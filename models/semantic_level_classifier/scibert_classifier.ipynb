{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01296be0-7fca-4f51-a104-9faed0756e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn.modules.loss import BCELoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class AltTextSentenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Example data entry\n",
    "        {\n",
    "          \"corpus_id\": 14745329,\n",
    "          \"sent_id\": 0,\n",
    "          \"text\": \"A graph of the latencies for each transcript (professional, automatic and crowd).\",\n",
    "          \"labels\": [\n",
    "            1,\n",
    "            0,\n",
    "            0,\n",
    "            0\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        current_item = self.data[idx]\n",
    "        text = current_item['text']\n",
    "        token_ids = self.tokenizer.encode(text, max_length=512, truncation=True)\n",
    "        labels = current_item['labels']\n",
    "        labels_float = [float(l) for l in labels]\n",
    "\n",
    "        return {\n",
    "            \"text\": token_ids,\n",
    "            \"labels\": labels,\n",
    "            \"labels_float\": labels_float\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        token_ids = [torch.tensor(entry[\"text\"]) for entry in data]\n",
    "        labels = [torch.tensor(entry[\"labels\"]) for entry in data]\n",
    "        labels_float = [torch.tensor(entry[\"labels_float\"]) for entry in data]\n",
    "        token_ids_tensor = torch.nn.utils.rnn.pad_sequence(token_ids, batch_first=True, padding_value=0)\n",
    "        labels_tensor = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        labels_float_tensor = torch.nn.utils.rnn.pad_sequence(labels_float, batch_first=True, padding_value=-100)\n",
    "        return {\n",
    "            \"input_ids\": token_ids_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "            \"labels_float\": labels_float_tensor\n",
    "        }\n",
    "\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name_or_path: str,\n",
    "            train_file: str,\n",
    "            val_file: str,\n",
    "            pred_file: str,\n",
    "            max_seq_length: int = 512,\n",
    "            batch_size: int = 4,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_file = train_file\n",
    "        self.val_file = val_file\n",
    "        self.pred_file = pred_file\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name_or_path, use_fast=True, max_length=max_seq_length\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=\"fit\"):\n",
    "        def load_data(path):\n",
    "            data = []\n",
    "            with open(path) as fin:\n",
    "                for line in fin:\n",
    "                    data.append(json.loads(line))\n",
    "            return data\n",
    "\n",
    "        if stage == 'fit':\n",
    "            train_data = load_data(self.train_file)\n",
    "            self.train_dataset = AltTextSentenceDataset(train_data, self.tokenizer)\n",
    "            val_data = load_data(self.val_file)\n",
    "            self.val_dataset = AltTextSentenceDataset(val_data, self.tokenizer)\n",
    "            \n",
    "        if stage == 'validate':\n",
    "            val_data = load_data(self.val_file)\n",
    "            self.val_dataset = AltTextSentenceDataset(val_data, self.tokenizer)\n",
    "\n",
    "        if stage == 'predict':\n",
    "            pred_data = load_data(self.pred_file)\n",
    "            self.pred_dataset = AltTextSentenceDataset(pred_data, self.tokenizer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=AltTextSentenceDataset.collate_fn,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=AltTextSentenceDataset.collate_fn,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.pred_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=AltTextSentenceDataset.collate_fn,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "class TransformerModule(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name_or_path: str,\n",
    "            num_labels: int = 4,\n",
    "            learning_rate: float = 3e-5,\n",
    "            adam_epsilon: float = 1e-8,\n",
    "            warmup_steps: int = 0,\n",
    "            weight_decay: float = 0.0,\n",
    "            max_seq_length: int = 512,\n",
    "            batch_size: int = 4,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.classifier = torch.nn.Linear(768, 4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.loss_fn = BCELoss()\n",
    "        self.metric_acc = torchmetrics.Accuracy()\n",
    "        self.metric_f1 = torchmetrics.F1()\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name_or_path, use_fast=True, max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        # inputs['input_ids'].shape -> [batch_size, max_len]\n",
    "        output = self.model(inputs[\"input_ids\"])\n",
    "        # cls_output_state.shape -> [batch_size, 768]\n",
    "        cls_output_state = output[\"last_hidden_state\"][inputs[\"input_ids\"] == self.tokenizer.cls_token_id]\n",
    "        # logits.shape -> [batch_size, num_labels] -> [num_labels * batch_size]\n",
    "        logits = self.classifier(cls_output_state)\n",
    "        probs = self.sigmoid(logits)\n",
    "        probs_flat = probs.view(-1)\n",
    "        # labels_flat.shape -> [num_labels * batch_size]\n",
    "        labels = inputs[\"labels\"]\n",
    "        # print(\"labels\",labels)\n",
    "        if 'labels_float' not in inputs:\n",
    "            temp_float = [float(l) for l in labels]\n",
    "            inputs['labels_float']=torch.tensor(temp_float)\n",
    "            # print(\"labels_float\", inputs[\"labels_float\"])\n",
    "        labels_float = inputs[\"labels_float\"]\n",
    "        labels_flat = labels_float.view(-1)\n",
    "        loss = self.loss_fn(probs_flat, labels_flat)\n",
    "        return loss, probs, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        acc = self.metric_acc(outputs[1].view(-1), outputs[2].view(-1))\n",
    "        self.log(\"loss\", loss)\n",
    "        self.log(\"acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, probs, labels = outputs\n",
    "        preds = torch.round(probs)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        # self.log(\"acc\", acc)\n",
    "        return {\"loss\": val_loss, \"val_loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", loss)\n",
    "        val_acc = self.metric_acc(preds.view(-1), labels.view(-1))\n",
    "        val_f1 = self.metric_f1(preds.view(-1), labels.view(-1))\n",
    "        self.log(\"val_acc\", val_acc, prog_bar=True)\n",
    "        self.log(\"val_f1\", val_f1, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        _, probs, _ = outputs\n",
    "        preds = torch.round(probs)\n",
    "        return {\"preds\": preds}\n",
    "        \n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "        train_loader = self.train_dataloader()\n",
    "        tb_size = self.hparams.batch_size * max(1, len(self.trainer.gpus) if self.trainer.gpus else 0)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "def train(model, train_file, val_file, pred_file, outdir, logname, device_type, devices):\n",
    "    dm = DataModule(train_file=train_file, val_file=val_file, pred_file=pred_file, model_name_or_path=model)\n",
    "    dm.setup(stage=\"fit\")\n",
    "    model = TransformerModule(warmup_steps=200, model_name_or_path=model)\n",
    "    logger = TensorBoardLogger(outdir, name=logname)\n",
    "    print(os.path.join(outdir, 'checkpoints'))\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=os.path.join(outdir, 'checkpoints'),\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        accelerator=device_type,\n",
    "        devices=devices,\n",
    "        progress_bar_refresh_rate=5,\n",
    "        max_epochs=15,\n",
    "        default_root_dir=outdir,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "    dm.setup(stage=\"validate\")\n",
    "\n",
    "    trainer.validate(model, dm.val_dataloader())\n",
    "\n",
    "    dm.setup(stage=\"predict\")\n",
    "    predictions = trainer.predict(model, dm.predict_dataloader())\n",
    "    pred_list = []\n",
    "    for pred in predictions:\n",
    "        pred_list.append(pred['preds'].cpu())\n",
    "    pred_tensor = torch.cat(pred_list, dim = 0)\n",
    "    return pred_tensor\n",
    "\n",
    "# def predict(model, model_path, pred_file):\n",
    "#     model = TransformerModule(model_name_or_path=model).load_from_checkpoint(model_path)\n",
    "#     dm = DataModule(model_name_or_path=model, train_file=None, val_file=None, pred_file=pred_file)\n",
    "#     dm.setup(stage=\"predict\")\n",
    "#     trainer = Trainer(\n",
    "#         accelerator=device_type,\n",
    "#         devices=devices,\n",
    "#         progress_bar_refresh_rate=5,)\n",
    "#     predictions = trainer.predict(model, dm)\n",
    "#     pred_list = []\n",
    "#     for pred in predictions:\n",
    "#         pred_list.append(pred['preds'])\n",
    "\n",
    "#     pred_tensor = torch.cat(pred_list, dim = 0)\n",
    "#     return pred_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7ac89-ba30-4803-b46b-8783db8a5858",
   "metadata": {},
   "source": [
    "## Model trained on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2194eb9d-0af3-432f-a06a-800903023911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data/allenai_scibert_scivocab_uncased\n",
      "all_data/train.jsonl all_data/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | model      | BertModel | 109 M \n",
      "1 | classifier | Linear    | 3.1 K \n",
      "2 | sigmoid    | Sigmoid   | 0     \n",
      "3 | loss_fn    | BCELoss   | 0     \n",
      "4 | metric_acc | Accuracy  | 0     \n",
      "5 | metric_f1  | F1        | 0     \n",
      "-----------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "439.686   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data/allenai_scibert_scivocab_uncased/checkpoints\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9789ef57db48a39ab84a26f8065fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03548fb6c65a4f35af7c9dc9326e587e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 419: val_loss reached 0.23826 (best 0.23826), saving model to \"all_data/allenai_scibert_scivocab_uncased/checkpoints/epoch=0-step=419-v1.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 839: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1259: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1679: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 2099: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 2519: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2939: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 3359: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 3779: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 4199: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 4619: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 5039: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 5459: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 5879: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 6299: val_loss was not in top 1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_acc': 0.9213759303092957,\n",
      " 'val_f1': 0.853881299495697,\n",
      " 'val_loss': 0.23826251924037933}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7366a8097aec493480159c16a4a38c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 420it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--model\", type=str, help=\"Name of model\")\n",
    "# parser.add_argument(\"--data\", type=str, help=\"Path to data directory\")\n",
    "# parser.add_argument(\"--outdir\", type=str, help=\"Path to save output\")\n",
    "# parser.add_argument(\"--preds\", type=str, help=\"Path to process file for predictions\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "data_dir = 'all_data/'\n",
    "out_dir = os.path.join('all_data/', model_name.replace('/', '_'))\n",
    "print(out_dir)\n",
    "pred_file = 'only_first.jsonl'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print('Data path does not exist!')\n",
    "    sys.exit(-1)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# check if GPUs available\n",
    "gpu_count = torch.cuda.device_count()\n",
    "if gpu_count == 0:\n",
    "    device_type = 'cpu'\n",
    "    devices = None\n",
    "else:\n",
    "    device_type = 'gpu'\n",
    "    devices = [0]\n",
    "\n",
    "predictions = torch.zeros(10, 4)\n",
    "train_file = os.path.join(data_dir, 'train.jsonl')\n",
    "val_file = os.path.join(data_dir, 'val.jsonl')\n",
    "print(train_file, val_file)\n",
    "if not os.path.exists(train_file):\n",
    "    raise FileNotFoundError(f\"{train_file} not found!\")\n",
    "if not os.path.exists(val_file):\n",
    "    raise FileNotFoundError(f\"{val_file} not found!\")\n",
    "out_subdir = out_dir\n",
    "os.makedirs(out_subdir, exist_ok=True)\n",
    "logger_name = 'logs'\n",
    "predictions+=train(model_name, train_file, val_file, pred_file, out_subdir, logger_name, device_type, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0b32a-b44a-4121-9f26-781546940a7c",
   "metadata": {},
   "source": [
    "## 5 fold cross validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15781c-7533-4e8d-8562-7224aeb58a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--model\", type=str, help=\"Name of model\")\n",
    "# parser.add_argument(\"--data\", type=str, help=\"Path to data directory\")\n",
    "# parser.add_argument(\"--outdir\", type=str, help=\"Path to save output\")\n",
    "# parser.add_argument(\"--preds\", type=str, help=\"Path to process file for predictions\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "data_dir = 'data_splits/'\n",
    "out_dir = os.path.join('.', model_name.replace('/', '_'))\n",
    "pred_file = 'only_first.jsonl'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print('Data path does not exist!')\n",
    "    sys.exit(-1)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# check if GPUs available\n",
    "gpu_count = torch.cuda.device_count()\n",
    "if gpu_count == 0:\n",
    "    device_type = 'cpu'\n",
    "    devices = None\n",
    "else:\n",
    "    device_type = 'gpu'\n",
    "    devices = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64a704-ff0a-43a8-9e4c-b75c97301e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.zeros(10, 4)\n",
    "# get folds\n",
    "for i in range(5):\n",
    "    print(f'Fold {i}')\n",
    "    train_file = os.path.join(data_dir, f'{i:02d}', 'train.jsonl')\n",
    "    val_file = os.path.join(data_dir, f'{i:02d}', 'val.jsonl')\n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"{train_file} not found!\")\n",
    "    if not os.path.exists(val_file):\n",
    "        raise FileNotFoundError(f\"{val_file} not found!\")\n",
    "    out_subdir = os.path.join(out_dir, f'fold_{i:02d}')\n",
    "    os.makedirs(out_subdir, exist_ok=True)\n",
    "    logger_name = 'logs'\n",
    "    predictions+=train(model_name, train_file, val_file, pred_file, out_subdir, logger_name, device_type, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d32e22-38cf-4ffa-92b7-f2d3e7d3a111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     out_subdir = os.path.join(out_dir, f'fold_{i:02d}', 'checkpoints')\n",
    "#     filename = os.listdir(out_subdir)[0]\n",
    "#     print(\"checkpoint\",out_subdir)\n",
    "#     model_path = out_subdir +'/'+filename\n",
    "#     model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "#     print(\"model path\", model_path)\n",
    "#     predictions+=predict(model = model_name, model_path = model_path, pred_file = pred_file).cpu()\n",
    "\n",
    "pred_index = predictions/5\n",
    "# print(pred_index)\n",
    "print(f\"Total Examples: {pred_index.shape[0]}\")\n",
    "print(f\"Level 1 Percentage: {torch.sum(pred_index[:, 0])/pred_index.shape[0]}\")\n",
    "print(f\"Level 2 Percentage: {torch.sum(pred_index[:, 1])/pred_index.shape[0]}\")\n",
    "print(f\"Level 3 Percentage: {torch.sum(pred_index[:, 2])/pred_index.shape[0]}\")\n",
    "print(f\"Level 4 Percentage: {torch.sum(pred_index[:, 3])/pred_index.shape[0]}\")\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd15783-d8d4-49f6-807e-ccb217186850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_id': 1000, 'sent_id': 0, 'text': 'Increasing from 0% of the population, both plots start at 100% differentiable and gradually fall to 80% differentiable at 75% of the population', 'labels': [-1, -1, -1, -1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-0.3669), tensor([[0.1911, 0.6914, 0.6671, 0.0174]]), [-1, -1, -1, -1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-1.2254), tensor([[0.0707, 0.3046, 0.6819, 0.0205]]), [-1, -1, -1, -1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.4609), tensor([[0.1916, 0.6992, 0.8915, 0.0343]]), [-1, -1, -1, -1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-0.6819), tensor([[0.1409, 0.4799, 0.6503, 0.0339]]), [-1, -1, -1, -1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-0.1918), tensor([[0.0540, 0.8409, 0.6925, 0.0298]]), [-1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "out_dir = os.path.join('.', model_name.replace('/', '_'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, max_length=512)\n",
    "sentence = {\"corpus_id\": 1000, \"sent_id\": 0, \"text\": \"Increasing from 0% of the population, both plots start at 100% differentiable and gradually fall to 80% differentiable at 75% of the population\", \"labels\": [-1, -1, -1, -1]}\n",
    "tokenized_input = tokenizer.encode(sentence['text'], max_length=512)\n",
    "input_ids = torch.tensor([tokenized_input])\n",
    "print(sentence)\n",
    "aggregated_predictions = torch.zeros(1, 4) #length of sentence\n",
    "\n",
    "for i in range(5):\n",
    "    out_subdir = os.path.join(out_dir, f'fold_{i:02d}', 'checkpoints')\n",
    "    filename = os.listdir(out_subdir)[0]\n",
    "    model_path = out_subdir +'/'+filename\n",
    "    # print(\"model path\", model_path)\n",
    "    model = TransformerModule.load_from_checkpoint(model_path)\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        all = model(input_ids=input_ids, labels=sentence['labels'])\n",
    "        # print(all)\n",
    "        logits = all[1]\n",
    "        aggregated_predictions += torch.round(logits)\n",
    "\n",
    "average_predictions = aggregated_predictions / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdf0274-8724-4193-87b4-4a5ab1bd1883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.6000, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85467c03-db1d-4bb6-a885-a173a27c44ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
