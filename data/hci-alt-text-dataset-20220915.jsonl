{"title": "Towards Efficacy-Centered Game Design Patterns For Brain Injury Rehabilitation: A Data-Driven Approach", "pdf_hash": "b48bf288b652f156808e742c91199ca1e93885f2", "year": 2015, "venue": "ASSETS", "alt_text": "A case includes information about session gaols, patient attributes, game attributes, and session outcomes.", "levels": null, "corpus_id": 14308372, "sentences": ["A case includes information about session gaols, patient attributes, game attributes, and session outcomes."], "caption": "Figure 1. Summary attributes of a case", "local_uri": ["b48bf288b652f156808e742c91199ca1e93885f2_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Towards Efficacy-Centered Game Design Patterns For Brain Injury Rehabilitation: A Data-Driven Approach", "pdf_hash": "b48bf288b652f156808e742c91199ca1e93885f2", "year": 2015, "venue": "ASSETS", "alt_text": "Two page diary form. The first page asks about patient information and the games played. The second page asks about subjective evaluation for the effectiveness of each game at meeting targeted therapeutic goals.", "levels": null, "corpus_id": 14308372, "sentences": ["Two page diary form.", "The first page asks about patient information and the games played.", "The second page asks about subjective evaluation for the effectiveness of each game at meeting targeted therapeutic goals."], "caption": "Figure 2. Final version of the diary form", "local_uri": ["b48bf288b652f156808e742c91199ca1e93885f2_Image_002.png"], "annotated": false, "compound": false}
{"title": "Towards Efficacy-Centered Game Design Patterns For Brain Injury Rehabilitation: A Data-Driven Approach", "pdf_hash": "b48bf288b652f156808e742c91199ca1e93885f2", "year": 2015, "venue": "ASSETS", "alt_text": "Dynamic balance (in 350 cases), Attention/concentration (in 319 cases), and Standing (in 291 cases) are the most frequently selected goals. Written expression (0 case), Reading comprehension (1 case), and Calculations (4 cases) are the least frequently selected goals.", "levels": null, "corpus_id": 14308372, "sentences": ["Dynamic balance (in 350 cases), Attention/concentration (in 319 cases), and Standing (in 291 cases) are the most frequently selected goals.", "Written expression (0 case), Reading comprehension (1 case), and Calculations (4 cases) are the least frequently selected goals."], "caption": "Figure 3. Goal selection frequency", "local_uri": ["b48bf288b652f156808e742c91199ca1e93885f2_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Towards Efficacy-Centered Game Design Patterns For Brain Injury Rehabilitation: A Data-Driven Approach", "pdf_hash": "b48bf288b652f156808e742c91199ca1e93885f2", "year": 2015, "venue": "ASSETS", "alt_text": "Most frequently used games are Wii sports Bowling (70 times), Wii fit table tilt (39 times), and Wii fit penguin slide (32 times).", "levels": null, "corpus_id": 14308372, "sentences": ["Most frequently used games are Wii sports Bowling (70 times), Wii fit table tilt (39 times), and Wii fit penguin slide (32 times)."], "caption": "", "local_uri": ["b48bf288b652f156808e742c91199ca1e93885f2_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Why Is Gesture Typing Promising for Older Adults?: Comparing Gesture and Tap Typing Behavior of Older with Young Adults", "pdf_hash": "9bf09f9aa7b3f445eee820cf1e99c92273b1ec42", "year": 2018, "venue": "ASSETS", "alt_text": "The subject is holding mobile phone with one hand, and use the other hand to perform text entry.", "levels": null, "corpus_id": 52936439, "sentences": ["The subject is holding mobile phone with one hand, and use the other hand to perform text entry."], "caption": "Figure 1. An older adult was gesture typing in the study.", "local_uri": ["9bf09f9aa7b3f445eee820cf1e99c92273b1ec42_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Why Is Gesture Typing Promising for Older Adults?: Comparing Gesture and Tap Typing Behavior of Older with Young Adults", "pdf_hash": "9bf09f9aa7b3f445eee820cf1e99c92273b1ec42", "year": 2018, "venue": "ASSETS", "alt_text": "This is an illustration of two sample gesture traces from an older adult and a young adult typing the word \"good\".", "levels": null, "corpus_id": 52936439, "sentences": ["This is an illustration of two sample gesture traces from an older adult and a young adult typing the word \"good\"."], "caption": "", "local_uri": ["9bf09f9aa7b3f445eee820cf1e99c92273b1ec42_Image_031.jpg"], "annotated": false, "compound": false}
{"title": "Why Is Gesture Typing Promising for Older Adults?: Comparing Gesture and Tap Typing Behavior of Older with Young Adults", "pdf_hash": "9bf09f9aa7b3f445eee820cf1e99c92273b1ec42", "year": 2018, "venue": "ASSETS", "alt_text": "This is an illustration of two sample gesture traces from an older adult and a young adult typing the word \"word\".", "levels": null, "corpus_id": 52936439, "sentences": ["This is an illustration of two sample gesture traces from an older adult and a young adult typing the word \"word\"."], "caption": "Figure 10. Gesture Samples for Older and Young Adults. The dot indicates the starting position and is for illustration only", "local_uri": ["9bf09f9aa7b3f445eee820cf1e99c92273b1ec42_Image_032.jpg"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Friends Graph represented using GSK. Each line below lists the name of a node, its number of edges, and nodes to which it is joined.\n\nAmy\t4\tOscar\tPat\tNeal\tHarry\t\t\t\nBob\t2\tDan\tCharlie\t\t\t\t\t\nCharlie\t1\tBob\t\t\t\t\t\t\nDan\t4\tBob\tFred\tKate\tNeal\t\t\t\nEmma\t1\tNeal\t\t\t\t\t\t\nFred\t4\tDan\tKate\tIke\tNeal\t\t\t\nGeorge\t3\tMark\tPat\tHarry\t\t\t\t\nHarry\t5\tAmy\tGeorge\tJon\tPat\tNeal\t\t\nIke\t4\tFred\tKate\tPat\tNeal\t\t\t\nJon\t1\tHarry\t\t\t\t\t\t\nKate\t5\tDan\tFred\tMark\tLou\tIke\t\t\nLou\t1\tKate\t\t\t\t\t\t\nMark\t2\tGeorge\tKate\t\t\t\t\t\nNeal\t7\tAmy\tDan\tFred\tIke\tPat\tEmma\tHarry\nOscar\t1\tAmy\t\t\t\t\t\t\nPat\t5\tAmy\tNeal\tHarry\tGeorge\tIke", "levels": [[1], [1], [2]], "corpus_id": 14830028, "sentences": ["Friends Graph represented using GSK.", "Each line below lists the name of a node, its number of edges, and nodes to which it is joined.", "Amy\t4\tOscar\tPat\tNeal\tHarry\t\t\t\nBob\t2\tDan\tCharlie\t\t\t\t\t\nCharlie\t1\tBob\t\t\t\t\t\t\nDan\t4\tBob\tFred\tKate\tNeal\t\t\t\nEmma\t1\tNeal\t\t\t\t\t\t\nFred\t4\tDan\tKate\tIke\tNeal\t\t\t\nGeorge\t3\tMark\tPat\tHarry\t\t\t\t\nHarry\t5\tAmy\tGeorge\tJon\tPat\tNeal\t\t\nIke\t4\tFred\tKate\tPat\tNeal\t\t\t\nJon\t1\tHarry\t\t\t\t\t\t\nKate\t5\tDan\tFred\tMark\tLou\tIke\t\t\nLou\t1\tKate\t\t\t\t\t\t\nMark\t2\tGeorge\tKate\t\t\t\t\t\nNeal\t7\tAmy\tDan\tFred\tIke\tPat\tEmma\tHarry\nOscar\t1\tAmy\t\t\t\t\t\t\nPat\t5\tAmy\tNeal\tHarry\tGeorge\tIke"], "caption": "Figure 1. GSK Friends Graph", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Friends Graph represented using Excel. Each line below lists the name of a node, its number of edges, and nodes to which it is joined.\n\nAmy\t4\tOscar\tPat\tNeal\tHarry\t\t\t\nBob\t2\tDan\tCharlie\t\t\t\t\t\nCharlie\t1\tBob\t\t\t\t\t\t\nDan\t4\tBob\tFred\tKate\tNeal\t\t\t\nEmma\t1\tNeal\t\t\t\t\t\t\nFred\t4\tDan\tKate\tIke\tNeal\t\t\t\nGeorge\t3\tMark\tPat\tHarry\t\t\t\t\nHarry\t5\tAmy\tGeorge\tJon\tPat\tNeal\t\t\nIke\t4\tFred\tKate\tPat\tNeal\t\t\t\nJon\t1\tHarry\t\t\t\t\t\t\nKate\t5\tDan\tFred\tMark\tLou\tIke\t\t\nLou\t1\tKate\t\t\t\t\t\t\nMark\t2\tGeorge\tKate\t\t\t\t\t\nNeal\t7\tAmy\tDan\tFred\tIke\tPat\tEmma\tHarry\nOscar\t1\tAmy\t\t\t\t\t\t\nPat\t5\tAmy\tNeal\tHarry\tGeorge\tIke", "levels": [[-1], [-1], [-1]], "corpus_id": 14830028, "sentences": ["Friends Graph represented using Excel.", "Each line below lists the name of a node, its number of edges, and nodes to which it is joined.", "Amy\t4\tOscar\tPat\tNeal\tHarry\t\t\t\nBob\t2\tDan\tCharlie\t\t\t\t\t\nCharlie\t1\tBob\t\t\t\t\t\t\nDan\t4\tBob\tFred\tKate\tNeal\t\t\t\nEmma\t1\tNeal\t\t\t\t\t\t\nFred\t4\tDan\tKate\tIke\tNeal\t\t\t\nGeorge\t3\tMark\tPat\tHarry\t\t\t\t\nHarry\t5\tAmy\tGeorge\tJon\tPat\tNeal\t\t\nIke\t4\tFred\tKate\tPat\tNeal\t\t\t\nJon\t1\tHarry\t\t\t\t\t\t\nKate\t5\tDan\tFred\tMark\tLou\tIke\t\t\nLou\t1\tKate\t\t\t\t\t\t\nMark\t2\tGeorge\tKate\t\t\t\t\t\nNeal\t7\tAmy\tDan\tFred\tIke\tPat\tEmma\tHarry\nOscar\t1\tAmy\t\t\t\t\t\t\nPat\t5\tAmy\tNeal\tHarry\tGeorge\tIke"], "caption": "Figure 2. Excel Friends Graph", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Town Graph represented using GSK. Each line below lists the name of a place, its number of outgoing directed edges, followed by the edge labels and nodes to which they lead.\n\nBank\t2\tApple to Grocery Store\tRose to Dry Cleaner\t\nBook Store\t1\tKing to Gas Station\t\t\nDry Cleaner\t1\tLily to Post Office\t\t\nGas Station\t2\tNut to School\tPine to Mall\t\nGrocery Store\t2\tSalmon to Mall\tJuniper to Post Office\t\nHome\t3   Umbrella to Library   Willow to Post Office\tDogwood to Dry Cleaner\nLibrary\t2\tMaple to Restaurant\tCherry to Gas Station\t\nMall\t2\tHolly to Book Store\tOak to Library\t\nPark\t2\tTiger to Gas Station\tBirch to Book Store\t\nPost Office\t3\tQuail to Bank\tElm to Mall\tVine to Library\nRestaurant\t2\tFir to Home\tIvy to School\t\nSchool\t2\tX-ray to Library\tGoose to Park", "levels": [[-1], [-1], [-1]], "corpus_id": 14830028, "sentences": ["Town Graph represented using GSK.", "Each line below lists the name of a place, its number of outgoing directed edges, followed by the edge labels and nodes to which they lead.", "Bank\t2\tApple to Grocery Store\tRose to Dry Cleaner\t\nBook Store\t1\tKing to Gas Station\t\t\nDry Cleaner\t1\tLily to Post Office\t\t\nGas Station\t2\tNut to School\tPine to Mall\t\nGrocery Store\t2\tSalmon to Mall\tJuniper to Post Office\t\nHome\t3   Umbrella to Library   Willow to Post Office\tDogwood to Dry Cleaner\nLibrary\t2\tMaple to Restaurant\tCherry to Gas Station\t\nMall\t2\tHolly to Book Store\tOak to Library\t\nPark\t2\tTiger to Gas Station\tBirch to Book Store\t\nPost Office\t3\tQuail to Bank\tElm to Mall\tVine to Library\nRestaurant\t2\tFir to Home\tIvy to School\t\nSchool\t2\tX-ray to Library\tGoose to Park"], "caption": "Figure 3. GSK Town Graph", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Town Graph represented using Excel. Each line below lists the name of a place, its number of outgoing directed edges, followed by the edge labels and nodes to which they lead.\n\nBank\t2\tApple to Grocery Store\tRose to Dry Cleaner\t\nBook Store\t1\tKing to Gas Station\t\t\nDry Cleaner\t1\tLily to Post Office\t\t\nGas Station\t2\tNut to School\tPine to Mall\t\nGrocery Store\t2\tSalmon to Mall\tJuniper to Post Office\t\nHome\t3   Umbrella to Library   Willow to Post Office\tDogwood to Dry Cleaner\nLibrary\t2\tMaple to Restaurant\tCherry to Gas Station\t\nMall\t2\tHolly to Book Store\tOak to Library\t\nPark\t2\tTiger to Gas Station\tBirch to Book Store\t\nPost Office\t3\tQuail to Bank\tElm to Mall\tVine to Library\nRestaurant\t2\tFir to Home\tIvy to School\t\nSchool\t2\tX-ray to Library\tGoose to Park", "levels": [[-1], [-1], [-1]], "corpus_id": 14830028, "sentences": ["Town Graph represented using Excel.", "Each line below lists the name of a place, its number of outgoing directed edges, followed by the edge labels and nodes to which they lead.", "Bank\t2\tApple to Grocery Store\tRose to Dry Cleaner\t\nBook Store\t1\tKing to Gas Station\t\t\nDry Cleaner\t1\tLily to Post Office\t\t\nGas Station\t2\tNut to School\tPine to Mall\t\nGrocery Store\t2\tSalmon to Mall\tJuniper to Post Office\t\nHome\t3   Umbrella to Library   Willow to Post Office\tDogwood to Dry Cleaner\nLibrary\t2\tMaple to Restaurant\tCherry to Gas Station\t\nMall\t2\tHolly to Book Store\tOak to Library\t\nPark\t2\tTiger to Gas Station\tBirch to Book Store\t\nPost Office\t3\tQuail to Bank\tElm to Mall\tVine to Library\nRestaurant\t2\tFir to Home\tIvy to School\t\nSchool\t2\tX-ray to Library\tGoose to Park"], "caption": "Figure 4. Excel Town Graph", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Examination Q1\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK). P9 and P1R used the improved GSK.\n\nP1\t3.1\t2.6\t0.5\nP2\t3.5\t2.5\t1\nP3\t3.2\t2\t1.2\nP4\t4.9\t4.6\t0.3\nP5\t6.5\t8.7\t-2.2\nP6\t3.3\t2.4\t0.9\nP7\t4.6\t5.3\t-0.7\nP8\t5.1\t6.5\t-1.4\nP9*\t3.9\t3.7\t0.2\nP1R*\t2.6\t2.4\t0.2", "levels": null, "corpus_id": 14830028, "sentences": ["Examination Q1\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK).", "P9 and P1R used the improved GSK.", "P1\t3.1\t2.6\t0.5\nP2\t3.5\t2.5\t1\nP3\t3.2\t2\t1.2\nP4\t4.9\t4.6\t0.3\nP5\t6.5\t8.7\t-2.2\nP6\t3.3\t2.4\t0.9\nP7\t4.6\t5.3\t-0.7\nP8\t5.1\t6.5\t-1.4\nP9*\t3.9\t3.7\t0.2\nP1R*\t2.6\t2.4\t0.2"], "caption": "", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_005.gif"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Navigation N\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK). P9 and P1R used the improved GSK.\n\nP1\t11.8\t11.8\t0\nP2\t14.8\t13.3\t1.5\nP3\t16.5\t18.1\t-1.6\nP4\t21.8\t16.8\t5\nP5\t25.7\t26.8\t-1.1\nP6\t14.2\t17.9\t-3.7\nP7\t29.9\t32.4\t-2.5\nP8\t24\t32.8\t-8.8\nP9*\t18.1\t9.4\t8.7\nP1R*\t12.1\t6.9\t5.2", "levels": null, "corpus_id": 14830028, "sentences": ["Navigation N\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK).", "P9 and P1R used the improved GSK.", "P1\t11.8\t11.8\t0\nP2\t14.8\t13.3\t1.5\nP3\t16.5\t18.1\t-1.6\nP4\t21.8\t16.8\t5\nP5\t25.7\t26.8\t-1.1\nP6\t14.2\t17.9\t-3.7\nP7\t29.9\t32.4\t-2.5\nP8\t24\t32.8\t-8.8\nP9*\t18.1\t9.4\t8.7\nP1R*\t12.1\t6.9\t5.2"], "caption": "Figure 5. Examination Study Q1", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_006.gif"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Examination Q2\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK). P9 and P1R used the improved GSK.\n\nP1\t3.7\t5.9\t-2.2\nP2\t4.4\t6.6\t-2.2\nP3\t3.4\t5.5\t-2.1\nP4\t4.7\t9.9\t-5.2\nP5\t7.2\t15.2\t-8\nP6\t7.2\t7.8\t-0.6\nP7\t5.7\t15\t-9.3\nP8\t5.8\t16.7\t-10.9\nP9*\t10.2\t10.5\t-0.3\nP1R*\t3\t3.5\t-0.5", "levels": null, "corpus_id": 14830028, "sentences": ["Examination Q2\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK).", "P9 and P1R used the improved GSK.", "P1\t3.7\t5.9\t-2.2\nP2\t4.4\t6.6\t-2.2\nP3\t3.4\t5.5\t-2.1\nP4\t4.7\t9.9\t-5.2\nP5\t7.2\t15.2\t-8\nP6\t7.2\t7.8\t-0.6\nP7\t5.7\t15\t-9.3\nP8\t5.8\t16.7\t-10.9\nP9*\t10.2\t10.5\t-0.3\nP1R*\t3\t3.5\t-0.5"], "caption": "Figure 6. Examination Study Q2", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_007.gif"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Examination Q3\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK). P9 and P1R used the improved GSK.\n\nP1\t10\t11.1\t-1.1\nP2\t10.8\t14.4\t-3.6\nP3\t13.5\t36.9\t-23.4\nP4\t15.4\t28.3\t-12.9\nP5\t19.2\t27.8\t-8.6\nP6\t18.7\t26\t-7.3\nP7\t28.4\t43.4\t-15\nP8\t18.5\t23.5\t-5\nP9*\t13.2\t15.1\t-1.9\nP1R*\t11.2\t9.6\t1.6", "levels": null, "corpus_id": 14830028, "sentences": ["Examination Q3\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK).", "P9 and P1R used the improved GSK.", "P1\t10\t11.1\t-1.1\nP2\t10.8\t14.4\t-3.6\nP3\t13.5\t36.9\t-23.4\nP4\t15.4\t28.3\t-12.9\nP5\t19.2\t27.8\t-8.6\nP6\t18.7\t26\t-7.3\nP7\t28.4\t43.4\t-15\nP8\t18.5\t23.5\t-5\nP9*\t13.2\t15.1\t-1.9\nP1R*\t11.2\t9.6\t1.6"], "caption": "Figure 7. Examination Study Q3", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_008.gif"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Examination Q4\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK). P9 and P1R used the improved GSK.\n\nP1\t12.1\t23.9\t-11.8\nP2\t15.7\t24.8\t-9.1\nP3\t15.6\t45.7\t-30.1\nP4\t15.3\t26.7\t-11.4\nP5\t55.6\t54.4\t1.2\nP6\t21.5\t54.6\t-33.1\nP7\t41.7\t46.7\t-5\nP8\t19.3\t41.5\t-22.2\nP9*\t16.9\t25.8\t-8.9\nP1R*\t17.3\t17.6\t-0.3", "levels": null, "corpus_id": 14830028, "sentences": ["Examination Q4\t\t\t\nEach line below contains the Participant, Excel  response time (sec), GSK response time (sec), and the difference in response times (Excel - GSK).", "P9 and P1R used the improved GSK.", "P1\t12.1\t23.9\t-11.8\nP2\t15.7\t24.8\t-9.1\nP3\t15.6\t45.7\t-30.1\nP4\t15.3\t26.7\t-11.4\nP5\t55.6\t54.4\t1.2\nP6\t21.5\t54.6\t-33.1\nP7\t41.7\t46.7\t-5\nP8\t19.3\t41.5\t-22.2\nP9*\t16.9\t25.8\t-8.9\nP1R*\t17.3\t17.6\t-0.3"], "caption": "Figure 8. Examination Study Q4", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_009.gif"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "G2: Graph with 4 nodes and 4 edges laid out as a square with one diagonal and a missing side.", "levels": [[-1]], "corpus_id": 14830028, "sentences": ["G2: Graph with 4 nodes and 4 edges laid out as a square with one diagonal and a missing side."], "caption": "G1                       G2", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Creation Study\t\t\t\nEach line below contains the Participant, G1  response time (sec), G2 response time (sec), G3  response time (sec), G4 response time (sec), and the total response time. P9 and P1R used the improved GSK.\n\nP1\t1.83\t1.18\t1.06\t3.95\t8.03\nP2\t2.58\t2.31\t6.26\t1.78\t12.95\nP3\t1.35\t1.75\t6.01\t2.03\t11.15\nP4\t2.41\t2.46\t2.23\t2.46\t9.56\nP5\t2.93\t4.9\t5.13\t7.08\t20.05\nP6\t2.98\t2.45\t4.03\t2.68\t12.15\nP7\t4.23\t4.28\t4.56\t4.35\t17.43\nP8\t6.56\t6.08\t4.35\t7.33\t24.33\nP9*\t5.21\t6.5\t4.78\t7.53\t24.03\nP1R*\t0.96\t1.2\t1.05\t1.3\t4.51", "levels": null, "corpus_id": 14830028, "sentences": ["Creation Study\t\t\t\nEach line below contains the Participant, G1  response time (sec), G2 response time (sec), G3  response time (sec), G4 response time (sec), and the total response time.", "P9 and P1R used the improved GSK.", "P1\t1.83\t1.18\t1.06\t3.95\t8.03\nP2\t2.58\t2.31\t6.26\t1.78\t12.95\nP3\t1.35\t1.75\t6.01\t2.03\t11.15\nP4\t2.41\t2.46\t2.23\t2.46\t9.56\nP5\t2.93\t4.9\t5.13\t7.08\t20.05\nP6\t2.98\t2.45\t4.03\t2.68\t12.15\nP7\t4.23\t4.28\t4.56\t4.35\t17.43\nP8\t6.56\t6.08\t4.35\t7.33\t24.33\nP9*\t5.21\t6.5\t4.78\t7.53\t24.03\nP1R*\t0.96\t1.2\t1.05\t1.3\t4.51"], "caption": "Figure 11. Creation Study", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_014.gif"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Preferences Dialog, Verbosity Tab, Radio buttons for Beginner (Most verbose), Advanced (Least verbose)", "levels": null, "corpus_id": 14830028, "sentences": ["Preferences Dialog, Verbosity Tab, Radio buttons for Beginner (Most verbose), Advanced (Least verbose)"], "caption": "", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Including blind people in computing through access to graphs", "pdf_hash": "e6011ab5c271d61735bfecdeb4e27bdce7a061e6", "year": 2014, "venue": "ASSETS", "alt_text": "Edge Filtering Dialog, Checkboxes for Show undirected edges (Toggle with CTRL+1), Show incoming edges (Toggle with CTRL+2), Show outgoing edges (Toggle with CTRL+3), Show bidirectional edges (Toggle with CTRL+4)", "levels": null, "corpus_id": 14830028, "sentences": ["Edge Filtering Dialog, Checkboxes for Show undirected edges (Toggle with CTRL+1), Show incoming edges (Toggle with CTRL+2), Show outgoing edges (Toggle with CTRL+3), Show bidirectional edges (Toggle with CTRL+4)"], "caption": "Figure 13. Edge Filtering Dialog", "local_uri": ["e6011ab5c271d61735bfecdeb4e27bdce7a061e6_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "Real time object scanning using a mobile phone and cloud-based visual search engine", "pdf_hash": "dcb297c2daed971d3cb1f7c2e18f049da3182822", "year": 2013, "venue": "ASSETS", "alt_text": "Stableness threshold equals to 0.8 divided by average time needed to process an individual frame.", "levels": [[-1]], "corpus_id": 9070423, "sentences": ["Stableness threshold equals to 0.8 divided by average time needed to process an individual frame."], "caption": "Where Ts is the stableness threshold, ti is the time used to process", "local_uri": ["dcb297c2daed971d3cb1f7c2e18f049da3182822_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Automatically generating tailored accessible user interfaces for ubiquitous services", "pdf_hash": "2a41713937a19dec4f56aa0d65299621279a6fd0", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 1. Architecture of the system. This figure illustrates the main components of the system. Using his/her device, the user interacts with an adapted and accessible user interface that is automatically generated. Several modules are involved in the process of generating the interface: a knowledge base, an abstract user interface (UIML document), the Resource Selector module, the Adaptation Engine module and finally the necessary resources and XSL and CSS style sheets. Once the user interface is generated, it communicates with the middleware URC-UCH in order to allow the user control the ubiquitous target devices. Some examples of these devices are information kiosks, televisions, cash machines or elevators.", "levels": null, "corpus_id": 17883592, "sentences": ["Figure 1.", "Architecture of the system.", "This figure illustrates the main components of the system.", "Using his/her device, the user interacts with an adapted and accessible user interface that is automatically generated.", "Several modules are involved in the process of generating the interface: a knowledge base, an abstract user interface (UIML document), the Resource Selector module, the Adaptation Engine module and finally the necessary resources and XSL and CSS style sheets.", "Once the user interface is generated, it communicates with the middleware URC-UCH in order to allow the user control the ubiquitous target devices.", "Some examples of these devices are information kiosks, televisions, cash machines or elevators."], "caption": "Figure 1.Architecture of the system", "local_uri": ["2a41713937a19dec4f56aa0d65299621279a6fd0_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Automatically generating tailored accessible user interfaces for ubiquitous services", "pdf_hash": "2a41713937a19dec4f56aa0d65299621279a6fd0", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 2. This figure illustrates a user interface automatically generated by Webclient.\n In the top there is the title of the interface which is \"Underground ticket vending machine\". Subsequently, there is a form displayed in a layout table which contains the interaction elements and their values in each row. \nThe headers of the table are UI Socket (column one header) and Value (column two header).\nIn the first row the row heading is \u201cPayment\u201d and its value in the adjacent cell is a button element containing the text \"initial\"\nThe second row heading is \u201cthe state of the ticket\u201d and its value in the adjacent cell is a read-only input textbox with the value \"unpaid\".\nThe third row heading is \u201cZone\u201d and its value in the next cell is a selectbox with the value \"zone 1\" selected.\nThe fourth row heading is \u201cTicket Price\u201d and its value next to it is an input textbox with the value \"3\".\nIn the last row the heading is \u201cType\u201d and its value next to it is a selectbox with the value \"Single Ticket\" selected.\nAt the bottom of the interface, below the table there is a link to return to the previous menu.", "levels": null, "corpus_id": 17883592, "sentences": ["Figure 2.", "This figure illustrates a user interface automatically generated by Webclient.", "In the top there is the title of the interface which is \"Underground ticket vending machine\".", "Subsequently, there is a form displayed in a layout table which contains the interaction elements and their values in each row.", "The headers of the table are UI Socket (column one header) and Value (column two header).", "In the first row the row heading is \u201cPayment\u201d and its value in the adjacent cell is a button element containing the text \"initial\"\nThe second row heading is \u201cthe state of the ticket\u201d and its value in the adjacent cell is a read-only input textbox with the value \"unpaid\".", "The third row heading is \u201cZone\u201d and its value in the next cell is a selectbox with the value \"zone 1\" selected.", "The fourth row heading is \u201cTicket Price\u201d and its value next to it is an input textbox with the value \"3\".", "In the last row the heading is \u201cType\u201d and its value next to it is a selectbox with the value \"Single Ticket\" selected.", "At the bottom of the interface, below the table there is a link to return to the previous menu."], "caption": "Figure 2.Automatically generated interface by UCH/Webclient.", "local_uri": ["2a41713937a19dec4f56aa0d65299621279a6fd0_Image_002.gif"], "annotated": false, "compound": false}
{"title": "Automatically generating tailored accessible user interfaces for ubiquitous services", "pdf_hash": "2a41713937a19dec4f56aa0d65299621279a6fd0", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 3. This image shows three different user interfaces that comprise the steps required to complete the task of buying a ticket. All three interfaces have the same title in the top part, \"Underground ticket vending machine\", and the same text in the footer, \"2011\". In addition, all of them contain two sections in the content part: one with the interaction elements and the other with the navigation menu. In the first interface corresponding to the initial screen, there are two selectbox elements with their corresponding label. The first one is the \u201cType\u201d label with the value \"Single Ticket\" selected and the second one the \u201cZone\u201d label with the value \"Zone 1\" selected. The second interface, which corresponds to the second screenshot, contains an input textbox with the label \"Check Price\" and the value \"3\". Below this, there is a button with the text \"Payment\" inside. In the last interface, the final screen, there is an input textbox, with the label \"Status of the ticket\" and the value \"unpaid\". In every interface there is a navigation menu with the \u201cback\u201d and \u201ccontinue\u201d options.", "levels": null, "corpus_id": 17883592, "sentences": ["Figure 3.", "This image shows three different user interfaces that comprise the steps required to complete the task of buying a ticket.", "All three interfaces have the same title in the top part, \"Underground ticket vending machine\", and the same text in the footer, \"2011\".", "In addition, all of them contain two sections in the content part: one with the interaction elements and the other with the navigation menu.", "In the first interface corresponding to the initial screen, there are two selectbox elements with their corresponding label.", "The first one is the \u201cType\u201d label with the value \"Single Ticket\" selected and the second one the \u201cZone\u201d label with the value \"Zone 1\" selected.", "The second interface, which corresponds to the second screenshot, contains an input textbox with the label \"Check Price\" and the value \"3\".", "Below this, there is a button with the text \"Payment\" inside.", "In the last interface, the final screen, there is an input textbox, with the label \"Status of the ticket\" and the value \"unpaid\".", "In every interface there is a navigation menu with the \u201cback\u201d and \u201ccontinue\u201d options."], "caption": "Figure 3. Navigational interfaces automatically generated by EGOKI for a blind user.", "local_uri": ["2a41713937a19dec4f56aa0d65299621279a6fd0_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "HandSee: Enabling Full Hand Interaction on Smartphone with Front Camera-based Stereo Vision", "pdf_hash": "341128fe9a74a4e119421d9f66497fe8edba2aaf", "year": 2019, "venue": "CHI", "alt_text": "This figure is divided into 4 parts. Figure A show that the right angle prism mirror placed on the front camera. Figure B show that the space above the touchscreen that can be covered. Figure C show that a sample image captured by the front camera. Figure D show that the derived depth image of the touching hand and the gripping hand.", "levels": null, "corpus_id": 140216222, "sentences": ["This figure is divided into 4 parts.", "Figure A show that the right angle prism mirror placed on the front camera.", "Figure B show that the space above the touchscreen that can be covered.", "Figure C show that a sample image captured by the front camera.", "Figure D show that the derived depth image of the touching hand and the gripping hand."], "caption": "", "local_uri": ["341128fe9a74a4e119421d9f66497fe8edba2aaf_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "HandSee: Enabling Full Hand Interaction on Smartphone with Front Camera-based Stereo Vision", "pdf_hash": "341128fe9a74a4e119421d9f66497fe8edba2aaf", "year": 2019, "venue": "CHI", "alt_text": "A picture show the field of view of the front camera and the interaction space above the touchscreen. The calculation is based on the assumption that the gripping hand is usually within the 10 cm range at the bottom of the screen.", "levels": null, "corpus_id": 140216222, "sentences": ["A picture show the field of view of the front camera and the interaction space above the touchscreen.", "The calculation is based on the assumption that the gripping hand is usually within the 10 cm range at the bottom of the screen."], "caption": "", "local_uri": ["341128fe9a74a4e119421d9f66497fe8edba2aaf_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "HandSee: Enabling Full Hand Interaction on Smartphone with Front Camera-based Stereo Vision", "pdf_hash": "341128fe9a74a4e119421d9f66497fe8edba2aaf", "year": 2019, "venue": "CHI", "alt_text": "a single object has two optical paths into the cellphone camera, which projects onto diferent locations on the image plane.", "levels": null, "corpus_id": 140216222, "sentences": ["a single object has two optical paths into the cellphone camera, which projects onto diferent locations on the image plane."], "caption": "Figure 3: Front camera-based stereo vision: The green and blue lines each represent an optical path that light takes in travelling from the same object to the camera", "local_uri": ["341128fe9a74a4e119421d9f66497fe8edba2aaf_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "HandSee: Enabling Full Hand Interaction on Smartphone with Front Camera-based Stereo Vision", "pdf_hash": "341128fe9a74a4e119421d9f66497fe8edba2aaf", "year": 2019, "venue": "CHI", "alt_text": "Pipeline of our real-time system for segmentation and stereo estimation of hands interacting with a smartphone.", "levels": null, "corpus_id": 140216222, "sentences": ["Pipeline of our real-time system for segmentation and stereo estimation of hands interacting with a smartphone."], "caption": "", "local_uri": ["341128fe9a74a4e119421d9f66497fe8edba2aaf_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "How Teens with Visual Impairments Take, Edit, and Share Photos on Social Media", "pdf_hash": "d6b580f82048a38193047754b3305304ec40cf70", "year": 2018, "venue": "CHI", "alt_text": "Left: P10 is in process of zooming out of a photo. Middle: a heart is animating over the photo to indicate P10 has liked it. Right: P10 has zoomed out of the photo, and we can see an unfilled outline of a heart below, indicating she has unliked the photo; the heart appearing below liked photos is filled in with red.", "levels": null, "corpus_id": 5049521, "sentences": ["Left: P10 is in process of zooming out of a photo.", "Middle: a heart is animating over the photo to indicate P10 has liked it.", "Right: P10 has zoomed out of the photo, and we can see an unfilled outline of a heart below, indicating she has unliked the photo; the heart appearing below liked photos is filled in with red."], "caption": "Figure 2. P10 used her smartphone\u2019s built-in Magnification accessibility feature to view this photo on Instagram. When finished, she did a three-finger tap to zoom out (left). This gesture was interpreted as a double tap, which liked the photo (middle). In frustration, she then zoomed out and un-liked the photo (right).", "local_uri": ["d6b580f82048a38193047754b3305304ec40cf70_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "How Teens with Visual Impairments Take, Edit, and Share Photos on Social Media", "pdf_hash": "d6b580f82048a38193047754b3305304ec40cf70", "year": 2018, "venue": "CHI", "alt_text": "Left: P12\u2019s Instagram feed is visible with the focus on a collage of two photos. In the left photo of the collage, P12 is holding a butterfly. In the right photo of the collage, a butterfly is on her head. Right: P12 has zoomed in to the right photo to emphasize the butterfly on her head", "levels": null, "corpus_id": 5049521, "sentences": ["Left: P12\u2019s Instagram feed is visible with the focus on a collage of two photos.", "In the left photo of the collage, P12 is holding a butterfly.", "In the right photo of the collage, a butterfly is on her head.", "Right: P12 has zoomed in to the right photo to emphasize the butterfly on her head"], "caption": "Figure 3. While telling researchers a story about her photos, P12 zoomed into the right photo in this collage to emphasize the butterfly on her head.", "local_uri": ["d6b580f82048a38193047754b3305304ec40cf70_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Supporting treatment of people living with HIV / AIDS in resource limited settings with IVRs", "pdf_hash": "72ec8cae501c78d24404003a9aa412da9a8676ed", "year": 2014, "venue": "CHI", "alt_text": "TAMA patient services daily (A) pill-time reminder, (B) symptoms look-up, (C) health tips, and (D) PIN.", "levels": null, "corpus_id": 15787623, "sentences": ["TAMA patient services daily (A) pill-time reminder, (B) symptoms look-up, (C) health tips, and (D) PIN."], "caption": "", "local_uri": ["72ec8cae501c78d24404003a9aa412da9a8676ed_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Modeling the functional area of the thumb on mobile touchscreen surfaces", "pdf_hash": "55e19bd9cfb7b8be4871a3cb8414e4b141238343", "year": 2014, "venue": "CHI", "alt_text": "Four example pictures of designers' estimates of the thumb functional area on mobile touchscreens.", "levels": null, "corpus_id": 568971, "sentences": ["Four example pictures of designers' estimates of the thumb functional area on mobile touchscreens."], "caption": "Figure 1. Designers (referred to from left) have previously used heuristic estimates for the functional area of the thumb [30, 13, 5, 6].", "local_uri": ["55e19bd9cfb7b8be4871a3cb8414e4b141238343_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Modeling the functional area of the thumb on mobile touchscreen surfaces", "pdf_hash": "55e19bd9cfb7b8be4871a3cb8414e4b141238343", "year": 2014, "venue": "CHI", "alt_text": "Four different grips on a device where index-finger distance from the edge on the back of the device is varied.", "levels": null, "corpus_id": 568971, "sentences": ["Four different grips on a device where index-finger distance from the edge on the back of the device is varied."], "caption": "Figure 5. Illustration of effects of index-\ufb01nger distance d: \u201cPushing\u201d the thumb further along the surface simultaneously \u201cpulls\u201d the index \ufb01nger from the back. d is the distance of the index \ufb01nger from the edge closest to the thumb.", "local_uri": ["55e19bd9cfb7b8be4871a3cb8414e4b141238343_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Subcontracting Microwork", "pdf_hash": "89a39a205f41afd5f4078fe0e753ee9d5c673797", "year": 2017, "venue": "CHI", "alt_text": "Table showing that skills and interests motivated workers to subtract more often than not, whereas money considerations more often motivated a choice to not subcontract.", "levels": null, "corpus_id": 12217908, "sentences": ["Table showing that skills and interests motivated workers to subtract more often than not, whereas money considerations more often motivated a choice to not subcontract."], "caption": "Figure 2. Overview of the motives crowd workers reported for their decisions to subcontract or not subcontract portions of the larger HIT. Sacrificing income was one of the main motives for not subcontracting.", "local_uri": ["89a39a205f41afd5f4078fe0e753ee9d5c673797_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Using an International Gaming Tournament to Study Individual Differences in MOBA Expertise and Cognitive Skills", "pdf_hash": "ee93a76437d7ba475c58b043c050fd78b77e7370", "year": 2016, "venue": "CHI", "alt_text": "Cumulative distribution function (blue) and histogram (N = number of participants within bin) for match making ranking (MMR).", "levels": [[1]], "corpus_id": 14633364, "sentences": ["Cumulative distribution function (blue) and histogram (N = number of participants within bin) for match making ranking (MMR)."], "caption": "Figure 1. Cumulative distribution function (blue) and histogram (N = number of participants within bin) for match making ranking (MMR).", "local_uri": ["ee93a76437d7ba475c58b043c050fd78b77e7370_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"It Broadens My Mind\": Empowering People with Cognitive Disabilities through Computing Education", "pdf_hash": "9e2f8d005f06748283e6eceafb54e8732956dafd", "year": 2019, "venue": "CHI", "alt_text": "Three male students sit in front of laptops programming. A female instructor leans over the shoulder of one of the students, providing him with feedback.", "levels": null, "corpus_id": 140263720, "sentences": ["Three male students sit in front of laptops programming.", "A female instructor leans over the shoulder of one of the students, providing him with feedback."], "caption": "Figure 1. Members of Code Club work together on programming projects. One member works with an instructor to solve a programming problem.", "local_uri": ["9e2f8d005f06748283e6eceafb54e8732956dafd_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "\"It Broadens My Mind\": Empowering People with Cognitive Disabilities through Computing Education", "pdf_hash": "9e2f8d005f06748283e6eceafb54e8732956dafd", "year": 2019, "venue": "CHI", "alt_text": "Smart pillow. A woman and a man sit side by side on a couch, holding a prototype smart pillow. The pillow has some sensor fabric installed on top of it.", "levels": null, "corpus_id": 140263720, "sentences": ["Smart pillow.", "A woman and a man sit side by side on a couch, holding a prototype smart pillow.", "The pillow has some sensor fabric installed on top of it."], "caption": "Figure 2. Code Club members demonstrate a prototype smart pillow. Sensors on the pillow detect taps and play an audio message based on the number of taps.", "local_uri": ["9e2f8d005f06748283e6eceafb54e8732956dafd_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"It Broadens My Mind\": Empowering People with Cognitive Disabilities through Computing Education", "pdf_hash": "9e2f8d005f06748283e6eceafb54e8732956dafd", "year": 2019, "venue": "CHI", "alt_text": "Photo of a whiteboard with a list of items on it describing a program. The list is labeled \"Driving Car Game\", and has the items \"car/truck\", \"backdrop - race track\", \"hit something - crash! (explosion and sound effect)\", and \"score.\"", "levels": null, "corpus_id": 140263720, "sentences": ["Photo of a whiteboard with a list of items on it describing a program.", "The list is labeled \"Driving Car Game\", and has the items \"car/truck\", \"backdrop - race track\", \"hit something - crash! (explosion and sound effect)\", and \"score.\""], "caption": "Figure 3. Plans for a driving game are sketched out on the whiteboard to help a student create the program.", "local_uri": ["9e2f8d005f06748283e6eceafb54e8732956dafd_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "\"It Broadens My Mind\": Empowering People with Cognitive Disabilities through Computing Education", "pdf_hash": "9e2f8d005f06748283e6eceafb54e8732956dafd", "year": 2019, "venue": "CHI", "alt_text": "Screen shot of a Scratch code editor window. Next to it is a tutorial document with images of code that could be confused with the code editor itself.", "levels": null, "corpus_id": 140263720, "sentences": ["Screen shot of a Scratch code editor window.", "Next to it is a tutorial document with images of code that could be confused with the code editor itself."], "caption": "Figure 4. Scratch code editor next to a tutorial. Tutorial images were sometimes mistaken for the code editor itself.", "local_uri": ["9e2f8d005f06748283e6eceafb54e8732956dafd_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories", "pdf_hash": "4b0ce9f935c070694c81348fc0312db7da2b62e6", "year": 2019, "venue": "CHI", "alt_text": "A user is teleporting herself in a Virtual Environment using the Curved Teleport. It allows her to teleport around an  obstacle and graphically choose the orientation, which she wants to face after teleportation only by using the curved trajectory  visualization with orientation indication, and without having to turn her body in the physical world.", "levels": [[-1], [-1]], "corpus_id": 140289579, "sentences": ["A user is teleporting herself in a Virtual Environment using the Curved Teleport.", "It allows her to teleport around an  obstacle and graphically choose the orientation, which she wants to face after teleportation only by using the curved trajectory  visualization with orientation indication, and without having to turn her body in the physical world."], "caption": "Figure 1: A user is teleporting herself in a Virtual Environment using the Curved Teleport. It allows her to teleport around an obstacle and graphically choose the orientation, which she wants to face after teleportation only by using the curved trajectory visualization with orientation indication, and without having to turn her body in the physical world.", "local_uri": ["4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories", "pdf_hash": "4b0ce9f935c070694c81348fc0312db7da2b62e6", "year": 2019, "venue": "CHI", "alt_text": "user is using the Linear Teleport to move onto a  target. The orientation that the user is facing is the forward  vector of the straight teleport line. The Parabola Teleport uses a parabola shaped visualization  to indicate the target position of the teleport. After  the teleport, users face the forward vector of the teleport.", "levels": null, "corpus_id": 140289579, "sentences": ["user is using the Linear Teleport to move onto a  target.", "The orientation that the user is facing is the forward  vector of the straight teleport line.", "The Parabola Teleport uses a parabola shaped visualization  to indicate the target position of the teleport.", "After  the teleport, users face the forward vector of the teleport."], "caption": "", "local_uri": ["4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_004.jpg", "4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories", "pdf_hash": "4b0ce9f935c070694c81348fc0312db7da2b62e6", "year": 2019, "venue": "CHI", "alt_text": "The AngleSelect Teleport uses a parabola visualization  to show the user the target position. Further it uses an  orientation indicator that lets the user select the orientation  that the user is facing after the teleport. The Curved Teleport visualizes the trajectory of the  teleportation in a curved line. The orientation that the user  is facing at the target location can be influenced by adjusting  the steepness of the curve.", "levels": null, "corpus_id": 140289579, "sentences": ["The AngleSelect Teleport uses a parabola visualization  to show the user the target position.", "Further it uses an  orientation indicator that lets the user select the orientation  that the user is facing after the teleport.", "The Curved Teleport visualizes the trajectory of the  teleportation in a curved line.", "The orientation that the user  is facing at the target location can be influenced by adjusting  the steepness of the curve."], "caption": "", "local_uri": ["4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_006.jpg", "4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories", "pdf_hash": "4b0ce9f935c070694c81348fc0312db7da2b62e6", "year": 2019, "venue": "CHI", "alt_text": "The visualization of the HPCurved Teleport first  uses a parabola that behaves like a state-of-the-art Parabola  Teleport, but at the high point of the parabola turns into a  Curved Teleport that lets the users chose the target orientation. We used round targets in our accuracy study. The  direction of the target is indicated by the green target area  and supported by the yellow target area. The rest of the target  is colored in red indicating the wrong direction. In this  screenshot of our accuracy study, the participant is using the  Parabola Teleport. (Picture of participant added for clarity).", "levels": null, "corpus_id": 140289579, "sentences": ["The visualization of the HPCurved Teleport first  uses a parabola that behaves like a state-of-the-art Parabola  Teleport, but at the high point of the parabola turns into a  Curved Teleport that lets the users chose the target orientation.", "We used round targets in our accuracy study.", "The  direction of the target is indicated by the green target area  and supported by the yellow target area.", "The rest of the target  is colored in red indicating the wrong direction.", "In this  screenshot of our accuracy study, the participant is using the  Parabola Teleport. (Picture of participant added for clarity)."], "caption": "", "local_uri": ["4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_009.jpg", "4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_010.jpg"], "annotated": false, "compound": true}
{"title": "Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories", "pdf_hash": "4b0ce9f935c070694c81348fc0312db7da2b62e6", "year": 2019, "venue": "CHI", "alt_text": "The VE that we created as the environment for conducting  the study. The castle in the middle of the environment  is the tutorial area, where participants can practice the  teleportation method before starting the study. Once participants  leave the tutorial castle, it disappears and becomes a  green lawn area.", "levels": null, "corpus_id": 140289579, "sentences": ["The VE that we created as the environment for conducting  the study.", "The castle in the middle of the environment  is the tutorial area, where participants can practice the  teleportation method before starting the study.", "Once participants  leave the tutorial castle, it disappears and becomes a  green lawn area."], "caption": "", "local_uri": ["4b0ce9f935c070694c81348fc0312db7da2b62e6_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Transcribing Across the Senses: Community Efforts to Create 3D Printable Accessible Tactile Pictures for Young Children with Visual Impairments", "pdf_hash": "84f267bc148557d479aadec296ea3ad1b28ee5fc", "year": 2015, "venue": "ASSETS", "alt_text": "This is a combination of four images. The image on the top left corner is of a plastic star cookie cutter taped to a page with braille underneith; the image to the top righte is a tactile map with a child's hand feeling the contours; the image on the bottom left is a 3DP-ATP of three bears sitting on chairs; the image on the lower right is of a 3DP-ATP of an elephant.", "levels": null, "corpus_id": 11602867, "sentences": ["This is a combination of four images.", "The image on the top left corner is of a plastic star cookie cutter taped to a page with braille underneith; the image to the top righte is a tactile map with a child's hand feeling the contours; the image on the bottom left is a 3DP-ATP of three bears sitting on chairs; the image on the lower right is of a 3DP-ATP of an elephant."], "caption": "Figure 1. Top: Handcrafted 2.5D-ATP/ATG (Source: Perkins School); Bottom 3D Printed 3DP-ATP (Source: Ref. 19, 39).", "local_uri": ["84f267bc148557d479aadec296ea3ad1b28ee5fc_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Transcribing Across the Senses: Community Efforts to Create 3D Printable Accessible Tactile Pictures for Young Children with Visual Impairments", "pdf_hash": "84f267bc148557d479aadec296ea3ad1b28ee5fc", "year": 2015, "venue": "ASSETS", "alt_text": "This is a combination of three images. To the left, two TVIs are holding a ATP book; in the middle a TVI is feeling a 3DP-ATP; to the right, two TVIs are using a computer to model a ATP.", "levels": null, "corpus_id": 11602867, "sentences": ["This is a combination of three images.", "To the left, two TVIs are holding a ATP book; in the middle a TVI is feeling a 3DP-ATP; to the right, two TVIs are using a computer to model a ATP."], "caption": "Figure 2 Workshop Activities: Discuss, Explore, and Model", "local_uri": ["84f267bc148557d479aadec296ea3ad1b28ee5fc_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Transcribing Across the Senses: Community Efforts to Create 3D Printable Accessible Tactile Pictures for Young Children with Visual Impairments", "pdf_hash": "84f267bc148557d479aadec296ea3ad1b28ee5fc", "year": 2015, "venue": "ASSETS", "alt_text": "This is a series of images of the models participants in each of the groups made.", "levels": null, "corpus_id": 11602867, "sentences": ["This is a series of images of the models participants in each of the groups made."], "caption": "Figure 3 Examples of Workshop Groups' Models. (From left to right) Accessibility Librarians, Children\u2019s Librarians, Engineers, Interaction Designers, O&M/TVIs. (Volunteers did not produce any models).", "local_uri": ["84f267bc148557d479aadec296ea3ad1b28ee5fc_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Learning non-visual graphical information using a touch-based vibro-audio interface", "pdf_hash": "8eb68362c1a01314312f4f1b123198758238df79", "year": 2012, "venue": "ASSETS '12", "alt_text": "Image showing a user tracing the Letter stimuli using the vibro-audio interface in the tablet mode.", "levels": null, "corpus_id": 16134558, "sentences": ["Image showing a user tracing the Letter stimuli using the vibro-audio interface in the tablet mode."], "caption": "", "local_uri": ["8eb68362c1a01314312f4f1b123198758238df79_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "pdf_hash": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "year": 2011, "venue": "ASSETS", "alt_text": "This figure presents a collum chart with the average WPM for each method. QWERTY ist the fastest followed by MultiTap, NavTouch and BrailleTouch.", "levels": [[1], [2]], "corpus_id": 3187042, "sentences": ["This figure presents a collum chart with the average WPM for each method.", "QWERTY ist the fastest followed by MultiTap, NavTouch and BrailleTouch."], "caption": "Figure 4. WPM (average) across the different methods. Error bars denote 95% CI.", "local_uri": ["3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "pdf_hash": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "year": 2011, "venue": "ASSETS", "alt_text": "This figure presents a collum chart with the MSD Error Rate for each method. MultiTap has the largest MSD Error Rate, followed by QWERTY, NavTouch and BrailleTouch. the method with thefastest followed by MultiTap, NavTouch and BrailleTouch.", "levels": [[1], [2], [2]], "corpus_id": 3187042, "sentences": ["This figure presents a collum chart with the MSD Error Rate for each method.", "MultiTap has the largest MSD Error Rate, followed by QWERTY, NavTouch and BrailleTouch.", "the method with thefastest followed by MultiTap, NavTouch and BrailleTouch."], "caption": "Figure 5. MSD Error Rates. Error bars denote 95% CI.", "local_uri": ["3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "pdf_hash": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "year": 2011, "venue": "ASSETS", "alt_text": "This figure presents a line chart with the average WPM for each method, for three age of onset groups (<=5, 6-20 and >=21). The early blind group was slower in every method. THe group with an age of onset between 6 and 20 is the fastest. These differeces in performance are more pronounced in QWERTY and MultiTap than with NavTouch and BrailleTouch.", "levels": [[1], [2], [2], [3]], "corpus_id": 3187042, "sentences": ["This figure presents a line chart with the average WPM for each method, for three age of onset groups (<=5, 6-20 and >=21).", "The early blind group was slower in every method.", "THe group with an age of onset between 6 and 20 is the fastest.", "These differeces in performance are more pronounced in QWERTY and MultiTap than with NavTouch and BrailleTouch."], "caption": "Figure 6. Age of onset impact on WPM.", "local_uri": ["3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "pdf_hash": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "year": 2011, "venue": "ASSETS", "alt_text": "This figure presents a line chart with the average WPM for each method, for two pressure sensitivity groups (<=3.61 and 3.62-4.31). The difference in performance of the two groups is considerable for QWERTY and MultiTap, especially for MultiTap, with the users with better sensitivity being much faster.", "levels": [[1], [3]], "corpus_id": 3187042, "sentences": ["This figure presents a line chart with the average WPM for each method, for two pressure sensitivity groups (<=3.61 and 3.62-4.31).", "The difference in performance of the two groups is considerable for QWERTY and MultiTap, especially for MultiTap, with the users with better sensitivity being much faster."], "caption": "Figure 7. Pressure sensitivity impact on WPM.", "local_uri": ["3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "pdf_hash": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "year": 2011, "venue": "ASSETS", "alt_text": "This figure presents a line chart with the average WPM for each method, for three spatial ability groups (<=4.75, 4.76-7.0 and >=7.01). The difference in performance of the group with the best spatial ability and the other two is big for QWERTY and MultiTap. Performance in NavTouch and BrailleTouch is similar among groups.", "levels": [[1], [3], [3, 2]], "corpus_id": 3187042, "sentences": ["This figure presents a line chart with the average WPM for each method, for three spatial ability groups (<=4.75, 4.76-7.0 and >=7.01).", "The difference in performance of the group with the best spatial ability and the other two is big for QWERTY and MultiTap.", "Performance in NavTouch and BrailleTouch is similar among groups."], "caption": "Figure 8. Spatial ability impact on WPM.", "local_uri": ["3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "The Gendered Geography of Contributions to OpenStreetMap: Complexities in Self-Focus Bias", "pdf_hash": "b0abced402bd11a7b43f42e4fd443c9150b4a9b0", "year": 2019, "venue": "CHI", "alt_text": "An overlaid histogram of edit counts of top 2,000 users and the 1,105 gender-inferred users. The top 2,000 users contributed  95.36% of all no-bots edits. The gender-identified users are equally distributed across the top 2,000 users in terms of edit counts.", "levels": null, "corpus_id": 140294123, "sentences": ["An overlaid histogram of edit counts of top 2,000 users and the 1,105 gender-inferred users.", "The top 2,000 users contributed  95.36% of all no-bots edits.", "The gender-identified users are equally distributed across the top 2,000 users in terms of edit counts."], "caption": "Figure 1: Overlaid histogram of edit counts of top 2,000 users and the 1,105 gender-inferred users. The top 2,000 users contributed 95.36% of all no-bots edits.", "local_uri": ["b0abced402bd11a7b43f42e4fd443c9150b4a9b0_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "The Gendered Geography of Contributions to OpenStreetMap: Complexities in Self-Focus Bias", "pdf_hash": "b0abced402bd11a7b43f42e4fd443c9150b4a9b0", "year": 2019, "venue": "CHI", "alt_text": "This figure shows a U.S. map where counties are marked using black separator lines. The U.S. counties filled with red color are those with at least one edit from the women power editors in our sample in the no-bots dataset. A prominent \"No Female Edits Belt\" (in white color) is visible running from the Northern Mountain West down through the Great Plains, Midwest, and Appalachians (note: these counties may be edited by non-power-editors or unidentified female editors).", "levels": [[1], [1], [3, 1]], "corpus_id": 140294123, "sentences": ["This figure shows a U.S. map where counties are marked using black separator lines.", "The U.S. counties filled with red color are those with at least one edit from the women power editors in our sample in the no-bots dataset.", "A prominent \"No Female Edits Belt\" (in white color) is visible running from the Northern Mountain West down through the Great Plains, Midwest, and Appalachians (note: these counties may be edited by non-power-editors or unidentified female editors)."], "caption": "", "local_uri": ["b0abced402bd11a7b43f42e4fd443c9150b4a9b0_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A readability evaluation of real-time crowd captions in the classroom", "pdf_hash": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 1a: A stenograph keyboard that shows its phonetic-based keys.  Figure 1b: A graph of a stenographer's typical Words Per Minute (WPM) limit and range.", "levels": [[1], [1]], "corpus_id": 14745329, "sentences": ["Figure 1a: A stenograph keyboard that shows its phonetic-based keys.", "Figure 1b: A graph of a stenographer's typical Words Per Minute (WPM) limit and range."], "caption": "A stenographer\u2019s typical Words Per Minute (WPM) limit and range.Figure 1: Professional Real-Time Captioning using a stenographrecognition (ASR). Both professional captioning and ASR provide a real-time word-for-word display of what is said in class, as well as options for saving the text after class for study. We discuss the readability of these approaches and a new approach, which utilizes crowd sourcing to generate real-time captions.Professional CaptioningThe most widely used approach, Communications Access Real Time (CART), is generated by professional captionists who use shorthand software to generate captions can keep up with natural speaking rates. Although popular, profes\u00ad sional captioners undergo years of training, which results in professional captioning services being expensive. Further\u00ad more, captionists usually have inadequate content knowl\u00ad edge and dictionaries to handle higher education lectures in speci\ufb01c \ufb01elds. is the most reliable transcription service, but is also the most expensive one. Trained stenographers type in shorthand on a stenographic (short hand writing sys\u00ad tem) keyboard as shown in Figure 1. This keyboard maps multiple key presses to phonemes that are expanded to ver\u00ad batim full text. Stenography requires 2-3 years of training to achieve at least 225 words per minute (WPM) and up to 300 WPM that is needed to consistently transcribe all real- time speech, which helps to explain the current cost of more than $100 an hour. CART stenographers need only to rec\u00ad ognize and type in the phonemes to create the transcript, which enables them to type fast enough to keep up with the natural speaking rate. But the software translation of phonemes to words requires a dictionary that already con\u00ad tains the words used in the lecture; typing in new words into the dictionary slows down the transcription speed con\u00ad siderably. The stenographer can transcribe speech even if the words or phonemes do not make sense to them, e.g., ifthe speech words appear to violate rules of grammar, pro\u00ad nunciation, or logic. If the captioner cannot understand the phoneme or word at all, then they cannot transcribe it.In response to the high costs of CART, computer-based macro expansion services like C-Print were developed and in\u00ad troduced. C-Print is a type of nearly-realtime transcription that was developed at the National Technical Institute for the Deaf. The captionist balances the tradeo\ufb00 between typ\u00ad ing speed and summarization, by including as much informa\u00ad tion as possible, generally providing a meaning-for-meaning but not verbatim translation of the spoken English content. This system enables operators who are trained in academic situations to consolidate and better organize the text with the goal of creating an end result more like class notes that may be more conducive to for learning. C-Print captionists need less training, and generally charge around $60 an hour. As the captionist normally cannot type as fast as the natural speaking rate, they are not able to produce a verbatim real- time transcript. Also, the captionist can only e\ufb00ectively convey classroom content if they understand that content themselves. The advantage is that the C-Print transcript accuracy and readability is high [21], but the disadvantage of this approach is that the transcript shows the summary that is based on the captionist\u2019s understanding of the ma\u00ad terial, which may be di\ufb00erent from the speaker or reader\u2019s understanding of the material.There are several captioning challenges in higher edu\u00ad cation. The \ufb01rst challenge is content knowledge - lecture information is dense and contains specialized vocabulary. This makes it hard to identify and schedule captionists who are both skilled in typing and have the appropriate content knowledge. Another captioning issue involves transcription delay, which occurs when captionists have to understand the phonemes or words and then type in what they have recognized. As a result, captionists tend to type the mate\u00ad rial to students with a delay of several seconds. This pre\u00ad vents students from e\ufb00ectively participating in an interac\u00ad tive classroom. Another challenge is speaker identi\ufb01cation, in which captionist are unfamiliar with participants and are challenged to properly identify the current speaker. They can simplify this by recognizing the speaker by name, or ask\u00ad ing the speaker to pause before beginning until the captionist has caught up and had an opportunity to identify the new speaker. In terms of availability, captionists typically are not available to transcribe live speech or dialogue for short periods or on-demand. Professional captionists usually need at least a few hours advance notice, and prefer to work in 1-hour increments so as to account for their commute times. As a result, students cannot easily decide at the last minute to attend a lecture or after class interactions with peers and teacher. Captionists used to need to be physically present at the event they were transcribing, but captioning services are increasingly being o\ufb00ered remotely [12, 1]. Captionists of\u00ad ten are simply not available for many technical \ufb01elds [21, 8]. Remote captioning o\ufb00ers the potential to recruit captionists familiar with a particular subject (e.g., organic chemistry) even if the captionist is located far away from an event. Se\u00ad lecting for expertise further reduces the pool of captionists. A \ufb01nal challenge is their cost - professional captionists are highly trained to keep up with speech with low errors rates, and so are highly paid. Experienced verbatim captionists\u2019 pay can exceed $200 an hour, and newly trained summariza\u00ad tion captionists can go as low as $60 an hour [21].Automatic Speech RecognitionASR platforms typically use probabilistic approaches to translate speech to text. These platforms face challenges in accurately capturing modern classroom lectures that can have one or more of the following challenges: extensive tech\u00ad nical vocabulary, poor acoustic quality, multiple informa\u00ad tion sources, speaker accents, or other problems. They also impose a processing delay of several seconds and the de\u00ad lay lengthens as the amount of data to be analyzed gets bigger. In other words, ASR works well under ideal situa\u00ad tions, but degrades quickly in many real settings. Kheir et al. [12] found that untrained ASR software had 75% accu\u00ad racy rate, but with training, could go to 90% under ideal single speaker, but this accuracy rate was still too low for use by deaf students. In the best possible case,  in which the speaker has trained the ASR and wears a high-quality, noise-canceling microphone, the accuracy can be above 90%. When recording a speaker using a standard microphone on ASR not trained for the speaker, accuracy rates plummet to far below 50%. Additionally, the errors made by ASR often change the meaning of the text, whereas we have found non- expert captionists are much more likely to simply omit words or make spelling errors. In Figure 2 for instance, the ASR changes \u2018two fold axis\u2019 to \u2018twenty four lexus\u2019, whereas the c typists typically omit words they do not understand or make spelling errors. Current ASR is speaker-dependent, has di\ufb03\u00ad culty recognizing domain-speci\ufb01c jargon, and adapts poorly to vocal changes, such as when the speaker is sick [6, 7]. ASR systems generally need substantial computing power and high-quality audio to work well, which means systems can be di\ufb03cult to transport. They are also ill-equipped to recognize and convey tone, attitudes, interest and emphasis, and to refer to visual information such as slides or demon\u00ad strations. ASR services charge about $15-20 an hour. How\u00ad ever, these systems are more easily integrated with other functions such as multimedia indexing.Crowd Captions in the ClassroomDeaf and hard of hearing students have had a long history of enhancing their classroom accessibility by collaborating with classmates. For example, they often arrange to copy notes from a classmate and share it with their study group. Crowdsourcing has been applied to o\ufb04ine transcription with great success [2], but has just recently been used for real- time transcription [15]. Applying a collaborative caption\u00ad ing approach among classmates enables real-time transcrip\u00ad tion from multiple non-experts, and crowd agreement mech\u00ad anisms can be utilized to vet transcript quality [14].We imagine a deaf or hard of hearing person eventually being able to capture aural speech with her cellphone any\u00ad where and have captions returned to her with a few seconds latency. She may use this to follow along in a lecture for which a professional captionist was not requested, to par\u00ad ticipate in informal conversation with peers after class, or enjoy a movie or other live event that lacks closed caption\u00ad ing. These use cases currently beyond the scope of ASR, and their serendipitous nature precludes pre-arranging a profes\u00ad sional captionist. Lasecki et al. have demonstrated that a modest number of people can provide reasonably high cov\u00ad erage over the caption stream, and introduces an algorithm that uses overlapping portions of the sequences to align and merge them using the Legion:Scribe system [15]. Scribe is based on the Legion [13] framework, which uses crowds of\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.that has a two fold axis\u2026\u2026.\u2026\u2026\u2026\u2026.have a crystal that\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.....we have a crystal\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.....we have a crystal that has a two fold axis\u2026..Figure 2: The crowd captioning interface. The in\u00ad terface provides a text input box at the bottom, and shifts text up as users type (either when the text hits the end of the box, or when the user presses the enter key). To encourage users to continue typing even when making mistakes, editing of text is dis\u00ad abled word by word. Partial captions are forwarded to the server in real-time, which uses overlapping segments and the order in segments are received to align and merge them.workers to accomplish tasks in real-time. Unlike Legion, Scribe merges responses to create a single, better, response instead of selecting from inputs to select the best sequence. This merger is done using an online multiple sequence align\u00ad ment algorithm that aligns worker input to both reconstruct the \ufb01nal stream and correct errors (such as spelling mis\u00ad takes) made by individual workers.Crowd captioning o\ufb00ers several potential bene\ufb01ts over ex\u00ad isting approaches. First, it is potentially much cheaper than hiring a professional captionist because non-expert caption\u00ad ists do not need extensive training to acquire a speci\ufb01c skill set, and thus may be drawn from a variety of sources, e.g. classmates, audience members, microtask marketplaces, vol\u00ad unteers, or a\ufb00ordable and readily available employees. Our workforce can be very large because, for people who can hear, speech recognition is relatively easy and most peo\u00ad ple can type accurately. The problem is that individually they cannot type quickly enough to keep up with natural speaking rates, and crowd captioning nicely remedies this problem.  Recent work has demonstrated that small crowds can be recruited quickly on-demand (in less than 2 seconds) from such sources[4, 3]. Scribe4Me enabled DHH users toreceive a transcript of a short sound sequence in a few min\u00ad utes, but is not able to produce verbatim captions over long periods of time [17].In previous work, we developed a crowd captioning sys\u00ad tem that accepts realtime transcription from multiple non- experts as shown in Figure 2. While non-experts cannot type as quickly as the natural speaking rate, we have found that they can provide accurate partial captions. Our system re\u00ad cruits fellow students with no training and compensates for slower typing speed and lower accuracy by combining the ef\u00ad forts of multiple captionists simultaneously and merges these partial captions in real-time. We have shown that groups of non-experts can achieve more timely captions than a pro\u00ad fessional captionist, that we can encourage them to focus on speci\ufb01c portions of the speech to improve global cover\u00ad age, and that it is possible to recombine partial captions and e\ufb00ectively tradeo\ufb00 coverage and precision [15].Real-time text reading versus listeningMost people only see real-time text on TV at the bar or gym in the form of closed captions, which tend to have no\u00ad ticeable errors. However, those programs are captioned by live captionists or stenographers. To reduce errors, these real-time transcripts are often corrected and made into a permanent part of the video \ufb01le by o\ufb00-line captionists who prepare captions from pre-recorded videotapes and thor\u00ad oughly review the work for errors before airing.The translation of speech to text is not direct, but rather is interpreted and changed in the course of each utterance. Markers like accent, tone, and timbre are stripped out and represented by standardized written words and symbols. Then the reader interprets these words and \ufb02ow to make meanings for themselves. Captionists tend not to include all spoken information so that readers can keep up with the transcript. Captionists are encouraged to alter the original transcrip\u00ad tion to provide time for the readers to completely read the caption and to synchronize with the audio. This is needed because, for a non-orthographic language like English, the length of a spoken utterance is not necessarily proportional to the length of a spelled word. In other words, reading speed is not the same as listening speed, especially for real- time scrolling text, as opposed to static pre-prepared text. For static text, reading speed has been measured at 291 wpm [19]. By contrast the average caption rate for TV programs is 141 wpm [11], while the most comfortable reading rate for hearing, hard-of-hearing, and deaf adults is around 145 wpm [10]. The reason is that the task of viewing real-time cap\u00ad tions involved di\ufb00erent processing demands in visual loca\u00ad tion and tracking of moving text on a dynamic background. English literacy rates among deaf and hard of hearing peo\u00ad ple who is low compared to hearing peers. Captioning re\u00ad search has shown that both rate and text reduction and viewer reading ability are important factors, and that cap\u00ad tions need to be provided within 5 seconds so that the reader can participate [20].The number of spoken words and their complexity can also in\ufb02uence the captioning decision on the amount of words to transcribe and degree of summarization to include so as to reduce the reader\u2019s total cognitive load. Jensema et al.[10] analyzed a large sample of captioned TV programs andlecture transcripts have a very di\ufb00erent pro\ufb01le. For compari\u00ad son purposes, we selected a 50 minute long clip from the MIT Open CourseWare (OCW) website1. The audio sample was picked from a lecture segment in which the speech was rela\u00ad tively clear.We chose this lecture because it combined both technical and non-technical components. We found that the lecture had 9137 words, of which 1428 were unique, at 182.7 wpm. Furthermore, over two thirds of the transcript con\u00ad sisted of around 500 words, which is double the size of the captioned TV word set.", "local_uri": ["5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc_Image_002.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A readability evaluation of real-time crowd captions in the classroom", "pdf_hash": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc", "year": 2012, "venue": "ASSETS '12", "alt_text": "A comparison of the flow for each transcript (cart, crowd and asr). Both CART and crowd captions exhibit a relatively smooth real-time flow.", "levels": null, "corpus_id": 14745329, "sentences": ["A comparison of the flow for each transcript (cart, crowd and asr).", "Both CART and crowd captions exhibit a relatively smooth real-time flow."], "caption": "", "local_uri": ["5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc_Image_006.png"], "annotated": false, "compound": false}
{"title": "A readability evaluation of real-time crowd captions in the classroom", "pdf_hash": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 5: A graph of the latencies for each transcript (professional, automatic and crowd). Professional and crowd captions have latencies under 5 seconds which allows students to keep up with the lecture.", "levels": [[1], [3, 2]], "corpus_id": 14745329, "sentences": ["Figure 5: A graph of the latencies for each transcript (professional, automatic and crowd).", "Professional and crowd captions have latencies under 5 seconds which allows students to keep up with the lecture."], "caption": "", "local_uri": ["5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Comparing Tactile, Auditory, and Visual Assembly Error-Feedback for Workers with Cognitive Impairments", "pdf_hash": "85bf475fb8a40de8ace3cd67c4fa963daa890fcb", "year": 2016, "venue": "ASSETS", "alt_text": "A participant is wearing an augmented glove providing tactile error feedback during assem- bly tasks.", "levels": [[-1]], "corpus_id": 14102317, "sentences": ["A participant is wearing an augmented glove providing tactile error feedback during assem- bly tasks."], "caption": "", "local_uri": ["85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparing Tactile, Auditory, and Visual Assembly Error-Feedback for Workers with Cognitive Impairments", "pdf_hash": "85bf475fb8a40de8ace3cd67c4fa963daa890fcb", "year": 2016, "venue": "ASSETS", "alt_text": "(a) The system uses a projector and a directed speaker for providing visual and auditory error feedback. Further, a Kinect v2 observes the picking of items from bins. (b) Red light providing visual error feedback. In case of an error, the whole working area is highlighted. (c) A glove equipped with vibration motors is further used to provide tactile error feedback.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 14102317, "sentences": ["(a) The system uses a projector and a directed speaker for providing visual and auditory error feedback.", "Further, a Kinect v2 observes the picking of items from bins. (", "b) Red light providing visual error feedback.", "In case of an error, the whole working area is highlighted. (", "c) A glove equipped with vibration motors is further used to provide tactile error feedback."], "caption": "(b)                                                                       (c)Figure 2: (a) The system uses a projector and a directed speaker for providing visual and auditory error feedback. Further, a Kinect v2 observes the picking of items from bins. (b) Red light providing visual error feedback. In case of an error, the whole working area is highlighted. (c) A glove equipped with vibration motors is further used to provide tactile error feedback.", "local_uri": ["85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_002.jpg", "85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_003.jpg", "85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Comparing Tactile, Auditory, and Visual Assembly Error-Feedback for Workers with Cognitive Impairments", "pdf_hash": "85bf475fb8a40de8ace3cd67c4fa963daa890fcb", "year": 2016, "venue": "ASSETS", "alt_text": "The assembly tasks used in the study. We used three different assembly tasks with equal complexity. The images depict the final step of the pictorial instructions.", "levels": null, "corpus_id": 14102317, "sentences": ["The assembly tasks used in the study.", "We used three different assembly tasks with equal complexity.", "The images depict the final step of the pictorial instructions."], "caption": "(a)                                                                          (b)                                                                      (c)", "local_uri": ["85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_009.jpg", "85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_010.jpg", "85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_011.jpg"], "annotated": false, "compound": true}
{"title": "Comparing Tactile, Auditory, and Visual Assembly Error-Feedback for Workers with Cognitive Impairments", "pdf_hash": "85bf475fb8a40de8ace3cd67c4fa963daa890fcb", "year": 2016, "venue": "ASSETS", "alt_text": "An overview of the average Task Completion Time to assemble the Lego Duplo construction using the different error feedback modalities and the average number of errors that were made using the different error feedback modalities for both assembly errors and picking errors. The error bars depict the standard error.", "levels": [[1], [1]], "corpus_id": 14102317, "sentences": ["An overview of the average Task Completion Time to assemble the Lego Duplo construction using the different error feedback modalities and the average number of errors that were made using the different error feedback modalities for both assembly errors and picking errors.", "The error bars depict the standard error."], "caption": "", "local_uri": ["85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_014.jpg", "85bf475fb8a40de8ace3cd67c4fa963daa890fcb_Image_015.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "pdf_hash": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "year": 2018, "venue": "ASSETS", "alt_text": "Screenshot of the mechanical turk interface.  See caption for the information collected", "levels": [[0], [0]], "corpus_id": 52937893, "sentences": ["Screenshot of the mechanical turk interface.", "See caption for the information collected"], "caption": "Figure 1. Screen shot of the HIT workspace presented to crowdsource workers on Amazon Mechanical Turk. The form contains an informa- tion graphic from our database, and asks the user to provide informa- tion from the graph, including the title, information about the x and y axes, and a classification category.", "local_uri": ["29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "pdf_hash": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "year": 2018, "venue": "ASSETS", "alt_text": "This image shows a donut chart of the six different categories in our dataset.  Rising trend is 36%, falling trend is 23%, stable trend is 33%, changing trend, big jump, and big fall are significantly lower", "levels": [[1], [2]], "corpus_id": 52937893, "sentences": ["This image shows a donut chart of the six different categories in our dataset.", "Rising trend is 36%, falling trend is 23%, stable trend is 33%, changing trend, big jump, and big fall are significantly lower"], "caption": "Figure 3. Distribution of the six different classes found in our dataset. Our data is imbalanced with the most frequent class represented being \u201crising trend\u201d at 36%.", "local_uri": ["29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "pdf_hash": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "year": 2018, "venue": "ASSETS", "alt_text": "This figure contains two line graphs a and b.  The line on a goes sharply down, but then rises and falls, rises and falls again.  For b, the line is barely stable and changes a bit before rising at the end.", "levels": [[3, 1], [3]], "corpus_id": 52937893, "sentences": ["This figure contains two line graphs a and b.  The line on a goes sharply down, but then rises and falls, rises and falls again.", "For b, the line is barely stable and changes a bit before rising at the end."], "caption": "(b)", "local_uri": ["29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_004.jpg", "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "pdf_hash": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "year": 2018, "venue": "ASSETS", "alt_text": "This figure contains two line graphs, a and b.  The a line graph represents the vision testing and training accuracy of the network over 80 epochs.  The training data rises consistently, but the testing accuracy maxes out at around 25 epochs.  In graph b, the line graph shows the training and testing accuracy of the bag of words model.  The training accuracy rises consistently, but the testing accuracy maxes out at round 45% at epoch 30 and slightly falls afterward.", "levels": [[1], [3, 2], [1], [3, 2]], "corpus_id": 52937893, "sentences": ["This figure contains two line graphs, a and b.  The a line graph represents the vision testing and training accuracy of the network over 80 epochs.", "The training data rises consistently, but the testing accuracy maxes out at around 25 epochs.", "In graph b, the line graph shows the training and testing accuracy of the bag of words model.", "The training accuracy rises consistently, but the testing accuracy maxes out at round 45% at epoch 30 and slightly falls afterward."], "caption": "Vision CNN Accuracy          (b) Bag of WordsAccuracyFigure 5. Training accuracy of the neural network using (a) just the vision part of the network, and (b) just the text part of the network. The maximum accuracy of the vision network is 69.67% obtained after 30 epochs. The maximum of the text network is 45% obtain after 32 epochs. Even though the training accuracy continues to trend upwards, the network begins to overfit to the training data as the network fails to generalize and improve on the testing data.Multimodal Accuracy", "local_uri": ["29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_006.jpg", "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": true}
{"title": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "pdf_hash": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "year": 2018, "venue": "ASSETS", "alt_text": "This line graph shows the training and testing accuracy of our multimodal model over 25 epochs.  The training accuracy consistently rises, but the testing accuracy peaks quickly at around 2 epochs and then falls due to the model overfitting to the training data.", "levels": [[1], [4, 3, 2]], "corpus_id": 52937893, "sentences": ["This line graph shows the training and testing accuracy of our multimodal model over 25 epochs.", "The training accuracy consistently rises, but the testing accuracy peaks quickly at around 2 epochs and then falls due to the model overfitting to the training data."], "caption": "Multimodal Accuracy", "local_uri": ["29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3, 4], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "pdf_hash": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "year": 2018, "venue": "ASSETS", "alt_text": "This bar chart shows the accuracy of the compared methods.  The baseline is at 36%.  The BOW is at 45.2%.  The Human (no vote) is at 58.5%.  The CNN is at 69.8%.  The Human is at 70.9% and the multimodal model is at 74.0%", "levels": [[1], [2], [2], [2], [2], [2]], "corpus_id": 52937893, "sentences": ["This bar chart shows the accuracy of the compared methods.", "The baseline is at 36%.", "The BOW is at 45.2%.", "The Human (no vote) is at 58.5%.", "The CNN is at 69.8%.", "The Human is at 70.9% and the multimodal model is at 74.0%"], "caption": "Figure 7. Comparison of different methods on the dataset. The baseline accuracy is based upon a naive classifier selecting the most frequent class in the dataset. The Bag of Words uses text only to perform classification. The Human (no vote) is the average human accuracy if their vote is left out of the ground truth label. CNN uses the visual features only. Human accuracy is the average turker accuracy if their vote counts towards the class label. Finally, our presented multimodal model performs the best with an accuracy of 74.0%.", "local_uri": ["29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "People with Visual Impairment Training Personal Object Recognizers: Feasibility and Challenges", "pdf_hash": "8e00a6c6b0ff5575c304fefbdad7c88b16de171d", "year": 2017, "venue": "CHI", "alt_text": "Two by four table of images. Each cell includes 5 consecutive photos of a cereal box. Two cells show similar variation both in the distance from the object but also in the variation of the viewpoint between the 5 photos. Those two cells include photos taken by the same blind participant.", "levels": null, "corpus_id": 16128412, "sentences": ["Two by four table of images.", "Each cell includes 5 consecutive photos of a cereal box.", "Two cells show similar variation both in the distance from the object but also in the variation of the viewpoint between the 5 photos.", "Those two cells include photos taken by the same blind participant."], "caption": "Figure 1. Object instances that participants in our study chose to train their personal object recognizers on. Can you tell which two objects were trained by the same participant? (4,2) (2,2) .rewsnA", "local_uri": ["8e00a6c6b0ff5575c304fefbdad7c88b16de171d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "People with Visual Impairment Training Personal Object Recognizers: Feasibility and Challenges", "pdf_hash": "8e00a6c6b0ff5575c304fefbdad7c88b16de171d", "year": 2017, "venue": "CHI", "alt_text": "Colorful network of terms with color denoting nodes under the same cluster. Some of the most prominent terms and their connections: Color: shirt, pant, shoe, dress, dog, flower Kind: dog, plant, seasoning, perfume, soda, soup, drink, tea, dinner, coffee, cereal Flavor: coffee, tea, cat food, dog food, seasoning Name: book, movie, perfume, medicine, snack,  Screen: display, photograph Card: hand, name, right side", "levels": [[-1], [-1]], "corpus_id": 16128412, "sentences": ["Colorful network of terms with color denoting nodes under the same cluster.", "Some of the most prominent terms and their connections: Color: shirt, pant, shoe, dress, dog, flower Kind: dog, plant, seasoning, perfume, soda, soup, drink, tea, dinner, coffee, cereal Flavor: coffee, tea, cat food, dog food, seasoning Name: book, movie, perfume, medicine, snack,  Screen: display, photograph Card: hand, name, right side"], "caption": "Figure 2. Term co-occurrence network for user questions in VizWiz dataset with sample images for the co-occurring terms \u2018soda\u2019 and \u2018kind\u2019.", "local_uri": ["8e00a6c6b0ff5575c304fefbdad7c88b16de171d_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "People with Visual Impairment Training Personal Object Recognizers: Feasibility and Challenges", "pdf_hash": "8e00a6c6b0ff5575c304fefbdad7c88b16de171d", "year": 2017, "venue": "CHI", "alt_text": "P1:  10 training photos portrait mode showing front, side, and back of the package.  2 testing photos landscape and 3 portrait. All testing photos show the back of the package.  P5:  9 training photos landscape all showing the front of the package. 3 testing photos landscape and 2 portrait. All testing photos show the front of the package.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 16128412, "sentences": ["P1:  10 training photos portrait mode showing front, side, and back of the package.", "2 testing photos landscape and 3 portrait.", "All testing photos show the back of the package.", "P5:  9 training photos landscape all showing the front of the package.", "3 testing photos landscape and 2 portrait.", "All testing photos show the front of the package."], "caption": "Figure 7. Training and testing images from P1 and P5 on \u201cchewy bars\u201d object illustrating distinct training strategies. (Images are uniformly sampled from the available 30 training images.)", "local_uri": ["8e00a6c6b0ff5575c304fefbdad7c88b16de171d_Image_019.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"just let the cane hit it\": how the blind and sighted see navigation differently", "pdf_hash": "5acce53a78c3dcc7f2bc28395f789516eefdfc41", "year": 2014, "venue": "ASSETS", "alt_text": "C5 is chasing N5 down to keep her from walking along the grass line on the right  because there are also bushes lining the sidewalk.", "levels": null, "corpus_id": 2320489, "sentences": ["C5 is chasing N5 down to keep her from walking along the grass line on the right  because there are also bushes lining the sidewalk."], "caption": "Figure 1 \u2013 During the study, a sighted companion (C5) said she didn\u2019t want her partner to walk along the grass area (like as shown) because she saw it as a hazard (not a walking cue).", "local_uri": ["5acce53a78c3dcc7f2bc28395f789516eefdfc41_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"just let the cane hit it\": how the blind and sighted see navigation differently", "pdf_hash": "5acce53a78c3dcc7f2bc28395f789516eefdfc41", "year": 2014, "venue": "ASSETS", "alt_text": "N4 and C4 stand at the back of 2 delivery trucks parked on each side of a wide sidewalk that doubles as a driveway. Only a narrow path in-between is passable.", "levels": null, "corpus_id": 2320489, "sentences": ["N4 and C4 stand at the back of 2 delivery trucks parked on each side of a wide sidewalk that doubles as a driveway.", "Only a narrow path in-between is passable."], "caption": "Figure 2 \u2013 As N4 uses his cane to tap between the two trucks to understand the width of the gap, his companion (C4) laughs and admonishes him for \u201chitting people\u2019s cars\u201d. She didn\u2019t understand his strategy to understand his boundaries.", "local_uri": ["5acce53a78c3dcc7f2bc28395f789516eefdfc41_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "\"just let the cane hit it\": how the blind and sighted see navigation differently", "pdf_hash": "5acce53a78c3dcc7f2bc28395f789516eefdfc41", "year": 2014, "venue": "ASSETS", "alt_text": "N1 is bending down somewhat to show his companion how his cane hits the concrete and then grass to know the difference and walk straight.", "levels": null, "corpus_id": 2320489, "sentences": ["N1 is bending down somewhat to show his companion how his cane hits the concrete and then grass to know the difference and walk straight."], "caption": "Figure 3 \u2013 N1 shows how he uses a grass line to walk straight.", "local_uri": ["5acce53a78c3dcc7f2bc28395f789516eefdfc41_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "\"just let the cane hit it\": how the blind and sighted see navigation differently", "pdf_hash": "5acce53a78c3dcc7f2bc28395f789516eefdfc41", "year": 2014, "venue": "ASSETS", "alt_text": "N3 is walking up the stairs on the left, along a brick wall and holding on to the handrail while is companion walks on the right.", "levels": null, "corpus_id": 2320489, "sentences": ["N3 is walking up the stairs on the left, along a brick wall and holding on to the handrail while is companion walks on the right."], "caption": "Figure 4 \u2013 C3 expressed concern that the handrail was close to the brick wall, but didn\u2019t warn N3 about the unusual stair length or that he was walking on the wrong side of the stairwell for oncoming traffic.", "local_uri": ["5acce53a78c3dcc7f2bc28395f789516eefdfc41_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "\"just let the cane hit it\": how the blind and sighted see navigation differently", "pdf_hash": "5acce53a78c3dcc7f2bc28395f789516eefdfc41", "year": 2014, "venue": "ASSETS", "alt_text": "N3 faces north while C3 faces west, just at the top landing of the set of stairs traversed in the prior photo.", "levels": null, "corpus_id": 2320489, "sentences": ["N3 faces north while C3 faces west, just at the top landing of the set of stairs traversed in the prior photo."], "caption": "Figure 5 \u2013 At the top of the stairs, N3 asks his companion, \u201cWhat\u2019s on the left?\u201d but their differing orientations causes the companion to give incorrect information based on his interpretation of \u201cleft\u201d as they stand perpendicular to one another.", "local_uri": ["5acce53a78c3dcc7f2bc28395f789516eefdfc41_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "DroneNavigator: Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers", "pdf_hash": "41b29f8c66e0281fa0544fae0a45764a58e83c3b", "year": 2017, "venue": "ASSETS", "alt_text": "A user is using a leashed quadcopter to navigate in unknown indoor environments. The user can perceive the directions both by hearing the au- ditory feedback that the quadcopter naturally emits and by feeling the tactile feedback on the handle.", "levels": null, "corpus_id": 26673257, "sentences": ["A user is using a leashed quadcopter to navigate in unknown indoor environments.", "The user can perceive the directions both by hearing the au- ditory feedback that the quadcopter naturally emits and by feeling the tactile feedback on the handle."], "caption": "ABSTRACT", "local_uri": ["41b29f8c66e0281fa0544fae0a45764a58e83c3b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "DroneNavigator: Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers", "pdf_hash": "41b29f8c66e0281fa0544fae0a45764a58e83c3b", "year": 2017, "venue": "ASSETS", "alt_text": "Our leashed quadcopter navigation pro- totype. The quadcopter is connected to a handle using an electric wire which acts as a leash. The handle has a movable pane with a 3D-printed arrow on top. This arrow can convey a direction as the leash lets the arrow always point into the direction of the quadcopter. Further, the handle is connected to an external USB battery pack, which can be car- ried by the user.", "levels": null, "corpus_id": 26673257, "sentences": ["Our leashed quadcopter navigation pro- totype.", "The quadcopter is connected to a handle using an electric wire which acts as a leash.", "The handle has a movable pane with a 3D-printed arrow on top.", "This arrow can convey a direction as the leash lets the arrow always point into the direction of the quadcopter.", "Further, the handle is connected to an external USB battery pack, which can be car- ried by the user."], "caption": "", "local_uri": ["41b29f8c66e0281fa0544fae0a45764a58e83c3b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "DroneNavigator: Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers", "pdf_hash": "41b29f8c66e0281fa0544fae0a45764a58e83c3b", "year": 2017, "venue": "ASSETS", "alt_text": "The results of the user study. (a) The Task Completion Time in seconds that was needed for each navigation method. (b) The average amount of navigation errors. (c) The perceived cognitive workload measured using the RTLX score. All error bars depict the standard error. The values depicted in this Figure are mentioned in the text.", "levels": [[1], [1], [1], [1], [1], [0]], "corpus_id": 26673257, "sentences": ["The results of the user study. (", "a) The Task Completion Time in seconds that was needed for each navigation method. (", "b) The average amount of navigation errors. (", "c) The perceived cognitive workload measured using the RTLX score.", "All error bars depict the standard error.", "The values depicted in this Figure are mentioned in the text."], "caption": "", "local_uri": ["41b29f8c66e0281fa0544fae0a45764a58e83c3b_Image_004.jpg", "41b29f8c66e0281fa0544fae0a45764a58e83c3b_Image_005.jpg", "41b29f8c66e0281fa0544fae0a45764a58e83c3b_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Incloodle: Evaluating an Interactive Application for Young Children with Mixed Abilities", "pdf_hash": "a14e1dde9d3a8a4eec6744478655020ec5d3998e", "year": 2016, "venue": "CHI", "alt_text": "Incloodle screenshots. The left column is Condition 2 (technology enforced cooperation and basic prompting) while the right column is Condition 3 (no enforcement and character prompting). The first row shows the start screen. On the left image, there is a matrix of 16 colored cartoon cameras. On the right image, there are 16 cartoon child faces. The second row shows the intro and question prompts. The left image says, \"What do your buddies do to make you happy?\" And the right says, \"Meet Rohan! My name is Rohan, and I feel happy when my buddies share with me. What do your buddies do to make you happy?\" and has a picture of a boy named Rohan. The third row has the picture prompt. The left says, \"Take a picture of yourselves making happy faces!\" The right says, \"Meet Rohan! Take a picture of yourselves making happy faces\" and has a picture of Rohan's face. The fourth row has only a left image of a girl with her face close to the camera so Incloodle recognizes one face, which has a disabled picture button. There is a red square around her face. In the fifth row, it shows enabled picture taking. The left has two children's faces in it (a girl and a boy) with green squares around their faces. On the right, there is the same picture of the girl from row four, which her face close up and the shutter button is still enabled. Lastly, both the left and the right show the picture taken from the fifth row.", "levels": null, "corpus_id": 15038075, "sentences": ["Incloodle screenshots.", "The left column is Condition 2 (technology enforced cooperation and basic prompting) while the right column is Condition 3 (no enforcement and character prompting).", "The first row shows the start screen.", "On the left image, there is a matrix of 16 colored cartoon cameras.", "On the right image, there are 16 cartoon child faces.", "The second row shows the intro and question prompts.", "The left image says, \"What do your buddies do to make you happy?\" And the right says, \"Meet Rohan! My name is Rohan, and I feel happy when my buddies share with me. What do your buddies do to make you happy?\" and has a picture of a boy named Rohan.", "The third row has the picture prompt.", "The left says, \"Take a picture of yourselves making happy faces!\" The right says, \"Meet Rohan! Take a picture of yourselves making happy faces\" and has a picture of Rohan's face.", "The fourth row has only a left image of a girl with her face close to the camera so Incloodle recognizes one face, which has a disabled picture button.", "There is a red square around her face.", "In the fifth row, it shows enabled picture taking.", "The left has two children's faces in it (a girl and a boy) with green squares around their faces.", "On the right, there is the same picture of the girl from row four, which her face close up and the shutter button is still enabled.", "Lastly, both the left and the right show the picture taken from the fifth row."], "caption": "", "local_uri": ["a14e1dde9d3a8a4eec6744478655020ec5d3998e_Image_001.jpg", "a14e1dde9d3a8a4eec6744478655020ec5d3998e_Image_002.png", "a14e1dde9d3a8a4eec6744478655020ec5d3998e_Image_003.png", "a14e1dde9d3a8a4eec6744478655020ec5d3998e_Image_004.jpg", "a14e1dde9d3a8a4eec6744478655020ec5d3998e_Image_005.jpg", "a14e1dde9d3a8a4eec6744478655020ec5d3998e_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Crowdsourcing Multi-label Audio Annotation Tasks with Citizen Scientists", "pdf_hash": "a2ec7b3f66cc921decfdfc34772afa32fe6d8d93", "year": 2019, "venue": "CHI", "alt_text": "Screenshot of binary and multi-label annotation tasks containing buttons with class labels on them.", "levels": null, "corpus_id": 113398962, "sentences": ["Screenshot of binary and multi-label annotation tasks containing buttons with class labels on them."], "caption": "Binary                       (c) Multi-label\u200cFigure 1: Screenshots of the binary and multi-label annota- tion tasks on Zooniverse along with the spectrogram sound visualization shown to annotators.task types, the audio was presented both aurally and visually (using a spectrogram representation; see Figure 1a).In the binary labeling task (see Figure 1b), volunteers were asked to decide whether a single suggested sound-source class was present or not in the recording. This task type provided both positive and negative labels explicitly.In the one-stage multi-labeling task (see Figure 1c), volun- teers were presented with a list of 30 class labels and an audio clip, and were asked to select all the sound-source classes present in the audio. The list of label options included our 22 sound-source classes plus labels for unknown or uncer- tain examples of the \u201csuperclasses\u201d; e.g. engines, construction machinery, or alert signals. This task type provided positive labels explicitly and negatively labels implicitly. Previous studies [50] indicate that requesting explicit negative labels reduces both precision and recall, and increases task comple- tion time in a multi-label task.In the two-stage hierarchical multi-label task, each stage was undertaken separately and by a diferent volunteer. In stage 1, the audio was presented to a volunteer alongside a list of 9 superclass labels; e.g. engines, or powered sawingTable 1: Annotator agreement.Task TypeUnanimous Agreement Pct.Binary Multi-labelHrchl. Multi-label Stg 1 Hrchl. Multi-label Stg 281%91%79%65%Task TypeKrippendorf\u2019s \u03b1 (95% CI)Binary Multi-labelHrchl. Multi-label Stg 1Hrchl. Multi-label Stg 20.52 [0.46, 0.58]0.53 [0.44, 0.60]0.45 [0.37, 0.52]0.45 [0.35, 0.54]tools. Identifcation of sounds in this stage provided a flter for possible class labels. For each selected superclass in stage 1, we posted a stage 2 task in which the same audio clip was presented alongside the sublist of our 22 class labels that correspond to the superclass. For example, if the audio had been identifed as containing engine sounds in stage 1, the list of possible labels shown in stage 2 would include: large- sounding engine, medium-sounding engine, small-sounding engine, other/unknown engine, artifcial interference noise, and other/unknown sound. Stage 2 tasks were undertaken at a later date, and the audio was presented to a diferent volun- teer. This task type provided positive labels explicitly and negative labels implicitly. We included this task as a compro- mise between the multi-labeling and binary labeling tasks\u2014 it requires fewer sound-source classes to be annotated si- multaneously and possibly fewer sound-source classes to be annotated overall for full multi-label annotation.", "local_uri": ["a2ec7b3f66cc921decfdfc34772afa32fe6d8d93_Image_003.jpg", "a2ec7b3f66cc921decfdfc34772afa32fe6d8d93_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Modeling the Engagement-Disengagement Cycle of Compulsive Phone Use", "pdf_hash": "4e1513c7d95386c44572801c755ac5bcd8be74ee", "year": 2019, "venue": "CHI", "alt_text": "Six sketches of phone screens that display padlocks, warning icons, and the word \"locked\" in capital letters.", "levels": null, "corpus_id": 140220316, "sentences": ["Six sketches of phone screens that display padlocks, warning icons, and the word \"locked\" in capital letters."], "caption": "", "local_uri": ["4e1513c7d95386c44572801c755ac5bcd8be74ee_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Is Now A Good Time?: An Empirical Study of Vehicle-Driver Communication Timing", "pdf_hash": "5acfe7252ab16d62d9ab07167cda8bb19e9bfb8f", "year": 2019, "venue": "CHI", "alt_text": "Overhead sketch of the sensor position in the car. Sensors include a face, road, driver side, and driver shoulder views. CANbus data collection, and GPS.", "levels": [[-1], [-1], [-1]], "corpus_id": 140323984, "sentences": ["Overhead sketch of the sensor position in the car.", "Sensors include a face, road, driver side, and driver shoulder views.", "CANbus data collection, and GPS."], "caption": "Figure 2: Instrumented 2016 Toyota Prius V.", "local_uri": ["5acfe7252ab16d62d9ab07167cda8bb19e9bfb8f_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Monitoring accessibility: large scale evaluations at a Geo political level", "pdf_hash": "2a9357697921609a904a78b6b6b5a6171a34d189", "year": 2011, "venue": "ASSETS", "alt_text": "A screenshot of an AMA instance. In particular this screenshot shows the Option Menu, a table which summarizes main analysis results related to the main evaluated area, a map which shows the evaluated area (with different colors for the different levels of accessibility of all the sub-areas), and a table which summarizes the analysis results related to the sub-areas.", "levels": null, "corpus_id": 18272514, "sentences": ["A screenshot of an AMA instance.", "In particular this screenshot shows the Option Menu, a table which summarizes main analysis results related to the main evaluated area, a map which shows the evaluated area (with different colors for the different levels of accessibility of all the sub-areas), and a table which summarizes the analysis results related to the sub-areas."], "caption": "Fig. 1 - A screenshot of an AMA instance", "local_uri": ["2a9357697921609a904a78b6b6b5a6171a34d189_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Monitoring accessibility: large scale evaluations at a Geo political level", "pdf_hash": "2a9357697921609a904a78b6b6b5a6171a34d189", "year": 2011, "venue": "ASSETS", "alt_text": "This figure shows the AMA Options Menu. By means of this menu user can choose the guidelines which will drive the evaluation (Select a guideline). For each of this set of guidelines it is possible to select the level (Select priority) and one or more requirements or success criteria (Requirements to check against). User can choose the categories of Web sites to be evaluated (Web site category). User can also set a previous date (Select a previous date) in order to compare most recent evaluations results with previous ones in a temporal dimension and the depth of the evaluations (Depth). User can choose the spatial-geo-politically related information, in order to enjoy accessibility evaluations results related to a specific area (Area).", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 18272514, "sentences": ["This figure shows the AMA Options Menu.", "By means of this menu user can choose the guidelines which will drive the evaluation (Select a guideline).", "For each of this set of guidelines it is possible to select the level (Select priority) and one or more requirements or success criteria (Requirements to check against).", "User can choose the categories of Web sites to be evaluated (Web site category).", "User can also set a previous date (Select a previous date) in order to compare most recent evaluations results with previous ones in a temporal dimension and the depth of the evaluations (Depth).", "User can choose the spatial-geo-politically related information, in order to enjoy accessibility evaluations results related to a specific area (Area)."], "caption": "Fig. 3 Options Menu", "local_uri": ["2a9357697921609a904a78b6b6b5a6171a34d189_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Monitoring accessibility: large scale evaluations at a Geo political level", "pdf_hash": "2a9357697921609a904a78b6b6b5a6171a34d189", "year": 2011, "venue": "ASSETS", "alt_text": "Screenshot of a map which shows accessibility evaluation results about European countries", "levels": null, "corpus_id": 18272514, "sentences": ["Screenshot of a map which shows accessibility evaluation results about European countries"], "caption": "Fig. 4 \u2013 Screenshot of the map which shows accessibility evaluation results", "local_uri": ["2a9357697921609a904a78b6b6b5a6171a34d189_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Monitoring accessibility: large scale evaluations at a Geo political level", "pdf_hash": "2a9357697921609a904a78b6b6b5a6171a34d189", "year": 2011, "venue": "ASSETS", "alt_text": "Table showing detailed accessibility evaluation results related to the Italian country. In particular, this table shows a summarized (number of evaluated Web sites, number of Web sites without any error, percentage of Web sites without any error, total number of errors, average number of errors per Web site) and detailed data (a table for each evaluated checkpoint or requirements, reporting the evaluated URL, the related category, the total number of errors and the number of evaluated Web pages).", "levels": [[-1], [-1]], "corpus_id": 18272514, "sentences": ["Table showing detailed accessibility evaluation results related to the Italian country.", "In particular, this table shows a summarized (number of evaluated Web sites, number of Web sites without any error, percentage of Web sites without any error, total number of errors, average number of errors per Web site) and detailed data (a table for each evaluated checkpoint or requirements, reporting the evaluated URL, the related category, the total number of errors and the number of evaluated Web pages)."], "caption": "Fig. 5 Table showing detailed accessibility evaluation results related to the Italian country", "local_uri": ["2a9357697921609a904a78b6b6b5a6171a34d189_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Monitoring accessibility: large scale evaluations at a Geo political level", "pdf_hash": "2a9357697921609a904a78b6b6b5a6171a34d189", "year": 2011, "venue": "ASSETS", "alt_text": "The recording manual evaluations prototype Web interface. For each warning user can decide if it is a real error or not (by means of a radio button).", "levels": [[-1], [-1]], "corpus_id": 18272514, "sentences": ["The recording manual evaluations prototype Web interface.", "For each warning user can decide if it is a real error or not (by means of a radio button)."], "caption": "Fig. 6 The recording manual evaluations prototype Web interface", "local_uri": ["2a9357697921609a904a78b6b6b5a6171a34d189_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior", "pdf_hash": "d39c93d0017363f1aaeb8e372cf8a124d33a8297", "year": 2017, "venue": "ASSETS", "alt_text": "This screenshot of the trian game shows a multi-car trian on a track on the top of screen. Then below proivdes two options for missing car that are similar but only one is an exact match. The users is to drag and drop the correct match to the identical place on the second train track below.", "levels": [[-1], [-1], [-1]], "corpus_id": 27876086, "sentences": ["This screenshot of the trian game shows a multi-car trian on a track on the top of screen.", "Then below proivdes two options for missing car that are similar but only one is an exact match.", "The users is to drag and drop the correct match to the identical place on the second train track below."], "caption": "Figure 1. Steps 1-4 show the user interaction in the train minigame: 1. Observe graphic prompt,2. Scan multiple options, 3. Select matching option, and 4. Drag and drop selected choice. \u00a9 Go Go Games Studios, LLC", "local_uri": ["d39c93d0017363f1aaeb8e372cf8a124d33a8297_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior", "pdf_hash": "d39c93d0017363f1aaeb8e372cf8a124d33a8297", "year": 2017, "venue": "ASSETS", "alt_text": "Two line graphs appear with Group A's findings stacked above Group B's findings. Each particpant that completed both the majority of game play and testing appear in their respective group's graph. For group A, that is P1, P2, and P5. For group B that is P7, P8, P11. Each particpant's score for each assessment is dispalyed in contionous line that run across the X axis that displays days of assessments  (1-3, 11-13, and days 24-26). The Y axis is the score on the MCR assessmet (0-14). The top graph has a phase line after day 3 to indicate that intervention began at this time for this group. The phase line is drawn down to graph below but jogs over to day 13 to indicate when treatment started for this group. Each participant's data is represetned by a uniqie shae and line style. Descriptive text of these data are provided in resutls section.", "levels": [[1], [1], [1], [1], [1], [1], [1], [1], [1], [0]], "corpus_id": 27876086, "sentences": ["Two line graphs appear with Group A's findings stacked above Group B's findings.", "Each particpant that completed both the majority of game play and testing appear in their respective group's graph.", "For group A, that is P1, P2, and P5.", "For group B that is P7, P8, P11.", "Each particpant's score for each assessment is dispalyed in contionous line that run across the X axis that displays days of assessments  (1-3, 11-13, and days 24-26).", "The Y axis is the score on the MCR assessmet (0-14).", "The top graph has a phase line after day 3 to indicate that intervention began at this time for this group.", "The phase line is drawn down to graph below but jogs over to day 13 to indicate when treatment started for this group.", "Each participant's data is represetned by a uniqie shae and line style.", "Descriptive text of these data are provided in resutls section."], "caption": "", "local_uri": ["d39c93d0017363f1aaeb8e372cf8a124d33a8297_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing", "pdf_hash": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c", "year": 2019, "venue": "CHI", "alt_text": "Titel: Setup with Open Probes - Beschreibung: A set of objects, each made out of white fabric. The shape of the objects reminds of belts, hoods or broochs.", "levels": null, "corpus_id": 140235080, "sentences": ["Titel: Setup with Open Probes - Beschreibung: A set of objects, each made out of white fabric.", "The shape of the objects reminds of belts, hoods or broochs."], "caption": "Figure 1: \u2018Open Probes\u2019.", "local_uri": ["6443e3632848430cfac5aeb3ab94a46acf0bbf0c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing", "pdf_hash": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c", "year": 2019, "venue": "CHI", "alt_text": "Titel: Woman performing with a belt - Beschreibung: Two pictures with the same woman who holds a white belt in her hand that is connected to another person. In one picture she pulls the belt.", "levels": null, "corpus_id": 140235080, "sentences": ["Titel: Woman performing with a belt - Beschreibung: Two pictures with the same woman who holds a white belt in her hand that is connected to another person.", "In one picture she pulls the belt."], "caption": "Figure 2: A participant performing the \u201cBossy Rope\u201d.", "local_uri": ["6443e3632848430cfac5aeb3ab94a46acf0bbf0c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing", "pdf_hash": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c", "year": 2019, "venue": "CHI", "alt_text": "Titel: Woman with info-stethoscope - Beschreibung: A woman standing next to an indoor plant with an object that connects her ear to the plant.", "levels": null, "corpus_id": 140235080, "sentences": ["Titel: Woman with info-stethoscope - Beschreibung: A woman standing next to an indoor plant with an object that connects her ear to the plant."], "caption": "Figure 3: Using the \u201cInfo-Stethoscope\u201d in a \u2018botanic garden\u2019.", "local_uri": ["6443e3632848430cfac5aeb3ab94a46acf0bbf0c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing", "pdf_hash": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c", "year": 2019, "venue": "CHI", "alt_text": "Titel: Man with hood and magnifying glass - Beschreibung: Two pictures with the same man. In one picture he wears an object with the shape of a scarf on his head as if it was a hood. In the other picture an object connects his ear to an object in his hands.", "levels": [[-1], [-1], [-1]], "corpus_id": 140235080, "sentences": ["Titel: Man with hood and magnifying glass - Beschreibung: Two pictures with the same man.", "In one picture he wears an object with the shape of a scarf on his head as if it was a hood.", "In the other picture an object connects his ear to an object in his hands."], "caption": "Figure 4: Feeling safe with the \u201cData Protection Hood\u201d and with the \u201cMagic Magnifying Glass\u201d.", "local_uri": ["6443e3632848430cfac5aeb3ab94a46acf0bbf0c_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing", "pdf_hash": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c", "year": 2019, "venue": "CHI", "alt_text": "Titel: Infographics \"Intra-active\" Dynamics - Beschreibung: The term \"activity\" is positioned in the middle of a triangular graph. The corners of the triangle are made out of the acronyms \"PPP\", \"OP\" and \"P\". The term \"activitiy\" and the acronyms are connected with double-ended arrows.", "levels": [[1], [1], [1]], "corpus_id": 140235080, "sentences": ["Titel: Infographics \"Intra-active\" Dynamics - Beschreibung: The term \"activity\" is positioned in the middle of a triangular graph.", "The corners of the triangle are made out of the acronyms \"PPP\", \"OP\" and \"P\".", "The term \"activitiy\" and the acronyms are connected with double-ended arrows."], "caption": "", "local_uri": ["6443e3632848430cfac5aeb3ab94a46acf0bbf0c_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mediating the Undercurrents: Using Social Media to Sustain a Social Movement", "pdf_hash": "207ad1a0b2976b271e27e9ef3ceda15f12572a01", "year": 2016, "venue": "CHI", "alt_text": "https://lh4.googleusercontent.com/UEQWy0KQUh60hfYCjC9XQ9WfVVuA_FBfqJGLDHDMZapyeSXG2e9sAoIhhVTG5lJk9hIqmxla8GUN2ezofZ4_NETSeBjB4NDJjeel_ZXH7Y90UYfSwBIwAni6vYum17AO6Aaleb8", "levels": null, "corpus_id": 16508300, "sentences": ["https://lh4.googleusercontent.com/UEQWy0KQUh60hfYCjC9XQ9WfVVuA_FBfqJGLDHDMZapyeSXG2e9sAoIhhVTG5lJk9hIqmxla8GUN2ezofZ4_NETSeBjB4NDJjeel_ZXH7Y90UYfSwBIwAni6vYum17AO6Aaleb8"], "caption": "Figure 1. An image of a protester holding up his umbrellas after tear gas was fired. Image: CC-BY-ND Pasu Au Yeung on Flickr.", "local_uri": ["207ad1a0b2976b271e27e9ef3ceda15f12572a01_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Mediating the Undercurrents: Using Social Media to Sustain a Social Movement", "pdf_hash": "207ad1a0b2976b271e27e9ef3ceda15f12572a01", "year": 2016, "venue": "CHI", "alt_text": "https://lh5.googleusercontent.com/9lCtqX9tBGxo7PF4R35wlwx1BwGPh7GrvcM5zPOHP89rafo0dhTs2LzoeqtwNv9t3oonyGRLzblI7Tl5f5xbKb4d1RQVHzNTkJh4ADRDqpWBqhc-0y0hs6gM5jsX-YC8AWdwS5w", "levels": null, "corpus_id": 16508300, "sentences": ["https://lh5.googleusercontent.com/9lCtqX9tBGxo7PF4R35wlwx1BwGPh7GrvcM5zPOHP89rafo0dhTs2LzoeqtwNv9t3oonyGRLzblI7Tl5f5xbKb4d1RQVHzNTkJh4ADRDqpWBqhc-0y0hs6gM5jsX-YC8AWdwS5w"], "caption": "Figure 2. Admiralty was crowded with protesters at the peak of the sit-in movement on September 29, 2014. Image: CC-BY- ND Pasu Au Yeung on Flickr.", "local_uri": ["207ad1a0b2976b271e27e9ef3ceda15f12572a01_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Mediating the Undercurrents: Using Social Media to Sustain a Social Movement", "pdf_hash": "207ad1a0b2976b271e27e9ef3ceda15f12572a01", "year": 2016, "venue": "CHI", "alt_text": "https://lh3.googleusercontent.com/JhsYGD5vgSQkxTKSwkeheUDT-vQj-_hWnbqpWyMD44XDlwX38XcHzjx-AZGXGRxnxFaYXZZdARB5nI84NsWyj93hD7jM9rZfWr98GK55ZWs3Xl27JYlk0rgKwzlTP4SCVKR-A-0", "levels": null, "corpus_id": 16508300, "sentences": ["https://lh3.googleusercontent.com/JhsYGD5vgSQkxTKSwkeheUDT-vQj-_hWnbqpWyMD44XDlwX38XcHzjx-AZGXGRxnxFaYXZZdARB5nI84NsWyj93hD7jM9rZfWr98GK55ZWs3Xl27JYlk0rgKwzlTP4SCVKR-A-0"], "caption": "Figure 3. A Facebook post shared by a pro-democracy newspaper reporting on the tear gas incident.", "local_uri": ["207ad1a0b2976b271e27e9ef3ceda15f12572a01_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Mediating the Undercurrents: Using Social Media to Sustain a Social Movement", "pdf_hash": "207ad1a0b2976b271e27e9ef3ceda15f12572a01", "year": 2016, "venue": "CHI", "alt_text": "https://lh6.googleusercontent.com/VW7y09K99V3g9Si2_1qjJNfgZcwVgbmr_o_vN21gcKwTKEVBK4WivieD2ADQPT62Neg_8STuOgjcBN6Lzwd4SZENyEEZ_9PL3f2OG3AXthaae0D0iYuXfpimuyu_i_DjqU-bruo", "levels": null, "corpus_id": 16508300, "sentences": ["https://lh6.googleusercontent.com/VW7y09K99V3g9Si2_1qjJNfgZcwVgbmr_o_vN21gcKwTKEVBK4WivieD2ADQPT62Neg_8STuOgjcBN6Lzwd4SZENyEEZ_9PL3f2OG3AXthaae0D0iYuXfpimuyu_i_DjqU-bruo"], "caption": "Figure 4 A schedule of public civil classes, publicly shared on Google Docs, that were taking place at the protest site.", "local_uri": ["207ad1a0b2976b271e27e9ef3ceda15f12572a01_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Towards crowd-based customer service: a mixed-initiative tool for managing Q&A sites", "pdf_hash": "08e0eae7fa3498dc1043733bb85cbf681daf7c5e", "year": 2014, "venue": "CHI", "alt_text": "Web-based simulator with 3 buttons for each question (Darkside, Brightside, Unknow)", "levels": null, "corpus_id": 17940702, "sentences": ["Web-based simulator with 3 buttons for each question (Darkside, Brightside, Unknow)"], "caption": "", "local_uri": ["08e0eae7fa3498dc1043733bb85cbf681daf7c5e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Towards crowd-based customer service: a mixed-initiative tool for managing Q&A sites", "pdf_hash": "08e0eae7fa3498dc1043733bb85cbf681daf7c5e", "year": 2014, "venue": "CHI", "alt_text": "Web based interface of our tool: filters on the left, new questions in the center panel, outdated questions on the right and operators list on the bottom.", "levels": null, "corpus_id": 17940702, "sentences": ["Web based interface of our tool: filters on the left, new questions in the center panel, outdated questions on the right and operators list on the bottom."], "caption": "Figure 2. The web-based user interface of our mixed-initiative tool.", "local_uri": ["08e0eae7fa3498dc1043733bb85cbf681daf7c5e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Let's Play Together: Adaptation Guidelines of Board Games for Players with Visual Impairment", "pdf_hash": "554915a6eac034f093512469153e21ae15c3a4e7", "year": 2019, "venue": "CHI", "alt_text": "Two different pictures, showing the cards that compose the game Coup. The first one shows two cards side by side: the original and adapted cards. The second picture shows one copy of each adapted card, side by side, with their distinct textures in the center of the card.", "levels": null, "corpus_id": 140319064, "sentences": ["Two different pictures, showing the cards that compose the game Coup.", "The first one shows two cards side by side: the original and adapted cards.", "The second picture shows one copy of each adapted card, side by side, with their distinct textures in the center of the card."], "caption": "Figure 1. The game Coup. Comparison of the adapted card with the original card (left) and all five adapted cards (right).", "local_uri": ["554915a6eac034f093512469153e21ae15c3a4e7_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "A Unifying Notification System To Scale Up Assistive Services", "pdf_hash": "08f617726fe2529e75d5b2760e725e18a80901eb", "year": 2015, "venue": "ASSETS", "alt_text": "This example of non-cricitical notification shows a green circle with a reminder message in the middle.", "levels": null, "corpus_id": 6666276, "sentences": ["This example of non-cricitical notification shows a green circle with a reminder message in the middle."], "caption": "", "local_uri": ["08f617726fe2529e75d5b2760e725e18a80901eb_Image_002.png"], "annotated": false, "compound": false}
{"title": "A Unifying Notification System To Scale Up Assistive Services", "pdf_hash": "08f617726fe2529e75d5b2760e725e18a80901eb", "year": 2015, "venue": "ASSETS", "alt_text": "This example shows a critical notification as an orange rectangle with a message in the middle.", "levels": null, "corpus_id": 6666276, "sentences": ["This example shows a critical notification as an orange rectangle with a message in the middle."], "caption": "", "local_uri": ["08f617726fe2529e75d5b2760e725e18a80901eb_Image_003.png"], "annotated": false, "compound": false}
{"title": "A Unifying Notification System To Scale Up Assistive Services", "pdf_hash": "08f617726fe2529e75d5b2760e725e18a80901eb", "year": 2015, "venue": "ASSETS", "alt_text": "The tablet shows a digital frame when no notifications are issues and it is idle.", "levels": null, "corpus_id": 6666276, "sentences": ["The tablet shows a digital frame when no notifications are issues and it is idle."], "caption": "", "local_uri": ["08f617726fe2529e75d5b2760e725e18a80901eb_Image_006.png"], "annotated": false, "compound": false}
{"title": "A Unifying Notification System To Scale Up Assistive Services", "pdf_hash": "08f617726fe2529e75d5b2760e725e18a80901eb", "year": 2015, "venue": "ASSETS", "alt_text": "This figure shows the evolution of the effectiveness for critical  and non-critical notifications, showed as diamonds and squares respectively.", "levels": null, "corpus_id": 6666276, "sentences": ["This figure shows the evolution of the effectiveness for critical  and non-critical notifications, showed as diamonds and squares respectively."], "caption": "Effectiveness scores range from 0 to 3.", "local_uri": ["08f617726fe2529e75d5b2760e725e18a80901eb_Image_008.png"], "annotated": false, "compound": false}
{"title": "A Unifying Notification System To Scale Up Assistive Services", "pdf_hash": "08f617726fe2529e75d5b2760e725e18a80901eb", "year": 2015, "venue": "ASSETS", "alt_text": "This shows the evolution of learnability for both critical and non-critical notifications.", "levels": null, "corpus_id": 6666276, "sentences": ["This shows the evolution of learnability for both critical and non-critical notifications."], "caption": "Time is measured in seconds.", "local_uri": ["08f617726fe2529e75d5b2760e725e18a80901eb_Image_009.png"], "annotated": false, "compound": false}
{"title": "BrailleSketch: A Gesture-based Text Input Method for People with Visual Impairments", "pdf_hash": "474a45ed6163754d93d7211f6490d3d38b240271", "year": 2017, "venue": "ASSETS", "alt_text": "The figure contains 5 images, which show the sequence of actions for drawing the letter N. First, the finger touches on the screen, then the finger moves to the right, then turns down and finally turns diagonally to the bottom left. The trace of the finger's motion connects all the dots in the Braille code for letter N.", "levels": null, "corpus_id": 32923215, "sentences": ["The figure contains 5 images, which show the sequence of actions for drawing the letter N. First, the finger touches on the screen, then the finger moves to the right, then turns down and finally turns diagonally to the bottom left.", "The trace of the finger's motion connects all the dots in the Braille code for letter N."], "caption": "", "local_uri": ["474a45ed6163754d93d7211f6490d3d38b240271_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "BrailleSketch: A Gesture-based Text Input Method for People with Visual Impairments", "pdf_hash": "474a45ed6163754d93d7211f6490d3d38b240271", "year": 2017, "venue": "ASSETS", "alt_text": "The figure shows a user touched the screen with one finger and the cellphone shows a 5x3 grid of dots.", "levels": null, "corpus_id": 32923215, "sentences": ["The figure shows a user touched the screen with one finger and the cellphone shows a 5x3 grid of dots."], "caption": "", "local_uri": ["474a45ed6163754d93d7211f6490d3d38b240271_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "BrailleSketch: A Gesture-based Text Input Method for People with Visual Impairments", "pdf_hash": "474a45ed6163754d93d7211f6490d3d38b240271", "year": 2017, "venue": "ASSETS", "alt_text": "The figure shows two different ways of sketching the Braille code for letter \"P\". The left figure shows the user touched the screen and started drawing from the top left dot and moved right, then moved diagonally towards bottom left and finally moved down.  The right figure shows the user touched the screen and started drawing from the left bottom dot and moved towards up twice. And finally the user moved right to finish sketching \"P\".", "levels": null, "corpus_id": 32923215, "sentences": ["The figure shows two different ways of sketching the Braille code for letter \"P\".", "The left figure shows the user touched the screen and started drawing from the top left dot and moved right, then moved diagonally towards bottom left and finally moved down.", "The right figure shows the user touched the screen and started drawing from the left bottom dot and moved towards up twice. And finally the user moved right to finish sketching \"P\"."], "caption": "Figure 5. Two different ways of sketching the Braille code for the letter \u201cP\u201d beyond the one shown in the Figure 1.", "local_uri": ["474a45ed6163754d93d7211f6490d3d38b240271_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "BrailleSketch: A Gesture-based Text Input Method for People with Visual Impairments", "pdf_hash": "474a45ed6163754d93d7211f6490d3d38b240271", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 6: The figure shows three additional keys implemented. From left to right of the figure, pressing volume down key to type the Space; swiping to the right and leaves the screen to delete a letter; pressing anywhere on the screen for more than 2s to type \"Enter\".", "levels": null, "corpus_id": 32923215, "sentences": ["Figure 6: The figure shows three additional keys implemented.", "From left to right of the figure, pressing volume down key to type the Space; swiping to the right and leaves the screen to delete a letter; pressing anywhere on the screen for more than 2s to type \"Enter\"."], "caption": "", "local_uri": ["474a45ed6163754d93d7211f6490d3d38b240271_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Effects of balancing for physical abilities on player performance, experience and self-esteem in exergames", "pdf_hash": "dd11d994cd3d819ddbd4c8b0f6bfcbb3690c9045", "year": 2014, "venue": "CHI", "alt_text": "Screenshot showing the custom-built version of Dance Dance Revolution that we used in our study. It shows how arrows appear at the top of the screen and need to be matched by players as they move to the bottom, and it shows player scores and interface elements.", "levels": null, "corpus_id": 12279384, "sentences": ["Screenshot showing the custom-built version of Dance Dance Revolution that we used in our study.", "It shows how arrows appear at the top of the screen and need to be matched by players as they move to the bottom, and it shows player scores and interface elements."], "caption": "Figure 1. Screen of the two-player dancing game we created.", "local_uri": ["dd11d994cd3d819ddbd4c8b0f6bfcbb3690c9045_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Effects of balancing for physical abilities on player performance, experience and self-esteem in exergames", "pdf_hash": "dd11d994cd3d819ddbd4c8b0f6bfcbb3690c9045", "year": 2014, "venue": "CHI", "alt_text": "Photographs showing players engaging with our game using two dance mats (right image) and a dance mat and a wheelchair (left image) as input devices.", "levels": [[-1]], "corpus_id": 12279384, "sentences": ["Photographs showing players engaging with our game using two dance mats (right image) and a dance mat and a wheelchair (left image) as input devices."], "caption": "Figure 2. Study setup for able-bodied players and players in wheelchairs (left) and able-bodied dyads (right).", "local_uri": ["dd11d994cd3d819ddbd4c8b0f6bfcbb3690c9045_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Effects of balancing for physical abilities on player performance, experience and self-esteem in exergames", "pdf_hash": "dd11d994cd3d819ddbd4c8b0f6bfcbb3690c9045", "year": 2014, "venue": "CHI", "alt_text": "This figure shows results for player experience of persons using wheelchairs and able-bodied persons playing our game. Mean results are given for the dimensons competence, autonomy, relatedness, control and enjoyment on a five point Likert scale. While most values are above average, the biggest difference in scores can be found for relatedness.", "levels": [[1], [1], [2]], "corpus_id": 12279384, "sentences": ["This figure shows results for player experience of persons using wheelchairs and able-bodied persons playing our game.", "Mean results are given for the dimensons competence, autonomy, relatedness, control and enjoyment on a five point Likert scale.", "While most values are above average, the biggest difference in scores can be found for relatedness."], "caption": "Figure 4. Mean (\uf0b1SE) scores for Study 2 by input device. The line represents a neutral response.", "local_uri": ["dd11d994cd3d819ddbd4c8b0f6bfcbb3690c9045_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "How Designing for People With and Without Disabilities Shapes Student Design Thinking", "pdf_hash": "e3b66b816df003addd7d393f644bcb4749825791", "year": 2016, "venue": "ASSETS", "alt_text": "Student designs (from left): G1\u2019s and G5\u2019s way-finding map interfaces. G11\u2019s captioning-in-progress.", "levels": null, "corpus_id": 14513233, "sentences": ["Student designs (from left): G1\u2019s and G5\u2019s way-finding map interfaces.", "G11\u2019s captioning-in-progress."], "caption": "Figure 2. Student designs (from left): G1\u2019s and G5\u2019s way- finding map interfaces. G11\u2019s captioning-in-progress.", "local_uri": ["e3b66b816df003addd7d393f644bcb4749825791_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "How Designing for People With and Without Disabilities Shapes Student Design Thinking", "pdf_hash": "e3b66b816df003addd7d393f644bcb4749825791", "year": 2016, "venue": "ASSETS", "alt_text": "A sketch of E11\u2019s group\u2019s glasses design, described as: \u201cdesigned to have a profile of modern \u2018hip\u2019 glasses.\u201d", "levels": null, "corpus_id": 14513233, "sentences": ["A sketch of E11\u2019s group\u2019s glasses design, described as: \u201cdesigned to have a profile of modern \u2018hip\u2019 glasses.\u201d"], "caption": "Figure 3. A sketch of E11\u2019s group\u2019s glasses design, described as: \u201cdesigned to have a profile of modern \u2018hip\u2019 glasses.\u201d", "local_uri": ["e3b66b816df003addd7d393f644bcb4749825791_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "How Designing for People With and Without Disabilities Shapes Student Design Thinking", "pdf_hash": "e3b66b816df003addd7d393f644bcb4749825791", "year": 2016, "venue": "ASSETS", "alt_text": "E9 tests a high-fidelity prototype, simulating glasses (he is wearing) displaying captions in real time (on the tablet).", "levels": null, "corpus_id": 14513233, "sentences": ["E9 tests a high-fidelity prototype, simulating glasses (he is wearing) displaying captions in real time (on the tablet)."], "caption": "Figure 4. E9 tests a high-fidelity prototype, simulating glasses (he is wearing) displaying captions in real time (on the tablet).", "local_uri": ["e3b66b816df003addd7d393f644bcb4749825791_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Transient and transitional states: pressure as an auxiliary input modality for bimanual interaction", "pdf_hash": "2b270028ed9d0e8f2851f67988e822ab434cdb9d", "year": 2014, "venue": "CHI", "alt_text": "Man is sitting and holding a tablet device that is resting on his leg. A pressure sensor is shown on the bezel of the device, under the thumb of his left hand.", "levels": null, "corpus_id": 15881107, "sentences": ["Man is sitting and holding a tablet device that is resting on his leg.", "A pressure sensor is shown on the bezel of the device, under the thumb of his left hand."], "caption": "Figure 1: People often dedicate one hand to holding a tablet, constraining their ability to use two hands on the touchscreen. We explore the use of pressure as a modality that can expand the available bimanual input techniques while the user is seated and holding the device.", "local_uri": ["2b270028ed9d0e8f2851f67988e822ab434cdb9d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Transient and transitional states: pressure as an auxiliary input modality for bimanual interaction", "pdf_hash": "2b270028ed9d0e8f2851f67988e822ab434cdb9d", "year": 2014, "venue": "CHI", "alt_text": "Participant sitting with tablet device resting on his lap. The index finger of his right hand is tapping on the screen while he holds the device with his left hand.", "levels": null, "corpus_id": 15881107, "sentences": ["Participant sitting with tablet device resting on his lap.", "The index finger of his right hand is tapping on the screen while he holds the device with his left hand."], "caption": "Figure 3: Participant during the experiment. Pressure input is controlled via an FSR under the thumb of his non- dominant hand (left-hand), while he holds the tablet as it rests on his lap. Targets are selected on the screen using his dominant hand.", "local_uri": ["2b270028ed9d0e8f2851f67988e822ab434cdb9d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Transient and transitional states: pressure as an auxiliary input modality for bimanual interaction", "pdf_hash": "2b270028ed9d0e8f2851f67988e822ab434cdb9d", "year": 2014, "venue": "CHI", "alt_text": "Graph showing the Mean Selection Time for each Menu Size, Direction and Distance in the Moving condition. Within Menu Size, 5 item is the fastest, then 7 item then 10 item. The Up Direction is faster than the Down Direction and the 2N Distance is fastest than the 4N distance which is faster than the 6N distance.", "levels": [[1], [3, 2], [3, 2]], "corpus_id": 15881107, "sentences": ["Graph showing the Mean Selection Time for each Menu Size, Direction and Distance in the Moving condition.", "Within Menu Size, 5 item is the fastest, then 7 item then 10 item.", "The Up Direction is faster than the Down Direction and the 2N Distance is fastest than the 4N distance which is faster than the 6N distance."], "caption": "Figure 5: Mean Selection Times for the Moving Condi-", "local_uri": ["2b270028ed9d0e8f2851f67988e822ab434cdb9d_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Transient and transitional states: pressure as an auxiliary input modality for bimanual interaction", "pdf_hash": "2b270028ed9d0e8f2851f67988e822ab434cdb9d", "year": 2014, "venue": "CHI", "alt_text": "Graph showing the mean selection time for for each Target Distance within each Direction. The graph shows, across directions, selection time increased as distance increased. The selection times for the Up Direction are generally faster than the Down Direction.", "levels": [[1], [3], [2]], "corpus_id": 15881107, "sentences": ["Graph showing the mean selection time for for each Target Distance within each Direction.", "The graph shows, across directions, selection time increased as distance increased.", "The selection times for the Up Direction are generally faster than the Down Direction."], "caption": "", "local_uri": ["2b270028ed9d0e8f2851f67988e822ab434cdb9d_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Learning from the Veg Box: Designing Unpredictability in Agency Delegation", "pdf_hash": "6bf90e822c2943819972642fe46487d3c81503d4", "year": 2018, "venue": "CHI", "alt_text": "Figure 2 \u2013 (Left) Screenshot of a participant's Diary entry (Ella). (Middle) \"Courgetti\" \u2013 The spaghetti noodles were replaced with spiralised courgettes (Daphne). (Right) A photo report of a soup made to hide the taste of some vegetables (Emily).", "levels": null, "corpus_id": 5040956, "sentences": ["Figure 2 \u2013 (Left) Screenshot of a participant's Diary entry (Ella). (Middle) \"Courgetti\" \u2013 The spaghetti noodles were replaced with spiralised courgettes (Daphne). (Right) A photo report of a soup made to hide the taste of some vegetables (Emily)."], "caption": "Table 2 \u2013 Summary of diary reports across the entire study.", "local_uri": ["6bf90e822c2943819972642fe46487d3c81503d4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "pdf_hash": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "year": 2016, "venue": "ASSETS", "alt_text": "Screenshot of the Learning Gems - Colors N Shapes app during the lesson on triangles.", "levels": null, "corpus_id": 8035131, "sentences": ["Screenshot of the Learning Gems - Colors N Shapes app during the lesson on triangles."], "caption": "Figure 1. LetterSchool app (left) and Learning Gems - Colors N Shapes app (right).", "local_uri": ["dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "pdf_hash": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "year": 2016, "venue": "ASSETS", "alt_text": "Description: In the first iteration of the activity on recognizing numbers (left), the options (or numbers in this case) were smaller and close together, there was a \u201cQuit\u201d button at the bottom, and the color scheme was not high-contrast. In later iterations (middle and then right), the options were bigger and more spaced out and the color scheme was high-contrast.", "levels": null, "corpus_id": 8035131, "sentences": ["Description: In the first iteration of the activity on recognizing numbers (left), the options (or numbers in this case) were smaller and close together, there was a \u201cQuit\u201d button at the bottom, and the color scheme was not high-contrast.", "In later iterations (middle and then right), the options were bigger and more spaced out and the color scheme was high-contrast."], "caption": "Figure 2. Iterations of activity on recognizing numbers.", "local_uri": ["dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_004.png"], "annotated": false, "compound": false}
{"title": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "pdf_hash": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the four screens that a user may see during the activity on recognizing Numbers. The first screen (on the top left corner) has a grid with all the numbers (black on white) and at the top there is a button that the user can press to \"Repeat prompt\" (which repeats the question) and information on which question the user is on out of the total number of questions in the lesson. This will be the same for all the screens. The second screen (on the top right corner) which appears after the user makes one incorrect response, has the same grid as screen 1 but now the correct number is highlighted (black on yellow). The third screen (on the bottom left corner) which appears after the user makes two consecutive incorrect responses, has the same grid as the previous screens but now all the incorrect numbers are gone and only the correct number (highlighted as on screen 2) remains. The fourth and final screen, which appears after the user has made three consecutive incorrect responses, has a big square that contains the correct number (highlighted as before).", "levels": null, "corpus_id": 8035131, "sentences": ["This figure shows the four screens that a user may see during the activity on recognizing Numbers.", "The first screen (on the top left corner) has a grid with all the numbers (black on white) and at the top there is a button that the user can press to \"Repeat prompt\" (which repeats the question) and information on which question the user is on out of the total number of questions in the lesson.", "This will be the same for all the screens.", "The second screen (on the top right corner) which appears after the user makes one incorrect response, has the same grid as screen 1 but now the correct number is highlighted (black on yellow).", "The third screen (on the bottom left corner) which appears after the user makes two consecutive incorrect responses, has the same grid as the previous screens but now all the incorrect numbers are gone and only the correct number (highlighted as on screen 2) remains.", "The fourth and final screen, which appears after the user has made three consecutive incorrect responses, has a big square that contains the correct number (highlighted as before)."], "caption": "Figure 4. Screens for the Numbers activity.", "local_uri": ["dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_007.png"], "annotated": false, "compound": false}
{"title": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "pdf_hash": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows a stacked column graph. The columns are grouped by activity (left to right): Colors, Money, Numbers, and Shapes. Within each activity there is a column for each participant (who demonstrated any reaction while using this activity) which depicts the quantity of negative (black) and positive (gray) reactions he/she demonstrated while trying that activity.\n\nParticipants\u2019 reactions during the Colors activity: For P1, 2 out of 2 were positive. For P2, 86 out of 95 were positive. For P3, 1 out of 1 were positive. For P4, 1 out of 1 were negative. For P5, 21 out of 25 were positive. For P6, 30 out of 30 were positive. P7 did not demonstrate any reactions. For P8, 22 out of 22 were positive. For P9, 31 out of 32 were positive.\n\nParticipants\u2019 reactions during the Money activity: For P1, 5 out of 5 were positive. For P2, 39 out of 39 were positive. For P3, 14 out of 14 were positive. For P4, 2 out of 3 were positive. For P5, 6 out of 6 were positive. For P6, 1 out of 1 were positive. P7 did not demonstrate any reactions. For P8, 13 out of 13 were positive. For P9, 21 out of 21 were positive. P10 did not demonstrate any reactions.\n\nParticipants\u2019 reactions during the Numbers activity: For P1, 8 out of 8 were positive. For P2, 64 out of 73 were positive. For P3, 2 out of 2 were positive. For P4, 10 out of 11 were positive. For P5, 21 out of 24 were positive. For P6, 19 out of 19 were positive. P7 did not demonstrate any reactions. For P8, 13 out of 13 were positive. For P9, 10 out of 10 were positive. P10 did not demonstrate any reactions.\n\nParticipants\u2019 reactions during the Shapes activity: For P1, 5 out of 5 were positive. For P2, 58 out of 59 were positive. For P3, 6 out of 6 were positive. For P4, 10 out of 11 were positive. For P5, 7 out of 8 were positive. For P6, 10 out of 10 were positive. P7 did not demonstrate any reactions. For P8, 8 out of 8 were positive. For P9, 31 out of 31 were positive.", "levels": [[1], [1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 8035131, "sentences": ["This figure shows a stacked column graph.", "The columns are grouped by activity (left to right): Colors, Money, Numbers, and Shapes.", "Within each activity there is a column for each participant (who demonstrated any reaction while using this activity) which depicts the quantity of negative (black) and positive (gray) reactions he/she demonstrated while trying that activity.", "Participants\u2019 reactions during the Colors activity: For P1, 2 out of 2 were positive.", "For P2, 86 out of 95 were positive.", "For P3, 1 out of 1 were positive.", "For P4, 1 out of 1 were negative.", "For P5, 21 out of 25 were positive.", "For P6, 30 out of 30 were positive.", "P7 did not demonstrate any reactions.", "For P8, 22 out of 22 were positive.", "For P9, 31 out of 32 were positive.", "Participants\u2019 reactions during the Money activity: For P1, 5 out of 5 were positive.", "For P2, 39 out of 39 were positive.", "For P3, 14 out of 14 were positive.", "For P4, 2 out of 3 were positive.", "For P5, 6 out of 6 were positive.", "For P6, 1 out of 1 were positive.", "P7 did not demonstrate any reactions.", "For P8, 13 out of 13 were positive.", "For P9, 21 out of 21 were positive.", "P10 did not demonstrate any reactions.", "Participants\u2019 reactions during the Numbers activity: For P1, 8 out of 8 were positive.", "For P2, 64 out of 73 were positive.", "For P3, 2 out of 2 were positive.", "For P4, 10 out of 11 were positive.", "For P5, 21 out of 24 were positive.", "For P6, 19 out of 19 were positive.", "P7 did not demonstrate any reactions.", "For P8, 13 out of 13 were positive.", "For P9, 10 out of 10 were positive.", "P10 did not demonstrate any reactions.", "Participants\u2019 reactions during the Shapes activity: For P1, 5 out of 5 were positive.", "For P2, 58 out of 59 were positive.", "For P3, 6 out of 6 were positive.", "For P4, 10 out of 11 were positive.", "For P5, 7 out of 8 were positive.", "For P6, 10 out of 10 were positive.", "P7 did not demonstrate any reactions.", "For P8, 8 out of 8 were positive.", "For P9, 31 out of 31 were positive."], "caption": "Figure 5. Positive and Negative Reactions with the Colors, Money, Numbers, and Shapes activities.", "local_uri": ["dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "pdf_hash": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows a stacked column graph. The columns are grouped by activity (left to right): Lowercase Letters, Uppercase Letters, and Money Addition. Within each activity there is a column for each participant (who demonstrated any reaction while using this activity) which depicts the quantity of negative (black) and positive (gray) reactions he/she demonstrated while trying that activity.\n\nParticipants\u2019 reactions during the Lowercase Letters activity: For P1, 12 out of 12 were positive. For P2, 75 out of 76 were positive. For P3, 1 out of 1 were positive. For P4, 18 out of 18 were positive. For P5, 20 out of 20 were positive. For P6, 4 out of 4 were positive. For P7, 1 out of 1 were negative. For P8, 18 out of 18 were positive. For P9, 30 out of 31 were positive. For P10, 7 out of 7 were positive.\n\nParticipants\u2019 reactions during the Uppercase Letters activity: For P1, 16 out of 16 were positive. For P2, 96 out of 99 were positive. For P3, 1 out of 1 were positive. For P4, 8 out of 9 were positive. For P5, 35 out of 36 were positive. For P6, 24 out of 24 were positive. P7 did not demonstrate any reactions. For P8, 13 out of 13 were positive. For P9, 37 out of 37 were positive.\n\nParticipants\u2019 reactions during the Money Addition activity: For P1, 12 out of 12 were positive. For P2, 67 out of 82 were positive. For P3, 39 out of 55 were positive. For P4, 14 out of 14 were negative. For P5, 9 out of 9 were positive. For P6, 24 out of 24 were positive. P7 did not demonstrate any reactions. For P8, 16 out of 16 were positive. For P9, 39 out of 43 were positive.", "levels": [[1], [1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 8035131, "sentences": ["This figure shows a stacked column graph.", "The columns are grouped by activity (left to right): Lowercase Letters, Uppercase Letters, and Money Addition.", "Within each activity there is a column for each participant (who demonstrated any reaction while using this activity) which depicts the quantity of negative (black) and positive (gray) reactions he/she demonstrated while trying that activity.", "Participants\u2019 reactions during the Lowercase Letters activity: For P1, 12 out of 12 were positive.", "For P2, 75 out of 76 were positive.", "For P3, 1 out of 1 were positive.", "For P4, 18 out of 18 were positive.", "For P5, 20 out of 20 were positive.", "For P6, 4 out of 4 were positive.", "For P7, 1 out of 1 were negative.", "For P8, 18 out of 18 were positive.", "For P9, 30 out of 31 were positive.", "For P10, 7 out of 7 were positive.", "Participants\u2019 reactions during the Uppercase Letters activity: For P1, 16 out of 16 were positive.", "For P2, 96 out of 99 were positive.", "For P3, 1 out of 1 were positive.", "For P4, 8 out of 9 were positive.", "For P5, 35 out of 36 were positive.", "For P6, 24 out of 24 were positive.", "P7 did not demonstrate any reactions.", "For P8, 13 out of 13 were positive.", "For P9, 37 out of 37 were positive.", "Participants\u2019 reactions during the Money Addition activity: For P1, 12 out of 12 were positive.", "For P2, 67 out of 82 were positive.", "For P3, 39 out of 55 were positive.", "For P4, 14 out of 14 were negative.", "For P5, 9 out of 9 were positive.", "For P6, 24 out of 24 were positive.", "P7 did not demonstrate any reactions.", "For P8, 16 out of 16 were positive.", "For P9, 39 out of 43 were positive."], "caption": "Figure 6. Positive and Negative Reactions with the LL, UL and MA activities.", "local_uri": ["dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_018.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Extracting Design Guidelines for Wearables and Movement in Tabletop Role-Playing Games via a Research Through Design Process", "pdf_hash": "1b0a0a16c12999e082ccd3ddd3cb2082eb03c953", "year": 2018, "venue": "CHI", "alt_text": "Experience Prototype of the Arm-Worn Device. On the left the state before the elemental stone ritual is shown. There is no stones attached to the prototype. On the right, experience prototype has one green and one red stone attached which are for earth and fire respectively. These stones are also props in the shape of a disc.", "levels": null, "corpus_id": 5083154, "sentences": ["Experience Prototype of the Arm-Worn Device.", "On the left the state before the elemental stone ritual is shown.", "There is no stones attached to the prototype.", "On the right, experience prototype has one green and one red stone attached which are for earth and fire respectively.", "These stones are also props in the shape of a disc."], "caption": "", "local_uri": ["1b0a0a16c12999e082ccd3ddd3cb2082eb03c953_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "pdf_hash": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "year": 2018, "venue": "CHI", "alt_text": "No-uncertainty visualization, showing a predicted arrival time bar at 9 minutes and the scheduled arrival time bar at 7 minutes.", "levels": [[1]], "corpus_id": 5065176, "sentences": ["No-uncertainty visualization, showing a predicted arrival time bar at 9 minutes and the scheduled arrival time bar at 7 minutes."], "caption": "This display type represents the sta\u00ad tus quo: it is informationally similar to the existing OneBusAway app, except we did not include annota\u00ad", "local_uri": ["825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "pdf_hash": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "year": 2018, "venue": "CHI", "alt_text": "Text predictive interval visualization. This reads \"Arriving in 9 minutes. Very good chacne (85%) of arriving 7 min from now or later\".", "levels": null, "corpus_id": 5065176, "sentences": ["Text predictive interval visualization.", "This reads \"Arriving in 9 minutes.", "Very good chacne (85%) of arriving 7 min from now or later\"."], "caption": "Compared to visual representations of probabilistic estimates, natural language representations provide a condensed illustration of a distri\u00ad", "local_uri": ["825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "pdf_hash": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "year": 2018, "venue": "CHI", "alt_text": "Quantile dotplots shown on a timeline. Dark vertical bar indicates predicted arrival time; light vertical bar indicates scheduled arrival time.", "levels": [[1], [1]], "corpus_id": 5065176, "sentences": ["Quantile dotplots shown on a timeline.", "Dark vertical bar indicates predicted arrival time; light vertical bar indicates scheduled arrival time."], "caption": "", "local_uri": ["825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "pdf_hash": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "year": 2018, "venue": "CHI", "alt_text": "Interval plot showing scheduled arrival time and predicted arrival time as lines, wtih a dark shaded region indicating the 50% quantile and light shaded region indicating the 95% quantile times.", "levels": [[-1]], "corpus_id": 5065176, "sentences": ["Interval plot showing scheduled arrival time and predicted arrival time as lines, wtih a dark shaded region indicating the 50% quantile and light shaded region indicating the 95% quantile times."], "caption": "Interval plots are perhaps the most common uncertainty visualization. We tested interval plots to see if the familiarity of the visualization", "local_uri": ["825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "pdf_hash": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "year": 2018, "venue": "CHI", "alt_text": "Probability density plot, showing the PDF as a lightly shaded interval, the predicted arrival time as a dark vertical bar, and the scheduled arrival time as a light vertical bar.", "levels": [[1]], "corpus_id": 5065176, "sentences": ["Probability density plot, showing the PDF as a lightly shaded interval, the predicted arrival time as a dark vertical bar, and the scheduled arrival time as a light vertical bar."], "caption": "Probability density function (PDF) plots use an area encoding for prob\u00ad ability that allows people to under\u00ad stand the shape of a distribution at", "local_uri": ["825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "pdf_hash": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "year": 2018, "venue": "CHI", "alt_text": "Complementary Cummulative Distribution Plot. Lightly shaded region indicates someone's chances of catching the bus if they arrive at the bus stop at a particular time. Dark vertical line indicates predicted arrival time; light vertical line indicates scheduled arrival time.", "levels": [[1], [1], [1]], "corpus_id": 5065176, "sentences": ["Complementary Cummulative Distribution Plot.", "Lightly shaded region indicates someone's chances of catching the bus if they arrive at the bus stop at a particular time.", "Dark vertical line indicates predicted arrival time; light vertical line indicates scheduled arrival time."], "caption": "Outside of the transit domain, prior work in uncertainty visualization found CDFs to be effective for con\u00ad veying some probabilities to the", "local_uri": ["825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Pok\u00e9mon GO Experience: A Location-Based Augmented Reality Mobile Game Goes Mainstream", "pdf_hash": "3728dfe5c65b169ffa427a59ad105370c4beae66", "year": 2017, "venue": "CHI", "alt_text": "https://lh3.googleusercontent.com/-8F-a-NXGkOMoej-FIQXV5LPePWFbyj2Ql7z_xCM4MkszVfKDXYmhYmNOfJNQY0Z_w=h900", "levels": null, "corpus_id": 407933, "sentences": ["https://lh3.googleusercontent.com/-8F-a-NXGkOMoej-FIQXV5LPePWFbyj2Ql7z_xCM4MkszVfKDXYmhYmNOfJNQY0Z_w=h900"], "caption": "Pok\u00e9mon GO", "local_uri": ["3728dfe5c65b169ffa427a59ad105370c4beae66_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "The Pok\u00e9mon GO Experience: A Location-Based Augmented Reality Mobile Game Goes Mainstream", "pdf_hash": "3728dfe5c65b169ffa427a59ad105370c4beae66", "year": 2017, "venue": "CHI", "alt_text": "https://cdn.theatlantic.com/assets/media/img/mt/2016/07/pokemon/lead_720_405.jpg?mod=1533691835", "levels": null, "corpus_id": 407933, "sentences": ["https://cdn.theatlantic.com/assets/media/img/mt/2016/07/pokemon/lead_720_405.jpg?mod=1533691835"], "caption": "", "local_uri": ["3728dfe5c65b169ffa427a59ad105370c4beae66_Image_019.gif"], "annotated": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "Figure a and b demonstrates how a visually impaired user use a screen-reader keyboard, especially when the touch location is not located in the intended key, the user performs a calibration to the correct key. Figure c illustrates the layout change of VIPBoard which reads out the predicted key name and enables the user to confirm the touch without calibration. Figure d illustrates that the new layout is based on the initial one and the user can still input any character on the layout.", "levels": null, "corpus_id": 140450868, "sentences": ["Figure a and b demonstrates how a visually impaired user use a screen-reader keyboard, especially when the touch location is not located in the intended key, the user performs a calibration to the correct key.", "Figure c illustrates the layout change of VIPBoard which reads out the predicted key name and enables the user to confirm the touch without calibration.", "Figure d illustrates that the new layout is based on the initial one and the user can still input any character on the layout."], "caption": "", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_001.png"], "annotated": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "Figure a illustrates keyboard layout adaption. The predicted key is translated to make sure the touch position is contained by the new key boundary, the other keys are changed accordingly. Figure b shows the offset vectors. The unoptimized offset vector starts from the key boundary to the touch position, while the optimized offset vector is longer, starts from inside the key to the touch position.", "levels": null, "corpus_id": 140450868, "sentences": ["Figure a illustrates keyboard layout adaption.", "The predicted key is translated to make sure the touch position is contained by the new key boundary, the other keys are changed accordingly.", "Figure b shows the offset vectors.", "The unoptimized offset vector starts from the key boundary to the touch position, while the optimized offset vector is longer, starts from inside the key to the touch position."], "caption": "= P (pos | c, pre)P (c, pre)", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_004.png"], "annotated": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "Figure a is a visually impaired user typing on a smartphone. Figure b is the interface of the study, which contains a keyboard and the target transcription sentence.", "levels": null, "corpus_id": 140450868, "sentences": ["Figure a is a visually impaired user typing on a smartphone.", "Figure b is the interface of the study, which contains a keyboard and the target transcription sentence."], "caption": "", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_007.png"], "annotated": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "The figure contains 26 ellipses which describe the 26 bivariate Gaussian distribution, which show the high input noise of visually impaired users.", "levels": [[-1]], "corpus_id": 140450868, "sentences": ["The figure contains 26 ellipses which describe the 26 bivariate Gaussian distribution, which show the high input noise of visually impaired users."], "caption": "Chinese characters. The corpus of Chinese words and the Pinyin decoding system is transferred from Google Pinyin IME [1]. We used the general touch model collected in Study 1.", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "The figure shows a sample English phrase: call for more details, a sample Chinese phrase: wish you a good dream, and the corresponding Pinyin string (xiwang ni keyi zuo yige haomeng).", "levels": null, "corpus_id": 140450868, "sentences": ["The figure shows a sample English phrase: call for more details, a sample Chinese phrase: wish you a good dream, and the corresponding Pinyin string (xiwang ni keyi zuo yige haomeng)."], "caption": "Figure 5: Examples of English, Chinese test phrase and the corresponding pinyin string of the Chinese phrase.", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_010.png"], "annotated": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "The figures show the time distribution of different keyboards and different languages. The entering time of VIPBoard is 1079.51 ms in English input and 1174.29 ms in Chinese input, the entering time of the traditional keyboard is 1298.95 ms in English input and 1359.74 ms in Chinese input.", "levels": [[1], [2]], "corpus_id": 140450868, "sentences": ["The figures show the time distribution of different keyboards and different languages.", "The entering time of VIPBoard is 1079.51 ms in English input and 1174.29 ms in Chinese input, the entering time of the traditional keyboard is 1298.95 ms in English input and 1359.74 ms in Chinese input."], "caption": "Table 3: Average miss rate (MR) of two keyboards", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "Figure a and b show average miss rate in each session for English and Chinese input respectively. The miss rates of VIPBoard are all lower than those of the traditional keyboard.", "levels": null, "corpus_id": 140450868, "sentences": ["Figure a and b show average miss rate in each session for English and Chinese input respectively.", "The miss rates of VIPBoard are all lower than those of the traditional keyboard."], "caption": "Figure 9: Average miss rate in each session. The error bars show the standard deviations.", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_019.jpg"], "annotated": false, "compound": false}
{"title": "VIPBoard: Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction", "pdf_hash": "76cf5f3238904787043a4417b306e85c717e60b3", "year": 2019, "venue": "CHI", "alt_text": "Figure a shows the number of available choices for the next input on the y-axis and the length of characters already input on the x-axis. Figure b shows the number of availabe keys within 2.5 key widths of the target key on y-axis and the length of characters already input on x-axis. Both figures show a shadow area indicates the percentage of words longer than the input length on the right axis.", "levels": null, "corpus_id": 140450868, "sentences": ["Figure a shows the number of available choices for the next input on the y-axis and the length of characters already input on the x-axis.", "Figure b shows the number of availabe keys within 2.5 key widths of the target key on y-axis and the length of characters already input on x-axis.", "Both figures show a shadow area indicates the percentage of words longer than the input length on the right axis."], "caption": "(a)                         (b)", "local_uri": ["76cf5f3238904787043a4417b306e85c717e60b3_Image_024.jpg", "76cf5f3238904787043a4417b306e85c717e60b3_Image_025.jpg"], "annotated": false, "compound": true}
{"title": "Design and Evaluation of a Social Media Writing Support Tool for People with Dyslexia", "pdf_hash": "8a00cd8478625ba44787d18015c6f747a71cbeda", "year": 2019, "venue": "CHI", "alt_text": "Relationship between perceived accuracy (x-axis) and overall satisfaction level with AWH (y-axis).", "levels": null, "corpus_id": 140233634, "sentences": ["Relationship between perceived accuracy (x-axis) and overall satisfaction level with AWH (y-axis)."], "caption": "", "local_uri": ["8a00cd8478625ba44787d18015c6f747a71cbeda_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Communicating Uncertainty in Fertility Prognosis", "pdf_hash": "d1177fed35f70e0442faed7f79433ce426de7a06", "year": 2019, "venue": "CHI", "alt_text": "This figure shows three screenshots that display three different views of our prototype each representing the fertility prognosis of a specific timeframe in the past. The first presents one day, the second six days and the thrid a whole month in the past. The first shows two bars on top of each other: an orange bar indicates that the system predicted 30% conception probability for the given day and a smaller blue bar indicates that the system retrospectively calculated a conception probability of 13% for that day. In the same way the second screenshows shows orange and blue bars indicating predicted and detected fertile phases for six consecutive days and the third visualization shows orange and blue bars within the tiles of a calendar. A problem with this visualization that we explain in the limitation section is that the orange bars are occuded by the blue ones so that only their dotted outline is visible if the orange bar is smaller. We did not optimze our prototype for this case, because the initial plan for our study and the data we received for prototyping purposes focused only on data in the future. However, this visualization of past data was very effective in helping users understand uncertainties in the predictions.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 140264944, "sentences": ["This figure shows three screenshots that display three different views of our prototype each representing the fertility prognosis of a specific timeframe in the past.", "The first presents one day, the second six days and the thrid a whole month in the past.", "The first shows two bars on top of each other: an orange bar indicates that the system predicted 30% conception probability for the given day and a smaller blue bar indicates that the system retrospectively calculated a conception probability of 13% for that day.", "In the same way the second screenshows shows orange and blue bars indicating predicted and detected fertile phases for six consecutive days and the third visualization shows orange and blue bars within the tiles of a calendar.", "A problem with this visualization that we explain in the limitation section is that the orange bars are occuded by the blue ones so that only their dotted outline is visible if the orange bar is smaller.", "We did not optimze our prototype for this case, because the initial plan for our study and the data we received for prototyping purposes focused only on data in the future.", "However, this visualization of past data was very effective in helping users understand uncertainties in the predictions."], "caption": "", "local_uri": ["d1177fed35f70e0442faed7f79433ce426de7a06_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Encumbered Interaction: a Study of Musicians Preparing to Perform", "pdf_hash": "804e261a3935febb2595d29c6e18a089e49469a8", "year": 2019, "venue": "CHI", "alt_text": "There is a desktop computer set on a small desk along with a printer, blank note paper, pencils and pens, and a desk chair on wheels (out of shot). To the left of the desk Mike has his bass guitar and a guitar stand which is positioned within easy reach of the desk area. The bass is connected to a small practice amplifier, also within close reach of the desk area.", "levels": null, "corpus_id": 140225054, "sentences": ["There is a desktop computer set on a small desk along with a printer, blank note paper, pencils and pens, and a desk chair on wheels (out of shot).", "To the left of the desk Mike has his bass guitar and a guitar stand which is positioned within easy reach of the desk area.", "The bass is connected to a small practice amplifier, also within close reach of the desk area."], "caption": "Figure 2: Mike's Home Set-up", "local_uri": ["804e261a3935febb2595d29c6e18a089e49469a8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Encumbered Interaction: a Study of Musicians Preparing to Perform", "pdf_hash": "804e261a3935febb2595d29c6e18a089e49469a8", "year": 2019, "venue": "CHI", "alt_text": "Mike is holding his mouse with the right hand to control the media player of a YouTube video on his computer whilst having his bass guitar on his lap.", "levels": null, "corpus_id": 140225054, "sentences": ["Mike is holding his mouse with the right hand to control the media player of a YouTube video on his computer whilst having his bass guitar on his lap."], "caption": "Figure 3: Mike playing along while operating his computer", "local_uri": ["804e261a3935febb2595d29c6e18a089e49469a8_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Encumbered Interaction: a Study of Musicians Preparing to Perform", "pdf_hash": "804e261a3935febb2595d29c6e18a089e49469a8", "year": 2019, "venue": "CHI", "alt_text": "A piece of paper containing a series of song titles with guitar chord changes written to the right. On of these chord progressions has been annotated with a pen (amending a mistake).", "levels": null, "corpus_id": 140225054, "sentences": ["A piece of paper containing a series of song titles with guitar chord changes written to the right.", "On of these chord progressions has been annotated with a pen (amending a mistake)."], "caption": "Figure 7: Marvin\u2019s annotated set-list", "local_uri": ["804e261a3935febb2595d29c6e18a089e49469a8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "pdf_hash": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "year": 2013, "venue": "ASSETS", "alt_text": "The image consists of 3 sub-images: first, a Braille representation of character 'h' using dots 1, 2, and 5; second, an figure with both users' hands and a representation of rings 1, 2, and 5 vibrating (character 'h'); third, the UbiBraille prototype with 6 rings augmented with a vibration motor. Rings are connected to a case.", "levels": null, "corpus_id": 16515725, "sentences": ["The image consists of 3 sub-images: first, a Braille representation of character 'h' using dots 1, 2, and 5; second, an figure with both users' hands and a representation of rings 1, 2, and 5 vibrating (character 'h'); third, the UbiBraille prototype with 6 rings augmented with a vibration motor.", "Rings are connected to a case."], "caption": "Figure 1. UbiBraille, our Braille-reading vibrotactile prototype, outputs individual characters using the same coding used for writing with a Braille typewriter. (a) The figure illustrates the matricial representation of \u2018h\u2019 using the Braille code: dots 1, 2, and", "local_uri": ["39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_001.png"], "annotated": false, "compound": false}
{"title": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "pdf_hash": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "year": 2013, "venue": "ASSETS", "alt_text": "Confusion matrix with each letter recognition rate. Y axis represents the required character and X axis represented the answered character. Most characters are above 90% accuracy rate. Most problematic characters are NOVYZ.", "levels": null, "corpus_id": 16515725, "sentences": ["Confusion matrix with each letter recognition rate.", "Y axis represents the required character and X axis represented the answered character.", "Most characters are above 90% accuracy rate.", "Most problematic characters are NOVYZ."], "caption": "", "local_uri": ["39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_004.gif"], "annotated": false, "compound": false}
{"title": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "pdf_hash": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "year": 2013, "venue": "ASSETS", "alt_text": "Blind participant during the user study. The participant is in a quite room, seated and with his hands on a table. He is wearing UbiBraille.", "levels": null, "corpus_id": 16515725, "sentences": ["Blind participant during the user study.", "The participant is in a quite room, seated and with his hands on a table.", "He is wearing UbiBraille."], "caption": "Figure 2. Participant during user study. His hands were resting on the table whilst receiving vibrotactile feedback.", "local_uri": ["39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "pdf_hash": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "year": 2013, "venue": "ASSETS", "alt_text": "Bar chart illustrating character recognition accuracy per participant: P1 - 94%, P2 - 65%, P3 - 92%, P4 - 100%, P5 - 100%, P6 - 69%, P7 - 94%, P8 - 96%, P9 - 65%, P10 - 67%, P11 -  54%", "levels": [[2, 1]], "corpus_id": 16515725, "sentences": ["Bar chart illustrating character recognition accuracy per participant: P1 - 94%, P2 - 65%, P3 - 92%, P4 - 100%, P5 - 100%, P6 - 69%, P7 - 94%, P8 - 96%, P9 - 65%, P10 - 67%, P11 -  54%"], "caption": "Figure 4. Character recognition accuracy per participant.", "local_uri": ["39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_013.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "pdf_hash": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "year": 2013, "venue": "ASSETS", "alt_text": "Bar chart representing word recognition accuracy per condition. 4000ms condition - 93%, 2000ms condition - 89%, 1000ms condition - 64%, 500ms condition - 33%. Accuracy decreases with demand of condition. Clearly there are no statistically significant differences between the first and second conditions.", "levels": [[1], [2], [3], [2]], "corpus_id": 16515725, "sentences": ["Bar chart representing word recognition accuracy per condition.", "4000ms condition - 93%, 2000ms condition - 89%, 1000ms condition - 64%, 500ms condition - 33%.", "Accuracy decreases with demand of condition.", "Clearly there are no statistically significant differences between the first and second conditions."], "caption": "Figure 5. Word recognition accuracy per duration condition.", "local_uri": ["39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_016.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This image illustrates how progressively downloaded data is displayed when a user pans a visualization that uses the Splash framework.", "levels": [[-1]], "corpus_id": 6618748, "sentences": ["This image illustrates how progressively downloaded data is displayed when a user pans a visualization that uses the Splash framework."], "caption": "Figure 1: The Splash framework enables real-time navigation for client-server visualization systems by progressively loading data.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This image illustrates the visual appearance of the target features in each of the study's three tasks.", "levels": [[1]], "corpus_id": 6618748, "sentences": ["This image illustrates the visual appearance of the target features in each of the study's three tasks."], "caption": "Figure 2: Example target features: (a) the maximum value of the dataset, (b) the shift in the mean value, and (c) a specific value. The coarse LOD offers varying utility to identifying the feature at the fine LOD. (Arrows are illustrative)", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This chart shows the average completion time (ms) across the three tasks, for each of the four conditions.", "levels": [[1]], "corpus_id": 6618748, "sentences": ["This chart shows the average completion time (ms) across the three tasks, for each of the four conditions."], "caption": "Figure 3: Task completion times. Bars indicate a 95% CI.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This image illustrates how the three components of the Splash framework integrate with an existing data server and visualization client.", "levels": [[-1]], "corpus_id": 6618748, "sentences": ["This image illustrates how the three components of the Splash framework integrate with an existing data server and visualization client."], "caption": "Figure 4: Overview of the Splash framework. The five steps required to integrate Splash into server- and client-side are divided amongst the data curator and visualization developer roles. Implementation responsibilities of each are indicated by asterisks.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This chart illustrates the theoretical performance of progressive downloading across different LOD compression ratios and SPI settings.", "levels": [[1]], "corpus_id": 6618748, "sentences": ["This chart illustrates the theoretical performance of progressive downloading across different LOD compression ratios and SPI settings."], "caption": "Figure 5: Modeled response times under varying LOD hierar- chy compression ratios and SPI settings. Progressive loading vastly improves interaction latency.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This image is a screenshot of the visualization produced by the first case study participant.", "levels": [[1]], "corpus_id": 6618748, "sentences": ["This image is a screenshot of the visualization produced by the first case study participant."], "caption": "Figure 6: Screenshot of the BlueGene dataset visualization. Three errors types in stacked charts with linked navigation.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This image is a screenshot of the visualization produced by the second case study participant.", "levels": [[-1]], "corpus_id": 6618748, "sentences": ["This image is a screenshot of the visualization produced by the second case study participant."], "caption": "Figure 7: Screenshot of the eQTL dataset visualization. Max, median, and the name of the max SNP are plotted.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "pdf_hash": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "year": 2014, "venue": "CHI", "alt_text": "This image is a screenshot of the visualization produced by the third case study participant.", "levels": [[-1]], "corpus_id": 6618748, "sentences": ["This image is a screenshot of the visualization produced by the third case study participant."], "caption": "Figure 8: Screenshot of the ATH CHR-M dataset visualiza- tion. Normalized frequencies of bases are displayed.", "local_uri": ["e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Sharing is Caring: Assistive Technology Designs on Thingiverse", "pdf_hash": "a3b2c700da67bccf500a7e8678c52ae1e4c453ac", "year": 2015, "venue": "CHI", "alt_text": "Grid showing top five most popular designs as tallied by likes, makes, and remixes. Images include prosthetic hands, pill boxes, a spinner ring, and a model of an atomic element.", "levels": null, "corpus_id": 17273816, "sentences": ["Grid showing top five most popular designs as tallied by likes, makes, and remixes.", "Images include prosthetic hands, pill boxes, a spinner ring, and a model of an atomic element."], "caption": "", "local_uri": ["a3b2c700da67bccf500a7e8678c52ae1e4c453ac_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Seemo: A Computational Approach to See Emotions", "pdf_hash": "763ccf9792cfa64cd00d7f180f6d82e7c5a68fa6", "year": 2018, "venue": "CHI", "alt_text": "A close up of text on a black background  Description generated with high confidence", "levels": null, "corpus_id": 5046142, "sentences": ["A close up of text on a black background  Description generated with high confidence"], "caption": "Figure 2. Model used to build Seemo - Given an emotion ek, it samples consecutive words {w1, w2, \u2026, wc} from the respective emotional document with a sliding window size of c, and uses them to learn emotion\u2019s representations WVxN.", "local_uri": ["763ccf9792cfa64cd00d7f180f6d82e7c5a68fa6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Investigating the Information Transfer Efficiency of a 3x3 Watch-back Tactile Display", "pdf_hash": "6dcb694f1f84f8370e298cc768a9a602e3211252", "year": 2015, "venue": "CHI", "alt_text": "Figure 1. Prototype and structure of the two types of tactors: (a) surface type (b) tip type.", "levels": null, "corpus_id": 7535566, "sentences": ["Figure 1.", "Prototype and structure of the two types of tactors: (a) surface type (b) tip type."], "caption": "Figure 1. Prototype and structure of the two types of tactors: (a) surface type (b) tip type.", "local_uri": ["6dcb694f1f84f8370e298cc768a9a602e3211252_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Investigating the Information Transfer Efficiency of a 3x3 Watch-back Tactile Display", "pdf_hash": "6dcb694f1f84f8370e298cc768a9a602e3211252", "year": 2015, "venue": "CHI", "alt_text": "Figure 2. Temporal patterns: (a) without sensory saltation (b) with sensory saltation", "levels": null, "corpus_id": 7535566, "sentences": ["Figure 2.", "Temporal patterns: (a) without sensory saltation (b) with sensory saltation"], "caption": "Figure 2. Temporal patterns: (a) without sensory saltation (b) with sensory saltation.", "local_uri": ["6dcb694f1f84f8370e298cc768a9a602e3211252_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Investigating the Information Transfer Efficiency of a 3x3 Watch-back Tactile Display", "pdf_hash": "6dcb694f1f84f8370e298cc768a9a602e3211252", "year": 2015, "venue": "CHI", "alt_text": "Figure 3. Spatial patterns: (a) Line, (b) Point, (c) Leftmost, and (d) Rightmost.", "levels": null, "corpus_id": 7535566, "sentences": ["Figure 3.", "Spatial patterns: (a) Line, (b) Point, (c) Leftmost, and (d) Rightmost."], "caption": "Figure 3. Spatial patterns: (a) Line, (b) Point, (c)", "local_uri": ["6dcb694f1f84f8370e298cc768a9a602e3211252_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Investigating the Information Transfer Efficiency of a 3x3 Watch-back Tactile Display", "pdf_hash": "6dcb694f1f84f8370e298cc768a9a602e3211252", "year": 2015, "venue": "CHI", "alt_text": "Figure 4. The stimulus-response confusion matrices and Mean and Standard Deviations (SD) of AC, IT, and RT from the experiments.", "levels": [[-1], [-1]], "corpus_id": 7535566, "sentences": ["Figure 4.", "The stimulus-response confusion matrices and Mean and Standard Deviations (SD) of AC, IT, and RT from the experiments."], "caption": "Figure 4. The stimulus-response confusion matrices and Mean and Standard Deviations (SD) of AC, IT, and RT from the experiments.", "local_uri": ["6dcb694f1f84f8370e298cc768a9a602e3211252_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "From screen reading to aural glancing: towards instant access to key page sections", "pdf_hash": "668c5c9a157e5a142993ba3e9d36b21b1f1466cf", "year": 2014, "venue": "ASSETS", "alt_text": "Screen-reader users performed basic tasks on Amazon and discussed their page-level navigation experience.", "levels": null, "corpus_id": 15878200, "sentences": ["Screen-reader users performed basic tasks on Amazon and discussed their page-level navigation experience."], "caption": "Figure 1. Screen-reader users performed basic tasks on Amazon.com and discussed their page-level navigation experience.", "local_uri": ["668c5c9a157e5a142993ba3e9d36b21b1f1466cf_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "From screen reading to aural glancing: towards instant access to key page sections", "pdf_hash": "668c5c9a157e5a142993ba3e9d36b21b1f1466cf", "year": 2014, "venue": "ASSETS", "alt_text": "Across page types, the serial navigation paradigm of screen readers buries highly-important sections necessary to complete common web tasks.", "levels": null, "corpus_id": 15878200, "sentences": ["Across page types, the serial navigation paradigm of screen readers buries highly-important sections necessary to complete common web tasks."], "caption": "Figure 6. The serial navigation paradigm of screen readers buries highly-important sections necessary to complete common web tasks.", "local_uri": ["668c5c9a157e5a142993ba3e9d36b21b1f1466cf_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "From screen reading to aural glancing: towards instant access to key page sections", "pdf_hash": "668c5c9a157e5a142993ba3e9d36b21b1f1466cf", "year": 2014, "venue": "ASSETS", "alt_text": "Expert performance suggests the DASX markedly shortens the gap in page navigation efficiency between sighted and screen-reader users.", "levels": null, "corpus_id": 15878200, "sentences": ["Expert performance suggests the DASX markedly shortens the gap in page navigation efficiency between sighted and screen-reader users."], "caption": "Figure 7. Expert performance suggests that DASX shortens the gap in page navigation efficiency between sighted and screen-reader users.", "local_uri": ["668c5c9a157e5a142993ba3e9d36b21b1f1466cf_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Always somewhere, never there: using critical design to understand database interactions", "pdf_hash": "81e5cd406ef26affda423e4020e5068753dee41c", "year": 2014, "venue": "CHI", "alt_text": "Composite figure showing the home page of the original Texas collection on the right, and a sample video metadata record with assigned descriptors on the left.", "levels": [[-1]], "corpus_id": 14118356, "sentences": ["Composite figure showing the home page of the original Texas collection on the right, and a sample video metadata record with assigned descriptors on the left."], "caption": "", "local_uri": ["81e5cd406ef26affda423e4020e5068753dee41c_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Always somewhere, never there: using critical design to understand database interactions", "pdf_hash": "81e5cd406ef26affda423e4020e5068753dee41c", "year": 2014, "venue": "CHI", "alt_text": "Composite figure showing the home page of the Here in Texas transformed video collection on the right, and a sample video metadata record with assigned descriptors on the left.", "levels": [[-1]], "corpus_id": 14118356, "sentences": ["Composite figure showing the home page of the Here in Texas transformed video collection on the right, and a sample video metadata record with assigned descriptors on the left."], "caption": "", "local_uri": ["81e5cd406ef26affda423e4020e5068753dee41c_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Always somewhere, never there: using critical design to understand database interactions", "pdf_hash": "81e5cd406ef26affda423e4020e5068753dee41c", "year": 2014, "venue": "CHI", "alt_text": "Composite figure showing the home page of the Kaleidescopic Texas transformed video collection on the right, and a sample video metadata record with assigned descriptors on the left.", "levels": [[-1]], "corpus_id": 14118356, "sentences": ["Composite figure showing the home page of the Kaleidescopic Texas transformed video collection on the right, and a sample video metadata record with assigned descriptors on the left."], "caption": "", "local_uri": ["81e5cd406ef26affda423e4020e5068753dee41c_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Always somewhere, never there: using critical design to understand database interactions", "pdf_hash": "81e5cd406ef26affda423e4020e5068753dee41c", "year": 2014, "venue": "CHI", "alt_text": "Composite figure showing the home page of the Post-Texas Index transformed video collection on the right, and a sample video metadata record with assigned descriptors on the left.", "levels": [[-1]], "corpus_id": 14118356, "sentences": ["Composite figure showing the home page of the Post-Texas Index transformed video collection on the right, and a sample video metadata record with assigned descriptors on the left."], "caption": "", "local_uri": ["81e5cd406ef26affda423e4020e5068753dee41c_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Kinecting with Orangutans: Zoo Visitors' Empathetic Responses to Animals' Use of Interactive Technology", "pdf_hash": "0216ea56ae8764bc5bc632a3c1dde1135ea9473a", "year": 2017, "venue": "CHI", "alt_text": "A young orangutan on its back with limbs spread wide, lying amongst dots of red, blue and green projected on the floor. In the foreground, a Microsoft Kinect sensor on tripod.", "levels": null, "corpus_id": 12401656, "sentences": ["A young orangutan on its back with limbs spread wide, lying amongst dots of red, blue and green projected on the floor.", "In the foreground, a Microsoft Kinect sensor on tripod."], "caption": "Figure 1. Interactive projections powered by a Microsoft Kinect in use by Dewi (6).", "local_uri": ["0216ea56ae8764bc5bc632a3c1dde1135ea9473a_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Kinecting with Orangutans: Zoo Visitors' Empathetic Responses to Animals' Use of Interactive Technology", "pdf_hash": "0216ea56ae8764bc5bc632a3c1dde1135ea9473a", "year": 2017, "venue": "CHI", "alt_text": "Adult orangutan investigates the Painting application projected on the floor of the enclosure, touching it with knuckle of right forefinger. Squares of black, red, gree, blue, yellow along near side of the projected application.", "levels": null, "corpus_id": 12401656, "sentences": ["Adult orangutan investigates the Painting application projected on the floor of the enclosure, touching it with knuckle of right forefinger.", "Squares of black, red, gree, blue, yellow along near side of the projected application."], "caption": "Figure 2. Gabby (35) using the Painting application.", "local_uri": ["0216ea56ae8764bc5bc632a3c1dde1135ea9473a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Kinecting with Orangutans: Zoo Visitors' Empathetic Responses to Animals' Use of Interactive Technology", "pdf_hash": "0216ea56ae8764bc5bc632a3c1dde1135ea9473a", "year": 2017, "venue": "CHI", "alt_text": "Photograph of young orangutan lying front down on projection area, with arm extended as though having 'hit' a projection with back of hand. In the foreground, a toddler crouches in front of the enclosure glass, watching the orangutan, with one hand pressed to the glass.", "levels": [[-1], [-1]], "corpus_id": 12401656, "sentences": ["Photograph of young orangutan lying front down on projection area, with arm extended as though having 'hit' a projection with back of hand.", "In the foreground, a toddler crouches in front of the enclosure glass, watching the orangutan, with one hand pressed to the glass."], "caption": "Figure 3. The system was configured to allow visitors to observe the orangutans\u2019 use in close proximity. Many respondents drew comparisons between the play of Dewi (6), and similar aged human children.", "local_uri": ["0216ea56ae8764bc5bc632a3c1dde1135ea9473a_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction", "pdf_hash": "c89f7cbca2134618e708ef111cb152a1c678fe12", "year": 2017, "venue": "ASSETS", "alt_text": "In Figure 1a, a user is performing swipe gestures at any body location such as palm, wrist, and thigh to navigate a list of available microinteractions. In Figure 1b, a user is tapping specific locations on palm such as left/right side of the palm to select specific microinteractions. In figure 1c, a user is tapping specific locations across the body such as wrist and ear to select specific microinteractions.", "levels": null, "corpus_id": 27193915, "sentences": ["In Figure 1a, a user is performing swipe gestures at any body location such as palm, wrist, and thigh to navigate a list of available microinteractions.", "In Figure 1b, a user is tapping specific locations on palm such as left/right side of the palm to select specific microinteractions.", "In figure 1c, a user is tapping specific locations across the body such as wrist and ear to select specific microinteractions."], "caption": "Figure 1. The three on-body interaction techniques explored for microinteractions in Study 2: (a) location-independent taps and swipes that can be performed anywhere on the body, (b) location-specific input that allows users to directly access a specific set of applications by tapping a dedicated location onthe palm only or (c) on the body. See also video figure.", "local_uri": ["c89f7cbca2134618e708ef111cb152a1c678fe12_Image_001.png"], "annotated": false, "compound": false}
{"title": "Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction", "pdf_hash": "c89f7cbca2134618e708ef111cb152a1c678fe12", "year": 2017, "venue": "ASSETS", "alt_text": "Those pictures demonstrate where each hardware compononet was located when it is being worn by our participants. For example, Two IR reflectance sensors were located at each side of a user's fingertip. An IMU sensor was located on top of the second joint of an index finger. On top of the IMU sensor, we had a camera with LED. All these sensors are secured with two Velcro straps around the finger. In addition to the finger-worn sensors, a microcontroller was mounted on partiicpants' wrist.", "levels": null, "corpus_id": 27193915, "sentences": ["Those pictures demonstrate where each hardware compononet was located when it is being worn by our participants.", "For example, Two IR reflectance sensors were located at each side of a user's fingertip.", "An IMU sensor was located on top of the second joint of an index finger.", "On top of the IMU sensor, we had a camera with LED.", "All these sensors are secured with two Velcro straps around the finger.", "In addition to the finger-worn sensors, a microcontroller was mounted on partiicpants' wrist."], "caption": "Figure 4. Our wearable, on-body interaction prototype system consists of a finger-worn package including a camera and LED, two IR reflectance sensors, and an IMU. Computer vision and machine learning algorithms are used to support location-specific, on-body gestural interaction.", "local_uri": ["c89f7cbca2134618e708ef111cb152a1c678fe12_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction", "pdf_hash": "c89f7cbca2134618e708ef111cb152a1c678fe12", "year": 2017, "venue": "ASSETS", "alt_text": "The figure illustrates five specific locations on a left palm for the LS palm condition. The five locations are labeled as up, down, left, right and center assuming a user's hand is rotated about 45 degreres clockwise. For example, \"left\" location is near the root of the thumb, and \"right\" location is near the root of the baby finger, where the \"up\" location is near the root of the index finger.", "levels": null, "corpus_id": 27193915, "sentences": ["The figure illustrates five specific locations on a left palm for the LS palm condition.", "The five locations are labeled as up, down, left, right and center assuming a user's hand is rotated about 45 degreres clockwise.", "For example, \"left\" location is near the root of the thumb, and \"right\" location is near the root of the baby finger, where the \"up\" location is near the root of the index finger."], "caption": "(a) Five LSpalm locations", "local_uri": ["c89f7cbca2134618e708ef111cb152a1c678fe12_Image_010.png"], "annotated": false, "compound": false}
{"title": "Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction", "pdf_hash": "c89f7cbca2134618e708ef111cb152a1c678fe12", "year": 2017, "venue": "ASSETS", "alt_text": "The figure illustrates five specific locations across the body for the LS palm condition. The five locations are labeled as palm, inner wrist, outer wrist, ear and thigh.", "levels": null, "corpus_id": 27193915, "sentences": ["The figure illustrates five specific locations across the body for the LS palm condition.", "The five locations are labeled as palm, inner wrist, outer wrist, ear and thigh."], "caption": "(b) Five LSbody locations", "local_uri": ["c89f7cbca2134618e708ef111cb152a1c678fe12_Image_011.png"], "annotated": false, "compound": false}
{"title": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "pdf_hash": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "year": 2013, "venue": "ASSETS", "alt_text": "This table contains exemplary images for different levels of visual complexity of the active game objects of the game created for the study. It covers the markers (net and hook), which are controlled by the position of the players' hands, the fish and the cans that the fish turn into after being caught by touching them with the hook marker. Each element is depicted for each version of the game. In the abstract version, the hook is reduced to a yellow circle, the net is reduced to a red square, the fish are reduced to red ellipses and the cans are reduced to grey rectangles. In the simple 2D version, the hook is a plain-colored, yellow cartoon hook, the net is red and grey and follows the same plain visual style. The fish is red and has a clear-cut silhouette. The can is silver/grey and orange and features a plain-black logo. The stylized 2D version uses the same shapes as the simple 2D version, but features more detailed textures and some highlights. The 3D Pre-render version also employs the same basic shapes, but the textures result from detailed 3D models.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 4977552, "sentences": ["This table contains exemplary images for different levels of visual complexity of the active game objects of the game created for the study.", "It covers the markers (net and hook), which are controlled by the position of the players' hands, the fish and the cans that the fish turn into after being caught by touching them with the hook marker.", "Each element is depicted for each version of the game.", "In the abstract version, the hook is reduced to a yellow circle, the net is reduced to a red square, the fish are reduced to red ellipses and the cans are reduced to grey rectangles.", "In the simple 2D version, the hook is a plain-colored, yellow cartoon hook, the net is red and grey and follows the same plain visual style.", "The fish is red and has a clear-cut silhouette.", "The can is silver/grey and orange and features a plain-black logo.", "The stylized 2D version uses the same shapes as the simple 2D version, but features more detailed textures and some highlights.", "The 3D Pre-render version also employs the same basic shapes, but the textures result from detailed 3D models."], "caption": "WuppDi! Games for PD Therapy: Visual Adjustments and Adaptation for Older AdultsThe game prototype used for this study was based on the WuppDi! suite of motion-based games for Parkinson\u2019s disease patients [3], more specifically the game Bremen Town Musicians (Figure 1). In this game, players have to first stop game elements that are moving across the screen on a stable trajectory by touching them with one hand and then collect these elements by touching them with their other hand. The games of the suite were implemented as 2D games in C# and Microsoft Game Studio (XNA) using differential images and color-blob tracking (red and yellow hand- held markers) for determining relative motion and the position of the players\u2019 hands.Figure 1: Screenshot of the original game called Bremen Town Musicians before adjustments and re-skinning.In order to utilize the game for the purpose of this study, a number of adjustments were made, resulting in a game called Fish Harvester (for screenshots see Figure 2). The basic game mechanics focus on senior-friendly user input, therapy- appropriate wide and fluent movements, positive encouragement without punishments, and a simple interaction scheme, which was the result of a user-centered game design process. An evaluation of the WuppDi! suite showed that the games were well received and the implemented motion-patters were deemed adequate for the participating PD patients by therapists [3].In the visually adapted (Fish Harvester) version of the game, we maintained the core mechanics. The scenario was changed to a fishing game in order to facilitate a more straightforward production of the abstract, simple 2D, stylized 2D, and 3D pre- render game skins. Background animations were removed, so that only active game objects display motion. The color scheme was adjusted to separate interactive objects (warm colors) from passive objects (cold colors) using complementary colors, in order to avoid adjacent parts of the hue circle. State animations of the collectible objects were adjusted to better separate stopped but not yet collected objects. Motion trajectory randomization was replaced by a predefined set of pseudo-randomized trajectories for collectible objects in order to support a better isolation of the independent variable of visual complexity across conditions. The graphical user interface was adjusted to match the new scenario, color scheme and the four levels of visual complexity. Figure 2 shows screenshots of the resulting four versions of the full game in an identical game-play situation. The game-situation pictured in these screenshots shows a number of fish-objects which aremoving across the screen on fixed trajectories. The players control the position of a net and a hook with their right and left hand. The hook can be used to stop the fish-objects and these can then be collected by touching them with the net.Figure 2: Screenshots of the four versions of Fish Harvester with different levels of visual complexity ranging from abstract (top-left), over simple 2D (top-right) and stylized 2D (bottom-left) to 3D pre-render (bottom-right).", "local_uri": ["a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "pdf_hash": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "year": 2013, "venue": "ASSETS", "alt_text": "The screenshot depicts the game-screen of the Bremen Town Musicians game in a typical game-play situation. On top of a background that shows the Bremen town musician animals in a musical performance on a stage (from left to right: the cat with a violin, the donkey, playing drums, the dog, playing guitar and the rooster, playing trumpet), some floating bubbles are visible that contain images of the mouthes of the animals (since they produce the respective animals sound when popped) together with a net (to stop the bubbles) and a needle (to pop the bubbles after they have been stopped). One of the bubbles is highlighted with an orange shillouette instead of the typical blue one, on order to indicate that it has already been stopped. An overlay on top of the screen shows the remaining time in the game (in this case 41 seconds) and the current score (in this case 5).", "levels": null, "corpus_id": 4977552, "sentences": ["The screenshot depicts the game-screen of the Bremen Town Musicians game in a typical game-play situation.", "On top of a background that shows the Bremen town musician animals in a musical performance on a stage (from left to right: the cat with a violin, the donkey, playing drums, the dog, playing guitar and the rooster, playing trumpet), some floating bubbles are visible that contain images of the mouthes of the animals (since they produce the respective animals sound when popped) together with a net (to stop the bubbles) and a needle (to pop the bubbles after they have been stopped).", "One of the bubbles is highlighted with an orange shillouette instead of the typical blue one, on order to indicate that it has already been stopped.", "An overlay on top of the screen shows the remaining time in the game (in this case 41 seconds) and the current score (in this case 5)."], "caption": "Figure 1: Screenshot of the original game called Bremen Town Musicians before adjustments and re-skinning.", "local_uri": ["a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "pdf_hash": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "year": 2013, "venue": "ASSETS", "alt_text": "This figure shows screenshots of the four versions of Fish Harvester with different levels of visual complexity ranging from abstract (top-left), over simple 2D (top-right) and stylized 2D (bottom-left) to 3D pre-render (bottom-right). Each screenshot shows the same situation in the game and they only differ in the level of visual complexity. The game elements which were described before and which were depicted in table 2 are shown on top of backgrounds which show according detail in their visual representation and are largely held in blue and green colors.", "levels": null, "corpus_id": 4977552, "sentences": ["This figure shows screenshots of the four versions of Fish Harvester with different levels of visual complexity ranging from abstract (top-left), over simple 2D (top-right) and stylized 2D (bottom-left) to 3D pre-render (bottom-right).", "Each screenshot shows the same situation in the game and they only differ in the level of visual complexity.", "The game elements which were described before and which were depicted in table 2 are shown on top of backgrounds which show according detail in their visual representation and are largely held in blue and green colors."], "caption": "Figure 2: Screenshots of the four versions of Fish Harvester with different levels of visual complexity ranging from abstract (top-left), over simple 2D (top-right) and stylized 2D (bottom-left) to 3D pre-render (bottom-right).", "local_uri": ["a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "pdf_hash": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "year": 2013, "venue": "ASSETS", "alt_text": "This figure shows the participants\u2019 mean responses to game experience statements S1 - S3 following each condition. The chart serves to provide a clear visual impression of the fact that the abstract version received the worst responses for each item. The means and standard deviations are listed in table 3.", "levels": [[1], [3], [0]], "corpus_id": 4977552, "sentences": ["This figure shows the participants\u2019 mean responses to game experience statements S1 - S3 following each condition.", "The chart serves to provide a clear visual impression of the fact that the abstract version received the worst responses for each item.", "The means and standard deviations are listed in table 3."], "caption": "Figure 4: Participants\u2019 mean responses to game experience statements S1 - S3 following each condition.", "local_uri": ["a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_006.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "pdf_hash": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "year": 2013, "venue": "ASSETS", "alt_text": "This figure shows a bar chart featuring the mean responses to Borg's RPE scale before gameplay and after playing each version of the game. The values are detailed in the following paragraph.", "levels": [[1], [0]], "corpus_id": 4977552, "sentences": ["This figure shows a bar chart featuring the mean responses to Borg's RPE scale before gameplay and after playing each version of the game.", "The values are detailed in the following paragraph."], "caption": "Figure 5. Mean response to Borg's RPE scale before gameplay and after playing each version of the game.", "local_uri": ["a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_007.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "pdf_hash": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "year": 2013, "venue": "ASSETS", "alt_text": "The responses of the participants to ranking item R1 (\u201cWhich version do you like best?\u201d). The abstract version is least preferred.", "levels": null, "corpus_id": 4977552, "sentences": ["The responses of the participants to ranking item R1 (\u201cWhich version do you like best?\u201d).", "The abstract version is least preferred."], "caption": "Figure 6: The responses of the participants to ranking item R1 (\u201cWhich version do you like best?\u201d). The abstract version is least preferred.", "local_uri": ["a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_008.gif"], "annotated": false, "compound": false}
{"title": "Modern Bereavement: A Model for Complicated Grief in the Digital Age", "pdf_hash": "0d6e575cab33706a47ab4106a841d2cc1f6c17a9", "year": 2018, "venue": "CHI", "alt_text": "New Model of Complicated Grief with the following phases: Fog, Isolation, Exploration, Immersion, Stabilization", "levels": null, "corpus_id": 5070471, "sentences": ["New Model of Complicated Grief with the following phases: Fog, Isolation, Exploration, Immersion, Stabilization"], "caption": "Figure 1. A behavioral model of grief", "local_uri": ["0d6e575cab33706a47ab4106a841d2cc1f6c17a9_Image_001.gif"], "annotated": false, "compound": false}
{"title": "Draco: bringing life to illustrations with kinetic textures", "pdf_hash": "93e5eda8fa04135f50da76c59d27813d11dd3c1f", "year": 2014, "venue": "CHI", "alt_text": "Twi diagrams illustrating the effect of cohesion. Particles stay closer to motion paths, and water avoids an object.", "levels": [[-1], [-1]], "corpus_id": 1423529, "sentences": ["Twi diagrams illustrating the effect of cohesion.", "Particles stay closer to motion paths, and water avoids an object."], "caption": "Figure 5: Impact of cohesion on the global motion of an object collection. (a) A lower cohesion produces a more uniform distribution between the motion paths. (b) Obstacle avoidance effects are obtained with a higher cohesion, where objects are more adhesive to the motion lines.", "local_uri": ["93e5eda8fa04135f50da76c59d27813d11dd3c1f_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Draco: bringing life to illustrations with kinetic textures", "pdf_hash": "93e5eda8fa04135f50da76c59d27813d11dd3c1f", "year": 2014, "venue": "CHI", "alt_text": "A workflow for creating an emmitting texture of rain falling on a city landscape.", "levels": null, "corpus_id": 1423529, "sentences": ["A workflow for creating an emmitting texture of rain falling on a city landscape."], "caption": "Figure 9: Creating an emitting texture. The user draws the source patch (inset: example raindrops) (a), then sketches a line emitter (b), which results in an emitting texture with a default motion (c). The user sketches a motion path (d), which instantaneously changes the global trajectory of the raindrops (e). Finally, she adjusts the granular motion by adding subtle translation to the raindrops (g), supplementing the global motion (f), with local variations (h).", "local_uri": ["93e5eda8fa04135f50da76c59d27813d11dd3c1f_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Draco: bringing life to illustrations with kinetic textures", "pdf_hash": "93e5eda8fa04135f50da76c59d27813d11dd3c1f", "year": 2014, "venue": "CHI", "alt_text": "An example worfklow for creating an animated illustration of a woman blowing bubbles across the sky.", "levels": null, "corpus_id": 1423529, "sentences": ["An example worfklow for creating an animated illustration of a woman blowing bubbles across the sky."], "caption": "Figure 12: Creating an emitting texture. The user draws the sample objects (a), then directly sketches the motion path (b). An emitting texture is automatically created with a default (blue) emitter, perpendicular to the motion path (c). The user then sketches additional motion paths in order to spread out the bubbles (d). Finally, she uses the motion profile widget (e) to adjust the scale (f) and the velocity profiles (g), so that the bubbles grow and decelerate as they move away from the emitter.", "local_uri": ["93e5eda8fa04135f50da76c59d27813d11dd3c1f_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Draco: bringing life to illustrations with kinetic textures", "pdf_hash": "93e5eda8fa04135f50da76c59d27813d11dd3c1f", "year": 2014, "venue": "CHI", "alt_text": "A photo of our study setup, and an image of the study task - champagne being poured into a wine glass, next to a steaming cup of coffee and a flickering candle.", "levels": null, "corpus_id": 1423529, "sentences": ["A photo of our study setup, and an image of the study task - champagne being poured into a wine glass, next to a steaming cup of coffee and a flickering candle."], "caption": "Figure 13: (a) A participant using our system to create animated illustrations. (b) The exercise task consisted of three emitting textures (blue) and two oscillating textures (red)", "local_uri": ["93e5eda8fa04135f50da76c59d27813d11dd3c1f_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Draco: bringing life to illustrations with kinetic textures", "pdf_hash": "93e5eda8fa04135f50da76c59d27813d11dd3c1f", "year": 2014, "venue": "CHI", "alt_text": "Two images created by particiapnts - an underwater suba diver and a rocket ship flying through space.", "levels": null, "corpus_id": 1423529, "sentences": ["Two images created by particiapnts - an underwater suba diver and a rocket ship flying through space."], "caption": "Figure 15: Artwork created by participants using Draco.", "local_uri": ["93e5eda8fa04135f50da76c59d27813d11dd3c1f_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation", "pdf_hash": "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e", "year": 2019, "venue": "CHI", "alt_text": "Figure 2: A game controller PCB designed using Pinpoint, featuring a thumbstick and capacitive button.", "levels": null, "corpus_id": 88510638, "sentences": ["Figure 2: A game controller PCB designed using Pinpoint, featuring a thumbstick and capacitive button."], "caption": "", "local_uri": ["4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation", "pdf_hash": "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e", "year": 2019, "venue": "CHI", "alt_text": "Figure 3: Game controller schematic (A) and board (B) before (top) and after (bottom) instrumentation on all signals. (C) The top and bottom (mirrored for clarity) of the instrumented game controller PCB with added jumper pads (callout). (D) The corresponding jig board generated by Pinpoint. (E) The jig board mates to the DUT with each pogo pin probing a pad.", "levels": null, "corpus_id": 88510638, "sentences": ["Figure 3: Game controller schematic (A) and board (B) before (top) and after (bottom) instrumentation on all signals. (C) The top and bottom (mirrored for clarity) of the instrumented game controller PCB with added jumper pads (callout).", "(D) The corresponding jig board generated by Pinpoint. (E) The jig board mates to the DUT with each pogo pin probing a pad."], "caption": "Before", "local_uri": ["4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_008.jpg", "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_009.jpg", "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_011.jpg"], "annotated": false, "compound": true}
{"title": "Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation", "pdf_hash": "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e", "year": 2019, "venue": "CHI", "alt_text": "Figure 6: Pinpoint enables designers to splice external circuits into instrumented sites on the DUT. Here, the designer splices in a breadboard to rapidly explore capacitor values.", "levels": null, "corpus_id": 88510638, "sentences": ["Figure 6: Pinpoint enables designers to splice external circuits into instrumented sites on the DUT.", "Here, the designer splices in a breadboard to rapidly explore capacitor values."], "caption": "Control", "local_uri": ["4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation", "pdf_hash": "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e", "year": 2019, "venue": "CHI", "alt_text": "Figure 7: Pinpoint introduces jumper pads with interruptible connections to enable the isolation of each element on a given net. Each pad connects on one end to a circuit element, and the remaining open ends are tied together.", "levels": null, "corpus_id": 88510638, "sentences": ["Figure 7: Pinpoint introduces jumper pads with interruptible connections to enable the isolation of each element on a given net.", "Each pad connects on one end to a circuit element, and the remaining open ends are tied together."], "caption": "Consider elements connected on the same netDelete wire segments, and replace with test pads connected to each elementConnect open ends of test pads together, following original layoutFigure 7: Pinpoint introduces jumper pads with interrupt- ible connections to enable the isolation of each element on a given net. Each pad connects on one end to a circuit ele- ment, and the remaining open ends are tied together.  Figure 8: A net with two components has only two states: connected or disconnected. Pinpoint needs only a single test pad to instrument nets in this confguration.Splice jumper pads into nets on the schematic. Pinpoint places jumper pads so as to enable any combination of connections to each element (component or subcircuit) on a given net (electrical connection shared by a set of compo- nents). We satisfy this requirement with a specifc topology: each jumper pad connects on one end to a single element, and the other ends of all pads on the net are connected together (Figure 7). We modify the schematic as follows:for net S in the original design do for element E on S dofor wire segment W in S and incident on E do Add a jumper pad P at the midpoint of W ; Delete W ;Connect one end of P to E on a new net S_E;end endConnect the open ends of all jumper pads togetheron the net S, following deleted segment paths to preserve the original appearance of the schematic;endThus, a net with n elements requires n jumper pads for full instrumentation. However, as shown in Figure 8, the case where n = 2 requires only a single pad to either connect orA.B.1         2        3Figure 9: In many cases, adding jumper pads using only the designer\u2019s original routing cannot support the individual isolation of each component, as in (A), where elements 1 and 3 cannot be connected with 2 disconnected. Instead, Pin- point must add new routes as in (B).disconnect the components. We reduce the total number of added pads by using a single splice in these cases.Remove problematic board traces. Unlike in the schematic, Pinpoint must consider both the topology and the geometry of connections on the board layout to enable individual element isolation. While we aim to minimize de- viation from the designer\u2019s original routing, many layouts require the removal or addition of traces, e.g. as shown in Figure 9, where the signal is routed \u201cthrough\u201d component 2. To preserve portions of the original routing while ensuring correctness, Pinpoint removes only wire segments incident upon elements, then routes with the remaining segments in place. Our limitations section describes in greater detail the considerations in preserving parts of the original routing.Lay out new jumper pads. To minimally impact the electrical properties of the circuit, placement of jumper pads should minimize the lengths of traces and number of vias required to route the resulting circuit. We defne a cost func- tion which weights free space on the board, distance from the original trace, and additional vias required in routing, then use a simulated annealing process to approximate the best available location for the new pad. Our current imple- mentation uses a greedy approach for faster instrumentation, optimizing the location of one pad before considering the next. We leave it to future work to develop a global cost min- imization procedure across all new pad locations to further optimize the placement.Route new connections. We fnalize the instrumen- tation using EAGLE\u2019s default autorouter to connect the new pads into the circuit. The designer can adjust routing as desired once the instrumentation is complete.", "local_uri": ["4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_015.jpg", "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_016.jpg", "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_017.jpg"], "annotated": false, "compound": true}
{"title": "Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation", "pdf_hash": "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e", "year": 2019, "venue": "CHI", "alt_text": "Figure 8: A net with two components has only two states: connected or disconnected. Pinpoint needs only a single test pad to instrument nets in this configuration.", "levels": null, "corpus_id": 88510638, "sentences": ["Figure 8: A net with two components has only two states: connected or disconnected.", "Pinpoint needs only a single test pad to instrument nets in this configuration."], "caption": "", "local_uri": ["4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_018.gif", "4e252feb5bc8e132ea6ec2606bfb1d6059f4869e_Image_020.gif"], "annotated": false, "compound": true}
{"title": "Speed-Dial: A Surrogate Mouse for Non-Visual Web Browsing", "pdf_hash": "d70046e4a525a66a853d01bfd586448bd0fad4bb", "year": 2017, "venue": "ASSETS", "alt_text": "Screen shot of an HTML data-grid view (background), and its equivalent custom data-grid view for Dial foreground).", "levels": [[-1]], "corpus_id": 7635152, "sentences": ["Screen shot of an HTML data-grid view (background), and its equivalent custom data-grid view for Dial foreground)."], "caption": "", "local_uri": ["d70046e4a525a66a853d01bfd586448bd0fad4bb_Image_013.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PitchPerfect: integrated rehearsal environment for structured presentation preparation", "pdf_hash": "4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793", "year": 2014, "venue": "CHI", "alt_text": "Extended authoring mode with visualized flow path, element notes, and time target.", "levels": null, "corpus_id": 15464905, "sentences": ["Extended authoring mode with visualized flow path, element notes, and time target."], "caption": "(c)", "local_uri": ["4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "PitchPerfect: integrated rehearsal environment for structured presentation preparation", "pdf_hash": "4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793", "year": 2014, "venue": "CHI", "alt_text": "Figure 2a: Element Note Rehearsal. The system highlights a visual element and prompts the presenter to anticipate the associated element note.", "levels": null, "corpus_id": 15464905, "sentences": ["Figure 2a: Element Note Rehearsal.", "The system highlights a visual element and prompts the presenter to anticipate the associated element note."], "caption": "Element Note Rehearsal(iv)(i)(ii)(iii)Flow Path RehearsalTimed Speech Rehearsal", "local_uri": ["4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "PitchPerfect: integrated rehearsal environment for structured presentation preparation", "pdf_hash": "4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793", "year": 2014, "venue": "CHI", "alt_text": "Figure 2b: Flow Path Rehearsal. The system hides visual elements further along the flow path and prompts the presenter to anticipate what is coming next.", "levels": null, "corpus_id": 15464905, "sentences": ["Figure 2b: Flow Path Rehearsal.", "The system hides visual elements further along the flow path and prompts the presenter to anticipate what is coming next."], "caption": "", "local_uri": ["4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "PitchPerfect: integrated rehearsal environment for structured presentation preparation", "pdf_hash": "4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793", "year": 2014, "venue": "CHI", "alt_text": "This mode encourages the presenter to practice timed and recorded verbalization, supported by visual time guides and structured notes.", "levels": null, "corpus_id": 15464905, "sentences": ["This mode encourages the presenter to practice timed and recorded verbalization, supported by visual time guides and structured notes."], "caption": "(iv)", "local_uri": ["4969e3b85ed9fcca2aa7a349cf0dd90d6ae79793_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating multimodal driver displays under varying situational urgency", "pdf_hash": "2ea1e1b13f160dbdc939506d10205528da14adfb", "year": 2014, "venue": "CHI", "alt_text": "Graphs of Response Times across situations, Response Times across Modalities and the Interaction between Situation and Modality", "levels": [[1]], "corpus_id": 14203195, "sentences": ["Graphs of Response Times across situations, Response Times across Modalities and the Interaction between Situation and Modality"], "caption": "Figure 3. (a) The response times across situations. (b) The response times across modalities, sorted by their mean values. (c) The interaction between Situation and Modali- ty with modalities sorted by their mean values. For all graphs, error bars represent 95% Confidence Intervals.", "local_uri": ["2ea1e1b13f160dbdc939506d10205528da14adfb_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "User Participation When Users have Mental and Cognitive Disabilities", "pdf_hash": "4d35cd67a8d1a3035a7e6f351fba4c9eaee44e76", "year": 2015, "venue": "ASSETS", "alt_text": "Fig 3. Example of prioritizing exercise. Sheets with statements on a wall. Red dots indication chosen options.", "levels": null, "corpus_id": 11469826, "sentences": ["Fig 3.", "Example of prioritizing exercise.", "Sheets with statements on a wall.", "Red dots indication chosen options."], "caption": "Fig 3: This is an example of a prioritization exercise where participants were asked to evaluate what is most important for a website to be cognitively available. The exercise was started as a discussion in this project and transformed to a stand-alone exercise that we tested with participants at the conference UD20145 and then at a workshop with 50 webmaster of municipal websites in Sweden. Visually, it is easy to spot the most important issues, in this case \"Content must not blink and move,\" \"simple and understandable language\" and \"You should not have to keep many things in mind.\"", "local_uri": ["4d35cd67a8d1a3035a7e6f351fba4c9eaee44e76_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "User Participation When Users have Mental and Cognitive Disabilities", "pdf_hash": "4d35cd67a8d1a3035a7e6f351fba4c9eaee44e76", "year": 2015, "venue": "ASSETS", "alt_text": "Fig 10.A wheel-layout presenting principles for universall design, described below.", "levels": [[-1]], "corpus_id": 11469826, "sentences": ["Fig 10.A wheel-layout presenting principles for universall design, described below."], "caption": "Fig. 10. From upper part of the circle and clockwise; Equitable Use, Flexibility in Use, Simple and Intuitive Use, Perceptible Information, Tolerance for Error, Low Physical Effort, Size and Space for Approach and Use", "local_uri": ["4d35cd67a8d1a3035a7e6f351fba4c9eaee44e76_Image_013.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Turn to the Self in Human-Computer Interaction: Care of the Self in Negotiating the Human-Technology Relationship", "pdf_hash": "3797d3e74f9f3aa1d651841235d2c55614bdac7b", "year": 2019, "venue": "CHI", "alt_text": "The screenshot in Figure 1 shows a player's quantified profile in League of Legends. The profile includes four major information blocks. The upper left one shows the player's rank in League of Legends. The upper right one shows the player's win rate in the recent 9 games. The lower left block shows a list of champions that the player have played the most with. The lower right block shows detailed information for each of the recent matches that the player has played.", "levels": null, "corpus_id": 96446656, "sentences": ["The screenshot in Figure 1 shows a player's quantified profile in League of Legends.", "The profile includes four major information blocks.", "The upper left one shows the player's rank in League of Legends. The upper right one shows the player's win rate in the recent 9 games.", "The lower left block shows a list of champions that the player have played the most with.", "The lower right block shows detailed information for each of the recent matches that the player has played."], "caption": "Figure 1. Quantified Performance in LoL.", "local_uri": ["3797d3e74f9f3aa1d651841235d2c55614bdac7b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "pdf_hash": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "year": 2017, "venue": "ASSETS", "alt_text": "This figure shows the multitude of factors that may impact app accessibility, as inspired by models in epidemiology. The factors are on a spectrum from Intrinsic to Extrinsic and grouped into categories. The categories and factors are as follows: Biologic and Genetic encompassing Source Code, and Visual Design. Individual Behaviors encompassing Code Reuse, Update frequency, tools, and testing techniques. Physical context encompassing physical device, operating system, assistive technologies. Social context encompassing popularity. Social relationships encompassing community engagement. Institutional context and policy encompassing education, company, and government. Social conditions encompassing cultural norms", "levels": null, "corpus_id": 24992331, "sentences": ["This figure shows the multitude of factors that may impact app accessibility, as inspired by models in epidemiology.", "The factors are on a spectrum from Intrinsic to Extrinsic and grouped into categories.", "The categories and factors are as follows: Biologic and Genetic encompassing Source Code, and Visual Design.", "Individual Behaviors encompassing Code Reuse, Update frequency, tools, and testing techniques.", "Physical context encompassing physical device, operating system, assistive technologies.", "Social context encompassing popularity.", "Social relationships encompassing community engagement.", "Institutional context and policy encompassing education, company, and government.", "Social conditions encompassing cultural norms"], "caption": "", "local_uri": ["0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "pdf_hash": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "year": 2017, "venue": "ASSETS", "alt_text": "Natural History of Development. The image shows progression through app development. The stages are conception, design, implementation, testing, release/birth . After birth, the app may cycle through updates. These stages may be iterative. The final stage is abandonment/death. preventative treatments can be applicable before release/birth. Therapeutic treatments are applied after birth, during update cycles. Life expectancy is a time measure from app's release to when it is abandoned by developers.", "levels": null, "corpus_id": 24992331, "sentences": ["Natural History of Development.", "The image shows progression through app development.", "The stages are conception, design, implementation, testing, release/birth .", "After birth, the app may cycle through updates.", "These stages may be iterative.", "The final stage is abandonment/death.", "preventative treatments can be applicable before release/birth.", "Therapeutic treatments are applied after birth, during update cycles.", "Life expectancy is a time measure from app's release to when it is abandoned by developers."], "caption": "Figure 2. The natural history of app development model represents the design and implementation process an app goes through pre-release and post-release. It serves as a framework for where new treatments might be introduced.", "local_uri": ["0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_002.gif"], "annotated": false, "compound": false}
{"title": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "pdf_hash": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "year": 2017, "venue": "ASSETS", "alt_text": "Natural History of Usage. This image depicts the stages of an app's usage. It starts with finding and downloading. Then is the birth of use. Within usage, there are potential barriers encountered and work arounds tried. These usage experiences may be cyclic. The final stage is app abandonment/death of usage. Preventative treatments can be applied to the pre-birth stages of finding and downloading. Therapeutic treatments can be applied within the usage stages. Life expectancy is a time measurement from birth of usage to death of usage or abandonment by user.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 24992331, "sentences": ["Natural History of Usage.", "This image depicts the stages of an app's usage.", "It starts with finding and downloading.", "Then is the birth of use.", "Within usage, there are potential barriers encountered and work arounds tried.", "These usage experiences may be cyclic.", "The final stage is app abandonment/death of usage.", "Preventative treatments can be applied to the pre-birth stages of finding and downloading.", "Therapeutic treatments can be applied within the usage stages.", "Life expectancy is a time measurement from birth of usage to death of usage or abandonment by user."], "caption": "Figure 3. The natural history of app usage model represents the process by which an end user finds, downloads, uses, and abandons an app. The usage stage includes first usage, or birth of usage. Within usage, someone might cycle through the stages of encountering a barrier and trying to work around it. The progression could ultimately end when a user abandons an app and usage dies.", "local_uri": ["0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_003.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "pdf_hash": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "year": 2017, "venue": "ASSETS", "alt_text": "Breaking the Chain of Infection. The 5 links in the chain of infection, examples of each link, and potential treatments are presented. The first link is infection agent (e.g., code, design, widget) with potential treatments of rapid diagnosis and treatment (e.g., screening protocols and tools). The next link is Reservoirs (e.g., tookits, design, guidelines repositories) with potential treatments of environment sanitation (e.g., removing or repairing inaccessible agents) and taking from clean sources (e.g., providing accessibility ratings). The next link is Mode of Transmission (e.g., copying code, OS update, tookit use) with potential treatments of Isolation (e.g., don't continue reusing and distributing known infectious agents). The next link is Portal of Entry (e.g., source code, design phase) with treatments of First Aid (e.g., fix barriers introduced through exposure to agent). The final link is a Suceptible Host (e.g., apps that use agent for essential functionality) with potential treatments of identifing high-risk (have and use knowledge of what is most impactful for accessibility) and treating underlying vulnerabitilites (integrate better accessibility considerations into bases, designs, development cycles, etc.). Breaking any link can disrupt the spread of \"inaccessibility diseases\" in apps", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 24992331, "sentences": ["Breaking the Chain of Infection.", "The 5 links in the chain of infection, examples of each link, and potential treatments are presented.", "The first link is infection agent (e.g., code, design, widget) with potential treatments of rapid diagnosis and treatment (e.g., screening protocols and tools).", "The next link is Reservoirs (e.g., tookits, design, guidelines repositories) with potential treatments of environment sanitation (e.g., removing or repairing inaccessible agents) and taking from clean sources (e.g., providing accessibility ratings).", "The next link is Mode of Transmission (e.g., copying code, OS update, tookit use) with potential treatments of Isolation (e.g., don't continue reusing and distributing known infectious agents).", "The next link is Portal of Entry (e.g., source code, design phase) with treatments of First Aid (e.g., fix barriers introduced through exposure to agent).", "The final link is a Suceptible Host (e.g., apps that use agent for essential functionality) with potential treatments of identifing high-risk (have and use knowledge of what is most impactful for accessibility) and treating underlying vulnerabitilites (integrate better accessibility considerations into bases, designs, development cycles, etc.).", "Breaking any link can disrupt the spread of \"inaccessibility diseases\" in apps"], "caption": "", "local_uri": ["0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "pdf_hash": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "year": 2017, "venue": "ASSETS", "alt_text": "Prevalence of Disease Determinants. A bar chart where each of the Scanning Errors (determinants) is shown with the number of apps out of 100 that were detected having that error. Values as follows in order the bars appear:    Item Description: 85 apps. Text contrast: 94 apps. Item Label: 94 apps. Item type label 20 apps. Clickable items: 57 apps. Touch Target: 95 apps. Image contrast: 85 apps. Editable Item Label: 10 apps. Link: 1 app.", "levels": [[1], [1], [1], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 24992331, "sentences": ["Prevalence of Disease Determinants.", "A bar chart where each of the Scanning Errors (determinants) is shown with the number of apps out of 100 that were detected having that error.", "Values as follows in order the bars appear:    Item Description: 85 apps.", "Text contrast: 94 apps.", "Item Label: 94 apps.", "Item type label 20 apps.", "Clickable items: 57 apps.", "Touch Target: 95 apps.", "Image contrast: 85 apps.", "Editable Item Label: 10 apps.", "Link: 1 app."], "caption": "", "local_uri": ["0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "pdf_hash": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "year": 2017, "venue": "ASSETS", "alt_text": "Number of Determinants per App. A bar chart of the number of apps out of 100 that presented with a given number of determinants out of the nine screened for.    Number of apps that presented:  zero determinants: 0 apps. one determinant: 3 apps. two determinants: 2 apps. three determinants: 2 apps. four determinanats: 9 apps. five determinants: 36 apps. six determinants: 36 apps. seven determinants: 10 apps. eight determinants: 3 apps. all nine determinants: 0 apps.", "levels": [[1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 24992331, "sentences": ["Number of Determinants per App.", "A bar chart of the number of apps out of 100 that presented with a given number of determinants out of the nine screened for.", "Number of apps that presented:  zero determinants: 0 apps.", "one determinant: 3 apps.", "two determinants: 2 apps.", "three determinants: 2 apps.", "four determinanats: 9 apps.", "five determinants: 36 apps.", "six determinants: 36 apps.", "seven determinants: 10 apps.", "eight determinants: 3 apps.", "all nine determinants: 0 apps."], "caption": "", "local_uri": ["0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The Adventures of Older Authors: Exploring Futures through Co-Design Fictions", "pdf_hash": "043e79dc858a46a055ac12297d2bd8a6121fe2fb", "year": 2019, "venue": "CHI", "alt_text": "A photo of the seven participants of the writing group sitting down and watching one of the prompts.", "levels": null, "corpus_id": 140216442, "sentences": ["A photo of the seven participants of the writing group sitting down and watching one of the prompts."], "caption": "Figure 1: (L-R) Judith, Brenda, Jenny, Winnie, Val, Catherine and Julie of the creative writing group viewing one of the prompts.", "local_uri": ["043e79dc858a46a055ac12297d2bd8a6121fe2fb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The Adventures of Older Authors: Exploring Futures through Co-Design Fictions", "pdf_hash": "043e79dc858a46a055ac12297d2bd8a6121fe2fb", "year": 2019, "venue": "CHI", "alt_text": "A photo of Greg and Tom writing their stories after viewing and discussing the prompts.", "levels": null, "corpus_id": 140216442, "sentences": ["A photo of Greg and Tom writing their stories after viewing and discussing the prompts."], "caption": "Figure 2: Greg and Tom writing after the group discussion.", "local_uri": ["043e79dc858a46a055ac12297d2bd8a6121fe2fb_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "ABC and 3D: opportunities and obstacles to 3D printing in special education environments", "pdf_hash": "f3c3b2e1fa082436f04953bde390bdd130d80a15", "year": 2014, "venue": "ASSETS", "alt_text": "Left: Shows a tactile Cartesian grid still in the print bed of the Z-printer. Loose resin composite powder surrounds the object waiting to be vacuumed off. Right: A decorative snowman figurine printed by the Z-printer. The figurine is approximately four inches tall and shows a jolly snowman next to a small pine tree made of stacked cones.", "levels": null, "corpus_id": 12285794, "sentences": ["Left: Shows a tactile Cartesian grid still in the print bed of the Z-printer.", "Loose resin composite powder surrounds the object waiting to be vacuumed off.", "Right: A decorative snowman figurine printed by the Z-printer.", "The figurine is approximately four inches tall and shows a jolly snowman next to a small pine tree made of stacked cones."], "caption": "", "local_uri": ["f3c3b2e1fa082436f04953bde390bdd130d80a15_Image_009.png", "f3c3b2e1fa082436f04953bde390bdd130d80a15_Image_010.png"], "annotated": false, "compound": true}
{"title": "ABC and 3D: opportunities and obstacles to 3D printing in special education environments", "pdf_hash": "f3c3b2e1fa082436f04953bde390bdd130d80a15", "year": 2014, "venue": "ASSETS", "alt_text": "A ranged of 3D printed plastic objects, included a planet, the Sphinx, the Chrysler building, a DNA helix, & a landmass. Each object is small enough to be held in one hand with the planet being the largest, roughly the size of a softball.", "levels": null, "corpus_id": 12285794, "sentences": ["A ranged of 3D printed plastic objects, included a planet, the Sphinx, the Chrysler building, a DNA helix, & a landmass.", "Each object is small enough to be held in one hand with the planet being the largest, roughly the size of a softball."], "caption": "Figure 5. Models printed at Site C to support information access for visually impaired users. Pictured: a planet, the Sphinx, the Chrysler building, a DNA helix, & a landmass.", "local_uri": ["f3c3b2e1fa082436f04953bde390bdd130d80a15_Image_012.png"], "annotated": false, "compound": false}
{"title": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "pdf_hash": "13eccfcfd4ce807e14da418115a386b918870582", "year": 2015, "venue": "CHI", "alt_text": "This is a screenshot of Eclipse with StructJumper open. The top half is the code editor and the bottom half is StructJumper.", "levels": null, "corpus_id": 17235000, "sentences": ["This is a screenshot of Eclipse with StructJumper open.", "The top half is the code editor and the bottom half is StructJumper."], "caption": "Figure 1. Screenshot of StructJumper with source code file on top and tree of nesting structure on bottom.", "local_uri": ["13eccfcfd4ce807e14da418115a386b918870582_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "pdf_hash": "13eccfcfd4ce807e14da418115a386b918870582", "year": 2015, "venue": "CHI", "alt_text": "This is the tree structure created by the code in figure 3. It is structured so that parents are on top of children. Below is the tree written out with * representing the level    Calculator  *Code Section  *Add  **Code section  *Subtract  **Code section  *Exponent  **Code section  **If  ***For  ****Code section  **Else  ***For  ****Code section  **Code section", "levels": null, "corpus_id": 17235000, "sentences": ["This is the tree structure created by the code in figure 3.", "It is structured so that parents are on top of children.", "Below is the tree written out with * representing the level    Calculator  *Code Section  *Add  **Code section  *Subtract  **Code section  *Exponent  **Code section  **If  ***For  ****Code section  **Else  ***For  ****Code section  **Code section"], "caption": "", "local_uri": ["13eccfcfd4ce807e14da418115a386b918870582_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "pdf_hash": "13eccfcfd4ce807e14da418115a386b918870582", "year": 2015, "venue": "CHI", "alt_text": "This is a bar chart of the mean time to complete the tasks.    With Keywords: Without Tool 3:57, with tool 3:32  Without Keywords: Without Tool 4:25, with tool 2:25  Conditions: Without Tool 5:01, with tool 2:24  All: Without Tool 4:28, with tool 2:47", "levels": [[1], [2]], "corpus_id": 17235000, "sentences": ["This is a bar chart of the mean time to complete the tasks.", "With Keywords: Without Tool 3:57, with tool 3:32  Without Keywords: Without Tool 4:25, with tool 2:25  Conditions: Without Tool 5:01, with tool 2:24  All: Without Tool 4:28, with tool 2:47"], "caption": "Figure 5. This chart shows the average completion time that it took participants to complete the three tasks bro- ken down by type. The bars represent the standard error.", "local_uri": ["13eccfcfd4ce807e14da418115a386b918870582_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "pdf_hash": "13eccfcfd4ce807e14da418115a386b918870582", "year": 2015, "venue": "CHI", "alt_text": "This is a bar chart of the mean score for the tasks.    With Keywords: Without Tool 2, with tool 2.42  Without Keywords: Without Tool 1.57, with tool 1.57  Conditions: Without Tool 2, with tool 2.7  All: Without Tool 1.9, with tool 2.2", "levels": [[1], [2]], "corpus_id": 17235000, "sentences": ["This is a bar chart of the mean score for the tasks.", "With Keywords: Without Tool 2, with tool 2.42  Without Keywords: Without Tool 1.57, with tool 1.57  Conditions: Without Tool 2, with tool 2.7  All: Without Tool 1.9, with tool 2.2"], "caption": "Figure 6. This chart shows the average score for all par- ticipants on the three tasks broken down by task. The bars represent the standard error.", "local_uri": ["13eccfcfd4ce807e14da418115a386b918870582_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "pdf_hash": "13eccfcfd4ce807e14da418115a386b918870582", "year": 2015, "venue": "CHI", "alt_text": "This is a bar chart of the mean response on the semantically anchored scales.    Easy: Without Tool 4.1, with tool 4.1  Frustration: Without Tool 4.3, with tool 5  Knew Location in the Code: Without Tool 4.1, with tool 5.6", "levels": [[1], [2, 1]], "corpus_id": 17235000, "sentences": ["This is a bar chart of the mean response on the semantically anchored scales.", "Easy: Without Tool 4.1, with tool 4.1  Frustration: Without Tool 4.3, with tool 5  Knew Location in the Code: Without Tool 4.1, with tool 5.6"], "caption": "Figure 7. This chart show the average score for the partic- ipants for the semantically anchored questions. A higher value is better for all three questions. The bars represent the standard error.", "local_uri": ["13eccfcfd4ce807e14da418115a386b918870582_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Exergames for Physiotherapy and Rehabilitation: A Medium-term Situated Study of Motivational Aspects and Impact on Functional Reach", "pdf_hash": "00de251fbb4aaeb4cae73a26f69b8a28e365c3a5", "year": 2015, "venue": "CHI", "alt_text": "The chart shows the pre and post means of the three conditions for the autonomy, presence, tension and importance dimensions of the motivational measures with error bars indicating standard error.", "levels": [[1]], "corpus_id": 12781621, "sentences": ["The chart shows the pre and post means of the three conditions for the autonomy, presence, tension and importance dimensions of the motivational measures with error bars indicating standard error."], "caption": "Figure 2: Means and standard errors for autonomy, presence, tension-pressure, and effort-importance collected in the first session (pre) and in the last session after five weeks (post).", "local_uri": ["00de251fbb4aaeb4cae73a26f69b8a28e365c3a5_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exergames for Physiotherapy and Rehabilitation: A Medium-term Situated Study of Motivational Aspects and Impact on Functional Reach", "pdf_hash": "00de251fbb4aaeb4cae73a26f69b8a28e365c3a5", "year": 2015, "venue": "CHI", "alt_text": "The chart shows bar plots for the mean difference in the FRT measure for all three conditions.", "levels": [[1]], "corpus_id": 12781621, "sentences": ["The chart shows bar plots for the mean difference in the FRT measure for all three conditions."], "caption": "Figure 3: Means and standard errors for the pre-post- differences of the functional reach test for all groups.", "local_uri": ["00de251fbb4aaeb4cae73a26f69b8a28e365c3a5_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Interdependence as a Frame for Assistive Technology Research and Design", "pdf_hash": "ab1e2303af454fa1c7c692c51f37290a0040827a", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 1 includes two diagrams, one on the left and the other on the right. The diagram on the left is titled \"Independence.\" The diagram is subdivided into a left and right subdivision. The left subdivision shows a circle labeled \"non-disabled person\" and a bi-directional arrow linking the circle to a square labeled \"environment.\" On the right subdivision, there is a partial circle with a scalloped edge labeled \"disabled person,\" with a bi-directional arrow linking it to a shape labeled \"assistive technology\" that completes the circle. The assistive technology shape is linked by a second bi-directional arrow to a square labeled \"environment.\" The diagram on the right side is titled \"Interdependence.\" The diagram has three circles, each labeled \"person,\" one square labeled \"environment,\" and one quarter-circle labeled \"assistive technology.\" Each shape is connected to every other shape with bi-directional arrows.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 52936154, "sentences": ["Figure 1 includes two diagrams, one on the left and the other on the right.", "The diagram on the left is titled \"Independence.\"", "The diagram is subdivided into a left and right subdivision.", "The left subdivision shows a circle labeled \"non-disabled person\" and a bi-directional arrow linking the circle to a square labeled \"environment.\"", "On the right subdivision, there is a partial circle with a scalloped edge labeled \"disabled person,\" with a bi-directional arrow linking it to a shape labeled \"assistive technology\" that completes the circle.", "The assistive technology shape is linked by a second bi-directional arrow to a square labeled \"environment.\"", "The diagram on the right side is titled \"Interdependence.\"", "The diagram has three circles, each labeled \"person,\" one square labeled \"environment,\" and one quarter-circle labeled \"assistive technology.\"", "Each shape is connected to every other shape with bi-directional arrows."], "caption": "Figure 1. An independence frame (left) emphasizes an individual\u2019s relationship with the environment. Assistive technology (AT) devices are meant to bridge a perceived gap between disabled bodies and environments designed for non- disabled people. An interdependence frame (right) emphasizes the relationships between people, ATs, and environments, drawing out the roles of those with disabilities during collective work they do to create access.", "local_uri": ["ab1e2303af454fa1c7c692c51f37290a0040827a_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gender Norms and Attitudes about Childcare Activities Presented on Father Blogs", "pdf_hash": "48e01e20a9ba9605d96f8c1c20e8724aeb8f9122", "year": 2017, "venue": "CHI", "alt_text": "Screenshot of a sample blog post with DIY-language and visuals. The blog post is titled the \"Layered Crib Sheets Hack.\" The banner of the blog features an image of a hammer and the body of the post has an image of a wrench.", "levels": null, "corpus_id": 1198140, "sentences": ["Screenshot of a sample blog post with DIY-language and visuals.", "The blog post is titled the \"Layered Crib Sheets Hack.\" The banner of the blog features an image of a hammer and the body of the post has an image of a wrench."], "caption": "Figure 1. Sample blog post for DIY condition.", "local_uri": ["48e01e20a9ba9605d96f8c1c20e8724aeb8f9122_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Now Check Your Input: Brief Task Lockouts Encourage Checking, Longer Lockouts Encourage Task Switching", "pdf_hash": "ca0b61b2c13a21075e6aac24d777185ed8b8d447", "year": 2016, "venue": "CHI", "alt_text": "Three Welwyn Winders are shown. These are cube-like mechanical switches wound using a handle on the side. The handle is wound until the signal is released.", "levels": null, "corpus_id": 9630768, "sentences": ["Three Welwyn Winders are shown.", "These are cube-like mechanical switches wound using a handle on the side.", "The handle is wound until the signal is released."], "caption": "", "local_uri": ["ca0b61b2c13a21075e6aac24d777185ed8b8d447_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "DDFSeeks same: sexual health-related language in online personal ads for men who have sex with men", "pdf_hash": "98474c14fa7ce6e06400788c51a7c0619abe26e5", "year": 2014, "venue": "CHI", "alt_text": "Village Voice 1978: 0.00% of ads contained SHR language Village Voice 1982: 1.70% of ads contained SHR language Village Voice 1985: 14.01% of ads contained SHR language Village Voice 1988: 22.99% of ads contained SHR language Craigslist 2013: 53.50% of ads contained SHR language", "levels": null, "corpus_id": 16712289, "sentences": ["Village Voice 1978: 0.00% of ads contained SHR language Village Voice 1982: 1.70% of ads contained SHR language Village Voice 1985: 14.01% of ads contained SHR language Village Voice 1988: 22.99% of ads contained SHR language Craigslist 2013: 53.50% of ads contained SHR language"], "caption": "CategoryVillage Voice (1988)Craigslist, NYC only (2013)Craigslist, 95 locations (2013)Overall SHR22.99%53.50%53.96%DiseaseNA46.30%48.34%HIV (sub-category of Disease)NA17.03%14.76%SafetyNA17.87%12.85%ProtectionNA3.56%4.78%Risk (sub-category of Protection)NA2.35%3.38%HealthNA2.12%1.23%Davidson\u2019s health- related dictionary [14]22.99%34.35%32.73%Davidson\u2019s sexual exclusivity subcode [14]13.41%0.07%0.15%", "local_uri": ["98474c14fa7ce6e06400788c51a7c0619abe26e5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "DDFSeeks same: sexual health-related language in online personal ads for men who have sex with men", "pdf_hash": "98474c14fa7ce6e06400788c51a7c0619abe26e5", "year": 2014, "venue": "CHI", "alt_text": "This figure is a scatterplot showing the relationship between HIV Estimated Prevalence Rate Per 100,000 Population (CDC, 2011) on the x-axis and Percentage of Ads Containing Sexual Health-Related Language on the y-axis in 95 locations. The relationship is estimated linearly by the equation SHR language = 46.360 + 0.009 * prevalence rate, p = 0.009. Each location is represented by a circle, with the size of the circle representing its population relative to the other locations. Outliers with a low HIV prevalence rate and a high % of SHR language in ads include SF Bay Area and Boise, ID. Outliers with a high HIV prevalence rate and a low % of SHR language in ads include Wichita, KS and Jackson, MS. New York City is close to the linear trend, with a high HIV prevalence rate and a high % of SHR language in ads. In general, locations with a higher population tend to have a greater percentage of ads containing SHR language.", "levels": [[1], [2], [1], [2], [2], [3], [3]], "corpus_id": 16712289, "sentences": ["This figure is a scatterplot showing the relationship between HIV Estimated Prevalence Rate Per 100,000 Population (CDC, 2011) on the x-axis and Percentage of Ads Containing Sexual Health-Related Language on the y-axis in 95 locations.", "The relationship is estimated linearly by the equation SHR language = 46.360 + 0.009 * prevalence rate, p = 0.009.", "Each location is represented by a circle, with the size of the circle representing its population relative to the other locations.", "Outliers with a low HIV prevalence rate and a high % of SHR language in ads include SF Bay Area and Boise, ID.", "Outliers with a high HIV prevalence rate and a low % of SHR language in ads include Wichita, KS and Jackson, MS.", "New York City is close to the linear trend, with a high HIV prevalence rate and a high % of SHR language in ads.", "In general, locations with a higher population tend to have a greater percentage of ads containing SHR language."], "caption": "SHR language = 46.360 + 0.009 * prevalence rate, p=0.009", "local_uri": ["98474c14fa7ce6e06400788c51a7c0619abe26e5_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Nothing to Hide: Aesthetic Customization of Hearing Aids and Cochlear Implants in an Online Community", "pdf_hash": "19ce98627b03c0814e74232c795fafb23d0ec980", "year": 2016, "venue": "ASSETS", "alt_text": "Two hearing aids decorated in red holographic tape with gems and green glitter christmas tree cutouts. Hearing aids situated next to crafting materials such as rolls of tape and scissors.", "levels": [[-1], [-1]], "corpus_id": 12676871, "sentences": ["Two hearing aids decorated in red holographic tape with gems and green glitter christmas tree cutouts.", "Hearing aids situated next to crafting materials such as rolls of tape and scissors."], "caption": "Figure 1: An online community member shares an example of customized hearing aids decorated to celebrate the Christmas holiday. The hearing aids are decorated with glitter tape, tube wraps, and rhinestones.", "local_uri": ["19ce98627b03c0814e74232c795fafb23d0ec980_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Nothing to Hide: Aesthetic Customization of Hearing Aids and Cochlear Implants in an Online Community", "pdf_hash": "19ce98627b03c0814e74232c795fafb23d0ec980", "year": 2016, "venue": "ASSETS", "alt_text": "A) Blue and silver glitter-decorated HAs with matching nails; B) HAs with My Little Pony character stickers and tube riders; C) HAs with silver dots, \u201cPOW\u201d stickers, and a sparkly ear mold. D) HA with purple ear base and shimmering emerald ear mold; E) Cochlear implant with ankle bracelet repurposed as bangle.", "levels": null, "corpus_id": 12676871, "sentences": ["A) Blue and silver glitter-decorated HAs with matching nails; B) HAs with My Little Pony character stickers and tube riders; C) HAs with silver dots, \u201cPOW\u201d stickers, and a sparkly ear mold. D) HA with purple ear base and shimmering emerald ear mold; E) Cochlear implant with ankle bracelet repurposed as bangle."], "caption": "Figure 2. DIY customization of HA and CIs. A) Blue and silver glitter-decorated HAs with matching nails; B) HAs with My Little Pony character stickers and tube riders; C) HAs with silver dots, \u201cPOW\u201d stickers, and a sparkly ear mold. D) HA with purple ear base and shimmering emerald ear mold; E) Cochlear Implant with ankle bracelet repurposed as a bangle.", "local_uri": ["19ce98627b03c0814e74232c795fafb23d0ec980_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Virtual Agents for Augmented Reality", "pdf_hash": "1d537d9ef32d2e12cd60ea1f551000d0671af8c9", "year": 2019, "venue": "CHI", "alt_text": "A user wears an augmented reality headset while sitting at a table in front of a touchscreen computer. The computer depicts a hidden object puzzle where the user has to locate an object in an image. Sitting across from the user is a virtual human that is not actually there, but displayed through the headset.", "levels": null, "corpus_id": 140299071, "sentences": ["A user wears an augmented reality headset while sitting at a table in front of a touchscreen computer.", "The computer depicts a hidden object puzzle where the user has to locate an object in an image.", "Sitting across from the user is a virtual human that is not actually there, but displayed through the headset."], "caption": "Figure 1. Example of a user wearing an augmented reality headset and interacting with a virtual agent projected onto the real world through the headset.\u200c", "local_uri": ["1d537d9ef32d2e12cd60ea1f551000d0671af8c9_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Virtual Agents for Augmented Reality", "pdf_hash": "1d537d9ef32d2e12cd60ea1f551000d0671af8c9", "year": 2019, "venue": "CHI", "alt_text": "Bargraph showing the number of likes, dislikes, and net score (likes minus dislikes) for each agent. Mikey had the highest net score, followed by Ava, Zee, and then Jake.", "levels": [[1], [2]], "corpus_id": 140299071, "sentences": ["Bargraph showing the number of likes, dislikes, and net score (likes minus dislikes) for each agent.", "Mikey had the highest net score, followed by Ava, Zee, and then Jake."], "caption": "", "local_uri": ["1d537d9ef32d2e12cd60ea1f551000d0671af8c9_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Drunk User Interfaces: Determining Blood Alcohol Level through Everyday Smartphone Tasks", "pdf_hash": "a57ecdb519e215328328ac8f96a8f2f4559283d5", "year": 2018, "venue": "CHI", "alt_text": "The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green lines show the user what they have drawn.", "levels": null, "corpus_id": 5048650, "sentences": ["The DUI swiping task resembles an Android 3\u00d73 lock screen.", "The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green lines show the user what they have drawn."], "caption": "Figure 1. The DUI swiping task resembles an Android 3\u00d73 lock screen. The straight dashed red lines show the ideal gesture (hidden from the user) for the code 1-5-8-9, while the curvy solid green path shows the user what they have drawn.", "local_uri": ["a57ecdb519e215328328ac8f96a8f2f4559283d5_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Comparing Methods of Displaying Language Feedback for Student Videos of American Sign Language", "pdf_hash": "0907dfb73fbc31f4f0035aa7f391bbeab2c55bd4", "year": 2015, "venue": "ASSETS", "alt_text": "Box plot of ASL Instructor Score Improvement from Round 1 to Round 1.  Comparison of the VIDEO condition and the NOTES + POPUP condition.  VIDEO median 0.05 quartiles 0 and 0.5. NOTES + POPUP median 0.5 quartiles 1.5 and 0.", "levels": [[1], [1], [2], [2]], "corpus_id": 18031368, "sentences": ["Box plot of ASL Instructor Score Improvement from Round 1 to Round 1.", "Comparison of the VIDEO condition and the NOTES + POPUP condition.", "VIDEO median 0.05 quartiles 0 and 0.5.", "NOTES + POPUP median 0.5 quartiles 1.5 and 0."], "caption": "", "local_uri": ["0907dfb73fbc31f4f0035aa7f391bbeab2c55bd4_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Trolled by the Trolley Problem: On What Matters for Ethical Decision Making in Automated Vehicles", "pdf_hash": "37d86ca758dfc54318ae3d2b5c27ab6abaf2fc4f", "year": 2019, "venue": "CHI", "alt_text": "This figure shows three undesirable outcomes of a trolley problem. In the left picture five people are killed. In the center picture one person is killed. In the right picture the person inside the trolley (i.e., an automated vehicle) is killed.", "levels": null, "corpus_id": 140223750, "sentences": ["This figure shows three undesirable outcomes of a trolley problem.", "In the left picture five people are killed.", "In the center picture one person is killed.", "In the right picture the person inside the trolley (i.e., an automated vehicle) is killed."], "caption": "", "local_uri": ["37d86ca758dfc54318ae3d2b5c27ab6abaf2fc4f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Trolled by the Trolley Problem: On What Matters for Ethical Decision Making in Automated Vehicles", "pdf_hash": "37d86ca758dfc54318ae3d2b5c27ab6abaf2fc4f", "year": 2019, "venue": "CHI", "alt_text": "This picture shows a simple version of the trolley problem. Either five people are killed (option A) or one person is killed (option B). Both outcomes are not desirable.", "levels": null, "corpus_id": 140223750, "sentences": ["This picture shows a simple version of the trolley problem.", "Either five people are killed (option A) or one person is killed (option B).", "Both outcomes are not desirable."], "caption": "", "local_uri": ["37d86ca758dfc54318ae3d2b5c27ab6abaf2fc4f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Effects of Public Commitments and Accountability in a Technology-Supported Physical Activity Intervention", "pdf_hash": "43add3db7c4cf21426cd70b4b564aab8e0608ead", "year": 2015, "venue": "CHI", "alt_text": "Figure 1. Conceptual model which informed the study design.  (A) Announcing commitments only is expected to increase social support, which would lead to more physical activity. It is also expected to decrease accountability.  (B) Announcing commitents and results is expected to increase accountability.   (D) Increased accountability is expected to increased commitment fulfillment, but decrease the challenge level and decrease chances of making a commitment.", "levels": null, "corpus_id": 11736056, "sentences": ["Figure 1.", "Conceptual model which informed the study design.", "(A) Announcing commitments only is expected to increase social support, which would lead to more physical activity.", "It is also expected to decrease accountability.", "(B) Announcing commitents and results is expected to increase accountability.", "(D) Increased accountability is expected to increased commitment fulfillment, but decrease the challenge level and decrease chances of making a commitment."], "caption": "Figure 1. Conceptual model", "local_uri": ["43add3db7c4cf21426cd70b4b564aab8e0608ead_Image_001.png"], "annotated": false, "compound": false}
{"title": "Effects of Public Commitments and Accountability in a Technology-Supported Physical Activity Intervention", "pdf_hash": "43add3db7c4cf21426cd70b4b564aab8e0608ead", "year": 2015, "venue": "CHI", "alt_text": "Figure 2. Study design. There were three conditions: a control condition in which commitments were made privately and two treatment conditions. In the announce without results condition, commitments but not the outcome were announced. In the announce with results, both commitments and the outcomes were annouced.", "levels": null, "corpus_id": 11736056, "sentences": ["Figure 2.", "Study design.", "There were three conditions: a control condition in which commitments were made privately and two treatment conditions.", "In the announce without results condition, commitments but not the outcome were announced.", "In the announce with results, both commitments and the outcomes were annouced."], "caption": "3", "local_uri": ["43add3db7c4cf21426cd70b4b564aab8e0608ead_Image_002.png"], "annotated": false, "compound": false}
{"title": "Effects of Public Commitments and Accountability in a Technology-Supported Physical Activity Intervention", "pdf_hash": "43add3db7c4cf21426cd70b4b564aab8e0608ead", "year": 2015, "venue": "CHI", "alt_text": "Figure 3. The Commit to Steps dashboard showed participants their progress toward their current commitment, their walking history, and their history of commitments.", "levels": null, "corpus_id": 11736056, "sentences": ["Figure 3. The Commit to Steps dashboard showed participants their progress toward their current commitment, their walking history, and their history of commitments."], "caption": "Figure 3. Commit to Steps dashboard", "local_uri": ["43add3db7c4cf21426cd70b4b564aab8e0608ead_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Effects of Public Commitments and Accountability in a Technology-Supported Physical Activity Intervention", "pdf_hash": "43add3db7c4cf21426cd70b4b564aab8e0608ead", "year": 2015, "venue": "CHI", "alt_text": "Figure 4. For a participant in the \"announce with results\" condition, the Facebook included an announcement of their new commitment and the results of their previous week's commitments. Participants could comment on these posts.", "levels": null, "corpus_id": 11736056, "sentences": ["Figure 4.", "For a participant in the \"announce with results\" condition, the Facebook included an announcement of their new commitment and the results of their previous week's commitments.", "Participants could comment on these posts."], "caption": "Figure 4. Example accountability post", "local_uri": ["43add3db7c4cf21426cd70b4b564aab8e0608ead_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Effects of Public Commitments and Accountability in a Technology-Supported Physical Activity Intervention", "pdf_hash": "43add3db7c4cf21426cd70b4b564aab8e0608ead", "year": 2015, "venue": "CHI", "alt_text": "Figure 5 - Responses (comments and likes) to Facebook posts declined as time went on.", "levels": null, "corpus_id": 11736056, "sentences": ["Figure 5 - Responses (comments and likes) to Facebook posts declined as time went on."], "caption": "", "local_uri": ["43add3db7c4cf21426cd70b4b564aab8e0608ead_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Effects of Public Commitments and Accountability in a Technology-Supported Physical Activity Intervention", "pdf_hash": "43add3db7c4cf21426cd70b4b564aab8e0608ead", "year": 2015, "venue": "CHI", "alt_text": "Figure 6. In the early weeks, participants step counts improved dramatically, but subsequent gains decreased with time.", "levels": null, "corpus_id": 11736056, "sentences": ["Figure 6.", "In the early weeks, participants step counts improved dramatically, but subsequent gains decreased with time."], "caption": "Figure 7. Week-to-week change in step count.", "local_uri": ["43add3db7c4cf21426cd70b4b564aab8e0608ead_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Smartphone-Based Gaze Gesture Communication for People with Motor Disabilities", "pdf_hash": "0c332f5fe56258ccee5bc89cda9f0ddd8f25e1e1", "year": 2017, "venue": "CHI", "alt_text": "Images of 3 styles of e-tran board with letters of the alphabet printed on large acrylic sheets.", "levels": null, "corpus_id": 6557268, "sentences": ["Images of 3 styles of e-tran board with letters of the alphabet printed on large acrylic sheets."], "caption": "Figure 2. Low-tech gaze input solutions: a) E-tran board;", "local_uri": ["0c332f5fe56258ccee5bc89cda9f0ddd8f25e1e1_Image_002.png"], "annotated": false, "compound": false}
{"title": "Smartphone-Based Gaze Gesture Communication for People with Motor Disabilities", "pdf_hash": "0c332f5fe56258ccee5bc89cda9f0ddd8f25e1e1", "year": 2017, "venue": "CHI", "alt_text": "Flow diagram of gesture recognition: detecting the face, findign landmarks, extracting the eye image, template matching.", "levels": [[-1]], "corpus_id": 6557268, "sentences": ["Flow diagram of gesture recognition: detecting the face, findign landmarks, extracting the eye image, template matching."], "caption": "Figure 5. Flowchart of eye gesture recognition algorithm.", "local_uri": ["0c332f5fe56258ccee5bc89cda9f0ddd8f25e1e1_Image_005.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Smartphone-Based Gaze Gesture Communication for People with Motor Disabilities", "pdf_hash": "0c332f5fe56258ccee5bc89cda9f0ddd8f25e1e1", "year": 2017, "venue": "CHI", "alt_text": "Figure showing how four gestures (down, up, right, left) are decoded to the word \"task\" by the word prediction system.", "levels": null, "corpus_id": 6557268, "sentences": ["Figure showing how four gestures (down, up, right, left) are decoded to the word \"task\" by the word prediction system."], "caption": "Figure 6. Word predictions update after each gesture in this example four-gesture sequence to spell the word \u201ctask\u201d.", "local_uri": ["0c332f5fe56258ccee5bc89cda9f0ddd8f25e1e1_Image_006.png"], "annotated": false, "compound": false}
{"title": "Designing an Animated Character System for American Sign Language", "pdf_hash": "8209b931c94fea5d107e6ef2461b64e00fd52249", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 3: How people communicate in ASL a) digitally, b) when taking notes, and c) when using an ASL writing system. This figure presents three bar charts (a, b, and c). The y-axis is % Participants, ranging from 0-90. The x-axis is a) Digital Communication Format, b) ASL Note Format, and c) Known Writing Systems. Each bar chart provides separate bars for DHH (light blue) and Hearing (dark blue) populations.    a) Digital ASL Communication Formats, sorted by DHH popularity (most popular first): Animated emoji, English gloss, Live video chat, Recorded videos, English descriptions, Non-animated emoji, Other, N/A.  b) ASL Note Formats, sorted by DHH popularity (most popular first): English gloss, English translation, ASL writing system, ASL video recording, English descriptions, Drawings of signs, Other, N/A.  c)Known ASL Writing Systems, sorted by DHH popularity (most popular first): English gloss, SignWriting, HamNoSys, Stokoe notation, si5s, Other, None.", "levels": [[1], [1], [1], [1], [1], [1]], "corpus_id": 52938981, "sentences": ["Figure 3: How people communicate in ASL a) digitally, b) when taking notes, and c) when using an ASL writing system.", "This figure presents three bar charts (a, b, and c).", "The y-axis is % Participants, ranging from 0-90.", "The x-axis is a) Digital Communication Format, b) ASL Note Format, and c) Known Writing Systems.", "Each bar chart provides separate bars for DHH (light blue) and Hearing (dark blue) populations.", "a) Digital ASL Communication Formats, sorted by DHH popularity (most popular first): Animated emoji, English gloss, Live video chat, Recorded videos, English descriptions, Non-animated emoji, Other, N/A.  b) ASL Note Formats, sorted by DHH popularity (most popular first): English gloss, English translation, ASL writing system, ASL video recording, English descriptions, Drawings of signs, Other, N/A.  c)Known ASL Writing Systems, sorted by DHH popularity (most popular first): English gloss, SignWriting, HamNoSys, Stokoe notation, si5s, Other, None."], "caption": "Figure 3: How people communicate in ASL a) digitally, b) when taking notes, and c) when using an ASL writing system.\u200c", "local_uri": ["8209b931c94fea5d107e6ef2461b64e00fd52249_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing an Animated Character System for American Sign Language", "pdf_hash": "8209b931c94fea5d107e6ef2461b64e00fd52249", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 4: Materials participants reported wanting to read in ASL text. This figure presents a bar chart, with separate bars for DHH (light blue) and hearing (dark blue) populations. Y-axis is % participants, ranging from 0-70. X-axis is Material Desired in ASL Text. sorted by DHH popularity (most popular first): Website content, Printed content, Email, Texts/SMS, Video captions, Other, None.", "levels": [[1], [1], [1], [1], [1]], "corpus_id": 52938981, "sentences": ["Figure 4: Materials participants reported wanting to read in ASL text.", "This figure presents a bar chart, with separate bars for DHH (light blue) and hearing (dark blue) populations.", "Y-axis is % participants, ranging from 0-70.", "X-axis is Material Desired in ASL Text.", "sorted by DHH popularity (most popular first): Website content, Printed content, Email, Texts/SMS, Video captions, Other, None."], "caption": "question provided fve multiple-choice answer options resem- bling the sign in question, ordered randomly. We selected featurally similar answer choices through Latent Semantic Analysis over a dataset of crowdsourced feature evaluations [8]. Each multiple-choice option was represented as an En- glish word, with a link to a signed video from SigningSavvy [48], an online English-to-ASL dictionary.", "local_uri": ["8209b931c94fea5d107e6ef2461b64e00fd52249_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing an Animated Character System for American Sign Language", "pdf_hash": "8209b931c94fea5d107e6ef2461b64e00fd52249", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 6: Identification Accuracy, the percent who identified signs from stationary vs. animated characters, without training. This figure presents a bar chart, with separate bars for stationary (light blue) and animated (dark blue) characters. Y-axis is Identification Accuracy, ranging from 0-80. X-axis is ASL Sign, including four signs: WHERE, UNDERSTAND, MAYBE, and MOTIVATION.", "levels": [[1], [1], [1], [1]], "corpus_id": 52938981, "sentences": ["Figure 6: Identification Accuracy, the percent who identified signs from stationary vs. animated characters, without training.", "This figure presents a bar chart, with separate bars for stationary (light blue) and animated (dark blue) characters.", "Y-axis is Identification Accuracy, ranging from 0-80.", "X-axis is ASL Sign, including four signs: WHERE, UNDERSTAND, MAYBE, and MOTIVATION."], "caption": "", "local_uri": ["8209b931c94fea5d107e6ef2461b64e00fd52249_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing an Animated Character System for American Sign Language", "pdf_hash": "8209b931c94fea5d107e6ef2461b64e00fd52249", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 5: Barriers to using ASL character systems reported by participants. This figure presents a bar chart, with separate bars for DHH (light blue) and hearing (dark blue) populations. Y-axis is % participants, ranging from 0-55. X-axis is Barrier to Adoption, sorted by DHH popularity (most popular first): Few printed material, Few online material, Hard to learn, Prefer English, Do not resemble SL, Dislike of reading, Other, None. Barriers with asterisks are potentially addressed by introducing animation to character systems. Barriers that have asterisks at the top of their bars: Hard to learn, and Do not resemble ASL.", "levels": [[1], [1], [1], [1], [1], [1]], "corpus_id": 52938981, "sentences": ["Figure 5: Barriers to using ASL character systems reported by participants.", "This figure presents a bar chart, with separate bars for DHH (light blue) and hearing (dark blue) populations.", "Y-axis is % participants, ranging from 0-55.", "X-axis is Barrier to Adoption, sorted by DHH popularity (most popular first): Few printed material, Few online material, Hard to learn, Prefer English, Do not resemble SL, Dislike of reading, Other, None.", "Barriers with asterisks are potentially addressed by introducing animation to character systems.", "Barriers that have asterisks at the top of their bars: Hard to learn, and Do not resemble ASL."], "caption": "Figure 5: Barriers to using ASL character systems reported by participants. Barriers with asterisks are potentially addressed by introducing animation to character systems.", "local_uri": ["8209b931c94fea5d107e6ef2461b64e00fd52249_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing an Animated Character System for American Sign Language", "pdf_hash": "8209b931c94fea5d107e6ef2461b64e00fd52249", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 8: Screen shots of the workshop website, in three parts (a, b, and c).    a) Rows of text in the animated character system are displayed. A drop-down at the top reads: Everything animated. Instructions at top read: Use the drop-down menu to explore 4 ways of viewing the story below.   b) Multiple-choice answer choices are displayed. Each has a radio button for selection, an animated character, and a description. The answer choice descriptions are: jump back to beginning, pause at beginning, fade in and fade out, black dot marker, and other (with a textbox input field). Instructions at top read: How should the animation indicate that a sign movement is beginning?  c) Sliders for adjusting the speed of two signs. Each sign is presented with English, ASL (as an animated character), and a slider. The two signs are TIRED and VERY TIRED. Instructions at top read: Adjust the sliders so each animation speed is appropriate.", "levels": null, "corpus_id": 52938981, "sentences": ["Figure 8: Screen shots of the workshop website, in three parts (a, b, and c).", "a) Rows of text in the animated character system are displayed.", "A drop-down at the top reads: Everything animated.", "Instructions at top read: Use the drop-down menu to explore 4 ways of viewing the story below.", "b) Multiple-choice answer choices are displayed.", "Each has a radio button for selection, an animated character, and a description.", "The answer choice descriptions are: jump back to beginning, pause at beginning, fade in and fade out, black dot marker, and other (with a textbox input field).", "Instructions at top read: How should the animation indicate that a sign movement is beginning?  c) Sliders for adjusting the speed of two signs.", "Each sign is presented with English, ASL (as an animated character), and a slider.", "The two signs are TIRED and VERY TIRED.", "Instructions at top read: Adjust the sliders so each animation speed is appropriate."], "caption": "", "local_uri": ["8209b931c94fea5d107e6ef2461b64e00fd52249_Image_012.png"], "annotated": false, "compound": false}
{"title": "PolarTrack: Optical Outside-In Device Tracking that Exploits Display Polarization", "pdf_hash": "6753e1e99273c1c0ec1700298a0c35979638c4fd", "year": 2018, "venue": "CHI", "alt_text": "PolarTrack is an optical tracking systemfor mobile devices that combines an off-the-shelf RGB camera with a rotating linear polarization filter mounted in front of the lens.PolarTrack exploits the use of polarized light in current displays to segment device screens in the camera feed from the backgroundby detecting periodical changesof display brightness while the linear polarizerrotates.", "levels": null, "corpus_id": 5056427, "sentences": ["PolarTrack is an optical tracking systemfor mobile devices that combines an off-the-shelf RGB camera with a rotating linear polarization filter mounted in front of the lens.", "PolarTrack exploits the use of polarized light in current displays to segment device screens in the camera feed from the backgroundby detecting periodical changesof display brightness while the linear polarizerrotates."], "caption": "", "local_uri": ["6753e1e99273c1c0ec1700298a0c35979638c4fd_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "PolarTrack: Optical Outside-In Device Tracking that Exploits Display Polarization", "pdf_hash": "6753e1e99273c1c0ec1700298a0c35979638c4fd", "year": 2018, "venue": "CHI", "alt_text": "AnAppleiPad Mini withtwo linearly polarized filtersput side-by-side on top of the screen. The filters are rotated 90\u00b0 to each other.", "levels": null, "corpus_id": 5056427, "sentences": ["AnAppleiPad Mini withtwo linearly polarized filtersput side-by-side on top of the screen.", "The filters are rotated 90\u00b0 to each other."], "caption": "Figure 2. An Apple iPad Mini with two linearly polarized \ufb01lters put side- by-side on top of the screen. The \ufb01lters are rotated 90\u00b0 to each other.", "local_uri": ["6753e1e99273c1c0ec1700298a0c35979638c4fd_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "PolarTrack: Optical Outside-In Device Tracking that Exploits Display Polarization", "pdf_hash": "6753e1e99273c1c0ec1700298a0c35979638c4fd", "year": 2018, "venue": "CHI", "alt_text": "Both pictures show an image of the same scene taken by a camera with a polarization filter. The rotation of the polarization filter is 0\u00b0 in the first picture and 90\u00b0 in the second picture.", "levels": null, "corpus_id": 5056427, "sentences": ["Both pictures show an image of the same scene taken by a camera with a polarization filter.", "The rotation of the polarization filter is 0\u00b0 in the first picture and 90\u00b0 in the second picture."], "caption": "Figure 3. Both pictures show an image of the same scene taken by a camera with a polarization \ufb01lter. The rotation of the polarization \ufb01lter is 0\u00b0 in the \ufb01rst picture and 90\u00b0 in the second picture.", "local_uri": ["6753e1e99273c1c0ec1700298a0c35979638c4fd_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "PolarTrack: Optical Outside-In Device Tracking that Exploits Display Polarization", "pdf_hash": "6753e1e99273c1c0ec1700298a0c35979638c4fd", "year": 2018, "venue": "CHI", "alt_text": "A sliding window approach averages the n (i.e. n = 3)past differential images with weightsto compensateforrolling shutter effects.", "levels": [[-1]], "corpus_id": 5056427, "sentences": ["A sliding window approach averages the n (i.e. n = 3)past differential images with weightsto compensateforrolling shutter effects."], "caption": "values where a change happened, either through change of light intensity or color changes of pixels. However, due to the camera\u2019s rolling shutter and the continuous rotation of the polarization \ufb01lter, the camera might have captured the pixels of an image at different times even though all of them are considered to be part of frame t. Consequently, this small time difference results in an image where pixels are captured with different polarizer angles, effectively resulting in a rolling shutter effect in IDG (see top row of images in Figure 4)", "local_uri": ["6753e1e99273c1c0ec1700298a0c35979638c4fd_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard", "pdf_hash": "feaab02b3d3545acf1964aaacc3ae4f29f889bda", "year": 2014, "venue": "CHI", "alt_text": "Gesturing on a physical keyboard. The green overlays show the keys that have been pressed and the red ones show the currently pressed keys. The red line running across keys illustrates the captured gesture stroke.", "levels": null, "corpus_id": 3891919, "sentences": ["Gesturing on a physical keyboard.", "The green overlays show the keys that have been pressed and the red ones show the currently pressed keys.", "The red line running across keys illustrates the captured gesture stroke."], "caption": "Figure 1: Gesturing on a physical keyboard. The green overlays show the keys that have been pressed and the red ones show the currently pressed keys. The red line running across keys illustrates the captured gesture stroke.", "local_uri": ["feaab02b3d3545acf1964aaacc3ae4f29f889bda_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard", "pdf_hash": "feaab02b3d3545acf1964aaacc3ae4f29f889bda", "year": 2014, "venue": "CHI", "alt_text": "Proposed set of 16 gestures including both sequence-sensitive (viz. straight strokes) and sequence-invariant (viz. symbolic strokes) gestures.", "levels": null, "corpus_id": 3891919, "sentences": ["Proposed set of 16 gestures including both sequence-sensitive (viz. straight strokes) and sequence-invariant (viz. symbolic strokes) gestures."], "caption": "Figure 3: The proposed gesture set.", "local_uri": ["feaab02b3d3545acf1964aaacc3ae4f29f889bda_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard", "pdf_hash": "feaab02b3d3545acf1964aaacc3ae4f29f889bda", "year": 2014, "venue": "CHI", "alt_text": "The empirical Cumulative Density Functions of intra-gesture idle durations. Inset shows the 99% cutoff.", "levels": null, "corpus_id": 3891919, "sentences": ["The empirical Cumulative Density Functions of intra-gesture idle durations.", "Inset shows the 99% cutoff."], "caption": "where Ei is the keycap name of the ith keystroke on the keyboard, and function D(Ei-1 , Ei) returns the segment-to- segment shortest distance between two keycaps, based on the specific keyboard geometry (see Figure 2). The variance, VK, is in turn calculated as:", "local_uri": ["feaab02b3d3545acf1964aaacc3ae4f29f889bda_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard", "pdf_hash": "feaab02b3d3545acf1964aaacc3ae4f29f889bda", "year": 2014, "venue": "CHI", "alt_text": "The ROC curves for mean keystroke distances for gesturing on the compact keyboard.", "levels": [[1]], "corpus_id": 3891919, "sentences": ["The ROC curves for mean keystroke distances for gesturing on the compact keyboard."], "caption": "Figure 8: The ROC curves for mean keystroke distances", "local_uri": ["feaab02b3d3545acf1964aaacc3ae4f29f889bda_Image_012.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard", "pdf_hash": "feaab02b3d3545acf1964aaacc3ae4f29f889bda", "year": 2014, "venue": "CHI", "alt_text": "The detection accuracy for different resolution timeouts and confidence thresholds on the compact keyboard.", "levels": null, "corpus_id": 3891919, "sentences": ["The detection accuracy for different resolution timeouts and confidence thresholds on the compact keyboard."], "caption": "C =", "local_uri": ["feaab02b3d3545acf1964aaacc3ae4f29f889bda_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard", "pdf_hash": "feaab02b3d3545acf1964aaacc3ae4f29f889bda", "year": 2014, "venue": "CHI", "alt_text": "The GestRun application launcher. The recognized gesture is shown above the corresponding application for the illustration purpose.", "levels": null, "corpus_id": 3891919, "sentences": ["The GestRun application launcher.", "The recognized gesture is shown above the corresponding application for the illustration purpose."], "caption": "Figure 12: The GestRun application launcher. The recognized gesture is shown above the corresponding application for the illustration purpose.", "local_uri": ["feaab02b3d3545acf1964aaacc3ae4f29f889bda_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Click on bake to get cookies: guiding word-finding with semantic associations", "pdf_hash": "4c8601af8c92ee5f6e84ec0ccfb1c3da374baf0d", "year": 2010, "venue": "ASSETS '10", "alt_text": "Home screen of the LG vocabulary interface with the phrase to be completed displayed at the bottom.", "levels": null, "corpus_id": 14707809, "sentences": ["Home screen of the LG vocabulary interface with the phrase to be completed displayed at the bottom."], "caption": "", "local_uri": ["4c8601af8c92ee5f6e84ec0ccfb1c3da374baf0d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Click on bake to get cookies: guiding word-finding with semantic associations", "pdf_hash": "4c8601af8c92ee5f6e84ec0ccfb1c3da374baf0d", "year": 2010, "venue": "ASSETS '10", "alt_text": "Bakes is checked as found after the user clicks on the plus button above the bakes icon.", "levels": null, "corpus_id": 14707809, "sentences": ["Bakes is checked as found after the user clicks on the plus button above the bakes icon."], "caption": "Figure 4. Bakes is checked as found after the user clicks on the plus button above the bakes icon.", "local_uri": ["4c8601af8c92ee5f6e84ec0ccfb1c3da374baf0d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Click on bake to get cookies: guiding word-finding with semantic associations", "pdf_hash": "4c8601af8c92ee5f6e84ec0ccfb1c3da374baf0d", "year": 2010, "venue": "ASSETS '10", "alt_text": "ViVA provides words related to the one the user has clicked on, e.g., food and kitchen are related to cook.", "levels": null, "corpus_id": 14707809, "sentences": ["ViVA provides words related to the one the user has clicked on, e.g., food and kitchen are related to cook."], "caption": "Figure 5. ViVA provides words related to the one the user has clicked on, e.g., food and kitchen are related to cook.", "local_uri": ["4c8601af8c92ee5f6e84ec0ccfb1c3da374baf0d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Stroke-Gesture Input for People with Motor Impairments: Empirical Results & Research Roadmap", "pdf_hash": "e8f307fe9618a608b5503003fd1490908dd2077e", "year": 2019, "venue": "CHI", "alt_text": "Thirty-five small photos showing people with motor impairments articulating stroke-gestures on a tablet", "levels": null, "corpus_id": 140323531, "sentences": ["Thirty-five small photos showing people with motor impairments articulating stroke-gestures on a tablet"], "caption": "Figure 1. Snapshots of 35 people with motor impairments articulating stroke-gestures on a tablet. Note the accessibility chal- lenges and coping strategies. In this work, we analyze 9,681 gestures from 70 people with and without motor impairments.", "local_uri": ["e8f307fe9618a608b5503003fd1490908dd2077e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Stroke-Gesture Input for People with Motor Impairments: Empirical Results & Research Roadmap", "pdf_hash": "e8f307fe9618a608b5503003fd1490908dd2077e", "year": 2019, "venue": "CHI", "alt_text": "Three line chart graphs showing recognition accuracy rates for stroke-gestures articulated by participants with and without motor impairments", "levels": [[1]], "corpus_id": 140323531, "sentences": ["Three line chart graphs showing recognition accuracy rates for stroke-gestures articulated by participants with and without motor impairments"], "caption": "Figure 3. Recognition accuracy rates for stroke-gestures articulated by participants with motor impairments (left) and without impairments (middle) under standard training, and recognition rates for participants with motor impairments (right) under mixed training (see text for description), function of the number of training participants P. Note: error bars show 95% CIs.", "local_uri": ["e8f307fe9618a608b5503003fd1490908dd2077e_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Stroke-Gesture Input for People with Motor Impairments: Empirical Results & Research Roadmap", "pdf_hash": "e8f307fe9618a608b5503003fd1490908dd2077e", "year": 2019, "venue": "CHI", "alt_text": "The figure shows a block diagram with the 10 items of the research roadmap for accessible stroke-gesture input for users with upper body motor impairments", "levels": null, "corpus_id": 140323531, "sentences": ["The figure shows a block diagram with the 10 items of the research roadmap for accessible stroke-gesture input for users with upper body motor impairments"], "caption": "Figure 4. Sketch of a research roadmap for accessible stroke-gesture input for users with upper body motor impairments.", "local_uri": ["e8f307fe9618a608b5503003fd1490908dd2077e_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "GymSoles: Improving Squats and Dead-Lifts by Visualizing the User's Center of Pressure", "pdf_hash": "c42fb8648292fa425fc93a12ee21ba10d25b6d1e", "year": 2019, "venue": "CHI", "alt_text": "Figure displays the major muscle groups being activated when executing dead-lift exercise. Trapezius, Spinal Erectors, Gluteus Maximus, Adductor Magnus, Hamstrings, Quadriceps Femoris", "levels": null, "corpus_id": 140323149, "sentences": ["Figure displays the major muscle groups being activated when executing dead-lift exercise.", "Trapezius, Spinal Erectors, Gluteus Maximus, Adductor Magnus, Hamstrings, Quadriceps Femoris"], "caption": "Trapezius      Spinal Erectors", "local_uri": ["c42fb8648292fa425fc93a12ee21ba10d25b6d1e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "GymSoles: Improving Squats and Dead-Lifts by Visualizing the User's Center of Pressure", "pdf_hash": "c42fb8648292fa425fc93a12ee21ba10d25b6d1e", "year": 2019, "venue": "CHI", "alt_text": "The figure shows the optical marker placements and a screen shot of a motive software. Twelve markers were attached to the leg, two markers were placed at the waist, and one marker was placed on the shoulder. OptiTrack motion capture system and Motive software was used to record marker data.", "levels": [[-1], [-1], [-1]], "corpus_id": 140323149, "sentences": ["The figure shows the optical marker placements and a screen shot of a motive software.", "Twelve markers were attached to the leg, two markers were placed at the waist, and one marker was placed on the shoulder.", "OptiTrack motion capture system and Motive software was used to record marker data."], "caption": "Optical Markers", "local_uri": ["c42fb8648292fa425fc93a12ee21ba10d25b6d1e_Image_010.jpg", "c42fb8648292fa425fc93a12ee21ba10d25b6d1e_Image_012.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "GymSoles: Improving Squats and Dead-Lifts by Visualizing the User's Center of Pressure", "pdf_hash": "c42fb8648292fa425fc93a12ee21ba10d25b6d1e", "year": 2019, "venue": "CHI", "alt_text": "Java based software was developed to provide visual feedback. The Visual feedback indicates the point of center of pressure", "levels": null, "corpus_id": 140323149, "sentences": ["Java based software was developed to provide visual feedback.", "The Visual feedback indicates the point of center of pressure"], "caption": "", "local_uri": ["c42fb8648292fa425fc93a12ee21ba10d25b6d1e_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Technology-Mediated Sight: A Case Study of Early Adopters of a Low Vision Assistive Technology", "pdf_hash": "c5f14f2b1f6cebc1c53463b1ab88925256bef4df", "year": 2017, "venue": "ASSETS", "alt_text": "User is wearing a device that sits like sunglasses on her head. She is looking down as she knits.", "levels": null, "corpus_id": 7026434, "sentences": ["User is wearing a device that sits like sunglasses on her head.", "She is looking down as she knits."], "caption": "", "local_uri": ["c5f14f2b1f6cebc1c53463b1ab88925256bef4df_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "pdf_hash": "b535ecced835fbcfffaa15a78abfa49f44709daa", "year": 2017, "venue": "ASSETS", "alt_text": "Fig1. (this alt text is in the caption)    A participant (P5) using NavCog3 heading to a movie theater from a station during Route 1 in Study 1. The smartphone was worn to free the participant's hands while walking.", "levels": null, "corpus_id": 24536453, "sentences": ["Fig1. (this alt text is in the caption)    A participant (P5) using NavCog3 heading to a movie theater from a station during Route 1 in Study 1.", "The smartphone was worn to free the participant's hands while walking."], "caption": "Figure 1. A participant (P5) using NavCog3 heading to a movie theater from a station during Route 1 in Study 1. The smart- phone was worn to free the participant\u2019s hands while walking.", "local_uri": ["b535ecced835fbcfffaa15a78abfa49f44709daa_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "pdf_hash": "b535ecced835fbcfffaa15a78abfa49f44709daa", "year": 2017, "venue": "ASSETS", "alt_text": "Fig4. (this alt text is in the caption)    Beacon deployment locations in the environment. Blue dots indicate beacon locations.", "levels": null, "corpus_id": 24536453, "sentences": ["Fig4. (this alt text is in the caption)    Beacon deployment locations in the environment.", "Blue dots indicate beacon locations."], "caption": "", "local_uri": ["b535ecced835fbcfffaa15a78abfa49f44709daa_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "pdf_hash": "b535ecced835fbcfffaa15a78abfa49f44709daa", "year": 2017, "venue": "ASSETS", "alt_text": "Fig3.    There are 4 major components; server, smartphone, Bluetooth LE beacons and user. server contains conversation and map & routing. smartphone contains location engine and sensor. Conversation, navigation and info. requests are interactions between the smartphone and the user.", "levels": null, "corpus_id": 24536453, "sentences": ["Fig3.", "There are 4 major components; server, smartphone, Bluetooth LE beacons and user.", "server contains conversation and map & routing.", "smartphone contains location engine and sensor.", "Conversation, navigation and info.", "requests are interactions between the smartphone and the user."], "caption": "Figure 3. An overview of the system: A user requests naviga- tion or surrounding information via speech-based interaction using a smartphone.", "local_uri": ["b535ecced835fbcfffaa15a78abfa49f44709daa_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "pdf_hash": "b535ecced835fbcfffaa15a78abfa49f44709daa", "year": 2017, "venue": "ASSETS", "alt_text": "Fig5. (this alt text is in the caption)    The visualization of three routes for Study 1 in the order of presentation: 1) a station to a movie theater (177m), 2) the theater to a candy shop (54 m), and 3) the shop to a sub- way station (176 m). Each route included a transition between floors via an elevator.", "levels": null, "corpus_id": 24536453, "sentences": ["Fig5. (this alt text is in the caption)    The visualization of three routes for Study 1 in the order of presentation: 1) a station to a movie theater (177m), 2) the theater to a candy shop (54 m), and 3) the shop to a sub- way station (176 m).", "Each route included a transition between floors via an elevator."], "caption": "Figure 5. The visualization of three routes for Study 1 in the order of presentation: 1) a station to a movie theater (177m), 2) the theater to a candy shop (54 m), and 3) the shop to a sub- way station (176 m). Each route included a transition between floors via an elevator.\u200c\u200c", "local_uri": ["b535ecced835fbcfffaa15a78abfa49f44709daa_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "pdf_hash": "b535ecced835fbcfffaa15a78abfa49f44709daa", "year": 2017, "venue": "ASSETS", "alt_text": "The localization error in meters per participant across three routes. All data is the following in a csv format.  participant,lower whisker,25th percentile,median,75th percentile,upper whisker,% of outliers,extremes,number of samples  P1,0.04,1.02,1.52,2.24,4.07,6.1%,48,787  P2,0.03,0.96,1.39,2.51,4.835,8.8%,66,751  P3,0.08,0.91,1.435,2.09,3.86,0.3%,2,644  P4,0.06,0.905,1.43,2.05,3.7675,2.6%,16,620  P5,0.03,0.95,1.42,2.03,3.65,2.2%,20,915  P6,0.02,0.76,1.15,1.59,2.835,6.4%,34,530  P7,0.03,0.81,1.29,1.93,3.61,1.0%,8,813  P8,0.12,1,1.53,2.305,4.2625,11.8%,115,971  P9,0.16,0.99,1.34,1.88,3.215,4.4%,36,810  P10,0.03,0.86,1.55,2.28,4.41,0.4%,3,800", "levels": [[1], [0], [2]], "corpus_id": 24536453, "sentences": ["The localization error in meters per participant across three routes.", "All data is the following in a csv format.", "participant,lower whisker,25th percentile,median,75th percentile,upper whisker,% of outliers,extremes,number of samples  P1,0.04,1.02,1.52,2.24,4.07,6.1%,48,787  P2,0.03,0.96,1.39,2.51,4.835,8.8%,66,751  P3,0.08,0.91,1.435,2.09,3.86,0.3%,2,644  P4,0.06,0.905,1.43,2.05,3.7675,2.6%,16,620  P5,0.03,0.95,1.42,2.03,3.65,2.2%,20,915  P6,0.02,0.76,1.15,1.59,2.835,6.4%,34,530  P7,0.03,0.81,1.29,1.93,3.61,1.0%,8,813  P8,0.12,1,1.53,2.305,4.2625,11.8%,115,971  P9,0.16,0.99,1.34,1.88,3.215,4.4%,36,810  P10,0.03,0.86,1.55,2.28,4.41,0.4%,3,800"], "caption": "", "local_uri": ["b535ecced835fbcfffaa15a78abfa49f44709daa_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "pdf_hash": "b535ecced835fbcfffaa15a78abfa49f44709daa", "year": 2017, "venue": "ASSETS", "alt_text": "Fig7.  The distribution of localization accuracy in meters. All the data is the following in a csv format.  Error in meters,Total,Success,Fail  0-0.5,16,15,1  0.5-1.0,87,79,8  1.0-1.5,61,56,5  1.5-2.0,55,46,9  2.0-2.5,18,14,4  2.5-3.0,12,8,4  3.0-3.5,3,2,1  3.5-4.0,1,0,1  5.5-6.0,1,1,0", "levels": [[0], [1], [0], [2, 1]], "corpus_id": 24536453, "sentences": ["Fig7.", "The distribution of localization accuracy in meters.", "All the data is the following in a csv format.", "Error in meters,Total,Success,Fail  0-0.5,16,15,1  0.5-1.0,87,79,8  1.0-1.5,61,56,5  1.5-2.0,55,46,9  2.0-2.5,18,14,4  2.5-3.0,12,8,4  3.0-3.5,3,2,1  3.5-4.0,1,0,1  5.5-6.0,1,1,0"], "caption": "Figure 7. The distribution of localization accuracy in meters (N", "local_uri": ["b535ecced835fbcfffaa15a78abfa49f44709daa_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "An Exploration of Speech-Based Productivity Support in the Car", "pdf_hash": "fb38ab08e7bd08780ef73370454fe91a949e6811", "year": 2019, "venue": "CHI", "alt_text": "A man sitting at a driving simulator. Text bubbles describe an in car speech-based assistant helping create presentation slides and alerting the driver to on-road events. The speech bubbles say: Person - \"Create a new slide\", Assistant - \"Ok. What is the title of this slide?\", Person - \"Outline\", Assistant - \"Ok. Please pay attention\". From these speech bubbles and arrow points towards generated presentation slides showing a title slide and an outline slide.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 117950543, "sentences": ["A man sitting at a driving simulator.", "Text bubbles describe an in car speech-based assistant helping create presentation slides and alerting the driver to on-road events.", "The speech bubbles say: Person - \"Create a new slide\", Assistant - \"Ok.", "What is the title of this slide?\",", "Person - \"Outline\", Assistant - \"Ok.", "Please pay attention\".", "From these speech bubbles and arrow points towards generated presentation slides showing a title slide and an outline slide."], "caption": "Figure 1: This paper explores how to design interfaces that safely allow drivers to be productive while commuting.", "local_uri": ["fb38ab08e7bd08780ef73370454fe91a949e6811_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Would You Be Mine: Appropriating Minecraft as an Assistive Technology for Youth with Autism", "pdf_hash": "bd4768c197be8771302b302710715e5a8bb94960", "year": 2016, "venue": "ASSETS", "alt_text": "A. Multi-sensory environment in a physical classroom. B. Brightly rainbow colored Sensory Room in Autcraft. C. Calming garden in Autcraft. D. Dark room where the lights can be turned on and off in Autcraft.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 31169018, "sentences": ["A. Multi-sensory environment in a physical classroom.", "B. Brightly rainbow colored Sensory Room in Autcraft. C. Calming garden in Autcraft. D. Dark room where the lights can be turned on and off in Autcraft."], "caption": "D.Figure 2. A. Multi-sensory environment in a physical classroom. B. Brightly rainbow colored Sensory Room in Autcraft.C. Calming garden in Autcraft. D. Dark room where the lights can be turned on and off in Autcraft.monitor were turned off, but the avatar is also experiencing the black space, which is distinct from the experience of turning off the screen. Players enact virtual embodiment while controlling their avatars, highlighting the complex experiences of virtual and physical worlds [5].Beyond simply  supporting people  with  SPD, and  other related challenges, when they need a break, therapeutic interventions also support teaching other coping processes. For example, multi- sensory environments (MSE) have been shown to help people support sensory integration. Typically, these physical environments, often called \u201csensory rooms,\u201d are saturated with visual, audible, and tactile stimuli and used therapeutically by trained professionals [47] (See Figure 2A).Certainly, the complexities and nuance of a clinically designed MSE cannot be easily replicated in a virtual space. However, in noticing the player-driven self-regulation practices, the Autcraft community administrators built their own version of Sensory Rooms within the virtual world as a quiet space for members to go and relax. In these Sensory Rooms, chat is disabled and the environment is meant to be tranquil and with minimal sensory input. The administrators usefully \u201cmodded\u201d and appropriated the Minecraft platform to create Autcraft, and the Autcraft platform to create carefully regulated spaces for sensory relief.Members can choose three different styles of room, each tailored to meet different kinds of sensory needs (i.e., a calm garden (See Figure 2C); a small, plain room with a light switch (See Figure 2D); and a brightly colored room (See Figure 2B)). In many ways, these rooms mimic the environments found in physical world Sensory Rooms. The community imbued the virtual spaces with assistive properties by mirroring physical therapeutic spaces.In keeping with other Autcraft research [44], we see here that administrators manage the rules and norms of these spaces through multiple venues. They use the mods as the primary infrastructure, but build upon that visible set of instructions and policies forabout the opening of these rooms, an administrator emphasized the importance of having the chat disabled in these rooms:The best part is that in these rooms, chat is disabled! You can still private message back and forth with people but the public chat will  be muted and you can't  talk into public chat  either. This means that you can experience the lights and the sounds and the calming nature of the rooms without a whole bunch of text flying across your screen. (forums, P29, age 30, m)3Other members also used these Sensory Rooms to take a break from being in the public chat, which can be helpful when trying to self-regulate exposure to chat conversations:This really helped me today there was a trigger for some bad memories in chat and it calmed me down wish i could visit this in real life4. (forums, P31, age 15, f)Because these are virtual avatars, members are able to \u201ctransport\u201d themselves to these Sensory Rooms at any time. Thus, in the virtual world players are able to regulate input in real time, nearly instantly. This player, however, points out that you cannot simply transport yourself to calming spaces in the physical world. This ability to instantly transport oneself into an environment that helps in self-regulation creates an assistive technology space\u2014which is potentially better or used differently than in the physical world because it gives the player an ability and experience in the virtual world they might not otherwise have.The actual interface of the game can also be overwhelming at times for some members, particularly if there are a lot of people logged in or a particularly chatty group are talking in chat. When the text scrolls too quickly in chat and visually becomes over- stimulating, players can seek relief simply by transporting to the Sensory Room:i like going there when chat is going to fast and i need to take a break [really] calming and relaxing (forums, P4, f)enforcing behavior. The instructions in reaching the Sensory          \u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0Rooms say, \u201cNeed a place to calm down? Quiet? Peaceful? Choose a Calm Room to visit here. In these rooms [t]here is no chat. It\u2019s a place to relax. Visit any time.\u201d In an announcement3 Each quote includes: (source of quote, participant number, age of participant, and identified gender if available in member profile)4 Here participants use \u201creal life\u201d to indicate the physical, offline parts of their lives. Also, \u201cirl\u201d seen later means the same.Not only do players use the community-created spaces for sensory self-regulation, they also contribute to these spaces. One member posted in the forums, informing others he had created an instruction manual of how to use the Sensory Rooms:I think sensory rooms are a fantastic idea. And I added a book In calm room 1 its about what to do and about calm rooms (forums, P33, m)Much like in the example above, the Autcraft virtual world is being shaped by each of the players as they participate in community life. This support can also be seen as administrators, noticing the players creating the sensory holes described at the beginning of this section, and then responding by building these sensory rooms. Each player, through their own acts of appropriation within the virtual space, shape what their virtual world looks like and how it functions to assist them as they engage with the world.These spaces are also different from other virtual worlds in that they are specifically built for members with autism. Unlike other online communities, where adolescents socialize and \u201chang out,\u201d these platforms are being augmented for this specific population. Interestingly, despite sensitivity to sensory input, members interacting within the Autcraft community do not seem to have a problem with the overwhelming amount of choices given to them both within the Autcraft virtual world interface and throughout the various platforms the community uses. Community members are able to deal with a lot of the visual stimuli of the virtual world interface in spite of their SPD symptoms. In fact, members seem to be able to choose from the various options to create a social and sensory experience that feels right for them, giving them the opportunity to have the embodied experience they want\u2014 something that is more easily done in a virtual space than a physical one. This may be because Minecraft, although not a typical game with \u201clevels\u201d and other stated goals, follows a classic game-style genre, allowing the players familiarity as they navigate the world like they would in many other games with a typical, first person perspective.Dealing with sensory overload can be a difficult experience for anyone with autism, particularly for children and adolescents who are still learning coping skills. Members of the Autcraft community have created spaces within the virtual world and the other platforms to help even the youngest members learn to deal with these sensory needs. As in the example of the sensory holes, one player appropriated materials at hand (in this case, virtual dirt) and inspired others to modify the actual software of Autcraft to create similar experiences for everyone. Individual players appropriate the Autcraft virtual  world to suit their  own needs, shaping their virtual environment, embodied experience, and, in time, influencing the overall experience for everyone in Autcraft.", "local_uri": ["bd4768c197be8771302b302710715e5a8bb94960_Image_002.jpg", "bd4768c197be8771302b302710715e5a8bb94960_Image_003.jpg", "bd4768c197be8771302b302710715e5a8bb94960_Image_004.jpg", "bd4768c197be8771302b302710715e5a8bb94960_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Am I a Bunny?: The Impact of High and Low Immersion Platforms and Viewers' Perceptions of Role on Presence, Narrative Engagement, and Empathy during an Animated 360\u00b0 Video", "pdf_hash": "0dcc1d51bf1c27120f504bb6e0c863fe2910bcfb", "year": 2018, "venue": "CHI", "alt_text": "Participants in the High Immersion VR headset condition were more likely to say they were a Character in the experience. Participants in the Smartphone condition were more likely to say they were an Observer.", "levels": null, "corpus_id": 5067103, "sentences": ["Participants in the High Immersion VR headset condition were more likely to say they were a Character in the experience.", "Participants in the Smartphone condition were more likely to say they were an Observer."], "caption": "", "local_uri": ["0dcc1d51bf1c27120f504bb6e0c863fe2910bcfb_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Using Participatory Design with Proxies with Children with Limited Communication", "pdf_hash": "2d6a89aaf467f92371757d0a8ba63c0503cb8d80", "year": 2017, "venue": "ASSETS", "alt_text": "This picture shows a prototype of the system on the left. This prototype consists of a custom-designed container with a living mushroom colony growing out of it. On the right, there is a schematic of the system that shows its different components including,", "levels": null, "corpus_id": 10688454, "sentences": ["This picture shows a prototype of the system on the left.", "This prototype consists of a custom-designed container with a living mushroom colony growing out of it.", "On the right, there is a schematic of the system that shows its different components including,"], "caption": "Figure 1. Rafigh (Prototype 3): the physical prototype (left), schematic with component names (right)", "local_uri": ["2d6a89aaf467f92371757d0a8ba63c0503cb8d80_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Using Participatory Design with Proxies with Children with Limited Communication", "pdf_hash": "2d6a89aaf467f92371757d0a8ba63c0503cb8d80", "year": 2017, "venue": "ASSETS", "alt_text": "This image shows prototype 3 which consists of a bubbleblower toy connected to a microcontroller and LEDs.", "levels": null, "corpus_id": 10688454, "sentences": ["This image shows prototype 3 which consists of a bubbleblower toy connected to a microcontroller and LEDs."], "caption": "Figure 3. Prototype 1 with LED lights and enclosure (left) and with internal electronics exposed (right)", "local_uri": ["2d6a89aaf467f92371757d0a8ba63c0503cb8d80_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Using Participatory Design with Proxies with Children with Limited Communication", "pdf_hash": "2d6a89aaf467f92371757d0a8ba63c0503cb8d80", "year": 2017, "venue": "ASSETS", "alt_text": "This picture shows prototype 2 which consists of a container with mushrooms growing out of it. An iPad is placed in the side of the container.", "levels": null, "corpus_id": 10688454, "sentences": ["This picture shows prototype 2 which consists of a container with mushrooms growing out of it.", "An iPad is placed in the side of the container."], "caption": "Figure 4. Prototype 2 with mushrooms and iPad", "local_uri": ["2d6a89aaf467f92371757d0a8ba63c0503cb8d80_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Using Participatory Design with Proxies with Children with Limited Communication", "pdf_hash": "2d6a89aaf467f92371757d0a8ba63c0503cb8d80", "year": 2017, "venue": "ASSETS", "alt_text": "This picture shows prototype 3, placed on a table in front of a monitor. There is a setup for a task to be completed using a cutome-make switch (also placed on the table).", "levels": null, "corpus_id": 10688454, "sentences": ["This picture shows prototype 3, placed on a table in front of a monitor.", "There is a setup for a task to be completed using a cutome-make switch (also placed on the table)."], "caption": "Figure 5. Prototype 3 in the school setting, with custom made switch and projected computer display.", "local_uri": ["2d6a89aaf467f92371757d0a8ba63c0503cb8d80_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "SwarmHaptics: Haptic Display with Swarm Robots", "pdf_hash": "34800feafee8d948615d7fc32dd8f13b2bb8ee97", "year": 2019, "venue": "CHI", "alt_text": "This is a teaser figure for SwarmHaptics. It demonstrates different use cases of haptic display with swarm robots.", "levels": null, "corpus_id": 85463044, "sentences": ["This is a teaser figure for SwarmHaptics.", "It demonstrates different use cases of haptic display with swarm robots."], "caption": "Figure 1: SwarmHaptics uses a swarm of robots to display various haptics patterns to diferent body parts that are on a surface or through external objects such as a mouse. It can be used to convey notifcations, social touch, directional cues, etc.", "local_uri": ["34800feafee8d948615d7fc32dd8f13b2bb8ee97_Image_001.png"], "annotated": false, "compound": false}
{"title": "SwarmHaptics: Haptic Display with Swarm Robots", "pdf_hash": "34800feafee8d948615d7fc32dd8f13b2bb8ee97", "year": 2019, "venue": "CHI", "alt_text": "This shows another example scenario: Remote social touch. On the left, it shows a control interface where you can draw different patterns and on the right, it shows the output where the robots move according to the drawn pattern.", "levels": null, "corpus_id": 85463044, "sentences": ["This shows another example scenario: Remote social touch.", "On the left, it shows a control interface where you can draw different patterns and on the right, it shows the output where the robots move according to the drawn pattern."], "caption": "", "local_uri": ["34800feafee8d948615d7fc32dd8f13b2bb8ee97_Image_014.png"], "annotated": false, "compound": false}
{"title": "SwarmHaptics: Haptic Display with Swarm Robots", "pdf_hash": "34800feafee8d948615d7fc32dd8f13b2bb8ee97", "year": 2019, "venue": "CHI", "alt_text": "On the left, it shows the normal force evaluation for the robots with PWM duty cycle in the x-axis and measured force on the y-axis. The max force is about 1.1 N.   On the right, it shows the perception study setup. There is a participant wearing a wristband for tracking and noise-cancelling headphone. Seven robots line up prior providing the haptic stimuli", "levels": null, "corpus_id": 85463044, "sentences": ["On the left, it shows the normal force evaluation for the robots with PWM duty cycle in the x-axis and measured force on the y-axis.", "The max force is about 1.1 N.   On the right, it shows the perception study setup.", "There is a participant wearing a wristband for tracking and noise-cancelling headphone.", "Seven robots line up prior providing the haptic stimuli"], "caption": "Figure 6: Remote Social Touch: Diferent haptic patterns can be drawn on a touch screen to convey remote social touch.", "local_uri": ["34800feafee8d948615d7fc32dd8f13b2bb8ee97_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Error related negativity in observing interactive tasks", "pdf_hash": "25e5f2ff3df3d731bbdbcd8485a45fe3658efd7f", "year": 2014, "venue": "CHI", "alt_text": "Topographic distribution of correct trials (top) and incorrect trials (bottom) in the \u2018far\u2019 layout at the intervals: 50ms before key press (left), key press (middle), and 50ms after key press (right).", "levels": [[1]], "corpus_id": 14048661, "sentences": ["Topographic distribution of correct trials (top) and incorrect trials (bottom) in the \u2018far\u2019 layout at the intervals: 50ms before key press (left), key press (middle), and 50ms after key press (right)."], "caption": "Figure 9. Topographic distribution of correct trials (top) and incorrect trials (bottom) in the \u2018far\u2019 layout at the intervals: 50ms before key press (left), key press (middle), and 50ms after key press (right).", "local_uri": ["25e5f2ff3df3d731bbdbcd8485a45fe3658efd7f_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Error related negativity in observing interactive tasks", "pdf_hash": "25e5f2ff3df3d731bbdbcd8485a45fe3658efd7f", "year": 2014, "venue": "CHI", "alt_text": "Average signals of the far layout at F3. The left vertical line is the averaged hand lift-off moments, the right vertical line is the button touched moments", "levels": [[1], [1]], "corpus_id": 14048661, "sentences": ["Average signals of the far layout at F3.", "The left vertical line is the averaged hand lift-off moments, the right vertical line is the button touched moments"], "caption": "Figure 13. Average signals of the far layout at F3. The left vertical line is the averaged hand lift-off moments, the right vertical line is the button touched moments.", "local_uri": ["25e5f2ff3df3d731bbdbcd8485a45fe3658efd7f_Image_017.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Sensation: Measuring the Effects of a Human-to-Human Social Touch Based Controller on the Player Experience", "pdf_hash": "69f71aac3c08ab6402379f0f346448affad5b155", "year": 2016, "venue": "CHI", "alt_text": "Gaming  environment which shows to player standinf face-to-face positions. There are two displays standing back-to-back between them. Figure includes a computer and the Sensation devices which attached to one of the players' arm.", "levels": null, "corpus_id": 11968800, "sentences": ["Gaming  environment which shows to player standinf face-to-face positions.", "There are two displays standing back-to-back between them.", "Figure includes a computer and the Sensation devices which attached to one of the players' arm."], "caption": "Figure 1: Gaming environment designed for the Sensation immersion is more likely to be achieved through human- human interaction.", "local_uri": ["69f71aac3c08ab6402379f0f346448affad5b155_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Sensation: Measuring the Effects of a Human-to-Human Social Touch Based Controller on the Player Experience", "pdf_hash": "69f71aac3c08ab6402379f0f346448affad5b155", "year": 2016, "venue": "CHI", "alt_text": "Figure shows four touch patterns consecutively. First, one finger touch which shows two players connect their index fingers. Second, brofist which is formed by two fists came together. Third, palm touch which requires both players to unite their whole palm. Forth is the four finger touch which is similar to one finger touch with difference of utilization of four fingers.", "levels": null, "corpus_id": 11968800, "sentences": ["Figure shows four touch patterns consecutively.", "First, one finger touch which shows two players connect their index fingers.", "Second, brofist which is formed by two fists came together.", "Third, palm touch which requires both players to unite their whole palm.", "Forth is the four finger touch which is similar to one finger touch with difference of utilization of four fingers."], "caption": "Figure 2: 1) 1-Finger 2) Bro-fist 3) Palm Touch 4) 4-Finger", "local_uri": ["69f71aac3c08ab6402379f0f346448affad5b155_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Sensation: Measuring the Effects of a Human-to-Human Social Touch Based Controller on the Player Experience", "pdf_hash": "69f71aac3c08ab6402379f0f346448affad5b155", "year": 2016, "venue": "CHI", "alt_text": "2-step Gameplay loop for Shape Destroy:    1) Only 1st player sees the shape and both players perform the required touch pattern. 2nd player needs to guess the touch pattern by looking at the hand of the 1st player.     2) Only 2nd player sees the shape and both players perform the required touch pattern. 1st player need to guess the touch pattern by looking at the hand of the 2nd player.", "levels": null, "corpus_id": 11968800, "sentences": ["2-step Gameplay loop for Shape Destroy:    1) Only 1st player sees the shape and both players perform the required touch pattern.", "2nd player needs to guess the touch pattern by looking at the hand of the 1st player.", "2) Only 2nd player sees the shape and both players perform the required touch pattern.", "1st player need to guess the touch pattern by looking at the hand of the 2nd player."], "caption": "Figure 3: 2-step Gameplay loop for Shape Destroy", "local_uri": ["69f71aac3c08ab6402379f0f346448affad5b155_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Sensation: Measuring the Effects of a Human-to-Human Social Touch Based Controller on the Player Experience", "pdf_hash": "69f71aac3c08ab6402379f0f346448affad5b155", "year": 2016, "venue": "CHI", "alt_text": "A diagram which shows the work flow of the sensation. Work flow of the sensation is explained in \"Technical Information\" section in detail.", "levels": null, "corpus_id": 11968800, "sentences": ["A diagram which shows the work flow of the sensation.", "Work flow of the sensation is explained in \"Technical Information\" section in detail."], "caption": "Figure 4: Sensation and its communication with unity.", "local_uri": ["69f71aac3c08ab6402379f0f346448affad5b155_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Sensation: Measuring the Effects of a Human-to-Human Social Touch Based Controller on the Player Experience", "pdf_hash": "69f71aac3c08ab6402379f0f346448affad5b155", "year": 2016, "venue": "CHI", "alt_text": "1) fill out the Immersive Tendency Questionnaire (ITQ), 2) play the game with Gamepad (A)/Sensation (B), 3) fill out the Immersive Experience Questionnaire for the 1st gameplay session, 4) play the game with Sensation (A)/Gamepad (B), 5) fill out the Immersive Experience Questionnaire for the 2nd gameplay session and 6) fill out the Experience Comparison", "levels": null, "corpus_id": 11968800, "sentences": ["1) fill out the Immersive Tendency Questionnaire (ITQ), 2) play the game with Gamepad (A)/Sensation (B), 3) fill out the Immersive Experience Questionnaire for the 1st gameplay session, 4) play the game with Sensation (A)/Gamepad (B), 5) fill out the Immersive Experience Questionnaire for the 2nd gameplay session and 6) fill out the Experience Comparison"], "caption": "", "local_uri": ["69f71aac3c08ab6402379f0f346448affad5b155_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Sensation: Measuring the Effects of a Human-to-Human Social Touch Based Controller on the Player Experience", "pdf_hash": "69f71aac3c08ab6402379f0f346448affad5b155", "year": 2016, "venue": "CHI", "alt_text": "Two pictures which show the user study process. The figure demonstrates a moment from the user studies. Two players play the game in the experiment setting with gamepad first and with the Sensation second.", "levels": null, "corpus_id": 11968800, "sentences": ["Two pictures which show the user study process.", "The figure demonstrates a moment from the user studies.", "Two players play the game in the experiment setting with gamepad first and with the Sensation second."], "caption": "Figure 6: 1) Gameplay session with a gamepad", "local_uri": ["69f71aac3c08ab6402379f0f346448affad5b155_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Never too old: engaging retired people inventing the future with MaKey MaKey", "pdf_hash": "1a5595724dd7d108750f246aa09006ef0773f975", "year": 2014, "venue": "CHI", "alt_text": "Figure 1. MaKey Makey components: board, alligator clips with plastic sheaths and USB cable to connect to computer", "levels": null, "corpus_id": 17731985, "sentences": ["Figure 1. MaKey Makey components: board, alligator clips with plastic sheaths and USB cable to connect to computer"], "caption": "Figure 1. MaKey MaKey components: board, alligator clips with plastic sheaths and USB cable to connect to computer", "local_uri": ["1a5595724dd7d108750f246aa09006ef0773f975_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Never too old: engaging retired people inventing the future with MaKey MaKey", "pdf_hash": "1a5595724dd7d108750f246aa09006ef0773f975", "year": 2014, "venue": "CHI", "alt_text": "Figure 3. Attaching the MaKey MaKey alligator clips to pieces of fruit and vegetables, the circuit board and the laptop", "levels": null, "corpus_id": 17731985, "sentences": ["Figure 3.", "Attaching the MaKey MaKey alligator clips to pieces of fruit and vegetables, the circuit board and the laptop"], "caption": "Figure 3. Attaching the MaKey MaKey alligator clips to pieces of fruit and vegetables, the circuit board and the laptop", "local_uri": ["1a5595724dd7d108750f246aa09006ef0773f975_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Never too old: engaging retired people inventing the future with MaKey MaKey", "pdf_hash": "1a5595724dd7d108750f246aa09006ef0773f975", "year": 2014, "venue": "CHI", "alt_text": "Figure 4. Group 6 holding hands and touching each others arms and each tapping a fruit/vegetable input to play music together", "levels": null, "corpus_id": 17731985, "sentences": ["Figure 4.", "Group 6 holding hands and touching each others arms and each tapping a fruit/vegetable input to play music together"], "caption": "Figure 4. Group 6 holding and touching each other and each tapping a fruit/vegetable input to play music together", "local_uri": ["1a5595724dd7d108750f246aa09006ef0773f975_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Never too old: engaging retired people inventing the future with MaKey MaKey", "pdf_hash": "1a5595724dd7d108750f246aa09006ef0773f975", "year": 2014, "venue": "CHI", "alt_text": "Figure 5.  The fruit and vegetables all lined up to match the notes of the piano keyboard", "levels": null, "corpus_id": 17731985, "sentences": ["Figure 5.", "The fruit and vegetables all lined up to match the notes of the piano keyboard"], "caption": "Figure 5. The fruit and vegetables all lined up to match the notes of the piano keyboard", "local_uri": ["1a5595724dd7d108750f246aa09006ef0773f975_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Broken display = broken interface': the impact of display damage on smartphone interaction", "pdf_hash": "6e03644ab2f3892b968a0122de11f985c6cdba27", "year": 2014, "venue": "CHI", "alt_text": "Figure 3 shows a rectified image of a damaged smartphone display in multiple stages of processing and image analysis. First only the photo, then photo with colored annotations, then only the colored annotations, and last a topology map of the visible and occluded display areas.", "levels": null, "corpus_id": 17182221, "sentences": ["Figure 3 shows a rectified image of a damaged smartphone display in multiple stages of processing and image analysis.", "First only the photo, then photo with colored annotations, then only the colored annotations, and last a topology map of the visible and occluded display areas."], "caption": "", "local_uri": ["6e03644ab2f3892b968a0122de11f985c6cdba27_Image_011.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_012.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_013.png", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_014.jpg"], "annotated": false, "compound": true}
{"title": "Broken display = broken interface': the impact of display damage on smartphone interaction", "pdf_hash": "6e03644ab2f3892b968a0122de11f985c6cdba27", "year": 2014, "venue": "CHI", "alt_text": "figure 4. Different damage types as used for the annotation of images. Scratches are superficial glass damage. Cracks constitute more severe glass damage. Spider webs are a pattern consisting of multiple cracks and scratches in close proximity. Screen issues can be discolorment, such as a dark spot, grayouts are areas that are partially occluded by screen failures. Blackouts are areas where the screen is not legible anymore.", "levels": null, "corpus_id": 17182221, "sentences": ["figure 4.", "Different damage types as used for the annotation of images.", "Scratches are superficial glass damage.", "Cracks constitute more severe glass damage.", "Spider webs are a pattern consisting of multiple cracks and scratches in close proximity.", "Screen issues can be discolorment, such as a dark spot, grayouts are areas that are partially occluded by screen failures.", "Blackouts are areas where the screen is not legible anymore."], "caption": "scratch   (b) crack    (c) spider      (d) color        (e) grayout   (f) blackout web               issue\u200c\u200c\u200c", "local_uri": ["6e03644ab2f3892b968a0122de11f985c6cdba27_Image_016.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_017.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_018.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_019.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_020.jpg", "6e03644ab2f3892b968a0122de11f985c6cdba27_Image_021.jpg"], "annotated": false, "compound": true}
{"title": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "pdf_hash": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "year": 2019, "venue": "CHI", "alt_text": "This figure shows a collection of touch offsets that are disordered on the left. On the right the touch offsets have been corrected using the Cluster Touch model.", "levels": null, "corpus_id": 140306385, "sentences": ["This figure shows a collection of touch offsets that are disordered on the left.", "On the right the touch offsets have been corrected using the Cluster Touch model."], "caption": "Figure 1: Cluster touch takes touch examples from individuals (left) and combines them with a user- independent model to create a user-specific touch model (right). Arrows convey corrective touch offsets by region.", "local_uri": ["2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_001.jpg", "2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_002.jpg"], "annotated": false, "compound": true}
{"title": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "pdf_hash": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "year": 2019, "venue": "CHI", "alt_text": "Figure 2    This figure depicts a touch offset by showing a touch that occurs below and to the right of the intended target.", "levels": null, "corpus_id": 140306385, "sentences": ["Figure 2    This figure depicts a touch offset by showing a touch that occurs below and to the right of the intended target."], "caption": "Figure 2: The difference between a user\u2019s touch location and the location of the intended target is a two- dimensional touch-offset vector.", "local_uri": ["2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "pdf_hash": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "year": 2019, "venue": "CHI", "alt_text": "This figure shows a participant with motor impairments interacting with the experiment testbed.", "levels": [[-1]], "corpus_id": 140306385, "sentences": ["This figure shows a participant with motor impairments interacting with the experiment testbed."], "caption": "Figure 3: A participant with motor impairments completes a target selection trial on the Google Nexus 6 smartphone.", "local_uri": ["2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "pdf_hash": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "year": 2019, "venue": "CHI", "alt_text": "Figure 4    This figure shows two heatmaps. The first is a heatmap of the percentage of touches that occur to the left or the right of their intended targets. The second is a heatmap of the percentage of touches that occur above or below their intended targets. Heatmaps are shown for users with motor impairments, and users without motor impairments who were walking and standing.", "levels": null, "corpus_id": 140306385, "sentences": ["Figure 4    This figure shows two heatmaps.", "The first is a heatmap of the percentage of touches that occur to the left or the right of their intended targets.", "The second is a heatmap of the percentage of touches that occur above or below their intended targets.", "Heatmaps are shown for users with motor impairments, and users without motor impairments who were walking and standing."], "caption": "Figure 4: Heat maps of percentages of touches that occurred to the left (yellow) or right (blue) of the intended crosshairs (top), and of touches that occurred below (yellow) or above (blue) the intended crosshairs (bottom). \u201cM.I.\u201d stands for users with motor impairments.", "local_uri": ["2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "pdf_hash": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "year": 2019, "venue": "CHI", "alt_text": "Figure 5    This figure shows two heatmaps. The first is a heatmap of the average x-offset error. The second is a heatmap of the average y-offset error. Heatmaps are shown for users with motor impairments, and users without motor impairments who were walking and standing.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 140306385, "sentences": ["Figure 5    This figure shows two heatmaps.", "The first is a heatmap of the average x-offset error.", "The second is a heatmap of the average y-offset error.", "Heatmaps are shown for users with motor impairments, and users without motor impairments who were walking and standing."], "caption": "Figure 5: Heat maps of the average x-offset error (top) and average y-offset error (bottom). Areas shown in red indicate more error. \u201cM.I.\u201d stands for users with motor impairments.", "local_uri": ["2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "pdf_hash": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "year": 2019, "venue": "CHI", "alt_text": "Figure 6  This figure shows two line graphs. The top graph shows the average binned offset along the x-axes of the smartphone screen and the location of clusters, three in total. The bottom graph shows the average binned offset along the y-axes of the smartphone screen and the location of the clusters, three in total.", "levels": [[1], [1], [1]], "corpus_id": 140306385, "sentences": ["Figure 6  This figure shows two line graphs.", "The top graph shows the average binned offset along the x-axes of the smartphone screen and the location of clusters, three in total.", "The bottom graph shows the average binned offset along the y-axes of the smartphone screen and the location of the clusters, three in total."], "caption": "\ud835\udc65", "local_uri": ["2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_016.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Note-taker 2.0: the next step toward enabling students who are legally blind to take notes in class", "pdf_hash": "1aabb40e2afea2ad042835504182fafdaf41dd90", "year": 2010, "venue": "ASSETS '10", "alt_text": "Photo showing a professor\u2019s-eye-view of the proof-of-concept Note-Taker in use by a student in a classroom.  The pan/tilt mechanism is clamped to the top, front edge of the student\u2019s desk, and the video camera is sitting on top of it, pointing toward the professor.  The Tablet PC display is laying flat on the top surface of the desk, and the student is bent forward, with a stylus in hand, viewing the video on the surface of the Tablet PC, and handwriting his notes with the stylus on the same.", "levels": null, "corpus_id": 16285492, "sentences": ["Photo showing a professor\u2019s-eye-view of the proof-of-concept Note-Taker in use by a student in a classroom.", "The pan/tilt mechanism is clamped to the top, front edge of the student\u2019s desk, and the video camera is sitting on top of it, pointing toward the professor.", "The Tablet PC display is laying flat on the top surface of the desk, and the student is bent forward, with a stylus in hand, viewing the video on the surface of the Tablet PC, and handwriting his notes with the stylus on the same."], "caption": "Figure 1: The proof-of-concept Note-Taker in use", "local_uri": ["1aabb40e2afea2ad042835504182fafdaf41dd90_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Note-taker 2.0: the next step toward enabling students who are legally blind to take notes in class", "pdf_hash": "1aabb40e2afea2ad042835504182fafdaf41dd90", "year": 2010, "venue": "ASSETS '10", "alt_text": "Screen capture of what is being displayed on the surface of the Tablet PC, as it is being used.  The left half of the display surface shows a digital notepad of horizontally ruled paper, with handwritten notes on it.  The upper portion of the right half of the display shows live video of a whiteboard at the front of the classroom, with writing on it.  Just below the video is a small rectangular window with up, down, left, and right buttons that can be tapped to move the camera.", "levels": null, "corpus_id": 16285492, "sentences": ["Screen capture of what is being displayed on the surface of the Tablet PC, as it is being used.", "The left half of the display surface shows a digital notepad of horizontally ruled paper, with handwritten notes on it.", "The upper portion of the right half of the display shows live video of a whiteboard at the front of the classroom, with writing on it.", "Just below the video is a small rectangular window with up, down, left, and right buttons that can be tapped to move the camera."], "caption": "Figure 2: The Proof-of-Concept Note-Taker user interface", "local_uri": ["1aabb40e2afea2ad042835504182fafdaf41dd90_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Note-taker 2.0: the next step toward enabling students who are legally blind to take notes in class", "pdf_hash": "1aabb40e2afea2ad042835504182fafdaf41dd90", "year": 2010, "venue": "ASSETS '10", "alt_text": "Photograph of the Note-Taker 2.0 PTZ camera.  It consists of a black plastic box about 3 cm high, 6 cm wide, and 8 cm long that lies flat on the surface of a desktop.  Extending upward from the top surface of that box is a black plastic swiveling mechanism that can tilt the aim of the camera upward and downward.  On top of tilt mechanism is a black cylindrical camera housing, that is currently pointing in a roughly horizontal direction.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 16285492, "sentences": ["Photograph of the Note-Taker 2.0 PTZ camera.", "It consists of a black plastic box about 3 cm high, 6 cm wide, and 8 cm long that lies flat on the surface of a desktop.", "Extending upward from the top surface of that box is a black plastic swiveling mechanism that can tilt the aim of the camera upward and downward.", "On top of tilt mechanism is a black cylindrical camera housing, that is currently pointing in a roughly horizontal direction."], "caption": "Figure 3: The Note-Taker 2.0 PTZ Camera", "local_uri": ["1aabb40e2afea2ad042835504182fafdaf41dd90_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Note-taker 2.0: the next step toward enabling students who are legally blind to take notes in class", "pdf_hash": "1aabb40e2afea2ad042835504182fafdaf41dd90", "year": 2010, "venue": "ASSETS '10", "alt_text": "Photograph with a \u201cstudent\u2019s eye view\u201d of the Note-taker, after it has been set up on the student\u2019s desk in the classroom.  The Tablet PC is lying directly in front of the student, with its display surface facing the ceiling. This display surface is oriented on the desktop in portrait mode, allowing the video to be displayed in the \u201cupper\u201d half of the display surface (as the student is looking downward at the display surface) and the digital notepad is displayed in the \u201clower\u201d half of the surface.", "levels": [[-1], [-1], [-1]], "corpus_id": 16285492, "sentences": ["Photograph with a \u201cstudent\u2019s eye view\u201d of the Note-taker, after it has been set up on the student\u2019s desk in the classroom.", "The Tablet PC is lying directly in front of the student, with its display surface facing the ceiling.", "This display surface is oriented on the desktop in portrait mode, allowing the video to be displayed in the \u201cupper\u201d half of the display surface (as the student is looking downward at the display surface) and the digital notepad is displayed in the \u201clower\u201d half of the surface."], "caption": "Figure 4: The Note-Taker 2.0 Prototype in Operation", "local_uri": ["1aabb40e2afea2ad042835504182fafdaf41dd90_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Note-taker 2.0: the next step toward enabling students who are legally blind to take notes in class", "pdf_hash": "1aabb40e2afea2ad042835504182fafdaf41dd90", "year": 2010, "venue": "ASSETS '10", "alt_text": "Screen capture of the Note-Taker 2.0 software interface. In the \"top half\" there is a zoom-in, contrast-enhanced view of an equation on what is presumably a blackboard. In the \"bottom half\" there is a digital notepad with a considerable amount of bulleted, handwritten notes.", "levels": null, "corpus_id": 16285492, "sentences": ["Screen capture of the Note-Taker 2.0 software interface.", "In the \"top half\" there is a zoom-in, contrast-enhanced view of an equation on what is presumably a blackboard.", "In the \"bottom half\" there is a digital notepad with a considerable amount of bulleted, handwritten notes."], "caption": "Figure 5. An example of histogram equalization and inversion", "local_uri": ["1aabb40e2afea2ad042835504182fafdaf41dd90_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Note-taker 2.0: the next step toward enabling students who are legally blind to take notes in class", "pdf_hash": "1aabb40e2afea2ad042835504182fafdaf41dd90", "year": 2010, "venue": "ASSETS '10", "alt_text": "Rendering of the Note-Taker 3.0 PTZ Camera. It has an aesthetic white and orange color scheme with a greater height than width.", "levels": null, "corpus_id": 16285492, "sentences": ["Rendering of the Note-Taker 3.0 PTZ Camera.", "It has an aesthetic white and orange color scheme with a greater height than width."], "caption": "", "local_uri": ["1aabb40e2afea2ad042835504182fafdaf41dd90_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Digital Strategies for Supporting Strengths- and Interests-based Learning with Children with Autism", "pdf_hash": "67bcc8f65e4a1535826f0c1c6235bcd017e77cdf", "year": 2017, "venue": "ASSETS", "alt_text": "Fig 1a. A screenshot featuring the Home screen of MeCalendar. The child has chosen a blue background and there is a circular photograph of the child in the centre of the screen. Below this are three other circles, one is a plus sign through which you add a new photo or video, one is an image of a calendar which takes you to your main calendar layout and the other is an image of letters and numbers which leads to the skill set area    Fig 1b. This is an image of an entry in the child's calendar which features a large photo to the right which has just been added by the child and tiles down the left-hand side which show other photos which have been taken on this day. The main photo is of two guinea pigs.    Fig 1c. This is an image of the calendar layout, featuring the month of October 2016. The child has populated the calendar with entries which are available at a glance in photo tile format.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 24410246, "sentences": ["Fig 1a.", "A screenshot featuring the Home screen of MeCalendar.", "The child has chosen a blue background and there is a circular photograph of the child in the centre of the screen.", "Below this are three other circles, one is a plus sign through which you add a new photo or video, one is an image of a calendar which takes you to your main calendar layout and the other is an image of letters and numbers which leads to the skill set area    Fig 1b.", "This is an image of an entry in the child's calendar which features a large photo to the right which has just been added by the child and tiles down the left-hand side which show other photos which have been taken on this day.", "The main photo is of two guinea pigs.", "Fig 1c.", "This is an image of the calendar layout, featuring the month of October 2016.", "The child has populated the calendar with entries which are available at a glance in photo tile format."], "caption": "", "local_uri": ["67bcc8f65e4a1535826f0c1c6235bcd017e77cdf_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Digital Strategies for Supporting Strengths- and Interests-based Learning with Children with Autism", "pdf_hash": "67bcc8f65e4a1535826f0c1c6235bcd017e77cdf", "year": 2017, "venue": "ASSETS", "alt_text": "This is an image of a calendar entry which features child sitting at his desk in class, focussing on his school work. There are two audio reordings of the teacher's praise attached to this entry.", "levels": null, "corpus_id": 24410246, "sentences": ["This is an image of a calendar entry which features child sitting at his desk in class, focussing on his school work.", "There are two audio reordings of the teacher's praise attached to this entry."], "caption": "Figure 2. An example of modelling - a child working at his desk", "local_uri": ["67bcc8f65e4a1535826f0c1c6235bcd017e77cdf_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Digital Strategies for Supporting Strengths- and Interests-based Learning with Children with Autism", "pdf_hash": "67bcc8f65e4a1535826f0c1c6235bcd017e77cdf", "year": 2017, "venue": "ASSETS", "alt_text": "This is an image of a classroom in which a child is standing at the front of the class beside the teacher who is bending forward to look at his iPad. The entry from his MeCalendar is shown behind him on the interactive whiteboard. He is disucssing his singing for Show and Share.", "levels": null, "corpus_id": 24410246, "sentences": ["This is an image of a classroom in which a child is standing at the front of the class beside the teacher who is bending forward to look at his iPad.", "The entry from his MeCalendar is shown behind him on the interactive whiteboard.", "He is disucssing his singing for Show and Share."], "caption": "Figure 3. Mikey breaking his routine in order to take part in Show and Share", "local_uri": ["67bcc8f65e4a1535826f0c1c6235bcd017e77cdf_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "EnhancedTouchX: Smart Bracelets for Augmenting Interpersonal Touch Interactions", "pdf_hash": "2d2eb73a83272e6dcdeebcff550c4c4e8e009905", "year": 2019, "venue": "CHI", "alt_text": "EnhancedTouchGo: An augmented reality game to facilitate interpersonal touch interactions.    EnhancedTouchPlay: A playware utilizing interpersonal touch interactions using real-time visual and vibration feedback according to the measured mode of touch.", "levels": null, "corpus_id": 140225404, "sentences": ["EnhancedTouchGo: An augmented reality game to facilitate interpersonal touch interactions.", "EnhancedTouchPlay: A playware utilizing interpersonal touch interactions using real-time visual and vibration feedback according to the measured mode of touch."], "caption": "", "local_uri": ["2d2eb73a83272e6dcdeebcff550c4c4e8e009905_Image_018.jpg", "2d2eb73a83272e6dcdeebcff550c4c4e8e009905_Image_019.jpg"], "annotated": false, "compound": true}
{"title": "Improving Smartphone Accessibility with Personalizable Static Overlays", "pdf_hash": "f253bf831ce7855ef7c0d234d35515074736aaca", "year": 2017, "venue": "ASSETS", "alt_text": "The image displayes a sketch of a smartphone. The screen is divided in two half with the top half tagged static area and the bottom half navigation area. The top half has a table with three rows. The first row has two cells, the first one is the button navigation reset, the second is order. The second row also has two cells, the first one is the button Apps, the second is Contacts. The last row has three cell.  The first  ocupies half the row and is the button Add Favorite. The other half is split by the two cells. First the Set and second the button with a minus signal representing the minimize function.", "levels": null, "corpus_id": 23751849, "sentences": ["The image displayes a sketch of a smartphone.", "The screen is divided in two half with the top half tagged static area and the bottom half navigation area.", "The top half has a table with three rows.", "The first row has two cells, the first one is the button navigation reset, the second is order.", "The second row also has two cells, the first one is the button Apps, the second is Contacts.", "The last row has three cell.", "The first  ocupies half the row and is the button Add Favorite.", "The other half is split by the two cells.", "First the Set and second the button with a minus signal representing the minimize function."], "caption": "Figure 1 - Default template for PSI.", "local_uri": ["f253bf831ce7855ef7c0d234d35515074736aaca_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "An iPad showing an abstract 2D Landscape with a rectangular frame in the middle and a small red rectangle located in the upper left cornor of the 2D landscape. On the frame there are four strippes of infared reflecting tape for the tracking system.", "levels": null, "corpus_id": 5543753, "sentences": ["An iPad showing an abstract 2D Landscape with a rectangular frame in the middle and a small red rectangle located in the upper left cornor of the 2D landscape.", "On the frame there are four strippes of infared reflecting tape for the tracking system."], "caption": "", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "An iPhone 4 showing the same scene as the iPad but far more zoomed out. The frame has the same size. On the iPhone sits a construction that has four marbel-like looking objects which consist of refelctive tape as well.", "levels": null, "corpus_id": 5543753, "sentences": ["An iPhone 4 showing the same scene as the iPad but far more zoomed out.", "The frame has the same size.", "On the iPhone sits a construction that has four marbel-like looking objects which consist of refelctive tape as well."], "caption": "(a) iPad(b) iPhoneFigure 1: We augmented an iPad and iPhone withIR reflective markers for the support of spatial tracking.", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "The composition of the task sequence illustrated with icons. To the far left all 128 rectangles are visible in the scene, which makes it quite colorful.", "levels": null, "corpus_id": 5543753, "sentences": ["The composition of the task sequence illustrated with icons.", "To the far left all 128 rectangles are visible in the scene, which makes it quite colorful."], "caption": "Figure 4: Basic composition rules for the task sequence", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "A bar diagram showing the factors navigation technique, gender and display size. The y axis ranges from 0 to 1000. The bar for the spatial navigation technique is about two thirds the size of the touch techiques . The bar of the male participants is about 4/5 the size of the female participants. The bar of the tablet is about the same hight of the male participants and similarly the bar of the phone is about the same hight as the bar of the female participants.", "levels": [[1], [1], [2], [2], [2]], "corpus_id": 5543753, "sentences": ["A bar diagram showing the factors navigation technique, gender and display size.", "The y axis ranges from 0 to 1000.", "The bar for the spatial navigation technique is about two thirds the size of the touch techiques .", "The bar of the male participants is about 4/5 the size of the female participants.", "The bar of the tablet is about the same hight of the male participants and similarly the bar of the phone is about the same hight as the bar of the female participants."], "caption": "Figure 5: Total completion times broken down by navigation technique, gender, and display size. Error bars denote standard deviations (95% CI).", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "Bar chart showing the diffrent compeltion time deppening on the visibility of the target. All numbers are in the text. In all cases the bars for visible tasks are much smaller than those for invisble.", "levels": [[1], [0], [2]], "corpus_id": 5543753, "sentences": ["Bar chart showing the diffrent compeltion time deppening on the visibility of the target.", "All numbers are in the text.", "In all cases the bars for visible tasks are much smaller than those for invisble."], "caption": "Figure 6: Average task completion times (gender-neutral).", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "A bar chart showing the average number of actions required for spatail conditon sperated by on and off screen targets as wel as for the phone and for the iPad. Likewise for the touch condition.", "levels": [[1], [1]], "corpus_id": 5543753, "sentences": ["A bar chart showing the average number of actions required for spatail conditon sperated by on and off screen targets as wel as for the phone and for the iPad.", "Likewise for the touch condition."], "caption": "", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "pdf_hash": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "year": 2014, "venue": "CHI", "alt_text": "The results of the questionaire shown as a bar chart. All bar look quiete similar except for ease of use, efficency to use, user experience and zooming.", "levels": [[1], [3]], "corpus_id": 5543753, "sentences": ["The results of the questionaire shown as a bar chart.", "All bar look quiete similar except for ease of use, efficency to use, user experience and zooming."], "caption": "Figure 8: Number of discrete actions (gender neutral).", "local_uri": ["f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Eyes-free yoga: an exergame using depth cameras for blind & low vision exercise", "pdf_hash": "e7635f15dd6753f7f0c3538b85f36d38da503ba5", "year": 2013, "venue": "ASSETS", "alt_text": "The participants needed to hear fewer corrections on average for each trial of the poses. Warrior Two Pose was only attempted once by all of the participants.", "levels": [[3], [3]], "corpus_id": 12667124, "sentences": ["The participants needed to hear fewer corrections on average for each trial of the poses.", "Warrior Two Pose was only attempted once by all of the participants."], "caption": "", "local_uri": ["e7635f15dd6753f7f0c3538b85f36d38da503ba5_Image_011.gif"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Eyes-free yoga: an exergame using depth cameras for blind & low vision exercise", "pdf_hash": "e7635f15dd6753f7f0c3538b85f36d38da503ba5", "year": 2013, "venue": "ASSETS", "alt_text": "Displays the number of participants who gave positive answers based on GameFlow.    Answers in order: 15, 10, 10, 9, 13, 15, 11", "levels": null, "corpus_id": 12667124, "sentences": ["Displays the number of participants who gave positive answers based on GameFlow.", "Answers in order: 15, 10, 10, 9, 13, 15, 11"], "caption": "How did you feel when trying to concen-trate on the game?", "local_uri": ["e7635f15dd6753f7f0c3538b85f36d38da503ba5_Image_012.gif"], "annotated": false, "compound": false}
{"title": "Incorporating Social Factors in Accessible Design", "pdf_hash": "eddd619545e1456affcf7064193c8a587ee09cf1", "year": 2018, "venue": "ASSETS", "alt_text": "Designer sits center and takes notes on users' brainstorm.  Users sit on either side of designer.", "levels": null, "corpus_id": 52941500, "sentences": ["Designer sits center and takes notes on users' brainstorm.", "Users sit on either side of designer."], "caption": "Figure 1. A professional designer (center) brainstorms with two users, one with a visual impairment (left) and one without (right).", "local_uri": ["eddd619545e1456affcf7064193c8a587ee09cf1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Incorporating Social Factors in Accessible Design", "pdf_hash": "eddd619545e1456affcf7064193c8a587ee09cf1", "year": 2018, "venue": "ASSETS", "alt_text": "Looking over the shoulder of D2, shows him aligning a strip of paper with a sketch on it.", "levels": null, "corpus_id": 52941500, "sentences": ["Looking over the shoulder of D2, shows him aligning a strip of paper with a sketch on it."], "caption": "Figure 2. D2 puts together a paper prototype.", "local_uri": ["eddd619545e1456affcf7064193c8a587ee09cf1_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Incorporating Social Factors in Accessible Design", "pdf_hash": "eddd619545e1456affcf7064193c8a587ee09cf1", "year": 2018, "venue": "ASSETS", "alt_text": "V5 holds out arm while other hand is poised over the wrist as if he were about to (or just did) touch the band on his wrist.", "levels": null, "corpus_id": 52941500, "sentences": ["V5 holds out arm while other hand is poised over the wrist as if he were about to (or just did) touch the band on his wrist."], "caption": "we then categorized by technologies. We defined a domain comprising issues uncovered in related work, i.e., users, technologies, places, and situations of use [30,31], and designed and piloted the prompt based on these constructs with graduate design and HCI students. We focused on an atypically inclusive environment where non-phone device or application would constitute a plausible solution.", "local_uri": ["eddd619545e1456affcf7064193c8a587ee09cf1_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Incorporating Social Factors in Accessible Design", "pdf_hash": "eddd619545e1456affcf7064193c8a587ee09cf1", "year": 2018, "venue": "ASSETS", "alt_text": "DSA framework: x-axis is social consideration; y-axis is functional consideration. Top row, left to right: Functionally usable (usable, useful, capable, etc.-- the definition of traditional accessibility); Socially Accessible (both functionally capable and usable in social situations). Bottom row, left to right: Inaccessible (neither functionally nor socially usable); Socially Usable (self-expressive, identity-enhancing, appealing, socially appropriate).", "levels": null, "corpus_id": 52941500, "sentences": ["DSA framework: x-axis is social consideration; y-axis is functional consideration.", "Top row, left to right: Functionally usable (usable, useful, capable, etc.-- the definition of traditional accessibility); Socially Accessible (both functionally capable and usable in social situations).", "Bottom row, left to right: Inaccessible (neither functionally nor socially usable); Socially Usable (self-expressive, identity-enhancing, appealing, socially appropriate)."], "caption": "", "local_uri": ["eddd619545e1456affcf7064193c8a587ee09cf1_Image_004.png"], "annotated": false, "compound": false}
{"title": "Incorporating Social Factors in Accessible Design", "pdf_hash": "eddd619545e1456affcf7064193c8a587ee09cf1", "year": 2018, "venue": "ASSETS", "alt_text": "Close up of board shows notes positioned within the framework (on white board).", "levels": null, "corpus_id": 52941500, "sentences": ["Close up of board shows notes positioned within the framework (on white board)."], "caption": "Figure 6. D1 uses the whiteboard to categorize ideas according to the Design for Social Accessibility framework.", "local_uri": ["eddd619545e1456affcf7064193c8a587ee09cf1_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Incorporating Social Factors in Accessible Design", "pdf_hash": "eddd619545e1456affcf7064193c8a587ee09cf1", "year": 2018, "venue": "ASSETS", "alt_text": "D3 stands in between two users, seated at table, while motioning with one hand over the other wrist. Both users watching D3.", "levels": null, "corpus_id": 52941500, "sentences": ["D3 stands in between two users, seated at table, while motioning with one hand over the other wrist.", "Both users watching D3."], "caption": "Figure 7. D3 confers with both users during brainstorming.", "local_uri": ["eddd619545e1456affcf7064193c8a587ee09cf1_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Improving social presence in human-agent interaction", "pdf_hash": "ce3fb22f6ccf8102e4f239bc205a5dab24441df5", "year": 2014, "venue": "CHI", "alt_text": "A social robot is positioned on one side of a digital multi-touch table and plays risk against three other players located on the other sides of the table.", "levels": null, "corpus_id": 11239330, "sentences": ["A social robot is positioned on one side of a digital multi-touch table and plays risk against three other players located on the other sides of the table."], "caption": "Figure 1. Risk case study", "local_uri": ["ce3fb22f6ccf8102e4f239bc205a5dab24441df5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound", "pdf_hash": "5ec58fd2b8a09a0bb14491e340a133a235e2f127", "year": 2019, "venue": "CHI", "alt_text": "graphic representing a 180 degree 'swing' (from left to right) of a radar arc around a person. A sound plats when the radar arm passes over an object on the radar sweep.", "levels": [[-1], [-1]], "corpus_id": 140280927, "sentences": ["graphic representing a 180 degree 'swing' (from left to right) of a radar arc around a person.", "A sound plats when the radar arm passes over an object on the radar sweep."], "caption": "Figure 1: A radar metaphor for directional stimuli", "local_uri": ["5ec58fd2b8a09a0bb14491e340a133a235e2f127_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound", "pdf_hash": "5ec58fd2b8a09a0bb14491e340a133a235e2f127", "year": 2019, "venue": "CHI", "alt_text": "Graphic illustrates four radar methods, Clockface (which has 7 delineations equally distributed across a 180 degree arc, that map to the points on a clock face, 9 oclock, 10 oclock, etc, starting at 9 oclock, on the left, round to 3 oclock on the right), Compass (which has 9 delineations equally distributed across a 180 degree radar arc. These points map roughly to the cardinal points on a compass, with West beginning at the left, moving through WNW, NW, NNW N etc. up to East on the right hand side of the radar arc. Scale, has the same number of delineations as compass, but replacing the compass points with musical notes stating with C on the left, moving up through a C major scale (C,D,E,F,G,A,B,C) to finish on a C note again, an octave higher on the right hand side of the radar arc. The final white noise condition is just a control condition, and the graphic just illustrates an arc with no delineations at all.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 140280927, "sentences": ["Graphic illustrates four radar methods, Clockface (which has 7 delineations equally distributed across a 180 degree arc, that map to the points on a clock face, 9 oclock, 10 oclock, etc, starting at 9 oclock, on the left, round to 3 oclock on the right), Compass (which has 9 delineations equally distributed across a 180 degree radar arc.", "These points map roughly to the cardinal points on a compass, with West beginning at the left, moving through WNW, NW, NNW N etc.", "up to East on the right hand side of the radar arc.", "Scale, has the same number of delineations as compass, but replacing the compass points with musical notes stating with C on the left, moving up through a C major scale (C,D,E,F,G,A,B,C) to finish on a C note again, an octave higher on the right hand side of the radar arc.", "The final white noise condition is just a control condition, and the graphic just illustrates an arc with no delineations at all."], "caption": "Figure 2. Four audio methods for delineating the passage of a radar arm for directional prompting. (a) clock face, (b) compass,", "local_uri": ["5ec58fd2b8a09a0bb14491e340a133a235e2f127_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound", "pdf_hash": "5ec58fd2b8a09a0bb14491e340a133a235e2f127", "year": 2019, "venue": "CHI", "alt_text": "response accuracy in degrees. (clock: -1.80\u00b0, compass: -0.58\u00b0, noise: 2.07\u00b0 scale: 2.12\u00b0))", "levels": null, "corpus_id": 140280927, "sentences": ["response accuracy in degrees. (clock: -1.80\u00b0, compass: -0.58\u00b0, noise: 2.07\u00b0 scale: 2.12\u00b0))"], "caption": "", "local_uri": ["5ec58fd2b8a09a0bb14491e340a133a235e2f127_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound", "pdf_hash": "5ec58fd2b8a09a0bb14491e340a133a235e2f127", "year": 2019, "venue": "CHI", "alt_text": "absolute response accuracy in degrees. Clockface 10.99, compass 9.89, white noise 12.97, scale 10.19", "levels": null, "corpus_id": 140280927, "sentences": ["absolute response accuracy in degrees.", "Clockface 10.99, compass 9.89, white noise 12.97, scale 10.19"], "caption": "", "local_uri": ["5ec58fd2b8a09a0bb14491e340a133a235e2f127_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound", "pdf_hash": "5ec58fd2b8a09a0bb14491e340a133a235e2f127", "year": 2019, "venue": "CHI", "alt_text": "figure illuatrates two distinct patterns, white noise and scale show an arc in performance, with worse response accuracy towards the centre of the radar sweep. clockface and compass methods show an M shape, with response accuracy improving slightly at the centre of the radar arc.", "levels": null, "corpus_id": 140280927, "sentences": ["figure illuatrates two distinct patterns, white noise and scale show an arc in performance, with worse response accuracy towards the centre of the radar sweep.", "clockface and compass methods show an M shape, with response accuracy improving slightly at the centre of the radar arc."], "caption": "Figure 8. Absolute response accuracy by location stimulus", "local_uri": ["5ec58fd2b8a09a0bb14491e340a133a235e2f127_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Substitutional Reality: Using the Physical Environment to Design Virtual Reality Experiences", "pdf_hash": "4fbeb21f5e8865df969f65801c3102abd283ae79", "year": 2015, "venue": "CHI", "alt_text": "In the picture each pair of objects exemplifying a different substitution type is shown. The replica mug used as baseline (a) then each pair of substitutive objects: (Aesthetic) glass (b) and wooden mug (c); hot (d) and icy mug (e); (Addition/Subtraction) big (f) and small mug (g); (Function) basket (h) and lamp (i); (Category) box (j) and sphere (k).", "levels": null, "corpus_id": 3338899, "sentences": ["In the picture each pair of objects exemplifying a different substitution type is shown.", "The replica mug used as baseline (a) then each pair of substitutive objects: (Aesthetic) glass (b) and wooden mug (c); hot (d) and icy mug (e); (Addition/Subtraction) big (f) and small mug (g); (Function) basket (h) and lamp (i); (Category) box (j) and sphere (k)."], "caption": "Figure 2. In the picture each pair of objects exemplifying a different substitution type is shown. The replica mug used as baseline (a) then each pair of substitutive objects: (Aesthetic) glass (b) and wooden mug (c); hot (d) and icy mug (e); (Addition/Subtraction) big (f) and small mug (g); (Function) basket (h) and lamp (i); (Category) box (j) and sphere (k).", "local_uri": ["4fbeb21f5e8865df969f65801c3102abd283ae79_Image_002.png"], "annotated": false, "compound": false}
{"title": "Empowering Families Facing English Literacy Challenges to Jointly Engage in Computer Programming", "pdf_hash": "ea704b68bc8030d2e9e988274bc6291f0c95e7b4", "year": 2018, "venue": "CHI", "alt_text": "A large rectangle in the middle of the screen forms the grid, where users assemble blocks, and demonstrate changes to these blocks in order to create rules.  This rectangle is flanked by narrow tall rectangles on the left and right, as well as a flat rectangle below. The large rectangle is flush with the top of the screen.  The rectangular area on the left displays the rules created by the user, while the one on the right shows the various screens set up by the user. The bottom rectangle contains a palette from which users may create new blocks, by dragging them onto the central rectangle.  The bottom-right corner has a single button, which runs the game.", "levels": null, "corpus_id": 3745210, "sentences": ["A large rectangle in the middle of the screen forms the grid, where users assemble blocks, and demonstrate changes to these blocks in order to create rules.", "This rectangle is flanked by narrow tall rectangles on the left and right, as well as a flat rectangle below.", "The large rectangle is flush with the top of the screen.", "The rectangular area on the left displays the rules created by the user, while the one on the right shows the various screens set up by the user.", "The bottom rectangle contains a palette from which users may create new blocks, by dragging them onto the central rectangle.", "The bottom-right corner has a single button, which runs the game."], "caption": "Figure 1: BlockStudio user interface (left) and main parts (right): Palette, Grid, Rules, Play button, and Screens.", "local_uri": ["ea704b68bc8030d2e9e988274bc6291f0c95e7b4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Empowering Families Facing English Literacy Challenges to Jointly Engage in Computer Programming", "pdf_hash": "ea704b68bc8030d2e9e988274bc6291f0c95e7b4", "year": 2018, "venue": "CHI", "alt_text": "The first image shows a spaceship on an empty background. At this point, the user clicks on the spaceship. The second image shows the result of clicking the spaceship. At this point, the game has paused in order to ask the user what should happen in response to clicking the spaceship. In addition, a grid has appeared behind the spaceship, to help with positioning, and a pair of buttons with a checkmark and cancel symbol are displayed on the left. The third image shows the spaceship being repositioned by the user, to a location that is vertically higher by five grid squares. At this point, the user clicks on the checkmark in order to indicate that they are done. The fourth image shows the spaceship on a black background again, with the grid, checkmark and cancel buttons hidden. At this point, the user has clicked the spaceship, and it now moves up by five grid squares in response.", "levels": null, "corpus_id": 3745210, "sentences": ["The first image shows a spaceship on an empty background.", "At this point, the user clicks on the spaceship.", "The second image shows the result of clicking the spaceship.", "At this point, the game has paused in order to ask the user what should happen in response to clicking the spaceship.", "In addition, a grid has appeared behind the spaceship, to help with positioning, and a pair of buttons with a checkmark and cancel symbol are displayed on the left.", "The third image shows the spaceship being repositioned by the user, to a location that is vertically higher by five grid squares.", "At this point, the user clicks on the checkmark in order to indicate that they are done.", "The fourth image shows the spaceship on a black background again, with the grid, checkmark and cancel buttons hidden.", "At this point, the user has clicked the spaceship, and it now moves up by five grid squares in response."], "caption": "Figure 2. How to create a rule in BlockStudio: (a) Click space- ship to trigger event, (b) Drag spaceship up to demonstrate the change, (c) Click \u2714 to end demonstration, (d) Clicking space- ship now moves it up. (See supplementary video for details)", "local_uri": ["ea704b68bc8030d2e9e988274bc6291f0c95e7b4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Empowering Families Facing English Literacy Challenges to Jointly Engage in Computer Programming", "pdf_hash": "ea704b68bc8030d2e9e988274bc6291f0c95e7b4", "year": 2018, "venue": "CHI", "alt_text": "There are four images, arranged as a pair per row. The top row shows the old interface, with locations that used text circled in red. The bottom row shows the new interface, which does not use text. Each row depicts the appearance of the interface in two different situations.", "levels": null, "corpus_id": 3745210, "sentences": ["There are four images, arranged as a pair per row.", "The top row shows the old interface, with locations that used text circled in red.", "The bottom row shows the new interface, which does not use text.", "Each row depicts the appearance of the interface in two different situations."], "caption": "Figure 3. The top row shows the prior system, with text circled in red. The bottom shows BlockStudio, without the text. The left shows block placement and the right shows rule creation.", "local_uri": ["ea704b68bc8030d2e9e988274bc6291f0c95e7b4_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Empowering Families Facing English Literacy Challenges to Jointly Engage in Computer Programming", "pdf_hash": "ea704b68bc8030d2e9e988274bc6291f0c95e7b4", "year": 2018, "venue": "CHI", "alt_text": "There are four photographs, arranged as a pair per row. Each image shows one family seated side-by-side in front of a laptop running our system.  The top left image shows a mother and her son. The mother is wearing a dark blue head scarf. The son has short curly hair. He is using his finger to point at a block on the laptop screen.  The top right image shows a different family, again a mother and her son. The mother is wearing a dark blue \"chador\", a variety of full-body shawl. The son is wearing a dark-colored jacket and his hair is very short. His left hand is on her right shoulder and he is turning to her and smiling.  The bottom left image shows a father and his two daughters. The father is wearing a white shirt and he is looking at the screen. His older daughter is next to him. She is wearing a pink shirt and has braided hair. She points at the screen with her finger, while her father uses the mouse to interact with the system. The younger daughter is next to her, wearing a polka-dotted dress. She is looking at the screen.  The bottom right image shows a mother and her daughter. The mother is wearing a light green jacket and a surgical mask, presumably to prevent spreading her cold. She is pointing at the screen. Her daughter is sitting next to her, wearing a black-and-white striped jacket and a pink baseball cap. The daughter is following along, using the mouse.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 3745210, "sentences": ["There are four photographs, arranged as a pair per row.", "Each image shows one family seated side-by-side in front of a laptop running our system.", "The top left image shows a mother and her son.", "The mother is wearing a dark blue head scarf.", "The son has short curly hair.", "He is using his finger to point at a block on the laptop screen.", "The top right image shows a different family, again a mother and her son.", "The mother is wearing a dark blue \"chador\", a variety of full-body shawl.", "The son is wearing a dark-colored jacket and his hair is very short.", "His left hand is on her right shoulder and he is turning to her and smiling.", "The bottom left image shows a father and his two daughters.", "The father is wearing a white shirt and he is looking at the screen.", "His older daughter is next to him.", "She is wearing a pink shirt and has braided hair.", "She points at the screen with her finger, while her father uses the mouse to interact with the system.", "The younger daughter is next to her, wearing a polka-dotted dress.", "She is looking at the screen.", "The bottom right image shows a mother and her daughter.", "The mother is wearing a light green jacket and a surgical mask, presumably to prevent spreading her cold.", "She is pointing at the screen.", "Her daughter is sitting next to her, wearing a black-and-white striped jacket and a pink baseball cap.", "The daughter is following along, using the mouse."], "caption": "Figure 4: (a) F1: Galad explaining a loop mechanic to his mother, (b) F3: Dalmar celebrating with his mother, (c) F5: Safiyo teaching her father how to create firing pattern, (d) F8: Falis guiding her daughter to create a rule (child does not understand English). Names are pseudonyms.", "local_uri": ["ea704b68bc8030d2e9e988274bc6291f0c95e7b4_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Empowering Families Facing English Literacy Challenges to Jointly Engage in Computer Programming", "pdf_hash": "ea704b68bc8030d2e9e988274bc6291f0c95e7b4", "year": 2018, "venue": "CHI", "alt_text": "There are four images showing screenshots of games that families created using our system.  The top-left shows a maze where the walls of the maze are comprised of various colored squares. Within this maze game, there is a smiley face near the bottom, and four arrows in the south-east corner, oriented up, down, left and right. The objective is to use the arrows to make the face navigate the maze.  The top-right shows a static arrangement of various colored square blocks, in a shape reminiscent of a church. It looks like a crucifix mounted atop an inverted U shape.  The bottom-left shows a rectangular region enclosed by blue squares and green dots, which are different kinds of blocks. There is a triangle inside this region, as well as other green dots. There are four arrows in the corner to control the triangle's motion. The objective is to move the triangle around and make it touch the green dots, which disappear when this happens.  The bottom-right shows a scene similar to the popular game called Flappy Bird. There is a white bird at the left, which must navigate a series of green vertical obstacles, left to right, by rising and falling. There is a blue rectangle at the right edge of the screen, and the player's goal is to have the bird reach this without touching the obstacles.", "levels": null, "corpus_id": 3745210, "sentences": ["There are four images showing screenshots of games that families created using our system.", "The top-left shows a maze where the walls of the maze are comprised of various colored squares.", "Within this maze game, there is a smiley face near the bottom, and four arrows in the south-east corner, oriented up, down, left and right.", "The objective is to use the arrows to make the face navigate the maze.", "The top-right shows a static arrangement of various colored square blocks, in a shape reminiscent of a church.", "It looks like a crucifix mounted atop an inverted U shape.", "The bottom-left shows a rectangular region enclosed by blue squares and green dots, which are different kinds of blocks.", "There is a triangle inside this region, as well as other green dots.", "There are four arrows in the corner to control the triangle's motion.", "The objective is to move the triangle around and make it touch the green dots, which disappear when this happens.", "The bottom-right shows a scene similar to the popular game called Flappy Bird. There is a white bird at the left, which must navigate a series of green vertical obstacles, left to right, by rising and falling.", "There is a blue rectangle at the right edge of the screen, and the player's goal is to have the bird reach this without touching the obstacles."], "caption": "Figure 5: (a) maze (b) static artifact (church), (c) game with green dots (collectibles) (d) clone of Flappy Bird\u2122", "local_uri": ["ea704b68bc8030d2e9e988274bc6291f0c95e7b4_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints", "pdf_hash": "87f5eb86ba8eb8ccd444326ff265e883f7c99948", "year": 2018, "venue": "CHI", "alt_text": "The planning process includes 5 steps: 1) Exercise guidelines overview (amount, balance, pattern, progression, compatibility), 2) Feature tour for making agood plan (e.g. a bar showing blaance of cardio and strength and explanation of it's meaning to the user), 3) Review client profile (e.g. goals for next week, long term goals, constraints, access, activities they like, dislike, or wnat to try), 4) schedule activities (a calendar showing the client schedule and exercise appointments, and information about calories burned if plan is followed, and strength and cardio distribution), 5) Plan is ready for client (overview of a plan by day, with activity name, time, duration, distribution of strength and cario, reasoning for why the activity matches the client, how to prepare for the activity, alternatives for the activity)", "levels": [[-1]], "corpus_id": 5040917, "sentences": ["The planning process includes 5 steps: 1) Exercise guidelines overview (amount, balance, pattern, progression, compatibility), 2) Feature tour for making agood plan (e.g. a bar showing blaance of cardio and strength and explanation of it's meaning to the user), 3) Review client profile (e.g. goals for next week, long term goals, constraints, access, activities they like, dislike, or wnat to try), 4) schedule activities (a calendar showing the client schedule and exercise appointments, and information about calories burned if plan is followed, and strength and cardio distribution), 5) Plan is ready for client (overview of a plan by day, with activity name, time, duration, distribution of strength and cario, reasoning for why the activity matches the client, how to prepare for the activity, alternatives for the activity)"], "caption": "Figure 1. In CrowdFit, planners schedule exercise activities following expert guidelines. Planners are first given a tour of the interactive system and presented with the guidelines, then review the client\u2019s profile, and schedule activities which fit within the client\u2019s schedule and match their preferences.", "local_uri": ["87f5eb86ba8eb8ccd444326ff265e883f7c99948_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints", "pdf_hash": "87f5eb86ba8eb8ccd444326ff265e883f7c99948", "year": 2018, "venue": "CHI", "alt_text": "Figure 2 contains a snapshot of the CrowdFit plannign page. The components shown on the page are (A) profile of client: Ray, Female, 29 years, 135 lb, Unemployed. Goals: Next week's goal: achieve more active days per week than sedentary, Long term goal: Become comfortable working out on a daily basis, Constraints: Limited access to workout facilities, Access to: Yoga mats, small weights, fitness ball, Activities: Likes: bodyweight exercises, can be done anywhere with limited resources. Interested in, but have not tried, (B) distribution of cardio and strength over the week: 32% cardio, 68% strength, (C) calories burnt if following the plan on the calendar: 649 calories of 536 to 1072 calories, (D) distribution of calories and strength-cardio per day: 7 day graph, showing around 150 calories of cardio on Monday, around 150 calories of mostly strength on Wednesday, about 400 calories of mostly strength on Friday, (E) client calendar with scheduled physical activities in green: e.g. swimming 9-10am on Sunday, (F) overview of last week\u2019s plan: Ray's plan was on Monday: Bodyweight workout for 45 min, on Tuesday Yoga for 45 min,  and (G) plysical activity the client performed: What Ray actually did: Monday bodyweight workout for 30 minutes - slightly shorter than taget, based on video length, on Tuesday walking 30 minutes I did not have time to go to yoga.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 5040917, "sentences": ["Figure 2 contains a snapshot of the CrowdFit plannign page.", "The components shown on the page are (A) profile of client: Ray, Female, 29 years, 135 lb, Unemployed.", "Goals: Next week's goal: achieve more active days per week than sedentary, Long term goal: Become comfortable working out on a daily basis, Constraints: Limited access to workout facilities, Access to: Yoga mats, small weights, fitness ball, Activities: Likes: bodyweight exercises, can be done anywhere with limited resources.", "Interested in, but have not tried, (B) distribution of cardio and strength over the week: 32% cardio, 68% strength, (C) calories burnt if following the plan on the calendar: 649 calories of 536 to 1072 calories, (D) distribution of calories and strength-cardio per day: 7 day graph, showing around 150 calories of cardio on Monday, around 150 calories of mostly strength on Wednesday, about 400 calories of mostly strength on Friday, (E) client calendar with scheduled physical activities in green: e.g. swimming 9-10am on Sunday, (F) overview of last week\u2019s plan: Ray's plan was on Monday: Bodyweight workout for 45 min, on Tuesday Yoga for 45 min,  and (G) plysical activity the client performed: What Ray actually did: Monday bodyweight workout for 30 minutes - slightly shorter than taget, based on video length, on Tuesday walking 30 minutes I did not have time to go to yoga."], "caption": "Figure 2. (A) profile of client, (B) distribution of cardio and strength over the week, (C) calories burnt if following the plan on the calendar, (D) distribution of calories and strength-cardio per day, (E) client calendar with scheduled physical activities in green, (F) overview of last week\u2019s plan and (G) plysical activity the client performed.", "local_uri": ["87f5eb86ba8eb8ccd444326ff265e883f7c99948_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Using an Interactive Avatar's Facial Expressiveness to Increase Persuasiveness and Socialness", "pdf_hash": "f935af40ed272c0e8d928a09674bf4dfae7484cc", "year": 2015, "venue": "CHI", "alt_text": "Fig. 1: On the left, a video frame shows the confederate opening her mouth with the AAM mesh overlaid on top to illustrate its tracking. On the right, the confederate's motion has been retargeted to the avatar.", "levels": null, "corpus_id": 17583203, "sentences": ["Fig. 1: On the left, a video frame shows the confederate opening her mouth with the AAM mesh overlaid on top to illustrate its tracking.", "On the right, the confederate's motion has been retargeted to the avatar."], "caption": "Figure 1. Example of the AAM tracking a confederate and retargeting her motion to the corresponding character.", "local_uri": ["f935af40ed272c0e8d928a09674bf4dfae7484cc_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Pose-Guided Level Design", "pdf_hash": "caba319c4419cf3ae1c6613f410133d883ef1ff0", "year": 2019, "venue": "CHI", "alt_text": "Bow Wave-L Lean-L,Kick-R Lean-L Kick-L Figure 3: Different types of poses for assembling a game levelfortheillustrativegame,JustExercise.", "levels": null, "corpus_id": 140263293, "sentences": ["Bow Wave-L Lean-L,Kick-R Lean-L Kick-L Figure 3: Different types of poses for assembling a game levelfortheillustrativegame,JustExercise."], "caption": "", "local_uri": ["caba319c4419cf3ae1c6613f410133d883ef1ff0_Image_009.png"], "annotated": false, "compound": false}
{"title": "Zebra Crossing Spotter: Automatic Population of Spatial Databases for Increased Safety of Blind Travelers", "pdf_hash": "c92082dfcf466cb7358094e5c09bb230abe19a3a", "year": 2015, "venue": "ASSETS", "alt_text": "Satellite image with line segments detected and the same image with crossing quadrilateral and direction.", "levels": null, "corpus_id": 3264188, "sentences": ["Satellite image with line segments detected and the same image with crossing quadrilateral and direction."], "caption": "Line segments              (b) CrossingFigure 4: Satellite detection stepsIf a set of stripes is not discarded, it is assumed to be a zebra crossing and it is characterized by its direction and po\u00ad sition. Its direction is easily derived as the angle of the line perpendicular to the stripes, which should all share nearly the same angle, due to the parallelism criterion (Line 8). The zebra crossing\u2019s position is represented as the quadri\u00ad lateral bounding the detected set of stripes, as depicted in Figure 4(b).Finally, the detected zebra crossing is added to the set of results Z. As mentioned above, the same zebra crossing may be recognized in two or more di\ufb00erent images. Hence, when inserting z in Z (Line 12), we \ufb01rst check if Z already contains a zebra crossing with approximately the same position and direction as z. If a similar crossing is found, the two crossings are merged.", "local_uri": ["c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_026.jpg", "c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_027.jpg"], "annotated": false, "compound": true}
{"title": "Zebra Crossing Spotter: Automatic Population of Spatial Databases for Increased Safety of Blind Travelers", "pdf_hash": "c92082dfcf466cb7358094e5c09bb230abe19a3a", "year": 2015, "venue": "ASSETS", "alt_text": "Reconstructed street view from above and the satellite image showing that the crossing detected from satellite image and the one from street view image do overlap but not completely.", "levels": [[-1]], "corpus_id": 3264188, "sentences": ["Reconstructed street view from above and the satellite image showing that the crossing detected from satellite image and the one from street view image do overlap but not completely."], "caption": "Satellite detection            (b) Street view detection     (c) Reconstructed street view      (d) imprecision in detectionFigure 6: Imprecision in GPS coordinates between satellite and street view detected crossingsA total of 141 zebra crossings and 152 transverse pedes\u00ad trian crossings have been detected in the satellite and street view images of the area by a human operator. In the follow\u00ad ing, a zebra crossing is considered to be detected correctly if at least 4 consecutive stripes of the crossing have been detected correctly.\u00d7\u00d7\u00d7Concerning satellite images, with the maximum zoom level available in Google maps for the considered area, each im\u00ad age with maximum resolution of 640 640 pixels covers 38m 38m. Thus, a total of 1149 satellite images are re\u00ad quired to cover A. Since the size of each image is approx\u00ad imately 46KB, the size of all images covering A is 52MB. The total number of street view panoramas available in A is 1425. As we show in the following, we acquired only a small portion of these panoramas, each having a resolution of 640 640 pixels and, on average, a size of 51KB.The tests were conducted on a laptop computer with Intelcore i7 4500u 1.8GHz CPU and 8GB RAM.", "local_uri": ["c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_035.jpg", "c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_036.jpg"], "annotated": false, "compound": true}
{"title": "Zebra Crossing Spotter: Automatic Population of Spatial Databases for Increased Safety of Blind Travelers", "pdf_hash": "c92082dfcf466cb7358094e5c09bb230abe19a3a", "year": 2015, "venue": "ASSETS", "alt_text": "Examples of false positives and false negatives in satellite and street view images.", "levels": [[-1]], "corpus_id": 3264188, "sentences": ["Examples of false positives and false negatives in satellite and street view images."], "caption": "FN - discoloration   (b) FN - hidden by trees    (c) FP - roof pattern                             (d) FP - stairs patternFigure 7: False negatives (FN) and false positives (FP) in satellite detection and street view validationSatellite detection Whole recognition10.95Precision0.90.850.80.750.70.650.7     0.75      0.8      0.85      0.9      0.95RecallFigure 8: Pareto frontier of the detection procedure\u2248nique requires acquiring and processing a total of 406 street view panoramas for A ( 2 for each candidate crossing). The total size of these images is 20.3MB.Cumulative distribution10.90.80.70.60.50.40.30.20.100     1     2     3     4     5     6     7     8Panoramas required for validation (actual crossings only)Figure 9: Used panoramas cumulative distributionThe street view-based validation (tuned for the best recall score) \ufb01lters out 58 out of 62 false positives identi\ufb01ed in the previous step, yielding a precision score of 0.97. The few false positives still present are caused by patterns very similar to zebra crossings, like the stairs in Figure 7(d). Of 137 true positives in Z, 134 are validated, resulting in a recall score of 0.98. Overall, the recall score of the whole procedure (including both satellite and street view detection) is 0.95.As with the satellite detection, di\ufb00erent parameter set\u00ad tings yield di\ufb00erent precision and recall scores during the validation. Figure 8 shows the settings that yield the best precision and recall trade-o\ufb00s during the validation.Regarding computation time, each street view image cansatellite images and street view panoramas), the total CPU\u00ad bound computation time to process A is 161s.", "local_uri": ["c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_037.jpg", "c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_038.jpg", "c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_039.jpg", "c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_040.jpg", "c92082dfcf466cb7358094e5c09bb230abe19a3a_Image_041.jpg"], "annotated": false, "compound": true}
{"title": "Introducing People with ASD to Crowd Work", "pdf_hash": "38e81f92a74dcdb62b7f35243f91e3d96a615b82", "year": 2017, "venue": "ASSETS", "alt_text": "(a) Two images used for image classifaication task. (b) Two images used for image tagging task. (c) An image of business card used for image transcription task.", "levels": null, "corpus_id": 7007084, "sentences": ["(a) Two images used for image classifaication task. (b) Two images used for image tagging task. (c) An image of business card used for image transcription task."], "caption": "Figure 1. The examples of images used in the Week 2 tasks. (a) Image classification: prompted yes/no question like \u201cis this this woman smiling?\u201d (b) Image description: prompted to describe what is in the image. (c) Image transcription: prompted to transcribe parts of the image. like the first name in the business card.", "local_uri": ["38e81f92a74dcdb62b7f35243f91e3d96a615b82_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Introducing People with ASD to Crowd Work", "pdf_hash": "38e81f92a74dcdb62b7f35243f91e3d96a615b82", "year": 2017, "venue": "ASSETS", "alt_text": "Five screenshots of web interfaces for the Week 3 tasks. (a) A survey interface. (b) A simplified survey interface. (c) A image transcription interface. (d) Image detection interface. (e) A simplified image transcription interface.", "levels": null, "corpus_id": 7007084, "sentences": ["Five screenshots of web interfaces for the Week 3 tasks. (a) A survey interface. (b) A simplified survey interface. (c) A image transcription interface. (d) Image detection interface. (e) A simplified image transcription interface."], "caption": "Figure 2. The task interfaces for Week 3. (a) Survey control: 10 questions from AQ were listed on a single page. (b) Survey mini: 10 questions from AQ were administered one-by-one to the participant. (c) Transcription control: we asked the participant to provide business name, transaction date, and total amount of payment for each receipt in a single page. (d) Text detection: the participant used the interface to draw a bounding box around the target information like business name. (e) Transcription mini: The participants were asked to transcribe what was on the cropped image of the receipts.", "local_uri": ["38e81f92a74dcdb62b7f35243f91e3d96a615b82_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Introducing People with ASD to Crowd Work", "pdf_hash": "38e81f92a74dcdb62b7f35243f91e3d96a615b82", "year": 2017, "venue": "ASSETS", "alt_text": "Y-axis is types of tasks. From top: survey control, survey mini, transcription control, transcription mini, and text detection. X-axis is time in seconds.", "levels": [[1], [1], [1]], "corpus_id": 7007084, "sentences": ["Y-axis is types of tasks.", "From top: survey control, survey mini, transcription control, transcription mini, and text detection.", "X-axis is time in seconds."], "caption": "Figure 3. The strip plot of task completion time by each participant. The x- axis is completion time in seconds and y-axis is the category of the task.", "local_uri": ["38e81f92a74dcdb62b7f35243f91e3d96a615b82_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Introducing People with ASD to Crowd Work", "pdf_hash": "38e81f92a74dcdb62b7f35243f91e3d96a615b82", "year": 2017, "venue": "ASSETS", "alt_text": "The screenshot of ATQ. The figure include the programmatically controlled Firefox web browser and the custom image transcription interface.", "levels": null, "corpus_id": 7007084, "sentences": ["The screenshot of ATQ.", "The figure include the programmatically controlled Firefox web browser and the custom image transcription interface."], "caption": "Figure 4. The ATQ prototype. The system has a custom desktop UI for image transcription, a programmatically controlled Firefox web browser, and a Selenium browser automation tool that runs on the background.", "local_uri": ["38e81f92a74dcdb62b7f35243f91e3d96a615b82_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Introducing People with ASD to Crowd Work", "pdf_hash": "38e81f92a74dcdb62b7f35243f91e3d96a615b82", "year": 2017, "venue": "ASSETS", "alt_text": "A comparison of time spent on control transcription and ATQ-based transcription. Y-axis represents the types of interface. From top: control and ATQ. X-axis represents time in (s)", "levels": [[1], [1], [1], [1]], "corpus_id": 7007084, "sentences": ["A comparison of time spent on control transcription and ATQ-based transcription.", "Y-axis represents the types of interface.", "From top: control and ATQ.", "X-axis represents time in (s)"], "caption": "Figure 5. A strip plot of time spent on transcribing each image. P1 and P3 took longer to complete transcription in the control condition.", "local_uri": ["38e81f92a74dcdb62b7f35243f91e3d96a615b82_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Physical accessibility of touchscreen smartphones", "pdf_hash": "b87db84f87997f4d8631b3c6eb539e49341ae559", "year": 2013, "venue": "ASSETS", "alt_text": "On the left: Tracing paper app screnshot. At the top and bottom of the app are rows of buttons.  The center is a grid overlaid on three purple circles above a horizontal rectangle.  Over two of the purple circles are long straight lines, mostly horizontal, usually starting or ending on a circle.  On the right: ArtTouch app screenshot. At the top and bottom of the screen are rows of controls.  The center area has a clear background, and shows a plane made of a series of uneven horizontal lines connecting two vertical lines that represent the path of a user's finger.  Near the top of the plane, a third touch point comes in, and is connected with the other two to form a triangle.", "levels": null, "corpus_id": 18205951, "sentences": ["On the left: Tracing paper app screnshot.", "At the top and bottom of the app are rows of buttons.", "The center is a grid overlaid on three purple circles above a horizontal rectangle.", "Over two of the purple circles are long straight lines, mostly horizontal, usually starting or ending on a circle.", "On the right: ArtTouch app screenshot.", "At the top and bottom of the screen are rows of controls.", "The center area has a clear background, and shows a plane made of a series of uneven horizontal lines connecting two vertical lines that represent the path of a user's finger.", "Near the top of the plane, a third touch point comes in, and is connected with the other two to form a triangle."], "caption": "", "local_uri": ["b87db84f87997f4d8631b3c6eb539e49341ae559_Image_001.jpg", "b87db84f87997f4d8631b3c6eb539e49341ae559_Image_002.jpg"], "annotated": false, "compound": true}
{"title": "Physical accessibility of touchscreen smartphones", "pdf_hash": "b87db84f87997f4d8631b3c6eb539e49341ae559", "year": 2013, "venue": "ASSETS", "alt_text": "Bar chart showing agreement that a smartphones takes a lot of mental effort to use.  The vertical axis shows the percentage of participants. The horizontal axis shows 'No device' and 'Device Owner' For each of these values, one bar is drawn, within which responses for disagree, neutral and agree are stacked. 60% of participants with no touch-screen device disagreed that a smartphone takes a lot of mental effort to use, 24% agreed. 37% of participants with a touch-screen device disagreed that a smartphone takes a lot of mental effort to use, 63% agreed.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 18205951, "sentences": ["Bar chart showing agreement that a smartphones takes a lot of mental effort to use.", "The vertical axis shows the percentage of participants.", "The horizontal axis shows 'No device' and 'Device Owner' For each of these values, one bar is drawn, within which responses for disagree, neutral and agree are stacked.", "60% of participants with no touch-screen device disagreed that a smartphone takes a lot of mental effort to use, 24% agreed.", "37% of participants with a touch-screen device disagreed that a smartphone takes a lot of mental effort to use, 63% agreed."], "caption": "Disagree", "local_uri": ["b87db84f87997f4d8631b3c6eb539e49341ae559_Image_009.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Physical accessibility of touchscreen smartphones", "pdf_hash": "b87db84f87997f4d8631b3c6eb539e49341ae559", "year": 2013, "venue": "ASSETS", "alt_text": "Two screen images showing a finger path over a rectangle.  In the first, the rectangle is wide and short, and the  finger path begins and ends above it on the left side. The path drops down to the middle of the rectangle, back to the left along the rectangle, then across to the right, and back to the left, leaving the rectangle again.  In the second, two short paths are shown, both starting near the lower middle of the rectangle and moving down off it.", "levels": null, "corpus_id": 18205951, "sentences": ["Two screen images showing a finger path over a rectangle.", "In the first, the rectangle is wide and short, and the  finger path begins and ends above it on the left side.", "The path drops down to the middle of the rectangle, back to the left along the rectangle, then across to the right, and back to the left, leaving the rectangle again.", "In the second, two short paths are shown, both starting near the lower middle of the rectangle and moving down off it."], "caption": "Figure 5. Two examples of sliding actions by participants who found this challenging.", "local_uri": ["b87db84f87997f4d8631b3c6eb539e49341ae559_Image_012.jpg", "b87db84f87997f4d8631b3c6eb539e49341ae559_Image_013.jpg"], "annotated": false, "compound": true}
{"title": "Accessibility in context: understanding the truly mobile experience of smartphone users with motor impairments", "pdf_hash": "aff58aaccdd03d0de8a9cb59941cd447ccc2c551", "year": 2014, "venue": "ASSETS", "alt_text": "Participant 2 in a wheelchair with phone in lap, and demonstrating text correction.", "levels": null, "corpus_id": 22591833, "sentences": ["Participant 2 in a wheelchair with phone in lap, and demonstrating text correction."], "caption": "", "local_uri": ["aff58aaccdd03d0de8a9cb59941cd447ccc2c551_Image_004.jpg", "aff58aaccdd03d0de8a9cb59941cd447ccc2c551_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Accessibility in context: understanding the truly mobile experience of smartphone users with motor impairments", "pdf_hash": "aff58aaccdd03d0de8a9cb59941cd447ccc2c551", "year": 2014, "venue": "ASSETS", "alt_text": "Participant 3 doing a one-handed pinch gesture and demonstrating how a tripod mount on his phone helps him hold it.", "levels": null, "corpus_id": 22591833, "sentences": ["Participant 3 doing a one-handed pinch gesture and demonstrating how a tripod mount on his phone helps him hold it."], "caption": "Figure 4. P2 with his phone on his lap while stationary (left),               Figure 5. P3\u2019s two-handed pinch gesture (left); holding the and showing challenges correcting text (right).                                phone by using the tripod mount on the case (right).", "local_uri": ["aff58aaccdd03d0de8a9cb59941cd447ccc2c551_Image_006.jpg", "aff58aaccdd03d0de8a9cb59941cd447ccc2c551_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Accessibility in context: understanding the truly mobile experience of smartphone users with motor impairments", "pdf_hash": "aff58aaccdd03d0de8a9cb59941cd447ccc2c551", "year": 2014, "venue": "ASSETS", "alt_text": "Participant 4 shopping at a pharmacy during the neighborhood activities, and holding his phone in his lap for use.", "levels": null, "corpus_id": 22591833, "sentences": ["Participant 4 shopping at a pharmacy during the neighborhood activities, and holding his phone in his lap for use."], "caption": "Figure 6. P4 in a pharmacy, stretching his arm to get items from an aisle (left); P4 holding the phone for use (right).", "local_uri": ["aff58aaccdd03d0de8a9cb59941cd447ccc2c551_Image_008.jpg", "aff58aaccdd03d0de8a9cb59941cd447ccc2c551_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "Genie: Input Retargeting on the Web through Command Reverse Engineering", "pdf_hash": "04fd41f7d10c173ace9af9aea70495ed81f30322", "year": 2017, "venue": "CHI", "alt_text": "The image shows a window with the label \"A Calculator\" and a standard basic calculator interface. Another window is shown to the left with the title \"Keyboard shortcuts\" and a list of keyboard shortcuts and their associated descriptions. Examples of generated shortcuts are \"Ctrl + 1\" with the description \"Number button\", and \"Ctrl + a\" with the descriptiion \"Add\", and \"Operator button\".", "levels": null, "corpus_id": 19007851, "sentences": ["The image shows a window with the label \"A Calculator\" and a standard basic calculator interface.", "Another window is shown to the left with the title \"Keyboard shortcuts\" and a list of keyboard shortcuts and their associated descriptions.", "Examples of generated shortcuts are \"Ctrl + 1\" with the description \"Number button\", and \"Ctrl + a\" with the descriptiion \"Add\", and \"Operator button\"."], "caption": "Figure 5. A calculator interface with incomplete keyboard support (a-calculator.com), enhanced to provide a keyboard shortcut for each command, as enabled by Genie\u2019s analyses.", "local_uri": ["04fd41f7d10c173ace9af9aea70495ed81f30322_Image_006.png"], "annotated": false, "compound": false}
{"title": "Genie: Input Retargeting on the Web through Command Reverse Engineering", "pdf_hash": "04fd41f7d10c173ace9af9aea70495ed81f30322", "year": 2017, "venue": "CHI", "alt_text": "The top of the image shows a graph builder interface with colored, numbered nodes and edges connecting some of the nodes. Options for the graph are to click on nodes, click on the canvas to add a new node, and drag and drop arrows between the nodes to create edges. Displayed over the image is a grid with cells and cell numbers over each cell. Typing these cell numbers places a node in that cell location. At the bottom of the image is a dialog titled \"keyboard shortcuts\" that contains a list of command options for working with the graph, including \"ctrl+i\" for \"Insert node: Insert new node at point.. \"", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 19007851, "sentences": ["The top of the image shows a graph builder interface with colored, numbered nodes and edges connecting some of the nodes.", "Options for the graph are to click on nodes, click on the canvas to add a new node, and drag and drop arrows between the nodes to create edges.", "Displayed over the image is a grid with cells and cell numbers over each cell.", "Typing these cell numbers places a node in that cell location.", "At the bottom of the image is a dialog titled \"keyboard shortcuts\" that contains a list of command options for working with the graph, including \"ctrl+i\" for \"Insert node: Insert new node at point.. \""], "caption": "", "local_uri": ["04fd41f7d10c173ace9af9aea70495ed81f30322_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Genie: Input Retargeting on the Web through Command Reverse Engineering", "pdf_hash": "04fd41f7d10c173ace9af9aea70495ed81f30322", "year": 2017, "venue": "CHI", "alt_text": "The top half of the image shows a web interface with \"My to-do list\" and a \"Save This List\" button. Below that is a field labeled \"Write your next task here\". Below that are three to-do items with checkboxes next to them: \"write review\", \"submit review\", and \"read paper\". Below that is a separate area of the interface that is a terminal style container. The terminal has a label \"Enter a command\" at the top and below says \"Type commands to see the list of commands\". The commands are shown below which include \"write task\" ,\"read paper\", \"save task\", etc. Below that line is typed a macro in the format described. Below that is \"help\" and a print out of the command labels and their associated text descriptions that Genie has extracted.", "levels": null, "corpus_id": 19007851, "sentences": ["The top half of the image shows a web interface with \"My to-do list\" and a \"Save This List\" button.", "Below that is a field labeled \"Write your next task here\".", "Below that are three to-do items with checkboxes next to them: \"write review\", \"submit review\", and \"read paper\".", "Below that is a separate area of the interface that is a terminal style container.", "The terminal has a label \"Enter a command\" at the top and below says \"Type commands to see the list of commands\".", "The commands are shown below which include \"write task\" ,\"read paper\", \"save task\", etc.", "Below that line is typed a macro in the format described.", "Below that is \"help\" and a print out of the command labels and their associated text descriptions that Genie has extracted."], "caption": "Figure 9. A Genie-enabled command line terminal that allows command automation and macro creation.", "local_uri": ["04fd41f7d10c173ace9af9aea70495ed81f30322_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "The paper teaser shows three images: left image shows a street intersection, middle     image shows a user wearing a VR headset while his gaze is being tracked, and the right     image shows the same intersection as the left but with an arrow showing the direction     the user should take to get to his/her destination.", "levels": null, "corpus_id": 140322449, "sentences": ["The paper teaser shows three images: left image shows a street intersection, middle     image shows a user wearing a VR headset while his gaze is being tracked, and the right     image shows the same intersection as the left but with an arrow showing the direction     the user should take to get to his/her destination."], "caption": "", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_001.jpg", "5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_002.png", "5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "Top image labeled with ``Training'' shows a gaze sequence.       An arrow connects the top image to a ``Navigation Aid Need Classifier''text box.        An arrow connects the ``Navigation Aid Need Classifier'' to a diamond which contains        ``Need help?'' connected to two arrows.        The first arrow marked with ``No'' connects it to an image of a street intersection.        The second arrow marked with ``Yes'' connects it to an image of the street intersection        with a navigation arrow pointing to the left.        An image of a user wearing a VR headset with his gaze extracted and inputed into the        ``Navigation Aid Need Classifier''.", "levels": null, "corpus_id": 140322449, "sentences": ["Top image labeled with ``Training'' shows a gaze sequence.", "An arrow connects the top image to a ``Navigation Aid Need Classifier''text box.", "An arrow connects the ``Navigation Aid Need Classifier'' to a diamond which contains        ``Need help?'' connected to two arrows.", "The first arrow marked with ``No'' connects it to an image of a street intersection.", "The second arrow marked with ``Yes'' connects it to an image of the street intersection        with a navigation arrow pointing to the left.", "An image of a user wearing a VR headset with his gaze extracted and inputed into the        ``Navigation Aid Need Classifier''."], "caption": "Figure 2: Overview of our approach.", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_006.png"], "annotated": false, "compound": false}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "Top left image: a flag post on a sidewalk.    Top right image: a street with an embedded path pointing toward an apple.    Bottom left image: a close-up of the apple with the embedded path.    Bottom right image: a barrier blocking a road.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 140322449, "sentences": ["Top left image: a flag post on a sidewalk.", "Top right image: a street with an embedded path pointing toward an apple.", "Bottom left image: a close-up of the apple with the embedded path.", "Bottom right image: a barrier blocking a road."], "caption": "Figure 5: We asked participants to fnish tasks during the data collection phase. (a) The participants were dropped at the starting point. (b) They were asked to follow the path to (c) a fruit. After fnding the fruit, participants were asked to fnd their way back to the starting point. The path was hidden from the participants unless triggered by pressing a button. (d) Barriers were randomly placed along the path back to the starting point to intentionally confuse the par- ticipants.", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_008.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "Top image shows a blue wavy line labeled with ``Gaze signal''.       Middle image shows a rectangle labeled with ``Aid shown?''.       The rectangle is subdivided into three rectangles, the middle rectangle       is labeled with ``aid-shown'' and the remaining labeled as ``no-aid''.       Bottom image shows a set of sliding rectangles in blue and red. The       red rectangles intersect with a one rectangle size area before ``aid-shown''.", "levels": [[1], [1], [1], [1], [1]], "corpus_id": 140322449, "sentences": ["Top image shows a blue wavy line labeled with ``Gaze signal''.", "Middle image shows a rectangle labeled with ``Aid shown?''.", "The rectangle is subdivided into three rectangles, the middle rectangle       is labeled with ``aid-shown'' and the remaining labeled as ``no-aid''.", "Bottom image shows a set of sliding rectangles in blue and red.", "The       red rectangles intersect with a one rectangle size area before ``aid-shown''."], "caption": "Figure 7: Windowing and labeling of gaze angle sequences. More details are provided in the Data Processing section.", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_010.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "A line plot showing the progression of training at each epoch. The     accuracy increases until 237 then plateaus.", "levels": [[1], [3, 2]], "corpus_id": 140322449, "sentences": ["A line plot showing the progression of training at each epoch.", "The     accuracy increases until 237 then plateaus."], "caption": "Figure 8: Validation accuracy of our model at every epoch of training. The accuracy was computed using the held-out val- idation data. The network corresponding to the 237th epoch was selected as our classifer.\u200c", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "A bar chart with 5 columns: ``no aid'', ``perm. arrow'', ``perm. map'',     ``adaptive arrow'' and adaptive map. The values range from 1 to 5 and are     labeled with Rating.", "levels": [[1], [1], [1], [1]], "corpus_id": 140322449, "sentences": ["A bar chart with 5 columns: ``no aid'', ``perm.", "arrow'', ``perm.", "map'',     ``adaptive arrow'' and adaptive map.", "The values range from 1 to 5 and are     labeled with Rating."], "caption": "", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_015.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "pdf_hash": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "year": 2019, "venue": "CHI", "alt_text": "A screen with a gaze heatmap overlaid. Values are higher in the center of the screen,     but there are some gaze values on the top of the screen.", "levels": null, "corpus_id": 140322449, "sentences": ["A screen with a gaze heatmap overlaid.", "Values are higher in the center of the screen,     but there are some gaze values on the top of the screen."], "caption": "", "local_uri": ["5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "From Gender Biases to Gender-Inclusive Design: An Empirical Investigation", "pdf_hash": "bb96dc3ce7c2196446c72e6d8c151abe578ee216", "year": 2019, "venue": "CHI", "alt_text": "Abby    Condensed version of the Abby persona, indicating the portion that is customized (the background information) in the upper right-hand corner of the image. Abby is 24 years old, and a graduate student.", "levels": null, "corpus_id": 115140495, "sentences": ["Abby    Condensed version of the Abby persona, indicating the portion that is customized (the background information) in the upper right-hand corner of the image.", "Abby is 24 years old, and a graduate student."], "caption": "Figure 1. Key  portions of  the Abby  persona used. The complete personas are in the supplemental document.", "local_uri": ["bb96dc3ce7c2196446c72e6d8c151abe578ee216_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "From Gender Biases to Gender-Inclusive Design: An Empirical Investigation", "pdf_hash": "bb96dc3ce7c2196446c72e6d8c151abe578ee216", "year": 2019, "venue": "CHI", "alt_text": "Gender Distribution    There was 1 woman with all 5 Abby facets, there were 5 women with 4 Abby facets, 2 women with 3 Abby facets, 1 woman with 2 Abby facets, 1 woman with 1 Abby facet, and 1 woman with 0 Abby facets. 1 man with 5 Tim facets, 3 men with 4 Tim facets, 3 men with 3 Tim facets, and 2 men with 2 Tim facets.", "levels": [[2, 1], [2]], "corpus_id": 115140495, "sentences": ["Gender Distribution    There was 1 woman with all 5 Abby facets, there were 5 women with 4 Abby facets, 2 women with 3 Abby facets, 1 woman with 2 Abby facets, 1 woman with 1 Abby facet, and 1 woman with 0 Abby facets.", "1 man with 5 Tim facets, 3 men with 4 Tim facets, 3 men with 3 Tim facets, and 2 men with 2 Tim facets."], "caption": "", "local_uri": ["bb96dc3ce7c2196446c72e6d8c151abe578ee216_Image_019.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Reinterpreting Schlemmer's Triadic Ballet: Interactive Costume for Unthinkable Movements", "pdf_hash": "69ccdba2e0e277506980c06db28eb5bb7dcfbaa9", "year": 2018, "venue": "CHI", "alt_text": "https://lh5.googleusercontent.com/KgOWgrPoIjYOdKxELn7lywUQePUYwYwkzs4k9yrv7h3p2VrRhGJTJcLKFm0bb1FOMqERggbovHRz3Sn9qH1JA-QUyRCQtR8-GOvi57XtWluortF29Y1ad762u1n0z3unGZ6MkdOZ", "levels": null, "corpus_id": 5039863, "sentences": ["https://lh5.googleusercontent.com/KgOWgrPoIjYOdKxELn7lywUQePUYwYwkzs4k9yrv7h3p2VrRhGJTJcLKFm0bb1FOMqERggbovHRz3Sn9qH1JA-QUyRCQtR8-GOvi57XtWluortF29Y1ad762u1n0z3unGZ6MkdOZ"], "caption": "Figure 2. Group photo of all figurines in Oskar Schlemmer\u2019s Triadic Ballett. (photo Ernst Schneider 1927) \u00a9 Bauhaus- Archiv Berlin/ Museum f\u00fcr Gestaltung", "local_uri": ["69ccdba2e0e277506980c06db28eb5bb7dcfbaa9_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "A Place to Play: The (Dis)Abled Embodied Experience for Autistic Children in Online Spaces", "pdf_hash": "9307e6b734d0120762ac9a402e31b17f015ba327", "year": 2019, "venue": "CHI", "alt_text": "Screen shot from cell phone. A chat channel that says, \"ResearcherKate: Connected using PickaxeChat for Android Unknown command! Try help for a list. ResearcherKate: Hello, World!\" Bottom half of screen shot is a virtual keyboard.", "levels": null, "corpus_id": 140218561, "sentences": ["Screen shot from cell phone.", "A chat channel that says, \"ResearcherKate: Connected using PickaxeChat for Android Unknown command! Try help for a list.", "ResearcherKate: Hello, World!\" Bottom half of screen shot is a virtual keyboard."], "caption": "Figure 1. A screen shot of the chat app for mobile devices.", "local_uri": ["9307e6b734d0120762ac9a402e31b17f015ba327_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "A Place to Play: The (Dis)Abled Embodied Experience for Autistic Children in Online Spaces", "pdf_hash": "9307e6b734d0120762ac9a402e31b17f015ba327", "year": 2019, "venue": "CHI", "alt_text": "Screen shot of microsoft paint program with a flattened minecraft character as the image.", "levels": null, "corpus_id": 140218561, "sentences": ["Screen shot of microsoft paint program with a flattened minecraft character as the image."], "caption": "Figure 2. Editing my Minecraft avatar in Microsoft's Paint program.", "local_uri": ["9307e6b734d0120762ac9a402e31b17f015ba327_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Everybody's Hacking: Participation and the Mainstreaming of Hackathons", "pdf_hash": "770f91ff7ef42db974320760acf8ab2ea4564ffa", "year": 2018, "venue": "CHI", "alt_text": "This work is licensed under a Creative Commons Attribution International 4.0License.", "levels": null, "corpus_id": 5069092, "sentences": ["This work is licensed under a Creative Commons Attribution International 4.0License."], "caption": "Loraine Clarke", "local_uri": ["770f91ff7ef42db974320760acf8ab2ea4564ffa_Image_001.png"], "annotated": false, "compound": false}
{"title": "Everybody's Hacking: Participation and the Mainstreaming of Hackathons", "pdf_hash": "770f91ff7ef42db974320760acf8ab2ea4564ffa", "year": 2018, "venue": "CHI", "alt_text": "Rather than an app, the Cycle Hack team produced stencils that could be shared online.", "levels": null, "corpus_id": 5069092, "sentences": ["Rather than an app, the Cycle Hack team produced stencils that could be shared online."], "caption": "Figure 2. Rather than an app, the Cycle Hack team produced stencils that could be shared online.", "local_uri": ["770f91ff7ef42db974320760acf8ab2ea4564ffa_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Everybody's Hacking: Participation and the Mainstreaming of Hackathons", "pdf_hash": "770f91ff7ef42db974320760acf8ab2ea4564ffa", "year": 2018, "venue": "CHI", "alt_text": "Barrier cards provided inspiration from cyclists and non-cyclists, creating conversation opportunities.", "levels": [[-1]], "corpus_id": 5069092, "sentences": ["Barrier cards provided inspiration from cyclists and non-cyclists, creating conversation opportunities."], "caption": "Figure 3. Barrier cards provided inspiration from cyclists and non-cyclists, creating conversation opportunities. Photo \u00a9 Zoe Prosser for Snook.", "local_uri": ["770f91ff7ef42db974320760acf8ab2ea4564ffa_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Interactions in Location-Based Gaming: A Case Study of Raiding in Pok\u00e9mon GO", "pdf_hash": "791526a6a087dbaf09cfb201cc9e2020121d134b", "year": 2019, "venue": "CHI", "alt_text": "A picture containing floor, indoor, cabinet, wall  Description automatically generated", "levels": null, "corpus_id": 78089341, "sentences": ["A picture containing floor, indoor, cabinet, wall  Description automatically generated"], "caption": "", "local_uri": ["791526a6a087dbaf09cfb201cc9e2020121d134b_Image_121.jpg"], "annotated": false, "compound": false}
{"title": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "pdf_hash": "747015237c3a1d786d88d2a6f03f1297354f2077", "year": 2016, "venue": "CHI", "alt_text": "Persona-Player-Person image where the Player is at the boundary between the Persona and the Person.", "levels": null, "corpus_id": 10238532, "sentences": ["Persona-Player-Person image where the Player is at the boundary between the Persona and the Person."], "caption": "Figure 1. Waskul and Lust\u2019s [36] persona-player-person boundaries. Prior work has studied actions as a function of a participant\u2019s person and player selves, but to our knowledge no work has studied actions as a function of their persona, which is what we analyze here.", "local_uri": ["747015237c3a1d786d88d2a6f03f1297354f2077_Image_001.png"], "annotated": false, "compound": false}
{"title": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "pdf_hash": "747015237c3a1d786d88d2a6f03f1297354f2077", "year": 2016, "venue": "CHI", "alt_text": "A graph showing the relationship between the roles Figher, Mage, and Rogue and their attributes. Fighters have the main attribute \"Strong\" and secondary attributes \"Non-stealthy/open\" and \"Non-magical\". Mages have primary attribute \"Magical\" and secondary attributes \"Frail\" and \"Non-stealthy\". Rogues have primary attribute \"Stealthy\" and secondary attributes \"Frail\" and \"Non-magical\".", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 10238532, "sentences": ["A graph showing the relationship between the roles Figher, Mage, and Rogue and their attributes.", "Fighters have the main attribute \"Strong\" and secondary attributes \"Non-stealthy/open\" and \"Non-magical\".", "Mages have primary attribute \"Magical\" and secondary attributes \"Frail\" and \"Non-stealthy\".", "Rogues have primary attribute \"Stealthy\" and secondary attributes \"Frail\" and \"Non-magical\"."], "caption": "Figure 2. Our triad of role-attribute mappings. We selected three attributes and identi\ufb01ed three corresponding roles we felt best represented the attributes. Nodes represent role-attribute mappings, and edges are attributes shared between the connected role-attribute mappings. The edge opposite a node is the antonymic attribute to the node\u2019s role-attribute.", "local_uri": ["747015237c3a1d786d88d2a6f03f1297354f2077_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "pdf_hash": "747015237c3a1d786d88d2a6f03f1297354f2077", "year": 2016, "venue": "CHI", "alt_text": "A sequence of images showing each frame of the player's avatar movement animation. The avatar is a gray triangular prism that walks on two vertices.", "levels": null, "corpus_id": 10238532, "sentences": ["A sequence of images showing each frame of the player's avatar movement animation.", "The avatar is a gray triangular prism that walks on two vertices."], "caption": "Figure 3. The player\u2019s avatar, which was modeled after Perlin\u2019s Polly [28] to avoid the Proteus Effect [39] \u2013 the phenomenon that users conform to expected behaviors associated with an avatar\u2019s appearance.", "local_uri": ["747015237c3a1d786d88d2a6f03f1297354f2077_Image_010.png"], "annotated": false, "compound": false}
{"title": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "pdf_hash": "747015237c3a1d786d88d2a6f03f1297354f2077", "year": 2016, "venue": "CHI", "alt_text": "Screenshot of a village that resembles a park. The player's avatar is centered in the screen walking on a tiled path. An NPC can be seen next to a bench and facing a lake. A few trees and a wooden fence can be seen around the edges.", "levels": null, "corpus_id": 10238532, "sentences": ["Screenshot of a village that resembles a park.", "The player's avatar is centered in the screen walking on a tiled path.", "An NPC can be seen next to a bench and facing a lake.", "A few trees and a wooden fence can be seen around the edges."], "caption": "Figure 4. Screenshot of a sample in-game level environment.", "local_uri": ["747015237c3a1d786d88d2a6f03f1297354f2077_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "pdf_hash": "747015237c3a1d786d88d2a6f03f1297354f2077", "year": 2016, "venue": "CHI", "alt_text": "Screenshot showing the inside of someone's house. A woman stands next to a fire and a kid can be seen awake on his bed. A dialog box is open showing the text \"It's hard to get my baby to sleep\" as said by the woman. The game interface prompts the player to \"Hit space to continue\" as part of the dialog box, and \"Press E to talk\" as part of the main interface.", "levels": null, "corpus_id": 10238532, "sentences": ["Screenshot showing the inside of someone's house.", "A woman stands next to a fire and a kid can be seen awake on his bed.", "A dialog box is open showing the text \"It's hard to get my baby to sleep\" as said by the woman.", "The game interface prompts the player to \"Hit space to continue\" as part of the dialog box, and \"Press E to talk\" as part of the main interface."], "caption": "Figure 5. Screenshot of a sample in-game dialog box.", "local_uri": ["747015237c3a1d786d88d2a6f03f1297354f2077_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "pdf_hash": "747015237c3a1d786d88d2a6f03f1297354f2077", "year": 2016, "venue": "CHI", "alt_text": "Screenshot of a cutscene showing a solid black background with a light brown, fancy old-styled frame. Inside the frame the player's avatar is portrayed as moving fast toward a dragon with visual indication that the dragon is being hit by the avatar. The dragon shows distress in its face.", "levels": null, "corpus_id": 10238532, "sentences": ["Screenshot of a cutscene showing a solid black background with a light brown, fancy old-styled frame.", "Inside the frame the player's avatar is portrayed as moving fast toward a dragon with visual indication that the dragon is being hit by the avatar.", "The dragon shows distress in its face."], "caption": "Figure 7. Screenshot of a sample in-game cutscene.", "local_uri": ["747015237c3a1d786d88d2a6f03f1297354f2077_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Our Story: Addressing Challenges in Development Contexts for Sustainable Participatory Video", "pdf_hash": "a1e031409784b3073f72ea708b943674248b516e", "year": 2019, "venue": "CHI", "alt_text": "Screenshot of the Our Story application editing screen, showing videos being placed onto a simplified timeline for editing.", "levels": null, "corpus_id": 140226118, "sentences": ["Screenshot of the Our Story application editing screen, showing videos being placed onto a simplified timeline for editing."], "caption": "Figure 2: Editing Screen of Our Story Application", "local_uri": ["a1e031409784b3073f72ea708b943674248b516e_Image_003.png"], "annotated": false, "compound": false}
{"title": "Gesture-Based Interactive Audio Guide on Tactile Reliefs", "pdf_hash": "e14d2a5957ffef240c33e0f03158bf0ecdfd540d", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 1:    This figure consists of 5 sub-figures.    Figure a) shows a tactile relief of the painting The Kiss of Gustav Klimt. The relief is white, made of Corean, approximately 42 by 42 centimeters large, approximately 2.5 centimeters of height variation.  It depicts a woman and a man embraced in a kiss, while standing on a meadow or cliff.    Figure b) shows the prototype setup of the Interactive Audio Guide. On a white table is a laptop running the software, the tactile relief, and over the relief a tripod on which the depth camera is mounted, looking down, observing the relief. The depth camera is attached to the computer.    Figure c) shows the content of the relief, but with colored labels. The relief is surounded by a black background. Between the labels are white lines.  The image is slightly distorted and is fully aligned with the images of the relief seen in Figures 1d), 1e) and 2a) and 2b).  A total of 20 differently colored labels can be seen. The six top-level regions are: the background, the meadow (subdivided in two parts), the signature, the male and the female figure and an ambiguous part between them.  The male and female figures are shown with a colored outline indicating their top-level region. They are further subdivided into parts of the clothing and body parts for a total of 20 labels.    Figure d) shows the depth image of the depth camera in portrait orientation. The black and white image is black where no measurements can be made (e.g. at the very top) and in shades of gray depending on the distance from the sensor. Dark is near, bright is far.  The relief can be seen almost filling the image. A hand goes from the bottom left edge up with an extended finger and touches the hands of the couple with a single extended fingertip.    Figure e) shows the same scene as Figure 1d) but as the Infrared Image from the sensor. It looks just as a black and white image, with a darker desk, a brighter relief and again darker hand and sleeve.   The sleeve of the pullover is quite wide.  Superimposed are the dividing lines between the labels in black color. The label that is touched is colored in purple, indicating a successful touch event.", "levels": null, "corpus_id": 1251090, "sentences": ["Figure 1:    This figure consists of 5 sub-figures.", "Figure a) shows a tactile relief of the painting The Kiss of Gustav Klimt.", "The relief is white, made of Corean, approximately 42 by 42 centimeters large, approximately 2.5 centimeters of height variation.", "It depicts a woman and a man embraced in a kiss, while standing on a meadow or cliff.", "Figure b) shows the prototype setup of the Interactive Audio Guide.", "On a white table is a laptop running the software, the tactile relief, and over the relief a tripod on which the depth camera is mounted, looking down, observing the relief.", "The depth camera is attached to the computer.", "Figure c) shows the content of the relief, but with colored labels.", "The relief is surounded by a black background.", "Between the labels are white lines.", "The image is slightly distorted and is fully aligned with the images of the relief seen in Figures 1d), 1e) and 2a) and 2b).", "A total of 20 differently colored labels can be seen.", "The six top-level regions are: the background, the meadow (subdivided in two parts), the signature, the male and the female figure and an ambiguous part between them.", "The male and female figures are shown with a colored outline indicating their top-level region.", "They are further subdivided into parts of the clothing and body parts for a total of 20 labels.", "Figure d) shows the depth image of the depth camera in portrait orientation.", "The black and white image is black where no measurements can be made (e.g. at the very top) and in shades of gray depending on the distance from the sensor.", "Dark is near, bright is far.", "The relief can be seen almost filling the image.", "A hand goes from the bottom left edge up with an extended finger and touches the hands of the couple with a single extended fingertip.", "Figure e) shows the same scene as Figure 1d) but as the Infrared Image from the sensor.", "It looks just as a black and white image, with a darker desk, a brighter relief and again darker hand and sleeve.", "The sleeve of the pullover is quite wide.", "Superimposed are the dividing lines between the labels in black color.", "The label that is touched is colored in purple, indicating a successful touch event."], "caption": "", "local_uri": ["e14d2a5957ffef240c33e0f03158bf0ecdfd540d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Gesture-Based Interactive Audio Guide on Tactile Reliefs", "pdf_hash": "e14d2a5957ffef240c33e0f03158bf0ecdfd540d", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 2:    This figure consists of 2 sub-figures.    Figure a) shows graphical debugging output of the hand-detection algorithm. It is the same scene as in Figures 1d+e and 2b.  The silhouette of the detected arm is shown with a thin gray outline, indicated as capital P_i.  On the bottom left corner where the arm touches the border, 2 yellow lines are painted at the border indicated as capital P_o.  On the widest part of silhouette P_i a circle is drawn indicated as C_err. The circle is approximately at the ellbow region and is the wrongly detected palm of the original algorithm.  The point on the single extended fintertip is indicated as P_max = p_i, and a yellow dotted line goes to the nearest point on the line capital P_o (labeled p_o) indicating that this point is indeed the fartherst point on P_i to any point on P_o.  Around P_max a circle indicated radius h is drawn. Inside this circle to the south of P_max is the palm.  Inside the palm is an inscribed red circle indicated C_a with radius r_a, which is the correct locaton and size of the palm.    Figure b) shows the standard deviation of the stored background computed from 100 frames. It is again the same scene, but only the background, i.e. without the user's hand.  High standard deviations are bright yellow (3 millimeter), low values are dark blue, red tones in between.  The steep outlines of the relief and the figures within the relief have quite high values, several pixels wide. The rest is rather low, with curved moir\u00e9-like patterns of higher values.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 1251090, "sentences": ["Figure 2:    This figure consists of 2 sub-figures.", "Figure a) shows graphical debugging output of the hand-detection algorithm.", "It is the same scene as in Figures 1d+e and 2b.", "The silhouette of the detected arm is shown with a thin gray outline, indicated as capital P_i.", "On the bottom left corner where the arm touches the border, 2 yellow lines are painted at the border indicated as capital P_o.", "On the widest part of silhouette P_i a circle is drawn indicated as C_err.", "The circle is approximately at the ellbow region and is the wrongly detected palm of the original algorithm.", "The point on the single extended fintertip is indicated as P_max = p_i, and a yellow dotted line goes to the nearest point on the line capital P_o (labeled p_o) indicating that this point is indeed the fartherst point on P_i to any point on P_o.", "Around P_max a circle indicated radius h is drawn.", "Inside this circle to the south of P_max is the palm.", "Inside the palm is an inscribed red circle indicated C_a with radius r_a, which is the correct locaton and size of the palm.", "Figure b) shows the standard deviation of the stored background computed from 100 frames.", "It is again the same scene, but only the background, i.e. without the user's hand.", "High standard deviations are bright yellow (3 millimeter), low values are dark blue, red tones in between.", "The steep outlines of the relief and the figures within the relief have quite high values, several pixels wide.", "The rest is rather low, with curved moir\u00e9-like patterns of higher values."], "caption": "", "local_uri": ["e14d2a5957ffef240c33e0f03158bf0ecdfd540d_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SonicExplorer: fluid exploration of audio parameters", "pdf_hash": "77a0ad87b2a7d04cd7f5b587418f240801c40c1d", "year": 2014, "venue": "CHI", "alt_text": "Left: Two blue buttons that look like MIDI input ports. Right: Dropdown menu with a selection of color names (e.g., Purple, Blue, Magenta, etc).", "levels": null, "corpus_id": 12092278, "sentences": ["Left: Two blue buttons that look like MIDI input ports.", "Right: Dropdown menu with a selection of color names (e.g., Purple, Blue, Magenta, etc)."], "caption": "MIDI Sync Buttons           b) Color Selection\u200c", "local_uri": ["77a0ad87b2a7d04cd7f5b587418f240801c40c1d_Image_012.jpg", "77a0ad87b2a7d04cd7f5b587418f240801c40c1d_Image_013.jpg"], "annotated": false, "compound": true}
{"title": "Do you see what I see?: designing a sensory substitution device to access non-verbal modes of communication", "pdf_hash": "8a2210bedeb1468f223c08eea4ad15a48d3bc894", "year": 2013, "venue": "ASSETS", "alt_text": "The picture in left is showing the location of landmark points detected by the face tracker and a mesh created by those points.    The picture in right marks the landmark points which are used to calculate the features.", "levels": [[-1], [-1]], "corpus_id": 7139073, "sentences": ["The picture in left is showing the location of landmark points detected by the face tracker and a mesh created by those points.", "The picture in right marks the landmark points which are used to calculate the features."], "caption": "(a)", "local_uri": ["8a2210bedeb1468f223c08eea4ad15a48d3bc894_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Do you see what I see?: designing a sensory substitution device to access non-verbal modes of communication", "pdf_hash": "8a2210bedeb1468f223c08eea4ad15a48d3bc894", "year": 2013, "venue": "ASSETS", "alt_text": "The left picture shows a blind person holding his smartphone and using its front camera to get feedback about his own facial expressions. The right picture shows another blind person is holding the camera to listen feedback on his interlocutor's facial expressions", "levels": null, "corpus_id": 7139073, "sentences": ["The left picture shows a blind person holding his smartphone and using its front camera to get feedback about his own facial expressions.", "The right picture shows another blind person is holding the camera to listen feedback on his interlocutor's facial expressions"], "caption": "95", "local_uri": ["8a2210bedeb1468f223c08eea4ad15a48d3bc894_Image_010.jpg", "8a2210bedeb1468f223c08eea4ad15a48d3bc894_Image_012.jpg"], "annotated": false, "compound": true}
{"title": "Strategies for Engaging Communities in Creating Physical Civic Technologies", "pdf_hash": "7c988d84bff3b420f01038569ca97ab53f35f262", "year": 2018, "venue": "CHI", "alt_text": "This work is licensed under a Creative Commons Attribution International 4.0 License", "levels": null, "corpus_id": 5047253, "sentences": ["This work is licensed under a Creative Commons Attribution International 4.0 License"], "caption": "4.0 License", "local_uri": ["7c988d84bff3b420f01038569ca97ab53f35f262_Image_003.png"], "annotated": false, "compound": false}
{"title": "Strategies for Engaging Communities in Creating Physical Civic Technologies", "pdf_hash": "7c988d84bff3b420f01038569ca97ab53f35f262", "year": 2018, "venue": "CHI", "alt_text": "Example of the Inventor Kit used to build the Info Box prototype, before addition of buttons.", "levels": null, "corpus_id": 5047253, "sentences": ["Example of the Inventor Kit used to build the Info Box prototype, before addition of buttons."], "caption": "Figure 3. Example of the Inventor Kit used to build the Info Box prototype, before addition of buttons.", "local_uri": ["7c988d84bff3b420f01038569ca97ab53f35f262_Image_006.jpg", "7c988d84bff3b420f01038569ca97ab53f35f262_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Towards Accessible Conversations in a Mobile Context for People who are Deaf and Hard of Hearing", "pdf_hash": "1564430b58f2e2027a8435cd214a23078c88f21b", "year": 2018, "venue": "ASSETS", "alt_text": "A close up of text on a black background  Description generated with high confidence", "levels": null, "corpus_id": 52219661, "sentences": ["A close up of text on a black background  Description generated with high confidence"], "caption": "SMARTPHONE SMARTWATCH HMD", "local_uri": ["1564430b58f2e2027a8435cd214a23078c88f21b_Image_069.jpg"], "annotated": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 2. Illustration of target patterns (the pairs of target circles on the paper inside the training box showing on the screen as in Figure 1). One group of subjects executed bottom up from the easiest task to the hardest task and then back to the easiest task (shown in the upper panel); another group of subjects executed bottom up from hard to easy tasks and then back to the hardest task (lower panel).  A1 to A3 represent the distances between targets and W1 to W3 represent the sizes of the targets of ID1 to ID3.", "levels": null, "corpus_id": 16776288, "sentences": ["Figure 2.", "Illustration of target patterns (the pairs of target circles on the paper inside the training box showing on the screen as in Figure 1).", "One group of subjects executed bottom up from the easiest task to the hardest task and then back to the easiest task (shown in the upper panel); another group of subjects executed bottom up from hard to easy tasks and then back to the hardest task (lower panel).", "A1 to A3 represent the distances between targets and W1 to W3 represent the sizes of the targets of ID1 to ID3."], "caption": "", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 3. Execution sequence of a trial (target setting 1), steps shown in parentheses. The arrows represent tool movement from one circle to another; before each tool movement, the tool stops on the circle for 10s.", "levels": [[-1], [-1], [-1]], "corpus_id": 16776288, "sentences": ["Figure 3.", "Execution sequence of a trial (target setting 1), steps shown in parentheses.", "The arrows represent tool movement from one circle to another; before each tool movement, the tool stops on the circle for 10s."], "caption": "Figure 3. Execution sequence of a trial (target setting 1), steps shown in parentheses. The arrows represent tool movement from one circle to another; before each tool movement, the tool stops on the circle for 10s.", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_005.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 4. Key steps in detecting tooltip from surgical videos. Panel A shows the binary-thresholded image with mostly the tool left. Panel B shows the recognized tool (in red rectangle) and tooltip (the blue dot).", "levels": null, "corpus_id": 16776288, "sentences": ["Figure 4.", "Key steps in detecting tooltip from surgical videos.", "Panel A shows the binary-thresholded image with mostly the tool left.", "Panel B shows the recognized tool (in red rectangle) and tooltip (the blue dot)."], "caption": "Figure 4. Key steps in detecting tooltip from surgical videos. Panel A shows the binary-thresholded image with mostly the tool left. Panel B shows the recognized tool (in red rectangle) and tooltip (the blue dot).", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 5. Kinematics of tool movement (subject 01, trial 01) during a typical horizontal movement between circles. The blue curve is the tooltip position in pixels along the horizontal line between the circles, and the red curve is the velocity of tooltip.", "levels": [[0], [1], [1]], "corpus_id": 16776288, "sentences": ["Figure 5.", "Kinematics of tool movement (subject 01, trial 01) during a typical horizontal movement between circles.", "The blue curve is the tooltip position in pixels along the horizontal line between the circles, and the red curve is the velocity of tooltip."], "caption": "of the window. All the data in the windows were aligned at the tooltip-start (3 seconds into the window), and the mean pupil diameter change was calculated for each time point in the window across all horizontal tooltip moves from all trials. Assuming there are m moves from all trials of all subjects and each move has n samples in a 7-second", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 6:  Example of tooltip movement (blue) and pupil size (black) over time for a complete trial (subject 01, trial 01). The blue curve is the tooltip position in pixels along the horizontal line between the circles. The dash and solid vertical lines represent the moments of tooltip-start and tooltip-reach.", "levels": [[1], [1], [1]], "corpus_id": 16776288, "sentences": ["Figure 6:  Example of tooltip movement (blue) and pupil size (black) over time for a complete trial (subject 01, trial 01).", "The blue curve is the tooltip position in pixels along the horizontal line between the circles.", "The dash and solid vertical lines represent the moments of tooltip-start and tooltip-reach."], "caption": "", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 7. Blow up of a segment (95s-120s) from Figure 6, showing how the pupil size (black curve) increases when tool movements occur. The blue curve is the tooltip position in pixels along the horizontal line between the circles. The dash and solid vertical lines represent the moments of tooltip-start and tooltip-reached.", "levels": null, "corpus_id": 16776288, "sentences": ["Figure 7.", "Blow up of a segment (95s-120s) from Figure 6, showing how the pupil size (black curve) increases when tool movements occur.", "The blue curve is the tooltip position in pixels along the horizontal line between the circles.", "The dash and solid vertical lines represent the moments of tooltip-start and tooltip-reached."], "caption": "Figure 7. Blow up of a segment (95s-120s) from Figure 6, showing how the pupil size (black curve) increases when tool movements occur. The blue curve is the tooltip position in pixels along the horizontal line between the circles. The dash and solid vertical lines represent the moments of tooltip-start and tooltip- reach.", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 9. Mean pupil diameter changes for 808 valid moves of 69 trials from 12 subjects. Data were aligned over a 7 second window 3 seconds before the tooltip-start. The baseline is defined as the mean diameter of the pupil over the first second of the window, and the solid black curve is the mean pupil diameter change from the baseline over time. The black vertical dashed line is tooltip-start where all the data are aligned and the vertical solid black line is the average tool-reach time. The error bars for 1 std. dev. are drawn every 400ms.", "levels": [[0], [2], [2], [2], [1], [1], [1], [1]], "corpus_id": 16776288, "sentences": ["Figure 9.", "Mean pupil diameter changes for 808 valid moves of 69 trials from 12 subjects.", "Data were aligned over a 7 second window 3 seconds before the tooltip-start.", "The baseline is defined as the mean diameter of the pupil over the first second of the window, and the solid black curve is the mean pupil diameter change from the baseline over time.", "The black vertical dashed line is tooltip-start where all the data are aligned and the vertical solid black line is the average tool-reach time.", "The error bars for 1 std.", "dev.", "are drawn every 400ms."], "caption": "Figure 9. Mean pupil diameter changes for 808 valid moves of 69 trials from 12 subjects. Data were aligned over a 7 second window 3 seconds before the tooltip-start. The baseline is defined as the mean diameter of the pupil over the first second of the window, and the solid black curve is the mean pupil diameter change from the baseline over time. The black vertical dashed line is tooltip-start where all the data are aligned and the vertical solid black line is the average tool-reach time. The error bars for 1 std. dev. are drawn every 400ms.", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_012.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 10. Mean pupil diameter changes against different IDs; data are aligned over a 7-second window around tooltip-start.  The vertical dash black line is the tooltip-start and other three solid color vertical lines represent the tooltip-reach moments of three IDs respectively. The three colors of bars at the bottom indicate significant differences in pupil dilation between Easy, Middle and Hard ID with black representing Easy vs. Hard, pink representing Easy vs. Middle, and green representing Middle vs. Hard. The error bars for 1 std. dev. are drawn every 400ms.", "levels": [[0], [1], [1], [1], [1], [1], [1]], "corpus_id": 16776288, "sentences": ["Figure 10.", "Mean pupil diameter changes against different IDs; data are aligned over a 7-second window around tooltip-start.", "The vertical dash black line is the tooltip-start and other three solid color vertical lines represent the tooltip-reach moments of three IDs respectively.", "The three colors of bars at the bottom indicate significant differences in pupil dilation between Easy, Middle and Hard ID with black representing Easy vs. Hard, pink representing Easy vs. Middle, and green representing Middle vs. Hard.", "The error bars for 1 std.", "dev.", "are drawn every 400ms."], "caption": "Figure 10. Mean pupil diameter changes against different IDs; data are aligned over a 7-second window around tooltip-start. The vertical dash black line is the tooltip-start and other three solid color vertical lines represent the tooltip-reach moments of three IDs respectively. The three colors of bars at the bottom indicate significant differences in pupil dilation between Easy, Middle and Hard ID with black representing Easy vs. Hard, pink representing Easy vs. Middle, and green representing Middle vs. Hard. The error bars for 1 std. dev. are drawn every 400ms.", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_013.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pupil responses during discrete goal-directed movements", "pdf_hash": "1a91496ad6d8adf41f12b157343321732872c0c1", "year": 2014, "venue": "CHI", "alt_text": "Figure 11. Box-whisker plot for Mean duration from tooltip-reach to the moment where the pupil peaked in size for three difficulty IDs.", "levels": [[0], [1]], "corpus_id": 16776288, "sentences": ["Figure 11.", "Box-whisker plot for Mean duration from tooltip-reach to the moment where the pupil peaked in size for three difficulty IDs."], "caption": "", "local_uri": ["1a91496ad6d8adf41f12b157343321732872c0c1_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"But, I Don't Want/Need a Power Wheelchair\": Toward Accessible Power Assistance for Manual Wheelchairs", "pdf_hash": "06aafc2bbdc48dbe300d767c98a6ad25befc70b8", "year": 2017, "venue": "ASSETS", "alt_text": "Sketch of a manual wheelchair with the sketch of an electric motor and transmission, battery and a battery status indication unit. One battery on each side (left and right) below the seat to distribute weight, only right battery visible on sketch. Also an alternative battery status indication unit, when the user cannot turn around to read LEDs on the back wheels hub or behind the chair, the unit is attachable to any part of the wheelchair frame.    Fig 1 \u00a9 Dafne Zuleima Morgado Ramirez", "levels": null, "corpus_id": 10301527, "sentences": ["Sketch of a manual wheelchair with the sketch of an electric motor and transmission, battery and a battery status indication unit.", "One battery on each side (left and right) below the seat to distribute weight, only right battery visible on sketch.", "Also an alternative battery status indication unit, when the user cannot turn around to read LEDs on the back wheels hub or behind the chair, the unit is attachable to any part of the wheelchair frame.", "Fig 1 \u00a9 Dafne Zuleima Morgado Ramirez"], "caption": "Figure 1 Sketch of power assist electric motor and transmission feeding the rear wheels through the axel. One battery on each side to distribute weight, only right battery visible on sketch. Also an alternative battery status indication unit, when the user cannot turn around to read LEDs on the back wheels hub or behind the chair, the unit is attachable to any part of the wheelchair frame. \u00a9 Dafne Zuleima Morgado Ramirez", "local_uri": ["06aafc2bbdc48dbe300d767c98a6ad25befc70b8_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "\"But, I Don't Want/Need a Power Wheelchair\": Toward Accessible Power Assistance for Manual Wheelchairs", "pdf_hash": "06aafc2bbdc48dbe300d767c98a6ad25befc70b8", "year": 2017, "venue": "ASSETS", "alt_text": "Sketch of man sitting on a wheelchair and performing a tapping gesture with his right hand on the right handrail of a hybrid handbike that is installed infront of him, gesture is being detected through a right wrist worn device.    Fig 2 \u00a9 Dafne Zuleima Morgado Ramirez", "levels": null, "corpus_id": 10301527, "sentences": ["Sketch of man sitting on a wheelchair and performing a tapping gesture with his right hand on the right handrail of a hybrid handbike that is installed infront of him, gesture is being detected through a right wrist worn device.", "Fig 2 \u00a9 Dafne Zuleima Morgado Ramirez"], "caption": "Figure 2 Sketch of tapping gesture on the handrail of a hybrid handbike being detected through a wrist worn device. \u00a9 Dafne Zuleima Morgado Ramirez", "local_uri": ["06aafc2bbdc48dbe300d767c98a6ad25befc70b8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"But, I Don't Want/Need a Power Wheelchair\": Toward Accessible Power Assistance for Manual Wheelchairs", "pdf_hash": "06aafc2bbdc48dbe300d767c98a6ad25befc70b8", "year": 2017, "venue": "ASSETS", "alt_text": "Five manual wheelchair wheels overlapping with each other. The handrim of each wheel has a different color indicating from left to right a change in the percentage of battery left. From left to right the colors displayed are: white, blue, green, orange and red. At the bottom of the wheels sketch there is an arrow with information inside that reads 100% at the left end, in the middle it reads Battery and the right end reads 10%.    Fig 3 \u00a9 Dafne Zuleima Morgado Ramirez", "levels": null, "corpus_id": 10301527, "sentences": ["Five manual wheelchair wheels overlapping with each other.", "The handrim of each wheel has a different color indicating from left to right a change in the percentage of battery left.", "From left to right the colors displayed are: white, blue, green, orange and red.", "At the bottom of the wheels sketch there is an arrow with information inside that reads 100% at the left end, in the middle it reads Battery and the right end reads 10%.", "Fig 3 \u00a9 Dafne Zuleima Morgado Ramirez"], "caption": "Figure 3 Sketch of battery indication option when user cannot turn around to read LEDs on the back wheels hub or behind the chair, thin LED film covering the handrim changes color according to battery status. Suggested by U8. \u00a9 Dafne Zuleima Morgado Ramirez", "local_uri": ["06aafc2bbdc48dbe300d767c98a6ad25befc70b8_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Blind Photographers and VizSnap: A Long-Term Study", "pdf_hash": "52de8e663626e2b9a25ff6f0c1db29fed8102dd9", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows four photographs. Description of photographs from right to left: photo outside on a snowy night; photo of construction on a sidewalk with a cone; photo of a stormy sky with palm trees; photo of a fence in front of oak trees.", "levels": [[-1], [-1]], "corpus_id": 9913462, "sentences": ["This figure shows four photographs.", "Description of photographs from right to left: photo outside on a snowy night; photo of construction on a sidewalk with a cone; photo of a stormy sky with palm trees; photo of a fence in front of oak trees."], "caption": "Figure 1. Sample photos taken by various participants after two weeks of use.", "local_uri": ["52de8e663626e2b9a25ff6f0c1db29fed8102dd9_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Blind Photographers and VizSnap: A Long-Term Study", "pdf_hash": "52de8e663626e2b9a25ff6f0c1db29fed8102dd9", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows eleven photographs, each of which was taken by a randomly chosen participant. Following the photographs from right to left, top to bottom, each photograph corresponds to these categories: Animal/pet, electronics, food/drink, group, household item, individual person, outdoor scenery, plant, toy/craft, vehicle, whole room.", "levels": [[-1], [-1]], "corpus_id": 9913462, "sentences": ["This figure shows eleven photographs, each of which was taken by a randomly chosen participant.", "Following the photographs from right to left, top to bottom, each photograph corresponds to these categories: Animal/pet, electronics, food/drink, group, household item, individual person, outdoor scenery, plant, toy/craft, vehicle, whole room."], "caption": "", "local_uri": ["52de8e663626e2b9a25ff6f0c1db29fed8102dd9_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploring Aural and Haptic Feedback for Visually Impaired People on a Track: A Wizard of Oz Study", "pdf_hash": "248bf6dc2f9ce1bc9af74bbfc824155fe4cca9be", "year": 2018, "venue": "ASSETS", "alt_text": "Aerial view of a 400 meter track. The track is in a oval shape, where the straightaways are complete straight and the curves are an exact semicircle.", "levels": null, "corpus_id": 51998167, "sentences": ["Aerial view of a 400 meter track.", "The track is in a oval shape, where the straightaways are complete straight and the curves are an exact semicircle."], "caption": "", "local_uri": ["248bf6dc2f9ce1bc9af74bbfc824155fe4cca9be_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Aural and Haptic Feedback for Visually Impaired People on a Track: A Wizard of Oz Study", "pdf_hash": "248bf6dc2f9ce1bc9af74bbfc824155fe4cca9be", "year": 2018, "venue": "ASSETS", "alt_text": "Picture of straightaway taken from a human level from behind a start line. Only lanes numbers 1-3 are included, but the photo encompasses the entire straightaway. The picture is included to show that the polyeurethane pebbles expand outside the oval of the track, and may be a misleading boundary to cane users.", "levels": null, "corpus_id": 51998167, "sentences": ["Picture of straightaway taken from a human level from behind a start line.", "Only lanes numbers 1-3 are included, but the photo encompasses the entire straightaway.", "The picture is included to show that the polyeurethane pebbles expand outside the oval of the track, and may be a misleading boundary to cane users."], "caption": "b)", "local_uri": ["248bf6dc2f9ce1bc9af74bbfc824155fe4cca9be_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Aural and Haptic Feedback for Visually Impaired People on a Track: A Wizard of Oz Study", "pdf_hash": "248bf6dc2f9ce1bc9af74bbfc824155fe4cca9be", "year": 2018, "venue": "ASSETS", "alt_text": "A diagram of the Observer and Participant walking on the track with particular callouts. The Observer held two phones: 1) to video record the participant for data anlaysis, and 2) The Sender phone which sent signals to the Participant. The Participant is a few feet in front of them, wearing vibrating wristbands, bone conduction headphones, and a custom phone belt. The phone belt is such that the phone screen is on their torso and facing away from them to simulate a future system.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 51998167, "sentences": ["A diagram of the Observer and Participant walking on the track with particular callouts.", "The Observer held two phones: 1) to video record the participant for data anlaysis, and 2) The Sender phone which sent signals to the Participant.", "The Participant is a few feet in front of them, wearing vibrating wristbands, bone conduction headphones, and a custom phone belt.", "The phone belt is such that the phone screen is on their torso and facing away from them to simulate a future system."], "caption": "", "local_uri": ["248bf6dc2f9ce1bc9af74bbfc824155fe4cca9be_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "a: sketch of field of view constraint of the personal cockpit b: a user wearing a currently available head-worn display c: the current head-worn display view from the perspective of the wearer", "levels": null, "corpus_id": 10637511, "sentences": ["a: sketch of field of view constraint of the personal cockpit b: a user wearing a currently available head-worn display c: the current head-worn display view from the perspective of the wearer"], "caption": "Figure 1. The Personal Cockpit (a) leverages an empirically- determined spatial layout of virtual windows. We investigate its design space, including field of view constraints of wearable displays. Our design is a shift from current interfaces (b, c), in which content remains fixed in the user\u2019s forward view.", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "left: the viewing frustra of both eyes right: the binocular overlap region for windows behind and in front of the image plane", "levels": null, "corpus_id": 10637511, "sentences": ["left: the viewing frustra of both eyes right: the binocular overlap region for windows behind and in front of the image plane"], "caption": "Figure 2. Binocular parallax creates an illusion of depth when objects appear in front of or behind the head-worn display\u2019s virtual image plane (left). If content appears wider than the available FoV (bottom right), binocular overlap is reduced.", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "a: a window with mutiple coloured shapes b: a userinteracting with a clipped region of the window in a c: a user touching a bullseye target within a window d: a user touching a bullseye target in one of two smaller windows e: representational depictions of four design parameters: angular width, distance, reference frame and angular separation", "levels": null, "corpus_id": 10637511, "sentences": ["a: a window with mutiple coloured shapes b: a userinteracting with a clipped region of the window in a c: a user touching a bullseye target within a window d: a user touching a bullseye target in one of two smaller windows e: representational depictions of four design parameters: angular width, distance, reference frame and angular separation"], "caption": "Figure 3. We used the results of our first 3 user studies (a-d) to tune the design parameters (e) of the personal cockpit.", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "left: mean time vs. angle middle: mean pointing error vs. angle right: mean pointing error vs. focus", "levels": [[1]], "corpus_id": 10637511, "sentences": ["left: mean time vs. angle middle: mean pointing error vs. angle right: mean pointing error vs. focus"], "caption": "Figure 6. Mean trial times by direction and angle (left). Mean pointing error by direction and angle (middle). Mean pointing error by point of focus (right). Bars show \u00b12 SE", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "left: arm fatigue and neck fatigue vs. direction middle: arm fatigue and neck fatigue vs. angle group right: arm fatigue and neck fatigue vs. point of focus", "levels": [[1]], "corpus_id": 10637511, "sentences": ["left: arm fatigue and neck fatigue vs. direction middle: arm fatigue and neck fatigue vs. angle group right: arm fatigue and neck fatigue vs. point of focus"], "caption": "Figure 7. Mean perceived arm fatigue and neck fatigue for", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "a: the personal cockpit layout in front of a user b: top and side views of the layout", "levels": null, "corpus_id": 10637511, "sentences": ["a: the personal cockpit layout in front of a user b: top and side views of the layout"], "caption": "", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "several application windows: start screen, messages, map, question, my contacts and calendar", "levels": null, "corpus_id": 10637511, "sentences": ["several application windows: start screen, messages, map, question, my contacts and calendar"], "caption": "Figure 9. Example of the application windows presented to participants in study 4.", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "a: a user touches a virtual display in the personal cockpit b: a user touches a view-fixed display c: a user touching a trackpad while looking at a view-fixed window", "levels": null, "corpus_id": 10637511, "sentences": ["a: a user touches a virtual display in the personal cockpit b: a user touches a view-fixed display c: a user touching a trackpad while looking at a view-fixed window"], "caption": "Figure10. Study 4 tested our design (a, shown without FoV constraint for demonstration) against techniques using direct", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "The personal cockpit: a spatial interface for effective task switching on head-worn displays", "pdf_hash": "00be9ed47bb08d4af373474345c79729d7514e2b", "year": 2014, "venue": "CHI", "alt_text": "a: miniature windows arranged in a sphere above a user's hand b: action sequence of the windows moving from world- to body-fixed layout c: action sequence of a user opening a new window d: a user touching a colour picker next to an image window", "levels": null, "corpus_id": 10637511, "sentences": ["a: miniature windows arranged in a sphere above a user's hand b: action sequence of the windows moving from world- to body-fixed layout c: action sequence of a user opening a new window d: a user touching a colour picker next to an image window"], "caption": "Figure 13. Personal Cockpit interactions scenarios: Changing from world-fixed to body-fixed layout (a); opening a new application window (b); window intercommunication (c); and a shrinking the Cockpit to a palm-sized overview (d).", "local_uri": ["00be9ed47bb08d4af373474345c79729d7514e2b_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis", "pdf_hash": "f7406bd6a43969a5471287344cb565bee972b376", "year": 2018, "venue": "ASSETS", "alt_text": "On the left, a portion of an ESPN app is shown. The menu button has a 3 line hamburger icon and is labeled (a). It is an example of an Image Button. The \"Watch ESPN\" logo button next to the manu button is labeled (b) and is an example of a Clickable Image. On the right, there is a portion of a map app interface. The directions buttons is on the bottom portion and is labeled (c). It has a direction arrow icon on it and is an example of a Floating Action Button.", "levels": null, "corpus_id": 52936776, "sentences": ["On the left, a portion of an ESPN app is shown.", "The menu button has a 3 line hamburger icon and is labeled (a).", "It is an example of an Image Button.", "The \"Watch ESPN\" logo button next to the manu button is labeled (b) and is an example of a Clickable Image.", "On the right, there is a portion of a map app interface.", "The directions buttons is on the bottom portion and is labeled (c).", "It has a direction arrow icon on it and is an example of a Floating Action Button."], "caption": "Figure 1: Screens from apps with examples of image-based buttons using (a) an Image Button, (b) a Clickable Image, and", "local_uri": ["f7406bd6a43969a5471287344cb565bee972b376_Image_001.png"], "annotated": false, "compound": false}
{"title": "Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis", "pdf_hash": "f7406bd6a43969a5471287344cb565bee972b376", "year": 2018, "venue": "ASSETS", "alt_text": "Number of apps is along the y-axis, proportion of image-based buttons with missing label is along the x-axis.    0-0.1: 2067 apps;  0.1-0.2: 119;  0.2-0.3: 124 apps;  0.3-0.4: 150 apps;  0.4-0.5: 152 apps;  0.5-0.6: 116 apps;  0.6-0.7: 109 apps;  0.7-0.8: 122 apps;  0.8-0.9: 156 apps;  0.9-1: 2638 apps.", "levels": null, "corpus_id": 52936776, "sentences": ["Number of apps is along the y-axis, proportion of image-based buttons with missing label is along the x-axis.    0-0.1: 2067 apps;  0.1-0.2: 119;  0.2-0.3: 124 apps;  0.3-0.4: 150 apps;  0.4-0.5: 152 apps;  0.5-0.6: 116 apps;  0.6-0.7: 109 apps;  0.7-0.8: 122 apps;  0.8-0.9: 156 apps;  0.9-1: 2638 apps."], "caption": "0", "local_uri": ["f7406bd6a43969a5471287344cb565bee972b376_Image_002.png"], "annotated": false, "compound": false}
{"title": "Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis", "pdf_hash": "f7406bd6a43969a5471287344cb565bee972b376", "year": 2018, "venue": "ASSETS", "alt_text": "number of apps is along the y-axis, proportion of labeled image-based buttons with duplicate labels is the x-axis.    proportion range: number of apps;  0-0.1: 2961 apps;  0.1-0.2: 42 apps;  0.2-0.3: 34 apps;  0.3-0.4: 60 apps;  0.4-05: 44 apps;  0.5-0.6: 14 apps;  0.6-0.7: 20 apps;  0.7-0.8: 25 apps;  0.8-0.9: 24 apps;  0.9-1: 174 apps.", "levels": [[1], [2]], "corpus_id": 52936776, "sentences": ["number of apps is along the y-axis, proportion of labeled image-based buttons with duplicate labels is the x-axis.", "proportion range: number of apps;  0-0.1: 2961 apps;  0.1-0.2: 42 apps;  0.2-0.3: 34 apps;  0.3-0.4: 60 apps;  0.4-05: 44 apps;  0.5-0.6: 14 apps;  0.6-0.7: 20 apps;  0.7-0.8: 25 apps;  0.8-0.9: 24 apps;  0.9-1: 174 apps."], "caption": "Figure 3: The distribution of the proportion of labeled image- based button elements within an app that have a duplicate label. A total of 3,398 apps were tested. Most apps have a very low proportion of their image-based buttons with the error. The more negative extreme of having 90%-100% of elements with the error has a small spike as well.", "local_uri": ["f7406bd6a43969a5471287344cb565bee972b376_Image_003.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis", "pdf_hash": "f7406bd6a43969a5471287344cb565bee972b376", "year": 2018, "venue": "ASSETS", "alt_text": "Two app interfaces are shown. On the left, a graphing app with the drawing options buttons highlighted and marked as all labeled \"Tool Image.\" On the right, a ghost sound maker app with all of the buttons for making different ghost sounds (such as mummy or undead), the settings button, and the image-based home button highlighted and marked as being labeled \"Ghost Sounds.\"", "levels": [[-1], [-1], [-1]], "corpus_id": 52936776, "sentences": ["Two app interfaces are shown.", "On the left, a graphing app with the drawing options buttons highlighted and marked as all labeled \"Tool Image.\"", "On the right, a ghost sound maker app with all of the buttons for making different ghost sounds (such as mummy or undead), the settings button, and the image-based home button highlighted and marked as being labeled \"Ghost Sounds.\""], "caption": "", "local_uri": ["f7406bd6a43969a5471287344cb565bee972b376_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis", "pdf_hash": "f7406bd6a43969a5471287344cb565bee972b376", "year": 2018, "venue": "ASSETS", "alt_text": "app rating is along the x-axis. proportion of missing labels is along the y-axis.    all but one app is rated above a 2 out of 5. Most apps are rated between 3.5-4. The proportion of missing labels is uniformly spread over the ratings.", "levels": null, "corpus_id": 52936776, "sentences": ["app rating is along the x-axis.", "proportion of missing labels is along the y-axis.", "all but one app is rated above a 2 out of 5.", "Most apps are rated between 3.5-4.", "The proportion of missing labels is uniformly spread over the ratings."], "caption": "Figure 5: There is high variability in the relationship between an app\u2019s rating and its proportion of image-based buttons with missing labels. A statistically significant, but very weak correlation exists between the two factors (\u03c1 = -0.05, p = .001). The weakness of the relationship suggests current ratings do not reflect the missing labels component of app accessibility.", "local_uri": ["f7406bd6a43969a5471287344cb565bee972b376_Image_005.png"], "annotated": false, "compound": false}
{"title": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "pdf_hash": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "year": 2019, "venue": "CHI", "alt_text": "(a)(b): ClassBeacons system (c): each lamp subtly depicts how long the teacher has been around it by changing from yellow (no time spent) to green (440 seconds spent) (d):  the system supports teachers\u2019 reflection-in-action on how they have divided time and attention over students in the classroom (e): the information display is based on teachers\u2019 real-time positioning data", "levels": [[-1]], "corpus_id": 140216720, "sentences": ["(a)(b): ClassBeacons system (c): each lamp subtly depicts how long the teacher has been around it by changing from yellow (no time spent) to green (440 seconds spent) (d):  the system supports teachers\u2019 reflection-in-action on how they have divided time and attention over students in the classroom (e): the information display is based on teachers\u2019 real-time positioning data"], "caption": "Figure 1. (a)(b): ClassBeacons system (c): each lamp depicts how long the teacher has been around it by changing from yellow (no time spent) to green (440 seconds spent) (d): the system supports teachers\u2019 reflection-in-action on how they have divided time and attention over students in the classroom (e): the display is based on teachers\u2019 real-time positioning data.", "local_uri": ["a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "pdf_hash": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "year": 2019, "venue": "CHI", "alt_text": "(a): ClassBeacons system. (b):ClassBeacons deployed in a classroom. (c): Wearable unit. (d),(e):Tracking anchors.", "levels": null, "corpus_id": 140216720, "sentences": ["(a): ClassBeacons system.", "(b):ClassBeacons deployed in a classroom.", "(c): Wearable unit.", "(d),(e):Tracking anchors."], "caption": "Figure 2. (a): ClassBeacons system (b): its deployment in a classroom (c): wearable unit (d)(e): tracking anchors.", "local_uri": ["a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "pdf_hash": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "year": 2019, "venue": "CHI", "alt_text": "Examples of how ClassBeacons (yellow dots) were implemented in classrooms with different layout.", "levels": null, "corpus_id": 140216720, "sentences": ["Examples of how ClassBeacons (yellow dots) were implemented in classrooms with different layout."], "caption": "Figure 3. Examples of how ClassBeacons (yellow dots) were implemented in classrooms with different layout.", "local_uri": ["a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "pdf_hash": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "year": 2019, "venue": "CHI", "alt_text": "Three types of reported RiA echo three stages of teachers\u2019 reflective process in ALACT model.", "levels": null, "corpus_id": 140216720, "sentences": ["Three types of reported RiA echo three stages of teachers\u2019 reflective process in ALACT model."], "caption": "Figure 4. Three types of reported RiA echo three stages of teachers\u2019 reflective process in ALACT model.", "local_uri": ["a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_004.gif"], "annotated": false, "compound": false}
{"title": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "pdf_hash": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "year": 2019, "venue": "CHI", "alt_text": "Heatmaps of positioning data from a lesson of T2 (L) and T5 (R) for illustrating their reported examples.", "levels": [[1]], "corpus_id": 140216720, "sentences": ["Heatmaps of positioning data from a lesson of T2 (L) and T5 (R) for illustrating their reported examples."], "caption": "Figure 5. Heatmaps of positioning data from a lesson of T2", "local_uri": ["a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "The Zoom/Picture condition viewing a picture of a report about an inaccessible building. The Picture condition does not have a zoom widget, the Zoom condition does not show photographs uploaded by other users, and the Control condition has both types of restrictions.", "levels": [[-1], [-1]], "corpus_id": 12348285, "sentences": ["The Zoom/Picture condition viewing a picture of a report about an inaccessible building.", "The Picture condition does not have a zoom widget, the Zoom condition does not show photographs uploaded by other users, and the Control condition has both types of restrictions."], "caption": "", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_001.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "Adding a new report was identical across all conditions. Users could add address, severoty of the report and a custom message.", "levels": [[-1], [-1]], "corpus_id": 12348285, "sentences": ["Adding a new report was identical across all conditions.", "Users could add address, severoty of the report and a custom message."], "caption": "Figure 1. Top: The Zoom/Picture condition viewing a picture of a report about an inaccessible building. The Picture condition does not have the zoom widget, the Zoom condition does not show photographs uploaded by other users, and the Control condition has both types of restrictions. Bottom: Adding a new report was identical across all conditions.", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_002.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "A steady flow of reports was observed mainly in the conditions that had pictures present. While some novelty effect was observed, the last 4 days of the study were amongst the highest in terms of participation.", "levels": null, "corpus_id": 12348285, "sentences": ["A steady flow of reports was observed mainly in the conditions that had pictures present.", "While some novelty effect was observed, the last 4 days of the study were amongst the highest in terms of participation."], "caption": "Figure 2. The number of daily reports given during the study for each condition.", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_003.gif"], "annotated": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "Those that had access to pictures (i.e. Picture and Zoom/Picture conditions) generally reported higher severity levels.", "levels": null, "corpus_id": 12348285, "sentences": ["Those that had access to pictures (i.e. Picture and Zoom/Picture conditions) generally reported higher severity levels."], "caption": "Figure 3. Assigned severity level across all conditions", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_004.gif"], "annotated": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "Participants' opinion regarding the city's accessibility declined on all conditions, although at a quicker pace amongst those with access to pictures.", "levels": null, "corpus_id": 12348285, "sentences": ["Participants' opinion regarding the city's accessibility declined on all conditions, although at a quicker pace amongst those with access to pictures."], "caption": "Figure 4. Answers given by our participants (7-point Likert scale) in which we asked them to rate the level of accessibility in the city for our three surveys across all conditions).", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_005.gif"], "annotated": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "Rise in awareness regarding inaccessible spots across all conditions, although much more accentuated amongst those with access to pictures.", "levels": null, "corpus_id": 12348285, "sentences": ["Rise in awareness regarding inaccessible spots across all conditions, although much more accentuated amongst those with access to pictures."], "caption": ". Figure 5. Answers given by our participants regarding how many inaccessible spots they remembered seeing during the previous week for our three surveys across all conditions.", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_006.gif"], "annotated": false, "compound": false}
{"title": "IncluCity: using contextual cues to raise awareness on environmental accessibility", "pdf_hash": "a4ab169b57ca737d81bff153fc855f15f2095670", "year": 2013, "venue": "ASSETS", "alt_text": "Rise in awareness regarding participants' feelings about their own potential mobility problems, although at a much more accentuated amongst those with access to pictures.", "levels": null, "corpus_id": 12348285, "sentences": ["Rise in awareness regarding participants' feelings about their own potential mobility problems, although at a much more accentuated amongst those with access to pictures."], "caption": "Figure 6. Answers using a 7-point Likert scale on how participants felt about mobility problems they had, taking into account their own physical incapability or bad infrastructure for our three surveys across all conditions.", "local_uri": ["a4ab169b57ca737d81bff153fc855f15f2095670_Image_007.gif"], "annotated": false, "compound": false}
{"title": "Coco's Videos: An Empirical Investigation of Video-Player Design Features and Children's Media Use", "pdf_hash": "623885990db455959efa7dbbc2b326a4aaba4122", "year": 2018, "venue": "CHI", "alt_text": "The screen is divided into a green pane on the left and a blue pane on the right. In the green pane, text says, \"Your playlist:\" Two thumbnails appear in the green playlist pane. At the bottom, a box says, \"Total: 6 min\" next to a play button.   At the top of the blue pane are three tabs that says: History, Recommended, Search. The \"Recommended\" tab is selected, and a grid of YouTube channels are displayed, including Cbeebies, Daniel Tiger's Neighborhood, and PBS Kids.", "levels": null, "corpus_id": 5070004, "sentences": ["The screen is divided into a green pane on the left and a blue pane on the right.", "In the green pane, text says, \"Your playlist:\" Two thumbnails appear in the green playlist pane.", "At the bottom, a box says, \"Total: 6 min\" next to a play button.", "At the top of the blue pane are three tabs that says: History, Recommended, Search.", "The \"Recommended\" tab is selected, and a grid of YouTube channels are displayed, including Cbeebies, Daniel Tiger's Neighborhood, and PBS Kids."], "caption": "Figure 1: The playlist-building screen: Coco\u2019s Videos sup- ports planning out a playlist of videos to watch. Here, playlist items accumulate on the left as they are selected.", "local_uri": ["623885990db455959efa7dbbc2b326a4aaba4122_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Coco's Videos: An Empirical Investigation of Video-Player Design Features and Children's Media Use", "pdf_hash": "623885990db455959efa7dbbc2b326a4aaba4122", "year": 2018, "venue": "CHI", "alt_text": "Left: A cartoon bear-like creature with an alarm clock is on the left side of the screen, with a speech bubble that says, \"How long should we watch videos?\" On the right side are two time pickers, with the labels \"hr\" and \"min\" and displays that read \"0\" and \"20.\" On the bottom right is an arrow button.  Middle: On the left is the same cartoon bear with a speech bubble that reads \"What will you do next?\" On the right are nine buttons that each have text and a related image. The text of these read: Eat, Leave, Play outside, Sleep, Play with toys, See friends, Read a book, Bath time, Something else! The one that says \"Play outside\" is highlighted with a light background. In the bottom right corner of the screen is an arrow button.  Right: A video player shows a cartoon. The video player is nearly full-screen, with player controls (seek bar, pause button, etc.) around the edges. A large transparent overlay is in the middle of the screen. On it is a drawing of an alarm clock with a speech bubble that says \"One minute left!\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 5070004, "sentences": ["Left: A cartoon bear-like creature with an alarm clock is on the left side of the screen, with a speech bubble that says, \"How long should we watch videos?\"", "On the right side are two time pickers, with the labels \"hr\" and \"min\" and displays that read \"0\" and \"20.\"", "On the bottom right is an arrow button.", "Middle: On the left is the same cartoon bear with a speech bubble that reads \"What will you do next?\"", "On the right are nine buttons that each have text and a related image.", "The text of these read: Eat, Leave, Play outside, Sleep, Play with toys, See friends, Read a book, Bath time, Something else!", "The one that says \"Play outside\" is highlighted with a light background.", "In the bottom right corner of the screen is an arrow button.", "Right: A video player shows a cartoon.", "The video player is nearly full-screen, with player controls (seek bar, pause button, etc.)", "around the edges.", "A large transparent overlay is in the middle of the screen.", "On it is a drawing of an alarm clock with a speech bubble that says \"One minute left!\""], "caption": "", "local_uri": ["623885990db455959efa7dbbc2b326a4aaba4122_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Coco's Videos: An Empirical Investigation of Video-Player Design Features and Children's Media Use", "pdf_hash": "623885990db455959efa7dbbc2b326a4aaba4122", "year": 2018, "venue": "CHI", "alt_text": "Part A on the left: This screenshot shows a picture of a book with the words \"Time to read!\" underneath. On the left side is a picture of a smiling bear-like cartoon character with an alarm clock. On the right side is a very large button with a house.  Part B in the middle: This screenshot is identical to the one in part A, except that a thumbnail-sized video is inset in the top right corner. The thumbnail shows the Daniel Tiger character as well as all the video controls of a YouTube player (e.g., pause, next, previous, volume, etc).  Part C on the right: This screenshot is identical to the one in part A, except that the home button is missing.", "levels": null, "corpus_id": 5070004, "sentences": ["Part A on the left: This screenshot shows a picture of a book with the words \"Time to read!\" underneath.", "On the left side is a picture of a smiling bear-like cartoon character with an alarm clock.", "On the right side is a very large button with a house.", "Part B in the middle: This screenshot is identical to the one in part A, except that a thumbnail-sized video is inset in the top right corner.", "The thumbnail shows the Daniel Tiger character as well as all the video controls of a YouTube player (e.g., pause, next, previous, volume, etc).", "Part C on the right: This screenshot is identical to the one in part A, except that the home button is missing."], "caption": "Figure 3: Transition screen experiences. Left: Neutral experience where child has the option to press to press the home button (bottom) right. Middle: Post-play experience where child has the option to press the home button and create a new playlist (bottom right) and videos related to the most recently viewed video play automatically (top right). Right: Controlled experience where no home button is displayed and no content on the screen is responsive.In all three conditions, the activity the child selected as his or her next activity when constructing the playlist isreflected on screen (here, reading). In all three conditions, when the screen is displayed, Coco says, \u201cNow it\u2019s time to read. Are you ready to read?\u201d", "local_uri": ["623885990db455959efa7dbbc2b326a4aaba4122_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "This diagram contains image symbols that represent the placement of either observers, participants or equipment. The diagram shows the layout of the experimental room where the study was held. The meeting room is described in the following points:\n\nLocation of participants and key equipment: There are three smartphones, two camera devices, one audio recording device, and one TV screen. All smartphones are placed front of the participants. The participants are separated with one table in the middle. The deaf or hard of hearing participant is sitting alone facing the hearing participants from the other side. Between them, there is only one audio recording device. The two camera devices (for video recording) are positioned next to each other, one is facing the deaf participant, whereas the second camera is facing the hearing participants. The TV screen is centered on the left side of the table so that all participants can easily see it. Front of each participant, there is one mobile application, one pen, and several paper sheets.\n\nObservation Room: In this room, there are two observers sitting next to each other facing the experimental room. The observation room has one table, two pens, several paper sheets, and one smartphone to record chat session via screen recording. The observation room has a one-way mirror for the observers to monitor the experiment session.\n\nThe diagram also lists all key equipment with a brief description of each.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 52941724, "sentences": ["This diagram contains image symbols that represent the placement of either observers, participants or equipment.", "The diagram shows the layout of the experimental room where the study was held.", "The meeting room is described in the following points:\n\nLocation of participants and key equipment: There are three smartphones, two camera devices, one audio recording device, and one TV screen.", "All smartphones are placed front of the participants.", "The participants are separated with one table in the middle.", "The deaf or hard of hearing participant is sitting alone facing the hearing participants from the other side.", "Between them, there is only one audio recording device.", "The two camera devices (for video recording) are positioned next to each other, one is facing the deaf participant, whereas the second camera is facing the hearing participants.", "The TV screen is centered on the left side of the table so that all participants can easily see it.", "Front of each participant, there is one mobile application, one pen, and several paper sheets.", "Observation Room: In this room, there are two observers sitting next to each other facing the experimental room.", "The observation room has one table, two pens, several paper sheets, and one smartphone to record chat session via screen recording.", "The observation room has a one-way mirror for the observers to monitor the experiment session.", "The diagram also lists all key equipment with a brief description of each."], "caption": "Figure 1: Floorplan of the meeting room and observation room, indicating the location of participants and observers, recording devices, and key equipment used in the study.", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "A male participant with two female participants facing each other in a group meeting while using the mobile application as a form of communication between them. The male participant is identified as a Deaf whereas the two female participants are identified as hearing. The hearing participants are sitting next to each other, while the deaf participant is sitting on the other side facing them.  All participants are using the mobile phone for communication purpose while discussing one of the study scenarios. The deaf participant is separated from the hearing participants by a table. In the table, there are paper sheets and pens for each participant, besides an audio recording device in the middle of the table to ensure proper recording of the study session.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 52941724, "sentences": ["A male participant with two female participants facing each other in a group meeting while using the mobile application as a form of communication between them.", "The male participant is identified as a Deaf whereas the two female participants are identified as hearing.", "The hearing participants are sitting next to each other, while the deaf participant is sitting on the other side facing them.", "All participants are using the mobile phone for communication purpose while discussing one of the study scenarios.", "The deaf participant is separated from the hearing participants by a table.", "In the table, there are paper sheets and pens for each participant, besides an audio recording device in the middle of the table to ensure proper recording of the study session."], "caption": "Figure 2: Photograph of participants using the mobile application, in a group meeting with three participants.", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "In this figure, two screenshots were taken from two different mobile application used during the study. The first screenshot was taken from the hearing participant whereas the second screenshot was taken from the deaf participant. The first screenshot contains a microphone button to record audio in order to generate text messages. It also contains a simple screen with a blue and yellow background. The screen is displaying the participant registered ID with a text message generated from the microphone. In the text message, one of the words is underlined because the ASR is not confident in its recognition. The second screenshot contains a keyboard for the deaf participant to text while communicating with the hearing participant. It also contains a simple display with the hearing participant ID as well as text messages.", "levels": null, "corpus_id": 52941724, "sentences": ["In this figure, two screenshots were taken from two different mobile application used during the study.", "The first screenshot was taken from the hearing participant whereas the second screenshot was taken from the deaf participant.", "The first screenshot contains a microphone button to record audio in order to generate text messages.", "It also contains a simple screen with a blue and yellow background.", "The screen is displaying the participant registered ID with a text message generated from the microphone.", "In the text message, one of the words is underlined because the ASR is not confident in its recognition.", "The second screenshot contains a keyboard for the deaf participant to text while communicating with the hearing participant.", "It also contains a simple display with the hearing participant ID as well as text messages."], "caption": "Figure 3: Screenshot of the application used during the study:", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "A box plot displaying the Median Intensity across each condition, with units in decibels (dB). First results are presented for the 9 participants who spoke in all three conditions, with median value of 48.882 for Markup, 41.828 for No ASR, and 50.541 for ASR.  There were significant differences between Markup and No ASR, as well as ASR and No ASR.  Next, results are shown for all 12 participants, with median value of 49.253 for Markup and 49.578 for ASR.  There was no significant difference between these two conditions.", "levels": [[1], [2, 1], [2], [2, 1], [2]], "corpus_id": 52941724, "sentences": ["A box plot displaying the Median Intensity across each condition, with units in decibels (dB).", "First results are presented for the 9 participants who spoke in all three conditions, with median value of 48.882 for Markup, 41.828 for No ASR, and 50.541 for ASR.", "There were significant differences between Markup and No ASR, as well as ASR and No ASR.", "Next, results are shown for all 12 participants, with median value of 49.253 for Markup and 49.578 for ASR.", "There was no significant difference between these two conditions."], "caption": "Figure 4: Box plots displaying the median intensity for each hearing participant\u2019s voice, across all three conditions", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_007.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "A box plot displaying the Mean Harmonicity across each condition, with units in decibels (dB). First results are presented for the 9 participants who spoke in all three conditions, with median value of 4.826 for Markup, 2.555 for No ASR, and 4.083 for ASR.  There was a significant difference between Markup and No ASR.  Next, results are shown for all 12 participants, with median value of 4.848 for Markup and 4.372 for ASR.  There was no significant difference between these two conditions.", "levels": [[1], [2], [2], [2], [2]], "corpus_id": 52941724, "sentences": ["A box plot displaying the Mean Harmonicity across each condition, with units in decibels (dB).", "First results are presented for the 9 participants who spoke in all three conditions, with median value of 4.826 for Markup, 2.555 for No ASR, and 4.083 for ASR.", "There was a significant difference between Markup and No ASR.", "Next, results are shown for all 12 participants, with median value of 4.848 for Markup and 4.372 for ASR.", "There was no significant difference between these two conditions."], "caption": "Figure 5: Box plots for harmonicity (mean harmonics-to-noise ratio), for each hearing participant, across the three conditions", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_008.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "A box plot displaying the F1 Mean across each condition, with units in Hertz. First results are presented for the 9 participants who spoke in all three conditions, with median value of 948.497 for Markup, 1152.505 for No ASR, and 905.744 for ASR.  There were significant differences between Markup and No ASR, as well as ASR and No ASR.  Next, results are shown for all 12 participants, with median value of 959.743 for Markup and 909.571 for ASR.  There was no significant difference between these two conditions.", "levels": [[1], [2, 1], [3], [2], [2]], "corpus_id": 52941724, "sentences": ["A box plot displaying the F1 Mean across each condition, with units in Hertz.", "First results are presented for the 9 participants who spoke in all three conditions, with median value of 948.497 for Markup, 1152.505 for No ASR, and 905.744 for ASR.", "There were significant differences between Markup and No ASR, as well as ASR and No ASR.", "Next, results are shown for all 12 participants, with median value of 959.743 for Markup and 909.571 for ASR.", "There was no significant difference between these two conditions."], "caption": "", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_009.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "A box plot displaying the F2 Mean across each condition, with units in Hertz. First results are presented for the 9 participants who spoke in all three conditions, with median value of 2510.066 for Markup, 2707.750 for No ASR, and 2577.583 for ASR.  There were significant differences between Markup and No ASR, as well as ASR and No ASR.  Next, results are shown for all 12 participants, with median value of 2524.027 for Markup and 2594.337 for ASR.  There was no significant difference between these two conditions.", "levels": [[1], [2], [2], [2, 1], [2]], "corpus_id": 52941724, "sentences": ["A box plot displaying the F2 Mean across each condition, with units in Hertz.", "First results are presented for the 9 participants who spoke in all three conditions, with median value of 2510.066 for Markup, 2707.750 for No ASR, and 2577.583 for ASR.", "There were significant differences between Markup and No ASR, as well as ASR and No ASR.", "Next, results are shown for all 12 participants, with median value of 2524.027 for Markup and 2594.337 for ASR.", "There was no significant difference between these two conditions."], "caption": "Figure 7: Box plots for F2 formant means across conditions", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_010.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "pdf_hash": "4cecd70a9e46a761774a54ed11d613b33721b95d", "year": 2018, "venue": "ASSETS", "alt_text": "A box plot displaying the Speech Rate across each condition, with units in words per minute. First results are presented for the 9 participants who spoke in all three conditions, with median value of 209.828 for Markup, 150.846 for No ASR, and 169.242 for ASR.  There was a significant difference between Markup and No ASR.   Next, results are shown for all 12 participants, with median value of 203.418 for Markup and 171.534 for ASR.  There was no significant difference between these two conditions.", "levels": [[1], [2, 1], [3], [2, 1], [3]], "corpus_id": 52941724, "sentences": ["A box plot displaying the Speech Rate across each condition, with units in words per minute.", "First results are presented for the 9 participants who spoke in all three conditions, with median value of 209.828 for Markup, 150.846 for No ASR, and 169.242 for ASR.", "There was a significant difference between Markup and No ASR.", "Next, results are shown for all 12 participants, with median value of 203.418 for Markup and 171.534 for ASR.", "There was no significant difference between these two conditions."], "caption": "Figure 8: Box plot of results for speech rate, in words per minute, across conditions", "local_uri": ["4cecd70a9e46a761774a54ed11d613b33721b95d_Image_011.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "HoloLearn: Wearable Mixed Reality for People with Neurodevelopmental Disorders (NDD)", "pdf_hash": "dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c", "year": 2018, "venue": "ASSETS", "alt_text": "Example of Mixed Reality with Microsoft HoloLens; on the background, what the user sees on the head-mounted device.", "levels": null, "corpus_id": 52942599, "sentences": ["Example of Mixed Reality with Microsoft HoloLens; on the background, what the user sees on the head-mounted device."], "caption": "Figure 1: Example of Mixed Reality with Microsoft HoloLens; on the background, what the user sees on the head-mounted device.", "local_uri": ["dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "HoloLearn: Wearable Mixed Reality for People with Neurodevelopmental Disorders (NDD)", "pdf_hash": "dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c", "year": 2018, "venue": "ASSETS", "alt_text": "Laying the table activity: on the table there are some cutlery, the positions on which they have to be repositioned are highlighted. There is also the virtual assistant waiting for the user to take action.", "levels": null, "corpus_id": 52942599, "sentences": ["Laying the table activity: on the table there are some cutlery, the positions on which they have to be repositioned are highlighted.", "There is also the virtual assistant waiting for the user to take action."], "caption": "Figure 3: \u201cLaying the table\u201d activity", "local_uri": ["dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "HoloLearn: Wearable Mixed Reality for People with Neurodevelopmental Disorders (NDD)", "pdf_hash": "dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c", "year": 2018, "venue": "ASSETS", "alt_text": "Garbage collection activity: on the floor there are two bins and some plastic bottles/cans. The virtual assistant indicates to the user which object s/he should collect.", "levels": null, "corpus_id": 52942599, "sentences": ["Garbage collection activity: on the floor there are two bins and some plastic bottles/cans.", "The virtual assistant indicates to the user which object s/he should collect."], "caption": "Figure 5: \u201cGarbage collection\u201d activity", "local_uri": ["dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "HoloLearn: Wearable Mixed Reality for People with Neurodevelopmental Disorders (NDD)", "pdf_hash": "dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c", "year": 2018, "venue": "ASSETS", "alt_text": "Virtual assistant in action: the Minion indicates a can to encourage the user to collect it.", "levels": null, "corpus_id": 52942599, "sentences": ["Virtual assistant in action: the Minion indicates a can to encourage the user to collect it."], "caption": "Figure 6: Virtual assistant in action", "local_uri": ["dcc4356d8d606ca4ead6ddb7c1d29748bdeade7c_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "PassChords: secure multi-touch authentication for blind people", "pdf_hash": "2d955bfeb3b7b43d5d1f45161b068a705190fde9", "year": 2012, "venue": "ASSETS '12", "alt_text": "A series of four images showing a hand entering a PassChord into a phone. From left to right: the user calibrates by pressing four fingers on the screen; the user taps the screen with her middle finger; the user taps the screen with the index, middle, and ring fingers; and the user taps the screen with her ring finger. The image shows that the phone vibrates with every touch.", "levels": null, "corpus_id": 13982453, "sentences": ["A series of four images showing a hand entering a PassChord into a phone.", "From left to right: the user calibrates by pressing four fingers on the screen; the user taps the screen with her middle finger; the user taps the screen with the index, middle, and ring fingers; and the user taps the screen with her ring finger.", "The image shows that the phone vibrates with every touch."], "caption": "", "local_uri": ["2d955bfeb3b7b43d5d1f45161b068a705190fde9_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "PassChords: secure multi-touch authentication for blind people", "pdf_hash": "2d955bfeb3b7b43d5d1f45161b068a705190fde9", "year": 2012, "venue": "ASSETS '12", "alt_text": "A bar graph with all possible finger tap patterns on the x-axis and percentages on the y-axis.", "levels": [[1]], "corpus_id": 13982453, "sentences": ["A bar graph with all possible finger tap patterns on the x-axis and percentages on the y-axis."], "caption": "", "local_uri": ["2d955bfeb3b7b43d5d1f45161b068a705190fde9_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PassChords: secure multi-touch authentication for blind people", "pdf_hash": "2d955bfeb3b7b43d5d1f45161b068a705190fde9", "year": 2012, "venue": "ASSETS '12", "alt_text": "A boxplot showing the password method and order (first or second in the session) on the x-axis and time (in seconds) on the y-axis. The boxes marking the range of PIN times are far higher than those for PassChord times.", "levels": [[1], [2]], "corpus_id": 13982453, "sentences": ["A boxplot showing the password method and order (first or second in the session) on the x-axis and time (in seconds) on the y-axis.", "The boxes marking the range of PIN times are far higher than those for PassChord times."], "caption": "", "local_uri": ["2d955bfeb3b7b43d5d1f45161b068a705190fde9_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "A person wearing Oculus headset standing and VR painting a fashion garment. The TutoriVR interface is to his left and has a video tutorial showing a screencast recording of how to paint the dress and the other associated interfaces of the widget", "levels": null, "corpus_id": 140216543, "sentences": ["A person wearing Oculus headset standing and VR painting a fashion garment.", "The TutoriVR interface is to his left and has a video tutorial showing a screencast recording of how to paint the dress and the other associated interfaces of the widget"], "caption": "", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "Shows a basic video player, having a video, a seek timeline, lay button, and a move button", "levels": null, "corpus_id": 140216543, "sentences": ["Shows a basic video player, having a video, a seek timeline, lay button, and a move button"], "caption": "", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "Shows the TutoriVR system having the improved video player, on the top, perspective thumbnail widget in the bottom. The video shows a green coil like structure being drawn, with a rendering of it being draw in real-time in the perspective thumbnail widget in the bottom.", "levels": null, "corpus_id": 140216543, "sentences": ["Shows the TutoriVR system having the improved video player, on the top, perspective thumbnail widget in the bottom.", "The video shows a green coil like structure being drawn, with a rendering of it being draw in real-time in the perspective thumbnail widget in the bottom."], "caption": "", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_005.png"], "annotated": false, "compound": false}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "Left: Ambiguous cube-like strokes when viewed on a video; Right: VIew in the Perspective thumbnail widget is shown, where the cube strokes become more interpretable when viewed from a different perspective.", "levels": null, "corpus_id": 140216543, "sentences": ["Left: Ambiguous cube-like strokes when viewed on a video; Right: VIew in the Perspective thumbnail widget is shown, where the cube strokes become more interpretable when viewed from a different perspective."], "caption": "Figure 4: Demonstration of the Perspective Thumbnail Wid- get. Left: Ambiguous cube-like strokes when viewed on a video; Right: The cube strokes become more interpretable when viewed from a diferent perspective in the Perspective Thumbnail Widget.", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_012.png"], "annotated": false, "compound": false}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "An image of the awareness Widget is shown It has a timeline marked from -30s/+30s with icons of events overlayed on the timeline. There is a thumbnail of the video to the right of the awareness widget.", "levels": null, "corpus_id": 140216543, "sentences": ["An image of the awareness Widget is shown It has a timeline marked from -30s/+30s with icons of events overlayed on the timeline.", "There is a thumbnail of the video to the right of the awareness widget."], "caption": "Figure 5: Awareness Widget. This widget is view-fxed in VR. It provides a minimal set of important tutorial content to help users be more aware of the tutorial progress.", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "Four examples of critical steps are shown. First is in top left, and shows multiple red hearts drawn with same center. Second is in Top right and shows a controller that is partly hidden by an object in the environment.", "levels": null, "corpus_id": 140216543, "sentences": ["Four examples of critical steps are shown.", "First is in top left, and shows multiple red hearts drawn with same center.", "Second is in Top right and shows a controller that is partly hidden by an object in the environment."], "caption": "Relative 3D depth         (b) Controller interactions (c) Intricate 3D strokes            (d) Relative 3D depthFigure 6: Examples of critical steps in the study tasks: (a) Drawing hearts that are on diferent planes; (b) An action performed with obstructed controller; (c) Intricate strokes involving 3D loops and coils; (d) assembling a 3D face por- trait", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_014.jpg", "1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_015.jpg"], "annotated": false, "compound": true}
{"title": "TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality", "pdf_hash": "1a6e2babf7ecffc32f194066eb3e7b7d470364c8", "year": 2019, "venue": "CHI", "alt_text": "Third is in the bottom left and shows intricate strokes involving 3D loops and coils. Fourth is in the bottom right that shows a 3D face portrait being in the process of assembly.", "levels": null, "corpus_id": 140216543, "sentences": ["Third is in the bottom left and shows intricate strokes involving 3D loops and coils.", "Fourth is in the bottom right that shows a 3D face portrait being in the process of assembly."], "caption": "(c) Intricate 3D strokes            (d) Relative 3D depth", "local_uri": ["1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_016.jpg", "1a6e2babf7ecffc32f194066eb3e7b7d470364c8_Image_017.jpg"], "annotated": false, "compound": true}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "Sequence of images showing a user using a keyboard to access the Airbnb website using SPRITEs. Each image sequence shows one action.", "levels": null, "corpus_id": 5046751, "sentences": ["Sequence of images showing a user using a keyboard to access the Airbnb website using SPRITEs.", "Each image sequence shows one action."], "caption": "", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "Description: It is an image of a hand with our hardware solution. The thumb, the middle finger, the pinky and the hand each have one vibration motor.", "levels": null, "corpus_id": 5046751, "sentences": ["Description: It is an image of a hand with our hardware solution.", "The thumb, the middle finger, the pinky and the hand each have one vibration motor."], "caption": "Figure 2: A Velcro band with the four tactors attached at different positions on the hand.", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "It is an image of a keyboard layout showing what regions of the keyboard are used in SPRITEs. Each edge has been marked as a separate region, and the number row has been marked as a separate region. The alphabet keys under the numeric rows are marked as a nested region for the numeric row.", "levels": null, "corpus_id": 5046751, "sentences": ["It is an image of a keyboard layout showing what regions of the keyboard are used in SPRITEs.", "Each edge has been marked as a separate region, and the number row has been marked as a separate region.", "The alphabet keys under the numeric rows are marked as a nested region for the numeric row."], "caption": "", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "The image shows a person's hand wearing our prototype. The hand is hovering over the keyboard as two Wii remotes mounted on top of the laptop track the user's hand.", "levels": null, "corpus_id": 5046751, "sentences": ["The image shows a person's hand wearing our prototype.", "The hand is hovering over the keyboard as two Wii remotes mounted on top of the laptop track the user's hand."], "caption": "Figure 3: Fingers prototype. The hand-wearing glove with infrared LEDs is being tracked by two Wii remotes.", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "Sequence of images showing a user using a keyboard to access a menubar using SPRITEs. Each image sequence shows one action.", "levels": [[-1], [-1]], "corpus_id": 5046751, "sentences": ["Sequence of images showing a user using a keyboard to access a menubar using SPRITEs.", "Each image sequence shows one action."], "caption": "", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "Sequence of images showing a user using a keyboard to access a table using SPRITEs. Each image sequence shows one action.", "levels": null, "corpus_id": 5046751, "sentences": ["Sequence of images showing a user using a keyboard to access a table using SPRITEs.", "Each image sequence shows one action."], "caption": "", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "Sequence of images showing a user using a keyboard to search in a table using SPRITEs. Each image sequence shows one action.", "levels": null, "corpus_id": 5046751, "sentences": ["Sequence of images showing a user using a keyboard to search in a table using SPRITEs.", "Each image sequence shows one action."], "caption": "", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Nonvisual Interaction Techniques at the Keyboard Surface", "pdf_hash": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "year": 2018, "venue": "CHI", "alt_text": "The graph shows that our system, Sprites peorformed 3x times better in task completion rate.", "levels": [[2]], "corpus_id": 5046751, "sentences": ["The graph shows that our system, Sprites peorformed 3x times better in task completion rate."], "caption": "Figure 9: Graph showing task completion rates for different kinds of tasks in our user study", "local_uri": ["5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [2], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "Shows 4 images from left to right-  (A) an extruder electrospinning a textile; (B) an interactive rigid plastic and electrospun lamp with controls being activated by a human's hand; (C) a electrospun and rigid plastic printed flower in a pot with other living plants that can sense water levels in the soil; (D) a comic strip of a sheep watching a television that is being touched by a human finger.", "levels": null, "corpus_id": 140220640, "sentences": ["Shows 4 images from left to right-  (A) an extruder electrospinning a textile; (B) an interactive rigid plastic and electrospun lamp with controls being activated by a human's hand; (C) a electrospun and rigid plastic printed flower in a pot with other living plants that can sense water levels in the soil; (D) a comic strip of a sheep watching a television that is being touched by a human finger."], "caption": "Figure 1: A range of objects fabricated on our novel 3D printer using rigid plastic and electrospun textiles: (A) a close-up of our printer electrospinning; (B) an origami-style folding lamp with piezoresistve brightness control and a soft custom-shaped capacitive toggle switch that use electrospun textiles for sensing and tactile experiences; (C) an actuated electrospun and rigid plastic fower that opens when an electrospun textile liquid sensor detects sufcient water in the soil; (D) a sheep comic that uses electrospun textile with capacitive sensing to create an interactive tactile experience.", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_001.png"], "annotated": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "An image of a small hinge that has electrospun regions for a soft bendable area and rigid plastic regions on its ends", "levels": null, "corpus_id": 140220640, "sentences": ["An image of a small hinge that has electrospun regions for a soft bendable area and rigid plastic regions on its ends"], "caption": "Figure 2: A simple hinge fabricated with our 3D printer us- ing rigid plastic printing and melt electrospinning on the same extruder.", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_003.png"], "annotated": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "Two images side by side: (A) a wide shot of the 3d printer showing two extruders; (B) a close-up of the extruder used for both rigid plastic and melt electrospinning.", "levels": null, "corpus_id": 140220640, "sentences": ["Two images side by side: (A) a wide shot of the 3d printer showing two extruders; (B) a close-up of the extruder used for both rigid plastic and melt electrospinning."], "caption": "Figure 5: Our 3D printer that supports melt electrospinning and rigid plastic printing using only the left extruder (A). A close-up of the extruder shows the high voltage ground wire connected to hotend nozzle (B). We note that (A) shows an ad- ditional extruder on the right\u2013 a recent change for exploring a third material type (i.e. conductive flament).", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_013.png"], "annotated": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "3 Rows of images show results from each test. Row 1 shows results of electrospinning swatches at decreasing extrusion rates from (1800 mm/min to 10 mm/min); Row 2 shows swatches made with increasing temperatures from 260 centigrade to 300 centrigrade; Row 3 shows swatches made with decreasing infill density from 100-10%", "levels": [[-1], [-1]], "corpus_id": 140220640, "sentences": ["3 Rows of images show results from each test.", "Row 1 shows results of electrospinning swatches at decreasing extrusion rates from (1800 mm/min to 10 mm/min); Row 2 shows swatches made with increasing temperatures from 260 centigrade to 300 centrigrade; Row 3 shows swatches made with decreasing infill density from 100-10%"], "caption": "Figure 6: A series of tests we performed on extrusion rate (A-E), temperature (F-J), and infll density (K-O) to determine melt electrospinning process parameters for our 3D printer. Tool-paths were generated from the geometry of a 30x30mm square swatch using a 3D printer slicer engine. Scale bar: 20mm.", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_015.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "Two images side-by-side. (A) shows a star-shaped capacitive sensor; (B) shows a finger interacting with the sensor", "levels": null, "corpus_id": 140220640, "sentences": ["Two images side-by-side. (A) shows a star-shaped capacitive sensor; (B) shows a finger interacting with the sensor"], "caption": "", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_016.png"], "annotated": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "Fig7: Two images of line graphs stacked vertically. The first shows a positive correlation between size of an object that is electrospun with its infill density. The second shows how scaling and object can produce an desired expected size based on an infill percentage", "levels": [[1], [3], [3]], "corpus_id": 140220640, "sentences": ["Fig7: Two images of line graphs stacked vertically.", "The first shows a positive correlation between size of an object that is electrospun with its infill density.", "The second shows how scaling and object can produce an desired expected size based on an infill percentage"], "caption": "Figure 7: Infll density is positively correlated with the size of fabricated electrospun objects (A). The inverse of this re- lationship determines a geometry scale factor that when ap- plied to objects prior to slicing will result in an electrospun object of desired size.", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_017.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles", "pdf_hash": "6a69750f5fba6953fc582e1c17c7313ac0cfddce", "year": 2019, "venue": "CHI", "alt_text": "Two figures side-by-side: (A) a piezoresitive electrospun sensor connected to electrical wires; (B) a human hand interacting with the sensor and results being visualized on a laptop screen", "levels": null, "corpus_id": 140220640, "sentences": ["Two figures side-by-side: (A) a piezoresitive electrospun sensor connected to electrical wires; (B) a human hand interacting with the sensor and results being visualized on a laptop screen"], "caption": "", "local_uri": ["6a69750f5fba6953fc582e1c17c7313ac0cfddce_Image_018.png"], "annotated": false, "compound": false}
{"title": "How to Work in the Car of the Future?: A Neuroergonomical Study Assessing Concentration, Performance and Workload Based on Subjective, Behavioral and Neurophysiological Insights", "pdf_hash": "6bc976b894794778fbb5ca9106e396b9f60b4418", "year": 2019, "venue": "CHI", "alt_text": "Prototypical autonomous car with interactive window screens, facing forward towards three large screens for displaying the self-driving scenario", "levels": null, "corpus_id": 140250712, "sentences": ["Prototypical autonomous car with interactive window screens, facing forward towards three large screens for displaying the self-driving scenario"], "caption": "Figure 1: Set-up of the stationary driving simulator (\u00a9Audi AG).", "local_uri": ["6bc976b894794778fbb5ca9106e396b9f60b4418_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "How to Work in the Car of the Future?: A Neuroergonomical Study Assessing Concentration, Performance and Workload Based on Subjective, Behavioral and Neurophysiological Insights", "pdf_hash": "6bc976b894794778fbb5ca9106e396b9f60b4418", "year": 2019, "venue": "CHI", "alt_text": "This configuration has large-area, bright light with high blue components and blurred window screens.", "levels": null, "corpus_id": 140250712, "sentences": ["This configuration has large-area, bright light with high blue components and blurred window screens."], "caption": "Figure 2: Concentration-focused interior configuration of the self-driving car prototype: large-area, bright light with high blue components and blurred window screens (\u00a9Audi AG).", "local_uri": ["6bc976b894794778fbb5ca9106e396b9f60b4418_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The Cost of Turning Heads: A Comparison of a Head-Worn Display to a Smartphone for Supporting Persons with Aphasia in Conversation", "pdf_hash": "49fa96c34cf091f5341f4d0969d6126820216e0b", "year": 2016, "venue": "ASSETS", "alt_text": "In the upper left-hand corner are a series of 8 small images showing views from the experimental software. The two left-most images show white icons on a black background for the categories actions and objects. The other six show black and white line drawings with a black gradient on the lower half of the image and white text for the word. The lower left-hand  corner shows three cards from the experimental card game with the same black and white images from the software glued on blue card stock. The middle image shows a tan colored Google Glass with a monocular display and thin titanium frame. In the same image is a 3D printed remote control with a white casing and black icons on the buttons. It is attached to an Arduino by a black wire appearing only partially in the picture. The right-most picture shows a black, contemporary smartphone with the software loaded on screen. The upper half of the screen shows the software's black and white line drawings, while the lower half show a white onscreen button layout in the same cross-shape as the remote control and with the same black icons for labels.", "levels": null, "corpus_id": 14524624, "sentences": ["In the upper left-hand corner are a series of 8 small images showing views from the experimental software.", "The two left-most images show white icons on a black background for the categories actions and objects.", "The other six show black and white line drawings with a black gradient on the lower half of the image and white text for the word.", "The lower left-hand  corner shows three cards from the experimental card game with the same black and white images from the software glued on blue card stock.", "The middle image shows a tan colored Google Glass with a monocular display and thin titanium frame.", "In the same image is a 3D printed remote control with a white casing and black icons on the buttons.", "It is attached to an Arduino by a black wire appearing only partially in the picture.", "The right-most picture shows a black, contemporary smartphone with the software loaded on screen.", "The upper half of the screen shows the software's black and white line drawings, while the lower half show a white onscreen button layout in the same cross-shape as the remote control and with the same black icons for labels."], "caption": "Figure 2. Top-left: Words from the vocabulary-prompting software, showing both the top level of the hierarchy (\u2018actions\u2019 and \u2018objects\u2019) and examples of words from the second level. Bottom-left: Examples of Go Fish cards used for introducing the game; card sets used for the training and testing tasks were identical except they did NOT include text. Middle: HWD with remote control for navigation: cycle through words in the current level of the hierarchy (left/right arrows), select a top-level category and view its contents (circle), cancel out of the second level and return to the top level (\u2018X\u2019), and play audio for the current word (speaker icon). Right: Smartphone version, showing an action word above the navigational controls.", "local_uri": ["49fa96c34cf091f5341f4d0969d6126820216e0b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Hybrid-Brailler: Combining Physical and Gestural Interaction for Mobile Braille Input and Editing", "pdf_hash": "c88e15d64a455eac71eb3d4a2d74a78f47632d07", "year": 2018, "venue": "CHI", "alt_text": "The image consists of 3 sub-figures. First, an illustration of the back of a mobile device with 9 buttons (3 by 3 grid) on landscape orientation; second, an illustration of a mobile touchscreen device on landscape mode where both thumbs are performing gestures on the screen; third, the Hybrid-Brailler prototype, which consists of a 3D-printed case, with 9 buttons, attached to a smartphone.", "levels": null, "corpus_id": 5049785, "sentences": ["The image consists of 3 sub-figures.", "First, an illustration of the back of a mobile device with 9 buttons (3 by 3 grid) on landscape orientation; second, an illustration of a mobile touchscreen device on landscape mode where both thumbs are performing gestures on the screen; third, the Hybrid-Brailler prototype, which consists of a 3D-printed case, with 9 buttons, attached to a smartphone."], "caption": "Figure 1. Hybrid-Brailler, a Braille input and editing system that combines physical and gestural interaction. (a) The system consists of 9 physical buttons used for Braille input. Notice that the 3x3 layout enables usage on both portrait and landscape modes.\u200c", "local_uri": ["c88e15d64a455eac71eb3d4a2d74a78f47632d07_Image_001.png"], "annotated": false, "compound": false}
{"title": "Hybrid-Brailler: Combining Physical and Gestural Interaction for Mobile Braille Input and Editing", "pdf_hash": "c88e15d64a455eac71eb3d4a2d74a78f47632d07", "year": 2018, "venue": "CHI", "alt_text": "The image consists of 3 sub-figures. First a user holding the prototype in landscape mode and performing an horizontal gesture with one of the thumbs. Second, a similar image, but both thumbs are on the screen: one of them just pressing down and the second performing an horizontal gesture to the left. Third, a similar image with a thumb pressing the screen and a second thumb performing an up gesture.", "levels": null, "corpus_id": 5049785, "sentences": ["The image consists of 3 sub-figures.", "First a user holding the prototype in landscape mode and performing an horizontal gesture with one of the thumbs.", "Second, a similar image, but both thumbs are on the screen: one of them just pressing down and the second performing an horizontal gesture to the left.", "Third, a similar image with a thumb pressing the screen and a second thumb performing an up gesture."], "caption": "Figure 2. Editing operations and matching gestures. (a) Caret movement using horizontal directional gestures. (b) Text selection using caret movement and touching the screen with a second thumb. (c) After selection, users cycle through the clipboard operations using vertical directional gestures and keeping the other thumb on the screen.", "local_uri": ["c88e15d64a455eac71eb3d4a2d74a78f47632d07_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Trigger-Action Programming for Personalising Humanoid Robot Behaviour", "pdf_hash": "0f88360b44f8c41b7c044f3c4d24538f4039eed8", "year": 2019, "venue": "CHI", "alt_text": "A high-level overview of the main logical components of the approach (Rule manager, Context Server, Tailoring Environment, Robot, Sensors) as well as their relationships.", "levels": null, "corpus_id": 140220651, "sentences": ["A high-level overview of the main logical components of the approach (Rule manager, Context Server, Tailoring Environment, Robot, Sensors) as well as their relationships."], "caption": "Figure 1: The Proposed Approach", "local_uri": ["0f88360b44f8c41b7c044f3c4d24538f4039eed8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Trigger-Action Programming for Personalising Humanoid Robot Behaviour", "pdf_hash": "0f88360b44f8c41b7c044f3c4d24538f4039eed8", "year": 2019, "venue": "CHI", "alt_text": "The hierarchy of triggers supported by the Tailoring Environment, in which the robot-related aspects have been expanded", "levels": null, "corpus_id": 140220651, "sentences": ["The hierarchy of triggers supported by the Tailoring Environment, in which the robot-related aspects have been expanded"], "caption": "Figure 2: Robot-Related Triggers in the Tailoring Tool", "local_uri": ["0f88360b44f8c41b7c044f3c4d24538f4039eed8_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Trigger-Action Programming for Personalising Humanoid Robot Behaviour", "pdf_hash": "0f88360b44f8c41b7c044f3c4d24538f4039eed8", "year": 2019, "venue": "CHI", "alt_text": "In the left-hand panel of the Tailoring Environment it is possible to compose the actions involved in a rule, through either a sequential operator or a parallel one.", "levels": null, "corpus_id": 140220651, "sentences": ["In the left-hand panel of the Tailoring Environment it is possible to compose the actions involved in a rule, through either a sequential operator or a parallel one."], "caption": "Figure 3: Action Composition Specification (See Left Hand Panel) in the Tailoring Environment", "local_uri": ["0f88360b44f8c41b7c044f3c4d24538f4039eed8_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Trigger-Action Programming for Personalising Humanoid Robot Behaviour", "pdf_hash": "0f88360b44f8c41b7c044f3c4d24538f4039eed8", "year": 2019, "venue": "CHI", "alt_text": "The hierarchy of triggers unfolded for specifying a IoT appliance -based trigger", "levels": null, "corpus_id": 140220651, "sentences": ["The hierarchy of triggers unfolded for specifying a IoT appliance -based trigger"], "caption": "Figure 4: Selection of a Trigger Involving IoT Appliances", "local_uri": ["0f88360b44f8c41b7c044f3c4d24538f4039eed8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Trigger-Action Programming for Personalising Humanoid Robot Behaviour", "pdf_hash": "0f88360b44f8c41b7c044f3c4d24538f4039eed8", "year": 2019, "venue": "CHI", "alt_text": "The main logical components of the architecture with details of their principal communications.", "levels": null, "corpus_id": 140220651, "sentences": ["The main logical components of the architecture with details of their principal communications."], "caption": "Figure 6: The Architecture of the Platform", "local_uri": ["0f88360b44f8c41b7c044f3c4d24538f4039eed8_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Trigger-Action Programming for Personalising Humanoid Robot Behaviour", "pdf_hash": "0f88360b44f8c41b7c044f3c4d24538f4039eed8", "year": 2019, "venue": "CHI", "alt_text": "Min, max and quartiles of time in seconds (rendered on the x axis) for creating R4-R11 rules (rendered on the Y axis).", "levels": null, "corpus_id": 140220651, "sentences": ["Min, max and quartiles of time in seconds (rendered on the x axis) for creating R4-R11 rules (rendered on the Y axis)."], "caption": "Figure 7: Box and Whisker Plot for the Execution Times", "local_uri": ["0f88360b44f8c41b7c044f3c4d24538f4039eed8_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Hidden in Plain Sight: an Exploration of a Visual Language for Near-Eye Out-of-Focus Displays in the Peripheral View", "pdf_hash": "fcdecb55ba0d72332570b2cea55e38208728e699", "year": 2016, "venue": "CHI", "alt_text": "Top view of peripheral displays inside a pair of glasses (indicated by the purple lines along the temples), and the corresponding periphery", "levels": null, "corpus_id": 14178982, "sentences": ["Top view of peripheral displays inside a pair of glasses (indicated by the purple lines along the temples), and the corresponding periphery"], "caption": "Figure 1. Top view of peripheral displays inside a pair of glasses (in\u00ad dicated by the purple lines along the temples), and the corresponding periphery, which is a \ufb01eld of about 180\u00b0. The displays are located near the border of the eyes, within the 20\u00b0 extremes of our periphery. The call-out shows a concept drawing of a tiny display that is attached to the leg of the glasses.", "local_uri": ["fcdecb55ba0d72332570b2cea55e38208728e699_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Hidden in Plain Sight: an Exploration of a Visual Language for Near-Eye Out-of-Focus Displays in the Peripheral View", "pdf_hash": "fcdecb55ba0d72332570b2cea55e38208728e699", "year": 2016, "venue": "CHI", "alt_text": "Left: experimental setup of glasses with peripheral displays used during our study. The near-eye out-of-focus peripheral displays were used to determine the usable display space, planar and retinal variables, and perception of motion. Middle: a participant wearing the glasses while sitting in front of an eye tracker. Right: closeup of a RoboPeak display that shows an arrow (same size as in the study).", "levels": null, "corpus_id": 14178982, "sentences": ["Left: experimental setup of glasses with peripheral displays used during our study.", "The near-eye out-of-focus peripheral displays were used to determine the usable display space, planar and retinal variables, and perception of motion.", "Middle: a participant wearing the glasses while sitting in front of an eye tracker.", "Right: closeup of a RoboPeak display that shows an arrow (same size as in the study)."], "caption": "Figure 2. Left: experimental setup of glasses with peripheral displays used during our study. The near-eye out-of-focus peripheral displays were used to determine the usable display space, planar and retinal variables, and perception of motion. Middle: a participant wearing the glasses while sitting in front of an eye tracker. Right: closeup of a RoboPeak display that shows an arrow (same size as in the study).", "local_uri": ["fcdecb55ba0d72332570b2cea55e38208728e699_Image_002.png", "fcdecb55ba0d72332570b2cea55e38208728e699_Image_003.jpg", "fcdecb55ba0d72332570b2cea55e38208728e699_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Hidden in Plain Sight: an Exploration of a Visual Language for Near-Eye Out-of-Focus Displays in the Peripheral View", "pdf_hash": "fcdecb55ba0d72332570b2cea55e38208728e699", "year": 2016, "venue": "CHI", "alt_text": "An overview of the static elements used during the second stage of our study. The set of graphical elements contains simple shapes, composite shapes, shapes with two different colors, and some more complex figures.", "levels": [[-1], [-1]], "corpus_id": 14178982, "sentences": ["An overview of the static elements used during the second stage of our study.", "The set of graphical elements contains simple shapes, composite shapes, shapes with two different colors, and some more complex figures."], "caption": "", "local_uri": ["fcdecb55ba0d72332570b2cea55e38208728e699_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Hidden in Plain Sight: an Exploration of a Visual Language for Near-Eye Out-of-Focus Displays in the Peripheral View", "pdf_hash": "fcdecb55ba0d72332570b2cea55e38208728e699", "year": 2016, "venue": "CHI", "alt_text": "Side view of the usable display area for each eye. The outer rectangle represents the peripheral display, the yellow rectangle the screen space that we used during the next stages of the study. The dots present the aggregated results of the boundary of the viewable region. The rendering of the eye in the background is for illustrative purposes and does not reflect exact measurements.", "levels": null, "corpus_id": 14178982, "sentences": ["Side view of the usable display area for each eye.", "The outer rectangle represents the peripheral display, the yellow rectangle the screen space that we used during the next stages of the study.", "The dots present the aggregated results of the boundary of the viewable region.", "The rendering of the eye in the background is for illustrative purposes and does not reflect exact measurements."], "caption": "Figure 5. Side view of the usable display area for each eye. The outer rectangle represents the peripheral display, the yellow rectangle the screen space that we used during the next stages of the study. The dots present the aggregated results of the boundary of the viewable region. The rendering of the eye in the background is for illustrative purposes and does not re\ufb02ect exact measurements.", "local_uri": ["fcdecb55ba0d72332570b2cea55e38208728e699_Image_007.jpg", "fcdecb55ba0d72332570b2cea55e38208728e699_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Hidden in Plain Sight: an Exploration of a Visual Language for Near-Eye Out-of-Focus Displays in the Peripheral View", "pdf_hash": "fcdecb55ba0d72332570b2cea55e38208728e699", "year": 2016, "venue": "CHI", "alt_text": "Recognition rates for static graphical elements without the use of a printed reference sheet. Figure 7. Recognition rates for static graphical elements when indicating the perceived shape on a printed reference sheet. ors tend to be perceived as blending together which sometimes leads to shapes becoming more difficult to recognize or colors being misinterpreted. It seems that orientation does contribute to the perceivability of shapes, which is probably caused by the specificities of how our eyes work. Scanning imagery up-down (vertical orientation) is often easier than scanning imagery from close by to further away (horizontal orientation). Because the displays are very close to the eyes, when a shape is oriented horizontally, the projected change in distance from the eye seems much bigger than the actual change in distance from the eye. Stage 3: Motion Overall, participants performed very well when recognizing motion. Since we added meaningful movement to shapes, such as an arrow moving in the direction in which the arrow is pointing, we expected the movement to help in recognizing the shape. Figure 8 presents the recognition rates of movement paths or changes of shapes, while Figure 9 presents the recognition rates", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 14178982, "sentences": ["Recognition rates for static graphical elements without the use of a printed reference sheet.", "Figure 7.", "Recognition rates for static graphical elements when indicating the perceived shape on a printed reference sheet.", "ors tend to be perceived as blending together which sometimes leads to shapes becoming more difficult to recognize or colors being misinterpreted.", "It seems that orientation does contribute to the perceivability of shapes, which is probably caused by the specificities of how our eyes work.", "Scanning imagery up-down (vertical orientation) is often easier than scanning imagery from close by to further away (horizontal orientation).", "Because the displays are very close to the eyes, when a shape is oriented horizontally, the projected change in distance from the eye seems much bigger than the actual change in distance from the eye.", "Stage 3: Motion Overall, participants performed very well when recognizing motion.", "Since we added meaningful movement to shapes, such as an arrow moving in the direction in which the arrow is pointing, we expected the movement to help in recognizing the shape.", "Figure 8 presents the recognition rates of movement paths or changes of shapes, while Figure 9 presents the recognition rates"], "caption": "Figure 6. Recognition rates for static graphical elements without the use of a printed reference sheet.", "local_uri": ["fcdecb55ba0d72332570b2cea55e38208728e699_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Hidden in Plain Sight: an Exploration of a Visual Language for Near-Eye Out-of-Focus Displays in the Peripheral View", "pdf_hash": "fcdecb55ba0d72332570b2cea55e38208728e699", "year": 2016, "venue": "CHI", "alt_text": "Recognition rates for static graphical elements when indicating the perceived shape on a printed reference sheet.", "levels": [[1]], "corpus_id": 14178982, "sentences": ["Recognition rates for static graphical elements when indicating the perceived shape on a printed reference sheet."], "caption": "Figure 7. Recognition rates for static graphical elements when indicating the perceived shape on a printed reference sheet.", "local_uri": ["fcdecb55ba0d72332570b2cea55e38208728e699_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EarTouch: Facilitating Smartphone Use for Visually Impaired People in Mobile and Public Scenarios", "pdf_hash": "354a22265b3bf594417920dc0b17be8cdae36bd6", "year": 2019, "venue": "CHI", "alt_text": "Figure 4 represents some example images of ear, finger and face after preprocessing. Usually face regions have a larger area than ear, and when using the top helix of ear to touch on the screen, it may seem like a fingertip.", "levels": null, "corpus_id": 140228710, "sentences": ["Figure 4 represents some example images of ear, finger and face after preprocessing.", "Usually face regions have a larger area than ear, and when using the top helix of ear to touch on the screen, it may seem like a fingertip."], "caption": "Figure 4: Some example images of touch after preprocessing:", "local_uri": ["354a22265b3bf594417920dc0b17be8cdae36bd6_Image_009.png"], "annotated": false, "compound": false}
{"title": "EarTouch: Facilitating Smartphone Use for Visually Impaired People in Mobile and Public Scenarios", "pdf_hash": "354a22265b3bf594417920dc0b17be8cdae36bd6", "year": 2019, "venue": "CHI", "alt_text": "Figure 5 represents the pipeline of image processing. After the interpolation and removing the background noise, we got a clear shape of ear. The adjacent regions were merged into a single one and then fitted in one bounding box. The weighted center and circle fitting of the ear region is also represented.", "levels": null, "corpus_id": 140228710, "sentences": ["Figure 5 represents the pipeline of image processing.", "After the interpolation and removing the background noise, we got a clear shape of ear.", "The adjacent regions were merged into a single one and then fitted in one bounding box.", "The weighted center and circle fitting of the ear region is also represented."], "caption": "", "local_uri": ["354a22265b3bf594417920dc0b17be8cdae36bd6_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "EarTouch: Facilitating Smartphone Use for Visually Impaired People in Mobile and Public Scenarios", "pdf_hash": "354a22265b3bf594417920dc0b17be8cdae36bd6", "year": 2019, "venue": "CHI", "alt_text": "Figure 6 represents two methods the users adopted for one-handed input, using the thumb finger of holding hand with palm facing the back of smartphone, or using the index finger of holding hand with palm facing the screen.", "levels": null, "corpus_id": 140228710, "sentences": ["Figure 6 represents two methods the users adopted for one-handed input, using the thumb finger of holding hand with palm facing the back of smartphone, or using the index finger of holding hand with palm facing the screen."], "caption": "Figure 6: thumb or index finger for one-handed input", "local_uri": ["354a22265b3bf594417920dc0b17be8cdae36bd6_Image_014.png"], "annotated": false, "compound": false}
{"title": "Inclusion and Education: 3D Printing for Integrated Classrooms", "pdf_hash": "9353cdaa11a95348c8e7e70988acdc99d5eb98aa", "year": 2015, "venue": "ASSETS", "alt_text": "A) Several hand-held size 3D printed objects in a pile. b) A screenshot of a 3D model of a table-top sized basketball hoop game. B) A pile of 3D printed finger rings with \"UMBC\" embossed on the outside in Braille characters.", "levels": null, "corpus_id": 1281896, "sentences": ["A) Several hand-held size 3D printed objects in a pile.", "b) A screenshot of a 3D model of a table-top sized basketball hoop game.", "B) A pile of 3D printed finger rings with \"UMBC\" embossed on the outside in Braille characters."], "caption": "Figure 3. Example output from the integrated class. A) Artifacts created by students including key chains, statues, and cookie cutters. B) 3D model of a basketball court made by ID1. C) Output from the class fulfilling a job for a client (Braille rings).", "local_uri": ["9353cdaa11a95348c8e7e70988acdc99d5eb98aa_Image_005.png"], "annotated": false, "compound": false}
{"title": "Inclusion and Education: 3D Printing for Integrated Classrooms", "pdf_hash": "9353cdaa11a95348c8e7e70988acdc99d5eb98aa", "year": 2015, "venue": "ASSETS", "alt_text": "A student with ID sits in front of a computer and 3D printer while he uses a mouse to manipulate an object on-screen while his UG partner watches from the neighboring seat.", "levels": null, "corpus_id": 1281896, "sentences": ["A student with ID sits in front of a computer and 3D printer while he uses a mouse to manipulate an object on-screen while his UG partner watches from the neighboring seat."], "caption": "Figure 5. A student with ID prepares to print a 3D model while his UG teammate observes and offers support.", "local_uri": ["9353cdaa11a95348c8e7e70988acdc99d5eb98aa_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "\"This is how I want to learn\": High Functioning Autistic Teens Co-Designing a Serious Game", "pdf_hash": "9a23c7510962eb626e941d559e3fe15d4d744867", "year": 2016, "venue": "CHI", "alt_text": "This is a flow chart that illustrates the sessions with the stakeholders. It states stakeholders' role, objective and tools used for each of the sessions.", "levels": null, "corpus_id": 7890187, "sentences": ["This is a flow chart that illustrates the sessions with the stakeholders.", "It states stakeholders' role, objective and tools used for each of the sessions."], "caption": "Figure 1: sequence of the PD sessions", "local_uri": ["9a23c7510962eb626e941d559e3fe15d4d744867_Image_001.png"], "annotated": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Diagrams demonstrating how to perform the two types of selection supported by WristTap. The menu is a 3x3 grid displayed on a mockup of a square watch face, with three menu buttons along the bottom row of the grid.\n\n(a) Two-Step Selection is performed by first touching a menu button with one finger or thumb. After a short delay the menu is displayed, and the user (while still keeping the first finger on the menu button), touches an item with another finger to make the selection.\n\n(b) One-Step Selection is performed by tapping the menu button with one finger or thumb, and the location of the desired item with another finger, in one action. Both fingers hit the menu together.", "levels": null, "corpus_id": 999354, "sentences": ["Diagrams demonstrating how to perform the two types of selection supported by WristTap.", "The menu is a 3x3 grid displayed on a mockup of a square watch face, with three menu buttons along the bottom row of the grid.", "(a) Two-Step Selection is performed by first touching a menu button with one finger or thumb.", "After a short delay the menu is displayed, and the user (while still keeping the first finger on the menu button), touches an item with another finger to make the selection.", "(b) One-Step Selection is performed by tapping the menu button with one finger or thumb, and the location of the desired item with another finger, in one action.", "Both fingers hit the menu together."], "caption": "Figure 1. WristTap supports two selection methods: (a) two-step selection, which requires visual search, and (b) one-step selection, which is faster, but requires the user to remember the locations of items.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Diagram demonstrating how to make a selection in TwoTap. The menu is a 3x3 grid displayed on a mockup of a square watch face, with three menu buttons along the bottom row of the grid\n\nFirst, a menu button is tapped, to open the menu. Next, the desired item is tapped.", "levels": null, "corpus_id": 999354, "sentences": ["Diagram demonstrating how to make a selection in TwoTap.", "The menu is a 3x3 grid displayed on a mockup of a square watch face, with three menu buttons along the bottom row of the grid\n\nFirst, a menu button is tapped, to open the menu.", "Next, the desired item is tapped."], "caption": "Figure 2. In TwoTap, selections are made with two sequential actions \u2013 a tap to open the menu, and another to select an item.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "The three menus used in Study 2 \u2013 Colors (left), Styles (middle), and Shapes (right).", "levels": null, "corpus_id": 999354, "sentences": ["The three menus used in Study 2 \u2013 Colors (left), Styles (middle), and Shapes (right)."], "caption": "Figure 3. The three-category menu used in Study 2.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Photos of two demo applications, each with a WristTap menu open. The music player application's menu has controls for play/pause, next track, previous track, favorite, volume up, and volume down. The home screen application's menu has toggles for wifi, cell data, notifications, gps, airplane mode, and a button to open settings.", "levels": [[-1], [-1], [-1]], "corpus_id": 999354, "sentences": ["Photos of two demo applications, each with a WristTap menu open.", "The music player application's menu has controls for play/pause, next track, previous track, favorite, volume up, and volume down.", "The home screen application's menu has toggles for wifi, cell data, notifications, gps, airplane mode, and a button to open settings."], "caption": "Figure 4. Demo applications: a music player (left), and a home screen application over the native watch face (right).", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Diagrams showing how to perform two-step and one-step selection trials for Study 1.", "levels": [[-1]], "corpus_id": 999354, "sentences": ["Diagrams showing how to perform two-step and one-step selection trials for Study 1."], "caption": "Figure 5. Trials for the two selection types. In two-step selection trials (1a,b,c), the user must first touch the indicated menu but- ton, then select the revealed item. In one-step selection trials (2a,b), the user touches both the menu button and item together.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Boxplots showing average participant trial completion times by grid size and selection method.", "levels": [[1]], "corpus_id": 999354, "sentences": ["Boxplots showing average participant trial completion times by grid size and selection method."], "caption": "Figure 6. Average participant trial completion times by grid size and selection method.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Box plots showing average participant error rates by grid size and selection method.", "levels": [[1]], "corpus_id": 999354, "sentences": ["Box plots showing average participant error rates by grid size and selection method."], "caption": "Figure 7. Average participant error rates by grid and method.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Diagram showing an example of a trial from Study 2. (a) a blue target is shown, (b) the user selects the blue item from the colors menu to set the current selection to 'blue', (c) the user draws a slice gesture through the blue target.", "levels": [[-1], [-1]], "corpus_id": 999354, "sentences": ["Diagram showing an example of a trial from Study 2. (", "a) a blue target is shown, (b) the user selects the blue item from the colors menu to set the current selection to 'blue', (c) the user draws a slice gesture through the blue target."], "caption": "Figure 8. Trials in Study 2. For each trial, an icon appears on the screen (a), the participant selects the corresponding item from the menu (b), and draws a stroke through the icon (c).", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "A series of box plots showing how the average time taken by participants to select the correct item changed as the blocks progressed. In the first 6 blocks, the median swoops downward from ~4000ms to ~1500ms, then stays around this level for the remaining blocks.", "levels": [[1], [3, 2]], "corpus_id": 999354, "sentences": ["A series of box plots showing how the average time taken by participants to select the correct item changed as the blocks progressed.", "In the first 6 blocks, the median swoops downward from ~4000ms to ~1500ms, then stays around this level for the remaining blocks."], "caption": "Figure 9. Per-participant average times to select the correct item in each block of nine trials.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Dot plot showing how the average percentage use of one-step selection changed over the study blocks. Shows a steady progression toward higher use of one-step selection, culminating in nearly 100% use in the final six blocks.", "levels": [[1], [3, 2]], "corpus_id": 999354, "sentences": ["Dot plot showing how the average percentage use of one-step selection changed over the study blocks.", "Shows a steady progression toward higher use of one-step selection, culminating in nearly 100% use in the final six blocks."], "caption": "Figure 10. Per-participant average use of the one-step selection method for each block. Error bars indicate standard error.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Diagrams showing the steps to complete an example trial for Study 3. A blue target is shown. The user swipes left to open a category menu (Colors, Styles, Shapes) and taps a category to bring up a vertical menu of items for that category. The user then swipes to the blue item matching the target, and taps it.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 999354, "sentences": ["Diagrams showing the steps to complete an example trial for Study 3.", "A blue target is shown.", "The user swipes left to open a category menu (Colors, Styles, Shapes) and taps a category to bring up a vertical menu of items for that category.", "The user then swipes to the blue item matching the target, and taps it."], "caption": "Figure 11. Swipe-and-Tap trials in Study 3. For each trial, an icon appears on the screen (a), the participant left swipes to re- veal the category menu and selects a category (b), then selects the item in a vertical list (c).", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Box plots showing the error-free trial completion times for the testing stage of Study 3. WristTap and TwoTap are close to one another, with medians around 900-1000ms / selection. Swipe-and-Tap has a median that is higher, closer to 2500ms, and a larger spread.", "levels": [[1], [2], [2]], "corpus_id": 999354, "sentences": ["Box plots showing the error-free trial completion times for the testing stage of Study 3.", "WristTap and TwoTap are close to one another, with medians around 900-1000ms / selection.", "Swipe-and-Tap has a median that is higher, closer to 2500ms, and a larger spread."], "caption": "Figure 12. Error-free trial completion times for the testing stage of Study 3, by technique.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Faster Command Selection on Touchscreen Watches", "pdf_hash": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "year": 2016, "venue": "CHI", "alt_text": "Three segmented bars, showing the median time taken for the component actions of the three selection techniques.\n\nWristTap - two-finger tap (900ms)\n\nTwoTap - tap category (637ms); tap item (344ms)\n\nSwipe-and-Tap - swipe to open menu (685ms); tap category (482ms); select item in vertical list (1209ms)", "levels": [[1], [2]], "corpus_id": 999354, "sentences": ["Three segmented bars, showing the median time taken for the component actions of the three selection techniques.", "WristTap - two-finger tap (900ms)\n\nTwoTap - tap category (637ms); tap item (344ms)\n\nSwipe-and-Tap - swipe to open menu (685ms); tap category (482ms); select item in vertical list (1209ms)"], "caption": "Figure 13. Median time taken for the component actions of selections in the testing stage of Study 3.", "local_uri": ["6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "pdf_hash": "becd79d567f86115aa509b992dd1a09a765043cb", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 1 (a): Single View: One window with male instructor on left, female sign language interpreter in center, laptop with real-time captions at center-bottom, and overhead slides on right.", "levels": null, "corpus_id": 1868554, "sentences": ["Figure 1 (a): Single View: One window with male instructor on left, female sign language interpreter in center, laptop with real-time captions at center-bottom, and overhead slides on right."], "caption": "A mainstream classroom with interpreter and captions. Important regions are highlighted.MVP with only the most important information presented visually closer together.Figure 1: Multiple View Perspectives (MVP) brings important aspects of the classroom visually closer together for deaf and hard of hearing students.viewing, and presenting those views on a single laptop screen may help by bringing the many sources of information vi\u00ad sually closer together [1, 7]. Figure 1 illustrates a typical classroom (a) compared to our Multiple View Perspective (MVP) interface designed to optimize viewing (b).Traditional lecture capture systems seem like promising solutions, but typically use only one camera to capture video, and, unless dedicated sta\ufb00 are on hand to manage the sys\u00ad tem, the view from the camera does not change and results in a video that is boring to watch [9]. With only a single view, users may lack the visual information required for adequate context [14]. Dedicated video production sta\ufb00 can improve video context and interest, but at a signi\ufb01cant cost. More\u00ad over, a camera operator operating a single video camera or even multiple video cameras may not be able to predict the learning needs of a deaf or hard of hearing student, let alone several deaf and hard of hearing students.MVP is a collection of video perspectives where a videoperspective is a context-aware video view of a speci\ufb01c class\u00ad room region.  MVP overcomes the limitations of traditional lecture capture systems and empowers deaf and hard of hear\u00ad ing students to independently bring regions of the classroom together onto one screen. This becomes useful in modern classrooms that often contain multiple visual sources of in\u00ad formation, such as overhead slides, demonstrations, white boards, and other students. It also provides a mechanism for deaf and hard of hearing users to easily customize their learning focus in the classroom for their own needs, by en\u00ad abling them to select, focus and customize their view.In this paper, we analyze the e\ufb00ectiveness of Multiple View Perspectives for improving visual access for deaf and hard of hearing students in viewing and recording accessi\u00ad ble mainstream lectures. We show that for deaf and hard of hearing students viewing and recording a classroom pre\u00ad sentation via the student\u2019s laptop and multiple mobile cam\u00ad eras, the MVP approach can reduce information loss and positively a\ufb00ect cognitive load.", "local_uri": ["becd79d567f86115aa509b992dd1a09a765043cb_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "pdf_hash": "becd79d567f86115aa509b992dd1a09a765043cb", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 2: An example MVP set up: On left, female instructor is pointing at overhead slides in the center. In the bottom-center, the student is looking at multiple views of classroom on his laptop. On right, female sign language interpreter is signing.", "levels": null, "corpus_id": 1868554, "sentences": ["Figure 2: An example MVP set up: On left, female instructor is pointing at overhead slides in the center.", "In the bottom-center, the student is looking at multiple views of classroom on his laptop.", "On right, female sign language interpreter is signing."], "caption": "", "local_uri": ["becd79d567f86115aa509b992dd1a09a765043cb_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "pdf_hash": "becd79d567f86115aa509b992dd1a09a765043cb", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 3: Picture on left shows single view layout: male instructor is on left, female sign language interpreter is in center, laptop real-time captions are at bottom center, and overhead slides are on right. Picture on right shows multiple view layout: four equal size windows are shown. Overhead slides are on top left, laptop real-time captions are shown on top right, male instructor is shown on bottom right, and female sign language interpreter is shown on bottom left.", "levels": null, "corpus_id": 1868554, "sentences": ["Figure 3: Picture on left shows single view layout: male instructor is on left, female sign language interpreter is in center, laptop real-time captions are at bottom center, and overhead slides are on right.", "Picture on right shows multiple view layout: four equal size windows are shown.", "Overhead slides are on top left, laptop real-time captions are shown on top right, male instructor is shown on bottom right, and female sign language interpreter is shown on bottom left."], "caption": "", "local_uri": ["becd79d567f86115aa509b992dd1a09a765043cb_Image_005.jpg", "becd79d567f86115aa509b992dd1a09a765043cb_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "pdf_hash": "becd79d567f86115aa509b992dd1a09a765043cb", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 5: Shows compact view layout with emphasis on captions, with two main windows and one sub-window. On top right is small window of male instructor. On left window is a laptop with real-time captions, and on right window is the overhead slides.", "levels": null, "corpus_id": 1868554, "sentences": ["Figure 5: Shows compact view layout with emphasis on captions, with two main windows and one sub-window.", "On top right is small window of male instructor.", "On left window is a laptop with real-time captions, and on right window is the overhead slides."], "caption": "", "local_uri": ["becd79d567f86115aa509b992dd1a09a765043cb_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "pdf_hash": "becd79d567f86115aa509b992dd1a09a765043cb", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 6: Shows compact multiple view with interpreter emphasis. On the top right is a small window showing male instructor. On left window is sign language interpreter. On right window is overhead slides.", "levels": null, "corpus_id": 1868554, "sentences": ["Figure 6: Shows compact multiple view with interpreter emphasis.", "On the top right is a small window showing male instructor.", "On left window is sign language interpreter.", "On right window is overhead slides."], "caption": "", "local_uri": ["becd79d567f86115aa509b992dd1a09a765043cb_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "pdf_hash": "becd79d567f86115aa509b992dd1a09a765043cb", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 7 shows tabular chart of statistics for participant responses in study. It describes questions and responses. The questions are \"What is your rating for single/multiple view\", \"Did it help\", \"is it easy to use\", etc. It shows significant difference between SVP/MVP and C-MVP.", "levels": [[1], [1], [1], [2]], "corpus_id": 1868554, "sentences": ["Figure 7 shows tabular chart of statistics for participant responses in study.", "It describes questions and responses.", "The questions are \"What is your rating for single/multiple view\", \"Did it help\", \"is it easy to use\", etc.", "It shows significant difference between SVP/MVP and C-MVP."], "caption": "", "local_uri": ["becd79d567f86115aa509b992dd1a09a765043cb_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig. 3. Transcription of multimodal text from interaction between Maya and a special needs assistant", "levels": null, "corpus_id": 4975252, "sentences": ["Fig. 3.", "Transcription of multimodal text from interaction between Maya and a special needs assistant"], "caption": "Fig. 3. Transcription of interaction between Maya (M) and Special needs assistant (S). Text in CAPS is electronic speech, underlined italicized text represents gaze, italicized text in brackets is gesture/action. Numbers in brackets are time in seconds.", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig. 4. Transcription of multimodal text from interaction between Maya and researcher", "levels": null, "corpus_id": 4975252, "sentences": ["Fig. 4.", "Transcription of multimodal text from interaction between Maya and researcher"], "caption": "Fig. 4. Transcription of researcher\u2019s (R) utterance to Maya. Italicized text in text in brackets represents gesture/action, numbers in brackets represents time in seconds.", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig. 5. Transcription of multimodal text from interaction between researcher, teacher and Maya", "levels": null, "corpus_id": 4975252, "sentences": ["Fig. 5.", "Transcription of multimodal text from interaction between researcher, teacher and Maya"], "caption": "Fig. 5. Transcription of interaction between Maya (M), the class teacher (CT) and researcher (R). Text in CAPS is electronic speech, underlined italicized text represents gaze, italicized text in brackets is gesture/action.", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig 6. line drawing from video still showing Grace (left), researcher (middle) and Maya (right). Grace's view is obstructed by her AAC screen.", "levels": null, "corpus_id": 4975252, "sentences": ["Fig 6.", "line drawing from video still showing Grace (left), researcher (middle) and Maya (right).", "Grace's view is obstructed by her AAC screen."], "caption": "Fig 6. Video still image of Maya (right) & researcher interacting whilst Grace (left) is distanced from the conversation.", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig. 7. Transcription of multimodal text from interaction between Clara, teacher and Maya", "levels": null, "corpus_id": 4975252, "sentences": ["Fig. 7.", "Transcription of multimodal text from interaction between Clara, teacher and Maya"], "caption": "Fig. 7. Transcription of interaction between Clara (C), teacher (CT) and Maya (M). Text in brackets is gesture/action, underlined italicized text represents gaze.", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig. 9. Transcription of multimodal text from interaction between researcher, Grace and Maya.", "levels": null, "corpus_id": 4975252, "sentences": ["Fig. 9.", "Transcription of multimodal text from interaction between researcher, Grace and Maya."], "caption": "Fig. 9. Transcription of interaction with researcher (R), Grace", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Design Opportunities for AAC and Children with Severe Speech and Physical Impairments", "pdf_hash": "79eddf4985cc6c58ed1366b22a3c3cf31e892bb6", "year": 2018, "venue": "CHI", "alt_text": "Fig. 10. Transcription of multimodal text from interaction between Maya, researcher and Grace.", "levels": null, "corpus_id": 4975252, "sentences": ["Fig. 10.", "Transcription of multimodal text from interaction between Maya, researcher and Grace."], "caption": "Fig. 10. Transcription of interaction between Maya (M), researcher (R) and Grace (G). Text in brackets is gesture/action, underlined italicized text represents gaze.", "local_uri": ["79eddf4985cc6c58ed1366b22a3c3cf31e892bb6_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "On the left, control condition is illustrated. In the illustration, there is a silluet of a man, is standing next to HMPD fixed on a tripot with a manikin head and watching projected screen while there is a table in front of him with a mouse on it. Middle, standing condition is illustrated. The illustration is basicly same as previous one. Only differences are that there is not a tripod and the man is wearing the HMPD. On the right, the setting of the illustration is same with the middle one except the table is longer. In this illustration, the man is walking while wearing the HMPD.", "levels": null, "corpus_id": 10409718, "sentences": ["On the left, control condition is illustrated.", "In the illustration, there is a silluet of a man, is standing next to HMPD fixed on a tripot with a manikin head and watching projected screen while there is a table in front of him with a mouse on it.", "Middle, standing condition is illustrated.", "The illustration is basicly same as previous one.", "Only differences are that there is not a tripod and the man is wearing the HMPD.", "On the right, the setting of the illustration is same with the middle one except the table is longer.", "In this illustration, the man is walking while wearing the HMPD."], "caption": "Figure 1. Experimental conditions used in the study. From left to right: (1) \u201ccontrol\u201d condition representing natural viewing cases where the displayed content is referenced to world; (2) \u201cstanding\u201d condition in which subject was immobile and wearing the HMPD prototype; (3) \u201cwalking\u201d condition in which subject was mobile and wearing the HMPD prototype.", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "Two common ways to present information from a HMD. First one: head referenced displays where the virtual object moves with the head. Second one: earth referenced displays where the virtual information is fixed on a point in real world.", "levels": null, "corpus_id": 10409718, "sentences": ["Two common ways to present information from a HMD.", "First one: head referenced displays where the virtual object moves with the head.", "Second one: earth referenced displays where the virtual information is fixed on a point in real world."], "caption": "", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "Head Mounted Projection Display Prototype including a mini pc, a battery, and projector.", "levels": null, "corpus_id": 10409718, "sentences": ["Head Mounted Projection Display Prototype including a mini pc, a battery, and projector."], "caption": "Figure 3. HMPD prototype used in the study.", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "Photos of a user (not an actual participant) performing both tasks in walking condition in the classroom. In the photo, on the top, user is performing VS task while walking. The photo on the bottom is a photo of a user completing MOT task while walking. In both photos the user is following a path arranged by tables in the classroom. A manniquin head fixed on a tripod stands on a table at the begining of the walking path.", "levels": null, "corpus_id": 10409718, "sentences": ["Photos of a user (not an actual participant) performing both tasks in walking condition in the classroom.", "In the photo, on the top, user is performing VS task while walking.", "The photo on the bottom is a photo of a user completing MOT task while walking.", "In both photos the user is following a path arranged by tables in the classroom.", "A manniquin head fixed on a tripod stands on a table at the begining of the walking path."], "caption": "Figure 4. A user (not an actual participant) performing tasks in walking condition by following a defined path. From top to bottom: (1) the user is performing a VS task, (2) the user is performing a MOT task. The room is illuminated in these pictures to increase the visibility of the experimental setup.", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "Illustrations of the MOT and VS tasks: On the top-left, MOT task illustrated. In the illustration there are 10 discs scattered on a black background. 5 of them are green to indicate their status as targets, others are white. On the bottom-left, MOT task is illustrated. On the illustration there are 10 discs. 7 of them are white, 2 of them are green and one of them is red. On the top-right VS task is illustrated. There are 18 L shaped 2d object scattered on a black background. On the bottom-right, VS task is illustrated. There are 17 L shaped 2d object and 1 T shaped 2d object scattered on a black background.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 10409718, "sentences": ["Illustrations of the MOT and VS tasks: On the top-left, MOT task illustrated.", "In the illustration there are 10 discs scattered on a black background.", "5 of them are green to indicate their status as targets, others are white.", "On the bottom-left, MOT task is illustrated.", "On the illustration there are 10 discs.", "7 of them are white, 2 of them are green and one of them is red.", "On the top-right VS task is illustrated.", "There are 18 L shaped 2d object scattered on a black background.", "On the bottom-right, VS task is illustrated.", "There are 17 L shaped 2d object and 1 T shaped 2d object scattered on a black background."], "caption": "Figure 5. Illustrations of the MOT and VS tasks: (1) MOT task; targets are blinking (2) MOT task; after the targets and distractors stopped moving, subject is selecting the targets. Green indicates correct choice of the participant, red indicates incorrect choice of the participants. (3) VS task; display consists 18 L shaped distractors (4) VS task, display consists 17 L shaped distractors and 1 T shaped target.", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "On the top-left, there is the mean response times chart for MOT task. Mean response time for control and standing conditinions are  5.59 s, and 5.41 s, respectively. On the  top-right, there is the mean response times chart for VS task. Meadn response time for control, standing, and walking conditions are 1.52 s, 1.50 s, 1.54 s, respectively.  On the bottom-left, there is the accuracy chart for MOT task. Mean accuracies for control, standing and walking conditions are %89.4, %87.9, and %78.3, respectively. The relationship between control vs. walking and standing vs. walking are marked with *.   On the bottom-right there is the mean chart of accuracy for VS task. Mean accuracies for control, standing, and walking conditions are %86.4, %87.6, and %86.7, respectively", "levels": [[1], [2], [1], [2], [1], [2], [1], [1], [2]], "corpus_id": 10409718, "sentences": ["On the top-left, there is the mean response times chart for MOT task.", "Mean response time for control and standing conditinions are  5.59 s, and 5.41 s, respectively.", "On the  top-right, there is the mean response times chart for VS task.", "Meadn response time for control, standing, and walking conditions are 1.52 s, 1.50 s, 1.54 s, respectively.", "On the bottom-left, there is the accuracy chart for MOT task.", "Mean accuracies for control, standing and walking conditions are %89.4, %87.9, and %78.3, respectively.", "The relationship between control vs. walking and standing vs. walking are marked with *.", "On the bottom-right there is the mean chart of accuracy for VS task.", "Mean accuracies for control, standing, and walking conditions are %86.4, %87.6, and %86.7, respectively"], "caption": "Figure 6. Mean (\u00b1SD) response times (top two panels) and accuracies (bottom two panels) for all conditions of the VS and MOT tasks. * marks significant differences between the corresponding groups.", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "pdf_hash": "94586032948e2c2ba39f17b94679efb4435237c9", "year": 2016, "venue": "CHI", "alt_text": "On the top, there are two illustrations describing a museum scenario for HMPDs. On top-left, HMPD user is walking while projecting a static display containing arrows and map of the museum onto wall. On the top right, same user is in front of an ancient vase and projecting a dynamic display on the wall about the vase.    On the midle, there are two illustrations describing a shopping environment scenario for HMPDs. On middle-left, HMPD user is walking while projecting a static display containing functional buttons and map of the mall with a juice box icon marked on it. On the middle-right, same user is in front of shelves projecting dynamic information about a specific brand of juice which contains an image of an orange next to a glass ( juice is animated to be pouring from the top into the glass), brand and price information.    On the bottom, there are two illustrations describing a surgery scenario for HMPDs. On the bottom left, surgeon is illustated while in operations. He or she is projecting a screen on the patient.  On the bottom right,  projected screen containing heart rate graphics, some detailed patient information and x-ray image of the patient's body is projected on an uniform surface on the patient. Screen is mark with the text \"dynamic or static display\".", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 10409718, "sentences": ["On the top, there are two illustrations describing a museum scenario for HMPDs.", "On top-left, HMPD user is walking while projecting a static display containing arrows and map of the museum onto wall.", "On the top right, same user is in front of an ancient vase and projecting a dynamic display on the wall about the vase.", "On the midle, there are two illustrations describing a shopping environment scenario for HMPDs.", "On middle-left, HMPD user is walking while projecting a static display containing functional buttons and map of the mall with a juice box icon marked on it.", "On the middle-right, same user is in front of shelves projecting dynamic information about a specific brand of juice which contains an image of an orange next to a glass ( juice is animated to be pouring from the top into the glass), brand and price information.", "On the bottom, there are two illustrations describing a surgery scenario for HMPDs.", "On the bottom left, surgeon is illustated while in operations.", "He or she is projecting a screen on the patient.", "On the bottom right,  projected screen containing heart rate graphics, some detailed patient information and x-ray image of the patient's body is projected on an uniform surface on the patient.", "Screen is mark with the text \"dynamic or static display\"."], "caption": "Figure 7. Examples for implementing our findings into real life scenarios: (1) Museum setting, (2) Shopping environment, (3) Surgery", "local_uri": ["94586032948e2c2ba39f17b94679efb4435237c9_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Eight images displaying the various grey coded patterns that were displayed by the IR projector.", "levels": null, "corpus_id": 3343150, "sentences": ["Eight images displaying the various grey coded patterns that were displayed by the IR projector."], "caption": "Figure 3. Gray-code patterns for an example 8x8 pixel area. The patterns include two synchronization patterns (1-2),", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "An exploded image of the stylus used in the experiment, highlighting the cap, momentary switch, PVC tubing, fiber optic cable, and 3D printed nib.", "levels": null, "corpus_id": 3343150, "sentences": ["An exploded image of the stylus used in the experiment, highlighting the cap, momentary switch, PVC tubing, fiber optic cable, and 3D printed nib."], "caption": "Figure 4. The fiber optic-based stylus used in the experiments.", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Block diagram of the processing performed on the FPGA highlighting the gray-code decoder, pixel generator, DMD controller, delay stages, and Microblaze processor.", "levels": null, "corpus_id": 3343150, "sentences": ["Block diagram of the processing performed on the FPGA highlighting the gray-code decoder, pixel generator, DMD controller, delay stages, and Microblaze processor."], "caption": "Figure 5. Block diagram of the processing performed on the IR pattern projector\u2019s FPGA.", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Block diagram of the processing performed on the FPGA highlighting the Microblaze processor, frame buffer, pixel generator, and DMD controller.", "levels": null, "corpus_id": 3343150, "sentences": ["Block diagram of the processing performed on the FPGA highlighting the Microblaze processor, frame buffer, pixel generator, and DMD controller."], "caption": "Figure 6. Block diagram of the processing performed on the visible projector\u2019s FPGA.", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Bar graph of the JND latency thresholds gathered from each participant during the large box dragging task.", "levels": [[1]], "corpus_id": 3343150, "sentences": ["Bar graph of the JND latency thresholds gathered from each participant during the large box dragging task."], "caption": "", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Bar graph of the JND latency thresholds gathered from each participant during the small box dragging task.", "levels": [[1]], "corpus_id": 3343150, "sentences": ["Bar graph of the JND latency thresholds gathered from each participant during the small box dragging task."], "caption": "The results from the scribbling task demonstrated that participants were able to discriminate between the 7- millisecond baseline latency and a median of 40 milliseconds (Figure 11; range 10 - 70 milliseconds). Although not directly comparable to the dragging tasks, such results suggest that task demands may play a role in the perception of latency. The higher perceived latency found while scribbling compared to dragging are likely due to the different visual feedback available, the strategies used to determine latency, or the cognitive loading encountered while scribbling versus dragging the box.", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Bar graph of the JND latency thresholds gathered from each participant during the scribbling task.", "levels": [[1]], "corpus_id": 3343150, "sentences": ["Bar graph of the JND latency thresholds gathered from each participant during the scribbling task."], "caption": "Figure 11. Minimum latency perceived by participants while performing the scribbling task. Results ranged from 10 to 70 milliseconds with a median latency of 40 milliseconds.", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "In the blink of an eye: investigating latency perception during stylus interaction", "pdf_hash": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "year": 2014, "venue": "CHI", "alt_text": "Demonstration of the latencies perceivable by participants, with the yellow arrow highlighting the nib location. From top: The nib was not centered in the box during the large box moving task; the nib is slightly off center in the small box moving task; there is a visible gap between the nib and ink while during scribbling.", "levels": null, "corpus_id": 3343150, "sentences": ["Demonstration of the latencies perceivable by participants, with the yellow arrow highlighting the nib location.", "From top: The nib was not centered in the box during the large box moving task; the nib is slightly off center in the small box moving task; there is a visible gap between the nib and ink while during scribbling."], "caption": "Figure 12. Demonstration of the latencies perceivable by participants, with the yellow arrow indicating the current nib location. From top: The nib was on the right side of the box during the large box dragging task, the nib was slightly off center during the small box dragging task, and there was a visible gap between the nib and ink while scribbling. The images were recorded using a high speed camera at 480 fps.", "local_uri": ["386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure shows 15 examples from the VizWiz dataset in the 3x5 format.  Three examples in the first column represent that images from users with visual impairments can be too blurry to be recognized.  The second column includes three examples of having uninformative information in the images; probably due to failure in aiming a camera toward the target.  The third column lists three representative examples of having low saliency due to cluttered backgrounds.  The fourth and fifth columns have examples where the object of interest is partially included or not included at all.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 203628527, "sentences": ["This figure shows 15 examples from the VizWiz dataset in the 3x5 format.", "Three examples in the first column represent that images from users with visual impairments can be too blurry to be recognized.", "The second column includes three examples of having uninformative information in the images; probably due to failure in aiming a camera toward the target.", "The third column lists three representative examples of having low saliency due to cluttered backgrounds.", "The fourth and fifth columns have examples where the object of interest is partially included or not included at all."], "caption": "Figure 1: Examples of challenging photos taken by people with visual impairments sent to Vizwiz crowdworkers. About 28% of all photos are categorized as unanswearable [26].", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_011.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure lists an original egocentric image, its hand segmentation, and its object center annotation from each of the five existing datasets: EgoHands, GTEA, GTEA Gaze+, Intel Egocentric, and TEgO.  The EgoHand example was captured by one of two people who are sighted when they were playing a card game.  The GTEA and GTEA Gaze+ examples were collected by people who are sighted when they were interacting with objects in the kitchen.  Similarly, the Intel Egocentric and TEgO examples showed the hand-object interaction in kitchen environments.  In particular, the TEgO example was collected by a person with visual impairment.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 203628527, "sentences": ["This figure lists an original egocentric image, its hand segmentation, and its object center annotation from each of the five existing datasets: EgoHands, GTEA, GTEA Gaze+, Intel Egocentric, and TEgO.  The EgoHand example was captured by one of two people who are sighted when they were playing a card game.", "The GTEA and GTEA Gaze+ examples were collected by people who are sighted when they were interacting with objects in the kitchen.", "Similarly, the Intel Egocentric and TEgO examples showed the hand-object interaction in kitchen environments.", "In particular, the TEgO example was collected by a person with visual impairment."], "caption": "Figure 2: Training examples in our hand-segmentation and object-localization models using the EgoHands, GTEA, GTEA Gaze+, Intel Egocentric, and TEgO datasets (in the left-to- right order). Original images are shown on the \ufb01rst row, hand segmentation on the second row, and object center annotations on the last row. EgoHands is used only in hand segmentation.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_017.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "There are two figures: one on the left and the other on the right. The left figure consists of two descriptive images: the tone feedback image on the left and the haptic feedback image on the right.  Each image shows that the camera frame is equally divided into nine regions: (the order goes from left to right and from top to bottom) A, B, C, D, E, F, G, H, and I.  Two different tone frequencies (200Hz and 500Hz) are used to indicate whether an object is estimated to locate on the center of the camera frame (region E) or not.  If the object is estimated on the center of the camera frame, it plays the high-frequency tone (500Hz); otherwise, the low-frequency tone (200Hz) is used as long as the object center is estimated to appear within the camera frame.  For an object appearing on the three left regions (A, D, G), it plays the left-stereophonic tone sound.  If an object appears on the three middle regions (B, E, H), it plays the middle-stereophonic tone sound.  For the three right regions (C, F, I), it plays the right-stereophonic tone sound.  The haptic feedback, shown on the right of this figure, is generated only when the object center is estimated to appear on the center of the camera frame (region E). The figure on the right illustrates the overall pipeline of our nonvisual feedback approach.  Using an example where a can of Pepsi cola and a user's hand appear on the center, we show how our nonvisual feedback approach works.  Once a user takes a photo that includes an object and a user's hand, the photo is sent to the back-end feedback module on which our object localization model is running.  The back-end feedback module then uses the localization model to estimate the location of an object of interest based on the hand information in the image.  This estimated location is sent back to the feedback generator to generate nonvisual feedback, accordingly.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 203628527, "sentences": ["There are two figures: one on the left and the other on the right.", "The left figure consists of two descriptive images: the tone feedback image on the left and the haptic feedback image on the right.", "Each image shows that the camera frame is equally divided into nine regions: (the order goes from left to right and from top to bottom) A, B, C, D, E, F, G, H, and I.  Two different tone frequencies (200Hz and 500Hz) are used to indicate whether an object is estimated to locate on the center of the camera frame (region E) or not.", "If the object is estimated on the center of the camera frame, it plays the high-frequency tone (500Hz); otherwise, the low-frequency tone (200Hz) is used as long as the object center is estimated to appear within the camera frame.", "For an object appearing on the three left regions (A, D, G), it plays the left-stereophonic tone sound.", "If an object appears on the three middle regions (B, E, H), it plays the middle-stereophonic tone sound.", "For the three right regions (C, F, I), it plays the right-stereophonic tone sound.", "The haptic feedback, shown on the right of this figure, is generated only when the object center is estimated to appear on the center of the camera frame (region E).", "The figure on the right illustrates the overall pipeline of our nonvisual feedback approach.", "Using an example where a can of Pepsi cola and a user's hand appear on the center, we show how our nonvisual feedback approach works.", "Once a user takes a photo that includes an object and a user's hand, the photo is sent to the back-end feedback module on which our object localization model is running.", "The back-end feedback module then uses the localization model to estimate the location of an object of interest based on the hand information in the image.", "This estimated location is sent back to the feedback generator to generate nonvisual feedback, accordingly."], "caption": "Our nonvisual feedback mechanism.                                (b) The architecture of our real-time feedback approach for better camera framing.Figure 3: Our testbed sends an image to the server, receives an estimated location, and triggers the corresponding feedback.ExplorationsOur initial attempts provided an increasingly fast and loud beep sound to communicate the proximity of the estimated object-center from the camera frame center conveying lim- ited spatial information as highlighted in V\u00e1zquez et al. [54], where participants preferred richer verbal instructions to the beep sound. To add spatial information, we considered stereo- phonic sound [18] that informs an object-center location in the x-axis with three distinguishable sounds for left, middle, and right, as shown in Figure 3(a). We piloted the stereophonic sound with a beep tone and a sinusoidal wave sound widely used in nonvisual shapes exploration for blind people [24]. We explored higher frequencies for the sinusoidal wave sound to convey a well-centered object. Also, we considered commu- nicating the presence of the object within the camera frame through vibrations [59, 8], which were more intense when the object was well-centered. A combination of the stereophonic sound with the vibration deemed more distinguishable.Our Feedback Mechanism\u00d7As shown in Figure 3(a), we coarsely divide the camera frame with a 3 3 grid alphabetically labeled from A to I. Given an estimated object-center location from our object localization model, we apply the following mapping between the region of the estimated location and the feedback modality:A, D, G: left-stereophonic sinusoidal wave (200Hz) C, F, I: right-stereophonic sinusoidal wave (200Hz) B, H: middle-stereophonic sinusoidal wave (200Hz)E: middle-stereophonic sinusoidal wave (500Hz) + vibrationThe intuition behind this mapping is that users can tell when the object is within the frame, have a sense of whether it is on the left or right, and quickly tell when it is well-centered. They receive a continuous sound with changing frequency and channel, and vibrations for well-centered objects. There is no sound or vibration for undetected or out-of-frame objects.STUDY AND DATA COLLECTIONTo understand the potential and limitations of our real-time feedback approach for camera manipulation in the context of teachable object recognizers, we replicate the in-lab user study by Kacorri et al. [32], where people with visual impairments train object recognizers without any feedback on the photo quality. We further extend the study design with a simulated real-world environment and additional open-end questions.Table 2: Participants\u2019 demographics and years of smartphone use. Light perception is indicated with an asterisk. \u00a0IDGenderAgeOnsetHandednessSmartphoneP1F5313R8P2F69birth*L-reading, R-other6P3F60birthL-reading, R-other8P4F66birthL4P5F6333R7P6F3428*L10P7F2919*R10P8F67birth*L-touchscreen, R-other7P9F64birth*R3ParticipantsWe recruited nine participants with visual impairments from our local community (IRB #1255427-2) each compensated at 55\u201380 (\u00b5: 63.3, \u03c3 : 7.1). As shown in Table 2, all participants identi\ufb01ed as legally blind; \ufb01ve of them reported having some light perception. They were all female age 29\u201369 (\u00b5: 56, \u03c3 : 15). None of them had used a smartphone for more than 10 years and only three (P3, P7, and P8) had previously taken photos more than once a week. Figure 4 shows participants\u2019 use and attitudes toward technology potentially affecting our task, based on the Rosen et al. [47] questionnaire.In open-ended questions, participants reported using Braille la- bels on kitchen appliances, electronic devices, food items (e.g. cans and bottles), cups, clothes, jewelry pouch, ID/credit/gift card, and mail. When a Braille label was not available, they reported using smell, isolating the object, coming up with some ad-hoc solution such as rubber bands for differentiating texture, asking for sighted help, or using an object recogni- tion application \u2014 though P1, P4, P7, and P9 had never used any technology for object recognition. The applications men- tioned across the other \ufb01ve participants were: KNFB reader [44], BeMyEyes [10], Aira [7], and SeeingAI [6].TestbedWe implement our feedback mechanism into a custom experi- mental testbed for iPhone 8, which connects over WiFi with a GPU server hosting the localization model. Every 333ms our testbed sends a resized (540x720) image to the server and receives an estimated location for the object center as shown in Figure 3(b). The estimated location is based on the image pixels classi\ufb01ed with the highest probability as the center ofFigure 4: Technology experience and attitude responses. All participants have smartphones; more than half are using apps for object recognition; and all are positive about technology.the object of interest. For this study, we use p > 0.3 as a threshold. There is no sound or vibration when the thresh- old is not met (e.g. for an undetected or out-of-frame object). For the object center within the frame, the testbed triggers the corresponding feedback to the A\u2013I regions. Given the high rate of 333ms, participants perceive the feedback with continuous sound changing frequencies and channels, on/off vibrations, and silent pauses, as they move the object or the camera around. Beyond the shutter sound, the testbed commu- nicates to the participant in real-time the count of photos taken and the completion of the training process for each object.Object StimuliAs shown in Figure 5(a), we use three objects for practice. The grill salt and mountain dew can are selected from the TEgO dataset [35] used to train our object localization model. Thus, the estimates for their object-center locations are anticipated to be more accurate and stable. On the other hand, the nut mix has not been seen by the object localization model, providing experience of less stable feedback. Given that our object localization model is error-prone like any machine learning algorithms, it is important to have participants learn about this and familiarize themselves with the fact that the feedback can be imperfect and thus it is not to be trusted at all times.While the premise of teachable object recognizers works best for unique objects (e.g. keychains or artisanal products that may or may not have readable texts or recognizable labels), researchers [32, 50] often use commercial products that allow for exploration of different shapes, sizes, materials, visual similarities, and more importantly, stimuli that allow for repli- cability of the study. We follow this approach and adopt the object stimuli from Kacorri et al. [32] (Figure 5(b)). Due to product changes, labels for some of the stimuli such as k-cups,a spice jar, soda can, and a snack box.15 stimuli: baking soda, caramel coffee, cheetos, chewy bars, chicken broth, coca cola, diced tomatoes, diet coke, dill, fritos, lacroix apricot, lacroix mango, lay\u2019s, oregano, pike place roast.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_018.jpg", "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_019.png"], "annotated": false, "compound": true}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure consists of two figures on the top and the bottom, respectively. The top figure includes, from left to right, grill salt, a can of mountain dew, and a box of nut mix. The bottom figure lists 15 objects in two rows.  The first row shows (the order goes from left to right) baking soda, a k-cup of caramel coffee, cheetos, a box of chewy bars, a can of chicken broth, a bottle of classic coca cola, a can of diced tomatoes, a bottle of diet coca cola. In the second row, dill weed, fritos, a can of lacroix apricot, a can of lacroix mango, lays, oregano, and a k-cup of pike place roast are listed.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 203628527, "sentences": ["This figure consists of two figures on the top and the bottom, respectively.", "The top figure includes, from left to right, grill salt, a can of mountain dew, and a box of nut mix.", "The bottom figure lists 15 objects in two rows.", "The first row shows (the order goes from left to right) baking soda, a k-cup of caramel coffee, cheetos, a box of chewy bars, a can of chicken broth, a bottle of classic coca cola, a can of diced tomatoes, a bottle of diet coca cola.", "In the second row, dill weed, fritos, a can of lacroix apricot, a can of lacroix mango, lays, oregano, and a k-cup of pike place roast are listed."], "caption": "15 stimuli: baking soda, caramel coffee, cheetos, chewy bars, chicken broth, coca cola, diced tomatoes, diet coke, dill, fritos, lacroix apricot, lacroix mango, lay\u2019s, oregano, pike place roast.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_020.png", "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_022.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure has two bar charts at the top and bottom. The bar chart at the top describes the proportion of participants who use a certain technology never, once a month, several times a month, once a week, several times a week, once a day, and several times a day as follows.  Experience of using a mobile device Never: 0.0% Once a month: 0.0% Several times a month: 0.0% Once a week: 0.0% Several times a week: 0.0% Once a day: 11.11% Several times a day: 88.89%  Experience of taking pictures using a mobile phone Never: 44.44% Once a month: 22.22% Several times a month: 0.0% Once a week: 0.0% Several times a week: 22.22% Once a day: 0.0% Several times a day: 11.11%  Experience of sharing your own photos or videos with others Never: 66.67% Once a month: 0.0% Several times a month: 22.22% Once a week: 0.0% Several times a week: 11.11% Once a day: 0.0% Several times a day: 0.0%  Experience of using apps (for any purpose) on a mobile phone Never: 0.0% Once a month: 0.0% Several times a month: 0.0% Once a week: 0.0% Several times a week: 0.0% Once a day: 22.22% Several times a day: 77.78%  Experience of using apps for object recognition (for example, Aipoly, TapTapSee) on a mobile phone Never: 44.44% Once a month: 0.0% Several times a month: 0.0% Once a week: 11.11% Several times a week: 11.11% Once a day: 0.0% Several times a day: 33.33%  Experience of using Braille labels to distinguish objects Never: 22.22% Once a month: 0.0% Several times a month: 11.11% Once a week: 0.0% Several times a week: 22.22% Once a day: 0.0% Several times a day: 44.44%  The bar chart at the bottom describes the proportion of participants who strongly disagree, disagree, neither agree nor disagree (neutral), agree, strongly agree with a statement about technology as follows.  Statement: I enjoy taking a photo on a mobile phone. Strongly disagree: 0.0% Disagree: 44.44% Neutral: 11.11% Agree: 22.22% Strongly agree: 22.22%  Statement: I think it is important to keep up with the latest trends in te", "levels": [[1], [2, 1], [2], [2], [2]], "corpus_id": 203628527, "sentences": ["This figure has two bar charts at the top and bottom.", "The bar chart at the top describes the proportion of participants who use a certain technology never, once a month, several times a month, once a week, several times a week, once a day, and several times a day as follows.", "Experience of using a mobile device Never: 0.0% Once a month: 0.0% Several times a month: 0.0% Once a week: 0.0% Several times a week: 0.0% Once a day: 11.11% Several times a day: 88.89%  Experience of taking pictures using a mobile phone Never: 44.44% Once a month: 22.22% Several times a month: 0.0% Once a week: 0.0% Several times a week: 22.22% Once a day: 0.0% Several times a day: 11.11%  Experience of sharing your own photos or videos with others Never: 66.67% Once a month: 0.0% Several times a month: 22.22% Once a week: 0.0% Several times a week: 11.11% Once a day: 0.0% Several times a day: 0.0%  Experience of using apps (for any purpose) on a mobile phone Never: 0.0% Once a month: 0.0% Several times a month: 0.0% Once a week: 0.0% Several times a week: 0.0% Once a day: 22.22% Several times a day: 77.78%  Experience of using apps for object recognition (for example, Aipoly, TapTapSee) on a mobile phone Never: 44.44% Once a month: 0.0% Several times a month: 0.0% Once a week: 11.11% Several times a week: 11.11% Once a day: 0.0% Several times a day: 33.33%  Experience of using Braille labels to distinguish objects Never: 22.22% Once a month: 0.0% Several times a month: 11.11% Once a week: 0.0% Several times a week: 22.22% Once a day: 0.0% Several times a day: 44.44%  The bar chart at the bottom describes the proportion of participants who strongly disagree, disagree, neither agree nor disagree (neutral), agree, strongly agree with a statement about technology as follows.", "Statement: I enjoy taking a photo on a mobile phone.", "Strongly disagree: 0.0% Disagree: 44.44% Neutral: 11.11% Agree: 22.22% Strongly agree: 22.22%  Statement: I think it is important to keep up with the latest trends in te"], "caption": "Figure 4: Technology experience and attitude responses. All participants have smartphones; more than half are using apps for object recognition; and all are positive about technology.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_021.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure shows the wild test setting in the cluttered environment where other objects, such as a coffee machine and cereal boxes, appear in the background.  It shows a table on the bottom-left of the figure and a bookshelf on the right.  The objects are placed on either the table and the bookshelf like following.  (From left to right) On the table: classic coke bottle, diet coke bottle, lacroix mango, lacroix apricot, lay's, fritos, cheetos, caramel coffee, pike place roast.  (From left to right) On the top shelf: baking soda, dill, oregano.  (From left to right) On the bottom shelf: chewy bars, diced tomatoes, chicken broth.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 203628527, "sentences": ["This figure shows the wild test setting in the cluttered environment where other objects, such as a coffee machine and cereal boxes, appear in the background.", "It shows a table on the bottom-left of the figure and a bookshelf on the right.", "The objects are placed on either the table and the bookshelf like following.", "(From left to right) On the table: classic coke bottle, diet coke bottle, lacroix mango, lacroix apricot, lay's, fritos, cheetos, caramel coffee, pike place roast.", "(From left to right) On the top shelf: baking soda, dill, oregano.", "(From left to right) On the bottom shelf: chewy bars, diced tomatoes, chicken broth."], "caption": "Figure 6: The wild test setting in the cluttered environment.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_023.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure includes two photos for each participant taking photos of the objects during the train session, 18 photos in total. Each photo shows a participant sitting in front of a desk. The surface of the desk is plain white color. Participants have different postures when they take photos. Some participants hold the object with a hand and a smartphone with the other hand. Some of the participants hold the smartphone with two hands to take photos, putting the object on the table. Participants took photos from the top or side of the objects.", "levels": null, "corpus_id": 203628527, "sentences": ["This figure includes two photos for each participant taking photos of the objects during the train session, 18 photos in total.", "Each photo shows a participant sitting in front of a desk.", "The surface of the desk is plain white color.", "Participants have different postures when they take photos.", "Some participants hold the object with a hand and a smartphone with the other hand.", "Some of the participants hold the smartphone with two hands to take photos, putting the object on the table.", "Participants took photos from the top or side of the objects."], "caption": "", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_024.jpg"], "annotated": false, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure shows two bar charts on the left and the right, respectively.  The left bar chart shows the average accuracy of each participant's model in the vanilla test and the right bar chart shows the average of each participant's model in the wild test.  The average accuracy of the models in the vanilla test P1: 49.86% (min: 44.60%, max: 54.00%, std: 3.16) P2: 62.88% (min: 57.20%, max: 68.00%, std: 3.47) P3: 70.46% (min: 66.20%, max: 76.40%, std: 3.06) P4: 61.78% (min: 57.20%, max: 66.60%, std: 3.57) P5: 53.38% (min: 47.80%, max: 57.60%, std: 2.84) P6: 74.56% (min: 72.60%, max: 77.00%, std: 1.46) P7: 84.14% (min: 80.20%, max: 89.40%, std: 3.32) P8: 55.12% (min: 52.60%, max: 59.80%, std: 2.39) P9: 55.28% (min: 50.00%, max: 59.80%, std: 3.17) S1: 82.30% (min: 79.20%, max: 86.20%, std: 2.38) S2: 94.58% (min: 92.60%, max: 96.00%, std: 1.16)  The average accuracy of the models in the wild test P1: 28.56% (min: 24.40%, max: 33.80%, std: 3.00) P2: 34.44% (min: 29.20%, max: 38.60%, std: 3.08) P3: N/A P4: 54.32% (min: 48.00%, max: 58.00%, std: 3.37) P5: 40.72% (min: 36.40%, max: 45.60%, std: 3.51) P6: 55.82% (min: 52.20%, max: 61.00%, std: 2.22) P7: 58.08% (min: 52.00%, max: 66.00%, std: 4.14) P8: 23.00% (min: 20.00%, max: 26.80%, std: 2.35) P9: 43.28% (min: 39.80%, max: 48.80%, std: 2.97) S1: 76.68% (min: 69.80%, max: 84.00%, std: 4.74) S2: 86.18% (min: 83.20%, max: 89.00%, std: 1.83)", "levels": [[1], [1], [2]], "corpus_id": 203628527, "sentences": ["This figure shows two bar charts on the left and the right, respectively.", "The left bar chart shows the average accuracy of each participant's model in the vanilla test and the right bar chart shows the average of each participant's model in the wild test.", "The average accuracy of the models in the vanilla test P1: 49.86% (min: 44.60%, max: 54.00%, std: 3.16) P2: 62.88% (min: 57.20%, max: 68.00%, std: 3.47) P3: 70.46% (min: 66.20%, max: 76.40%, std: 3.06) P4: 61.78% (min: 57.20%, max: 66.60%, std: 3.57) P5: 53.38% (min: 47.80%, max: 57.60%, std: 2.84) P6: 74.56% (min: 72.60%, max: 77.00%, std: 1.46) P7: 84.14% (min: 80.20%, max: 89.40%, std: 3.32) P8: 55.12% (min: 52.60%, max: 59.80%, std: 2.39) P9: 55.28% (min: 50.00%, max: 59.80%, std: 3.17) S1: 82.30% (min: 79.20%, max: 86.20%, std: 2.38) S2: 94.58% (min: 92.60%, max: 96.00%, std: 1.16)  The average accuracy of the models in the wild test P1: 28.56% (min: 24.40%, max: 33.80%, std: 3.00) P2: 34.44% (min: 29.20%, max: 38.60%, std: 3.08) P3: N/A P4: 54.32% (min: 48.00%, max: 58.00%, std: 3.37) P5: 40.72% (min: 36.40%, max: 45.60%, std: 3.51) P6: 55.82% (min: 52.20%, max: 61.00%, std: 2.22) P7: 58.08% (min: 52.00%, max: 66.00%, std: 4.14) P8: 23.00% (min: 20.00%, max: 26.80%, std: 2.35) P9: 43.28% (min: 39.80%, max: 48.80%, std: 2.97) S1: 76.68% (min: 69.80%, max: 84.00%, std: 4.74) S2: 86.18% (min: 83.20%, max: 89.00%, std: 1.83)"], "caption": "Figure 8: Average model accuracy per participant.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_025.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure has two line graphs. A line graph on the left describes the average proportion of photos that included the full, part of, and no object.   Train Full: average=0.68, variance=0.05 Partial: average=0.29, variance=0.05 No: average=0.02, variance=0.001 Vanilla test Full: average=0.77, variance=0.03 Partial: average=0.20, variance=0.02 No: average=0.02, variance=0.002 Wild test Full: average=0.62, variance=0.10 Partial: average=0.30, variance=0.05 No: average=0.08, variance=0.01  The other line graph on the right describes the average proportion of photos that included the full, part of, and no hand.  Train Full: average=0.53, variance=0.14 Partial: average=0.13, variance=0.01 No: average=0.34, variance=0.14 Vanilla test Full: average=0.58, variance=0.18 Partial: average=0.11, variance=0.01 No: average=0.31, variance=0.19 Wild test Full: average=0.66, variance=0.12 Partial: average=0.19, variance=0.01 No: average=0.16, variance=0.08", "levels": [[1], [1], [2], [2]], "corpus_id": 203628527, "sentences": ["This figure has two line graphs.", "A line graph on the left describes the average proportion of photos that included the full, part of, and no object.", "Train Full: average=0.68, variance=0.05 Partial: average=0.29, variance=0.05 No: average=0.02, variance=0.001 Vanilla test Full: average=0.77, variance=0.03 Partial: average=0.20, variance=0.02 No: average=0.02, variance=0.002 Wild test Full: average=0.62, variance=0.10 Partial: average=0.30, variance=0.05 No: average=0.08, variance=0.01  The other line graph on the right describes the average proportion of photos that included the full, part of, and no hand.", "Train Full: average=0.53, variance=0.14 Partial: average=0.13, variance=0.01 No: average=0.34, variance=0.14 Vanilla test Full: average=0.58, variance=0.18 Partial: average=0.11, variance=0.01 No: average=0.31, variance=0.19 Wild test Full: average=0.66, variance=0.12 Partial: average=0.19, variance=0.01 No: average=0.16, variance=0.08"], "caption": "Figure 9: Proportion of photos with fully, partially, or not included objects and hands. Errors bars show variance among participants, who were able to fully capture the object in 60\u2013 80% of their photos and include their hand in more than 50%.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_026.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "pdf_hash": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "year": 2019, "venue": "ASSETS", "alt_text": "This figure consists of four box plots that are located from left to right.  The first boxplot shows word error rate per feedback cluster in the train session. The second boxplot shows the ratio of including hands in train photos per feedback cluster. The third boxplot shows the ratio of fully including an object in train photos per feedback cluster. The fourth boxplot shows the photography time per feedback cluster in the train session.  Word error rate per feedback cluster in train C1: 0.65 (min: 0.27, max: 0.96, std: 0.15) C2: 0.27 (min: 0.07, max: 0.64, std: 0.13) C3: 0.22 (min: 0.04, max: 0.52, std: 0.14)  Ratio of including hands in traing photos per feedback cluster. C1: 0.64 (min: 0, max: 1, std: 0.379, median: 0.84) C2: 0.55 (min: 0, max: 1, std: 0.447, median: 0.68) C3: 0.88 (min: 0, max: 1, std: 0.3, median: 1.0)  Ratio of including entire object in train photos per feedback cluster. C1: 0.96 (min: 0.93, max: 1.00, std: 0.02) C2: 0.995 (min: 0.960, max: 1.00, std: 0.008) C3: 0.974 (min: 0.933, max: 1.00, std: 0.023)  Photography time per feedback cluster in train C1: 227.94 seconds (min: 106.57 seconds, max: 867.25 seconds, std: 140.32) C2: 220.28 seconds (min: 40.81, max: 1039.57, std: 194.55) C3: 129.90 seconds (min: 76.74, max: 346.58, std: 65.63)", "levels": [[1], [1], [1], [1], [1], [2], [2], [2]], "corpus_id": 203628527, "sentences": ["This figure consists of four box plots that are located from left to right.", "The first boxplot shows word error rate per feedback cluster in the train session.", "The second boxplot shows the ratio of including hands in train photos per feedback cluster.", "The third boxplot shows the ratio of fully including an object in train photos per feedback cluster.", "The fourth boxplot shows the photography time per feedback cluster in the train session.", "Word error rate per feedback cluster in train C1: 0.65 (min: 0.27, max: 0.96, std: 0.15) C2: 0.27 (min: 0.07, max: 0.64, std: 0.13) C3: 0.22 (min: 0.04, max: 0.52, std: 0.14)  Ratio of including hands in traing photos per feedback cluster.", "C1: 0.64 (min: 0, max: 1, std: 0.379, median: 0.84) C2: 0.55 (min: 0, max: 1, std: 0.447, median: 0.68) C3: 0.88 (min: 0, max: 1, std: 0.3, median: 1.0)  Ratio of including entire object in train photos per feedback cluster.", "C1: 0.96 (min: 0.93, max: 1.00, std: 0.02) C2: 0.995 (min: 0.960, max: 1.00, std: 0.008) C3: 0.974 (min: 0.933, max: 1.00, std: 0.023)  Photography time per feedback cluster in train C1: 227.94 seconds (min: 106.57 seconds, max: 867.25 seconds, std: 140.32) C2: 220.28 seconds (min: 40.81, max: 1039.57, std: 194.55) C3: 129.90 seconds (min: 76.74, max: 346.58, std: 65.63)"], "caption": "(a) Feedback WER across pairs.         (b) % of photos including hands.         (c) % of photos with full object.              (d) Time spent in training.", "local_uri": ["854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_030.jpg", "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_031.jpg", "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_032.jpg", "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_033.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "BBeep: A Sonic Collision Avoidance System for Blind Travellers and Nearby Pedestrians", "pdf_hash": "ccbbca3cb5a9ecbc87a9d48eecbf0a18bdf8d515", "year": 2019, "venue": "CHI", "alt_text": "This figure shows an example situation of our user evaluation in an airport. A blind participant walk through a crowded environment with our system. A researcher is walking behind them to guarantee their safety.", "levels": null, "corpus_id": 140321520, "sentences": ["This figure shows an example situation of our user evaluation in an airport.", "A blind participant walk through a crowded environment with our system.", "A researcher is walking behind them to guarantee their safety."], "caption": "Our system", "local_uri": ["ccbbca3cb5a9ecbc87a9d48eecbf0a18bdf8d515_Image_013.png"], "annotated": false, "compound": false}
{"title": "Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery", "pdf_hash": "d48bc52255517276c0ddaa46a7893a8ec4477983", "year": 2019, "venue": "CHI", "alt_text": "Our approach automatically places audio iles as sound sources on a 360\u00b0 panorama image to enhance the immersionwhen experienced through a virtual reality headset. In this example, a background audio ile with sounds for a town is placedby our approach. Audio iles for people chatting (in blue), walking (in purple), and of cars (in green) are automatically assignedas sound sources for the detected objects, and are placed at estimated depths in the scene with respect to the user. Please referto the supplementary material for the audio results.", "levels": null, "corpus_id": 140245577, "sentences": ["Our approach automatically places audio iles as sound sources on a 360\u00b0 panorama image to enhance the immersionwhen experienced through a virtual reality headset.", "In this example, a background audio ile with sounds for a town is placedby our approach.", "Audio iles for people chatting (in blue), walking (in purple), and of cars (in green) are automatically assignedas sound sources for the detected objects, and are placed at estimated depths in the scene with respect to the user.", "Please referto the supplementary material for the audio results."], "caption": "Figure 1: Our approach automatically places audio fles as sound sources on a 360\u00b0 panorama image to enhance the immersion when experienced through a virtual reality headset. In this example, a background audio fle with sounds for a town is placed by our approach. Audio fles for people chatting (in blue), walking (in purple), and of cars (in green) are automatically assigned as sound sources for the detected objects, and are placed at estimated depths in the scene with respect to the user. Please refer to the supplementary material for the audio results.", "local_uri": ["d48bc52255517276c0ddaa46a7893a8ec4477983_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery", "pdf_hash": "d48bc52255517276c0ddaa46a7893a8ec4477983", "year": 2019, "venue": "CHI", "alt_text": "Overview of our approach. Given a panorama image, our approach performs scene classiication and object detectionon images sampled horizontally from the panorama image. Next, it performs object recognition. Based on the scene knowledge,it assigns a corresponding background sound for the scene; it also places appropriate sound sources at the estimated objectlocations accordingly.", "levels": null, "corpus_id": 140245577, "sentences": ["Overview of our approach.", "Given a panorama image, our approach performs scene classiication and object detectionon images sampled horizontally from the panorama image.", "Next, it performs object recognition.", "Based on the scene knowledge,it assigns a corresponding background sound for the scene; it also places appropriate sound sources at the estimated objectlocations accordingly."], "caption": "", "local_uri": ["d48bc52255517276c0ddaa46a7893a8ec4477983_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery", "pdf_hash": "d48bc52255517276c0ddaa46a7893a8ec4477983", "year": 2019, "venue": "CHI", "alt_text": "A sample image and the corresponding tags assigned by ourautomatic scene classiier. In this case, some scene tags assignedwere \"Crowd\", \"Transport\", and \"Town\". \"Town\" was ultimatelythe highest scored and most frequently occurring tag across alltested image segments, so it was selected as the scene tag for theChinatownscene.", "levels": null, "corpus_id": 140245577, "sentences": ["A sample image and the corresponding tags assigned by ourautomatic scene classiier.", "In this case, some scene tags assignedwere \"Crowd\", \"Transport\", and \"Town\". \"Town\" was ultimatelythe highest scored and most frequently occurring tag across alltested image segments, so it was selected as the scene tag for theChinatownscene."], "caption": "A sample image and the corresponding tags assigned by our automatic scene classifer. In this case, some scene tags assigned were \"Crowd\", \"Transport\", and \"Town\". \"Town\" was ultimately the highest scored and most frequently occurring tag across all tested image segments, so it was selected as the scene tag for the Chinatown scene.Figure 3: An illustrative example, Chinatown.", "local_uri": ["d48bc52255517276c0ddaa46a7893a8ec4477983_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery", "pdf_hash": "d48bc52255517276c0ddaa46a7893a8ec4477983", "year": 2019, "venue": "CHI", "alt_text": "To perform object detection, our approach samplesimages of the scene by rotating the camera horizontally by36\u00b0 each time until the whole scene is covered. Here we illus-trate the process forChinatown, with the bounding boxes inthe enlarged image showing the detected objects.", "levels": null, "corpus_id": 140245577, "sentences": ["To perform object detection, our approach samplesimages of the scene by rotating the camera horizontally by36\u00b0 each time until the whole scene is covered.", "Here we illus-trate the process forChinatown, with the bounding boxes inthe enlarged image showing the detected objects."], "caption": "", "local_uri": ["d48bc52255517276c0ddaa46a7893a8ec4477983_Image_008.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_009.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_010.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_011.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_012.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_013.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_014.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_015.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_016.jpg", "d48bc52255517276c0ddaa46a7893a8ec4477983_Image_017.jpg"], "annotated": false, "compound": true}
{"title": "Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery", "pdf_hash": "d48bc52255517276c0ddaa46a7893a8ec4477983", "year": 2019, "venue": "CHI", "alt_text": "Depth estimation. We assume the average real-world heights of diferent objects categories based on estimationand previously recorded averages, which are shown in our supplementary materials. In this example, we show both people(average height:5.3ft) and a car (average height5.0ft). The person highlighted in red represents the reference object and thered line represents the baseline depth speciied by the user. The depth estimation estimates the depth of the black objects bythese inputs. (a) The system chooses the reference object, which corresponds to the object that received the highest conidencescore during object detection. (b) The designer speciies the baseline depth to the reference object. (c) The system estimatesthe depths of the other objects. (d) audio sources corresponding to the objects will be placed at their estimated depths to createspatial audio.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 140245577, "sentences": ["Depth estimation.", "We assume the average real-world heights of diferent objects categories based on estimationand previously recorded averages, which are shown in our supplementary materials.", "In this example, we show both people(average height:5.3ft) and a car (average height5.0ft).", "The person highlighted in red represents the reference object and thered line represents the baseline depth speciied by the user.", "The depth estimation estimates the depth of the black objects bythese inputs. (", "a) The system chooses the reference object, which corresponds to the object that received the highest conidencescore during object detection. (", "b) The designer speciies the baseline depth to the reference object. (", "c) The system estimatesthe depths of the other objects. (", "d) audio sources corresponding to the objects will be placed at their estimated depths to createspatial audio."], "caption": "", "local_uri": ["d48bc52255517276c0ddaa46a7893a8ec4477983_Image_019.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Older People Inventing their Personal Internet of Things with the IoT Un-Kit Experience", "pdf_hash": "81489890d86db6c14e438682a0bcffaafcc12aba", "year": 2019, "venue": "CHI", "alt_text": "Two images side by side. On the left are the IoT Un-Kit elements of sensors, actuators and media elements. On the right is the co-design workshop with the researcher and participant interacting with the IoT elements and the cards.", "levels": null, "corpus_id": 140220846, "sentences": ["Two images side by side.", "On the left are the IoT Un-Kit elements of sensors, actuators and media elements.", "On the right is the co-design workshop with the researcher and participant interacting with the IoT elements and the cards."], "caption": "Figure 1: (L) The Un-Kit IoT elements and (R) the in-home co-design using the IoT Un-Kit Experience.", "local_uri": ["81489890d86db6c14e438682a0bcffaafcc12aba_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Older People Inventing their Personal Internet of Things with the IoT Un-Kit Experience", "pdf_hash": "81489890d86db6c14e438682a0bcffaafcc12aba", "year": 2019, "venue": "CHI", "alt_text": "A photo of the IoT Unkit Elements on top view - sesnors and actuators sorrounding a Raspberry Pi that makes connection combination possible.", "levels": null, "corpus_id": 140220846, "sentences": ["A photo of the IoT Unkit Elements on top view - sesnors and actuators sorrounding a Raspberry Pi that makes connection combination possible."], "caption": "", "local_uri": ["81489890d86db6c14e438682a0bcffaafcc12aba_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Older People Inventing their Personal Internet of Things with the IoT Un-Kit Experience", "pdf_hash": "81489890d86db6c14e438682a0bcffaafcc12aba", "year": 2019, "venue": "CHI", "alt_text": "A top view of the cards with picture of body senses to represent sensors, of light, sound and movements to represent actuators and silhouettes of people to represent relations. A small box of yarn is in the middle to represent connection combination possibilities.", "levels": null, "corpus_id": 140220846, "sentences": ["A top view of the cards with picture of body senses to represent sensors, of light, sound and movements to represent actuators and silhouettes of people to represent relations.", "A small box of yarn is in the middle to represent connection combination possibilities."], "caption": "Figure 3: Cards used to simplify the concept of sensing.", "local_uri": ["81489890d86db6c14e438682a0bcffaafcc12aba_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Older People Inventing their Personal Internet of Things with the IoT Un-Kit Experience", "pdf_hash": "81489890d86db6c14e438682a0bcffaafcc12aba", "year": 2019, "venue": "CHI", "alt_text": "Two images side by side. On the left is Val checking her garden. On the right is Ann and her complicated griller.", "levels": null, "corpus_id": 140220846, "sentences": ["Two images side by side.", "On the left is Val checking her garden.", "On the right is Ann and her complicated griller."], "caption": "Figure 4: Val in her garden and Ann with her fancy griller.", "local_uri": ["81489890d86db6c14e438682a0bcffaafcc12aba_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Older People Inventing their Personal Internet of Things with the IoT Un-Kit Experience", "pdf_hash": "81489890d86db6c14e438682a0bcffaafcc12aba", "year": 2019, "venue": "CHI", "alt_text": "Three images are shown. First is of Ann sitting at her back porch. Second is Ann knitting inside her home. Third is Val checking her cooking.", "levels": null, "corpus_id": 140220846, "sentences": ["Three images are shown.", "First is of Ann sitting at her back porch.", "Second is Ann knitting inside her home.", "Third is Val checking her cooking."], "caption": "Figure 5: a. Ann\u2019s spot at her back porch, b. Ann\u2019s knitting setup inside her house, c. Val\u2019s cooking area", "local_uri": ["81489890d86db6c14e438682a0bcffaafcc12aba_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Older People Inventing their Personal Internet of Things with the IoT Un-Kit Experience", "pdf_hash": "81489890d86db6c14e438682a0bcffaafcc12aba", "year": 2019, "venue": "CHI", "alt_text": "Three images are shown. First is the photo of Val's garden showing her 'alert' plant rack idea. Second is Ann witnessing the interaction of a moisture sensor that is to be used in her proposed idea. Third is the linen cabinet where Ann wants to use her proposed personal IoT.", "levels": null, "corpus_id": 140220846, "sentences": ["Three images are shown.", "First is the photo of Val's garden showing her 'alert' plant rack idea.", "Second is Ann witnessing the interaction of a moisture sensor that is to be used in her proposed idea.", "Third is the linen cabinet where Ann wants to use her proposed personal IoT."], "caption": "Figure 6: a. Val\u2019s \u2018alert\u2019 plant rack idea, b. Ann experiencing the interaction of her idea and c. her linen cabinet.", "local_uri": ["81489890d86db6c14e438682a0bcffaafcc12aba_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times", "pdf_hash": "415c0ef55b1c603af7ed537f4aede720d80f34bd", "year": 2017, "venue": "CHI", "alt_text": "The keyboard used in our longitudinal study, including a target phrase and output area.", "levels": null, "corpus_id": 11870996, "sentences": ["The keyboard used in our longitudinal study, including a target phrase and output area."], "caption": "Figure 2. The keyboard used in our longitudinal study, including a target phrase and output area.", "local_uri": ["415c0ef55b1c603af7ed537f4aede720d80f34bd_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times", "pdf_hash": "415c0ef55b1c603af7ed537f4aede720d80f34bd", "year": 2017, "venue": "CHI", "alt_text": "The image dispalys a text phrase that the participant must transcribe. There are also three user controls. A \"Start\" button and an increment and decrement time button.", "levels": null, "corpus_id": 11870996, "sentences": ["The image dispalys a text phrase that the participant must transcribe.", "There are also three user controls.", "A \"Start\" button and an increment and decrement time button."], "caption": "Figure 3. Participant were given an opportunity to memorize the target phrase and to adjust their dwell time using the time adjustment controls located at the right of the UI.", "local_uri": ["415c0ef55b1c603af7ed537f4aede720d80f34bd_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times", "pdf_hash": "415c0ef55b1c603af7ed537f4aede720d80f34bd", "year": 2017, "venue": "CHI", "alt_text": "Two images of the keyboard area. The top image shows the 'e' key with a highlighted border. The bottom image shows the 'e' key with a highlighted background.", "levels": null, "corpus_id": 11870996, "sentences": ["Two images of the keyboard area.", "The top image shows the 'e' key with a highlighted border.", "The bottom image shows the 'e' key with a highlighted background."], "caption": "(B)", "local_uri": ["415c0ef55b1c603af7ed537f4aede720d80f34bd_Image_005.jpg", "415c0ef55b1c603af7ed537f4aede720d80f34bd_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times", "pdf_hash": "415c0ef55b1c603af7ed537f4aede720d80f34bd", "year": 2017, "venue": "CHI", "alt_text": "Two graphs. The graph on the left shows the text entry speeds for each technique over all eight sessions with learning curves fit to the speed data. The graph on the right shows corrected error rates for each technique over all sessions.", "levels": [[1], [1], [1]], "corpus_id": 11870996, "sentences": ["Two graphs.", "The graph on the left shows the text entry speeds for each technique over all eight sessions with learning curves fit to the speed data.", "The graph on the right shows corrected error rates for each technique over all sessions."], "caption": "Figure 5. (Left) Text entry speeds (in WPM) for each technique over all eight sessions and learning curves fit to the speed data modeled by the function y=axb. (Right) Corrected error rates for each technique over all sessions.", "local_uri": ["415c0ef55b1c603af7ed537f4aede720d80f34bd_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Spokespeople: Exploring Routes to Action through Citizen-Generated Data", "pdf_hash": "2e0fcd70b499fe6971f147bd855bfc3411972ff5", "year": 2018, "venue": "CHI", "alt_text": "Infrastructure 27%, Near miss 19%, Positive experience 18%, It felt unsafe 14%, Uncategorised 12%, Other 8%, Air pollution 1%, Noise 1%", "levels": null, "corpus_id": 5040303, "sentences": ["Infrastructure 27%, Near miss 19%, Positive experience 18%, It felt unsafe 14%, Uncategorised 12%, Other 8%, Air pollution 1%, Noise 1%"], "caption": "", "local_uri": ["2e0fcd70b499fe6971f147bd855bfc3411972ff5_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Volunteer-Based Online Studies With Older Adults and People with Disabilities", "pdf_hash": "17cbe37a0d356c0e6721d3a62a24818439616bd1", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 1: Overview of the stimuli used in four of our LabintheWild experiments. From left to right: (a) Stimuli in the Weather Prediction Study, showing three cards with different geometries on the top, and asking participants to choose whether the associated weather is sunny or rainy at the bottom. (b) Stimuli in the Memory study: on the left shows five numbers (probes) that people will need to memorize, on the right shows one probe that participants need to recall whether it is in the set or not. (c) Stimuli in the Social Intelligence study, showing a person's eyes and four words participants need to choose from. (d) Stimuli in Fitt's Law study, demonstration of reciprocal task on the left, and one-at-a-time task on the right.", "levels": null, "corpus_id": 51901855, "sentences": ["Figure 1: Overview of the stimuli used in four of our LabintheWild experiments.", "From left to right: (a) Stimuli in the Weather Prediction Study, showing three cards with different geometries on the top, and asking participants to choose whether the associated weather is sunny or rainy at the bottom. (b) Stimuli in the Memory study: on the left shows five numbers (probes) that people will need to memorize, on the right shows one probe that participants need to recall whether it is in the set or not. (c) Stimuli in the Social Intelligence study, showing a person's eyes and four words participants need to choose from. (d) Stimuli in Fitt's Law study, demonstration of reciprocal task on the left, and one-at-a-time task on the right."], "caption": "Weather Prediction Study                         (b) Memory Study                         (c) Social Intelligence Study                        (d) Fitt\u2019s Law Study", "local_uri": ["17cbe37a0d356c0e6721d3a62a24818439616bd1_Image_001.jpg", "17cbe37a0d356c0e6721d3a62a24818439616bd1_Image_002.jpg", "17cbe37a0d356c0e6721d3a62a24818439616bd1_Image_003.jpg", "17cbe37a0d356c0e6721d3a62a24818439616bd1_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "The impact of membership overlap on the survival of online communities", "pdf_hash": "f504e35bd95dd13e5692c126fb272244dd765d39", "year": 2013, "venue": "ICIS", "alt_text": "The figure shows that communities with higher overlap with mature communities are more likely to survive than communities with lower overlap with mature communities.", "levels": [[3]], "corpus_id": 207210224, "sentences": ["The figure shows that communities with higher overlap with mature communities are more likely to survive than communities with lower overlap with mature communities."], "caption": "Figure 2. Average survival rate for communities with different levels of overlap with mature intersecting communities. This visualization corresponds to Model 1 in Table 4", "local_uri": ["f504e35bd95dd13e5692c126fb272244dd765d39_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The impact of membership overlap on the survival of online communities", "pdf_hash": "f504e35bd95dd13e5692c126fb272244dd765d39", "year": 2013, "venue": "ICIS", "alt_text": "The figure shows that communities which have more core members participating in other communities are less likely to survive than communities which have fewer core members participating in other communities.", "levels": [[3, 1]], "corpus_id": 207210224, "sentences": ["The figure shows that communities which have more core members participating in other communities are less likely to survive than communities which have fewer core members participating in other communities."], "caption": "Figure 3. Average survival rate for communities varying core in focal community (i.e., shared members who are core members in focal community). This visualization corresponds to Model 2 in Table 5", "local_uri": ["f504e35bd95dd13e5692c126fb272244dd765d39_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"But, I don't take steps\": Examining the Inaccessibility of Fitness Trackers for Wheelchair Athletes", "pdf_hash": "5451400f3bf185d1907f5c08e503fc1a23af9c4a", "year": 2015, "venue": "ASSETS", "alt_text": "Left is an image of the fitbit charge wearable fitness band. On the right is the NIke+ GPS Watch.", "levels": null, "corpus_id": 14779210, "sentences": ["Left is an image of the fitbit charge wearable fitness band.", "On the right is the NIke+ GPS Watch."], "caption": "Figure 1. This figure highlights the accessibility challenges in three wearable fitness trackers currently on the market. The Fitbit charge (left) is used as a step counter, which is not a useful measurement for wheelchair users who don\u2019t walk. The clasp used on the Nike+ GPS Watch (right) can be difficult for users with limited hand dexterity to use independently.", "local_uri": ["5451400f3bf185d1907f5c08e503fc1a23af9c4a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"But, I don't take steps\": Examining the Inaccessibility of Fitness Trackers for Wheelchair Athletes", "pdf_hash": "5451400f3bf185d1907f5c08e503fc1a23af9c4a", "year": 2015, "venue": "ASSETS", "alt_text": "Left is a hand cyclist using an adaptive hand bike. On the right is a wheelchair basketball player taking a free throw shot.", "levels": null, "corpus_id": 14779210, "sentences": ["Left is a hand cyclist using an adaptive hand bike.", "On the right is a wheelchair basketball player taking a free throw shot."], "caption": "Figure 3. Specialized and often custom equipment is used by adaptive sport athletes to improve the safety and accessibility of each sport. (Left) Adaptive Hand bike (Right) Basketball Wheelchair.", "local_uri": ["5451400f3bf185d1907f5c08e503fc1a23af9c4a_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "\"But, I don't take steps\": Examining the Inaccessibility of Fitness Trackers for Wheelchair Athletes", "pdf_hash": "5451400f3bf185d1907f5c08e503fc1a23af9c4a", "year": 2015, "venue": "ASSETS", "alt_text": "Diagram showing aesthetics and feedback with equal sizes. Analysis is within feedback and sensing and fit are withing analysis.", "levels": null, "corpus_id": 14779210, "sentences": ["Diagram showing aesthetics and feedback with equal sizes.", "Analysis is within feedback and sensing and fit are withing analysis."], "caption": "Figure 4. This diagram represents the relationship of the five thematic areas we identified that influence wearable system design choices.", "local_uri": ["5451400f3bf185d1907f5c08e503fc1a23af9c4a_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Wheelchair-based game design for older adults", "pdf_hash": "f8361de968dc956b7a2e1f19b3f8ccfc4ba18b0d", "year": 2013, "venue": "ASSETS", "alt_text": "A graph showing the average completion times per gesture. Body-based gestures average around 2500 milliseconds while wheelchair-based gestures take less time for moving forward and backward with an average of about 2000 milliseconds, and turning gestures taking longer with about 3400 milliseconds.", "levels": [[1], [3, 2]], "corpus_id": 141703, "sentences": ["A graph showing the average completion times per gesture.", "Body-based gestures average around 2500 milliseconds while wheelchair-based gestures take less time for moving forward and backward with an average of about 2000 milliseconds, and turning gestures taking longer with about 3400 milliseconds."], "caption": "Figure 1. Avg. completion time per gesture in ms (CI=95%).", "local_uri": ["f8361de968dc956b7a2e1f19b3f8ccfc4ba18b0d_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Wheelchair-based game design for older adults", "pdf_hash": "f8361de968dc956b7a2e1f19b3f8ccfc4ba18b0d", "year": 2013, "venue": "ASSETS", "alt_text": "A screenshot showing the game, Cupcake Heaven. The image displays two lanes of candy and vegetables at the bottom and the top of the screen, and a middle lane in which the player can feed candy to the child that is displayed on the left side of the screen. The player avatar is a hand.", "levels": null, "corpus_id": 141703, "sentences": ["A screenshot showing the game, Cupcake Heaven.", "The image displays two lanes of candy and vegetables at the bottom and the top of the screen, and a middle lane in which the player can feed candy to the child that is displayed on the left side of the screen.", "The player avatar is a hand."], "caption": "Figure 2. Overview of Cupcake Heaven.", "local_uri": ["f8361de968dc956b7a2e1f19b3f8ccfc4ba18b0d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "VirtualComponent: A Mixed-Reality Tool for Designing and Tuning Breadboarded Circuits", "pdf_hash": "4c774672aff0e09dd86dbcc54cb4f80df631f1be", "year": 2019, "venue": "CHI", "alt_text": "An image presenting an overview of the prototype, consisiting of a breadboard, a custom modulo plugged into the breadboard, and a tablet with a software application used to control the values of different virtual components.", "levels": null, "corpus_id": 140230742, "sentences": ["An image presenting an overview of the prototype, consisiting of a breadboard, a custom modulo plugged into the breadboard, and a tablet with a software application used to control the values of different virtual components."], "caption": "Figure 1: VirtualComponent allows placing and tuning electrical components in software, while the underlying physical circuit reflects these changes in real-time. Both external tools (OpenScopeMZ) and custom modules can be used as virtual components when plugged into the breadboard.", "local_uri": ["4c774672aff0e09dd86dbcc54cb4f80df631f1be_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "VirtualComponent: A Mixed-Reality Tool for Designing and Tuning Breadboarded Circuits", "pdf_hash": "4c774672aff0e09dd86dbcc54cb4f80df631f1be", "year": 2019, "venue": "CHI", "alt_text": "The virtual components are available after plugging the module into the breadboard.", "levels": null, "corpus_id": 140230742, "sentences": ["The virtual components are available after plugging the module into the breadboard."], "caption": "Figure 4: The virtual components are available after plug- ging the module into the breadboard.", "local_uri": ["4c774672aff0e09dd86dbcc54cb4f80df631f1be_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "VirtualComponent: A Mixed-Reality Tool for Designing and Tuning Breadboarded Circuits", "pdf_hash": "4c774672aff0e09dd86dbcc54cb4f80df631f1be", "year": 2019, "venue": "CHI", "alt_text": "Probing voltage after tuning the resistors' values with sliders. The values of the resistors can be constrained to be equal (top) or controlled independently (bottom).", "levels": null, "corpus_id": 140230742, "sentences": ["Probing voltage after tuning the resistors' values with sliders.", "The values of the resistors can be constrained to be equal (top) or controlled independently (bottom)."], "caption": "Figure 6: Probing voltage after tuning the resistors\u2019 values with sliders. The values of the resistors can be constrained to be equal (top) or controlled independently (bottom).", "local_uri": ["4c774672aff0e09dd86dbcc54cb4f80df631f1be_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "VirtualComponent: A Mixed-Reality Tool for Designing and Tuning Breadboarded Circuits", "pdf_hash": "4c774672aff0e09dd86dbcc54cb4f80df631f1be", "year": 2019, "venue": "CHI", "alt_text": "Virtual resistors are created using potentiometer ICs, while virtual inductors and capacitors are created by chaining in series or in parallel eight individual components through a port expander.", "levels": null, "corpus_id": 140230742, "sentences": ["Virtual resistors are created using potentiometer ICs, while virtual inductors and capacitors are created by chaining in series or in parallel eight individual components through a port expander."], "caption": "Figure 9: A virtual resistor (left), inductor (center), and capac- itor (right). Virtual resistors are created using potentiome- ter ICs, and virtual inductors and capacitors are created by chaining eight individual components in series or in paral- lel. For example, the image shows a virtual inductor with value of 11 \u00b5H and a virtual capacitor of 4.9 nF.", "local_uri": ["4c774672aff0e09dd86dbcc54cb4f80df631f1be_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage", "pdf_hash": "b14a1d1480426fbe92d85db69f0b79ada279eb90", "year": 2015, "venue": "ASSETS", "alt_text": "Lift points are scattered over intended keys. There is no clear offset pattern. Keys nears edges have lower variability.", "levels": [[-1], [-1], [-1]], "corpus_id": 2293352, "sentences": ["Lift points are scattered over intended keys.", "There is no clear offset pattern.", "Keys nears edges have lower variability."], "caption": "Figure 1. Lift points for all participants in week eight.", "local_uri": ["b14a1d1480426fbe92d85db69f0b79ada279eb90_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage", "pdf_hash": "b14a1d1480426fbe92d85db69f0b79ada279eb90", "year": 2015, "venue": "ASSETS", "alt_text": "Participants slowly improve performance from week to week; however, for P2 there is a significant performance drop in week 3, followed by a steady improvement, and for P4 there is a sudden performance improvement in week 7.", "levels": null, "corpus_id": 2293352, "sentences": ["Participants slowly improve performance from week to week; however, for P2 there is a significant performance drop in week 3, followed by a steady improvement, and for P4 there is a sudden performance improvement in week 7."], "caption": "", "local_uri": ["b14a1d1480426fbe92d85db69f0b79ada279eb90_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage", "pdf_hash": "b14a1d1480426fbe92d85db69f0b79ada279eb90", "year": 2015, "venue": "ASSETS", "alt_text": "Error rates decrease over the 8-week period. Substitutions are clearly the most common error type.", "levels": null, "corpus_id": 2293352, "sentences": ["Error rates decrease over the 8-week period.", "Substitutions are clearly the most common error type."], "caption": "Figure 4. Types of error over 8 weeks.", "local_uri": ["b14a1d1480426fbe92d85db69f0b79ada279eb90_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage", "pdf_hash": "b14a1d1480426fbe92d85db69f0b79ada279eb90", "year": 2015, "venue": "ASSETS", "alt_text": "An overall decrease of error rates is visible from week 1 to week 8. There is not a clear pattern of most challenging keys.", "levels": [[-1], [-1]], "corpus_id": 2293352, "sentences": ["An overall decrease of error rates is visible from week 1 to week 8.", "There is not a clear pattern of most challenging keys."], "caption": "Figure 5. Substitution error rates per key. Gray keys were not used in the trials. Darker colors indicate higher error rates.", "local_uri": ["b14a1d1480426fbe92d85db69f0b79ada279eb90_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage", "pdf_hash": "b14a1d1480426fbe92d85db69f0b79ada279eb90", "year": 2015, "venue": "ASSETS", "alt_text": "There is a clear predominance of horizontal overlaps, particularly on keys N and M.", "levels": [[-1]], "corpus_id": 2293352, "sentences": ["There is a clear predominance of horizontal overlaps, particularly on keys N and M."], "caption": "Figure 6. Polygons encompass hit points within a standard deviation of key centroid.", "local_uri": ["b14a1d1480426fbe92d85db69f0b79ada279eb90_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage", "pdf_hash": "b14a1d1480426fbe92d85db69f0b79ada279eb90", "year": 2015, "venue": "ASSETS", "alt_text": "Bubbles are clearly larger in week one (twice the size of week 8). Some keys such as S, E, and backspace have larger bubbles in week 1.", "levels": null, "corpus_id": 2293352, "sentences": ["Bubbles are clearly larger in week one (twice the size of week 8).", "Some keys such as S, E, and backspace have larger bubbles in week 1."], "caption": "Figure 7. A circle indicates a pause; size represents its duration. Left - week 1 for P1, Right - week 8 for P1.", "local_uri": ["b14a1d1480426fbe92d85db69f0b79ada279eb90_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Typing on Split Keyboards with Peripheral Vision", "pdf_hash": "a579f1acfcc6eb51605078e79f1478fbfb2fedf3", "year": 2019, "venue": "CHI", "alt_text": "Collected touch points across all participants in three modes. The ellipse covers 90% of touch points corresponding to an individual key.", "levels": [[-1], [-1]], "corpus_id": 140313811, "sentences": ["Collected touch points across all participants in three modes.", "The ellipse covers 90% of touch points corresponding to an individual key."], "caption": "We used a unigram Bayesian algorithm based on Goodman et al.\u2019s method [13], with which given tap location sequences", "local_uri": ["a579f1acfcc6eb51605078e79f1478fbfb2fedf3_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Typing on Split Keyboards with Peripheral Vision", "pdf_hash": "a579f1acfcc6eb51605078e79f1478fbfb2fedf3", "year": 2019, "venue": "CHI", "alt_text": "Heat maps of gaze points on the tablet while typing in Eyes-on mode and Peripheral mode.", "levels": [[1]], "corpus_id": 140313811, "sentences": ["Heat maps of gaze points on the tablet while typing in Eyes-on mode and Peripheral mode."], "caption": "Figure 6: Heat maps of gaze points on the tablet while typing in Eyes-on mode (left) and Peripheral mode (right).", "local_uri": ["a579f1acfcc6eb51605078e79f1478fbfb2fedf3_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Typing on Split Keyboards with Peripheral Vision", "pdf_hash": "a579f1acfcc6eb51605078e79f1478fbfb2fedf3", "year": 2019, "venue": "CHI", "alt_text": "Interactions of GlanceType. (a) Tap on both sides of the split keyboard to input letters in either peripheral or eyes-on mode. (b) When performing peripheral typing, gaze fixes at the text field. Swipe right to select the first candidate or drag indirectly to select top-5 candidates from the list beneath the text. The target is highlighted in red. (c) When performing eyes-on typing, direct tap on the candidate to select OOV words from the list above the keyboard. (d) Swipe upward to trigger an additional keyboard for entering punctuation symbols and digits.", "levels": null, "corpus_id": 140313811, "sentences": ["Interactions of GlanceType. (a) Tap on both sides of the split keyboard to input letters in either peripheral or eyes-on mode.", "(b) When performing peripheral typing, gaze fixes at the text field.", "Swipe right to select the first candidate or drag indirectly to select top-5 candidates from the list beneath the text.", "The target is highlighted in red. (c) When performing eyes-on typing, direct tap on the candidate to select OOV words from the list above the keyboard.", "(d) Swipe upward to trigger an additional keyboard for entering punctuation symbols and digits."], "caption": "Figure 8: Interactions of GlanceType. (a) Tap on both sides of the split keyboard to input letters in either peripheral or eyes-on mode. (b) When performing peripheral typing, gaze fxes at the text feld. Swipe right to select the frst candidate or drag indirectly to select top-5 candidates from the list beneath the text. The target is highlighted in red. (c) When performing eyes- on typing, direct tap on the candidate to select OOV words from the list above the keyboard. (d) Swipe upward to trigger an additional keyboard for entering punctuation symbols and digits.", "local_uri": ["a579f1acfcc6eb51605078e79f1478fbfb2fedf3_Image_011.png"], "annotated": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "First image: In the foreground, a 3D-printed mountain relief hovered by a single finger. In the background, a set of images of the actual mountain on a laptop screen.  Second image: In the foreground, a 3D-printed molecule touched by a single finger. In the background, a 3D visualization of the molecule on a laptop screen.  Third image: A finger that presses the head of a 3D-printed duck. As a result, a sound is played.", "levels": null, "corpus_id": 108395187, "sentences": ["First image: In the foreground, a 3D-printed mountain relief hovered by a single finger.", "In the background, a set of images of the actual mountain on a laptop screen.", "Second image: In the foreground, a 3D-printed molecule touched by a single finger.", "In the background, a 3D visualization of the molecule on a laptop screen.", "Third image: A finger that presses the head of a 3D-printed duck.", "As a result, a sound is played."], "caption": "Figure 1: Trilaterate is a fabrication pipeline that enables users to 3D print hover-, touch-, and force-sensitive objects.", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "Image A shows a duck equipped with four colored circular electrodes, each encapsulated with shields except in a certain spherical segment which is visually shown. Image B shows the same duck equipped with four colored circular electrodes. Also, a finger is shown hovering the duck and three distances, i.e. three circular arcs illustrated the finger's location at their intersection point.", "levels": null, "corpus_id": 108395187, "sentences": ["Image A shows a duck equipped with four colored circular electrodes, each encapsulated with shields except in a certain spherical segment which is visually shown.", "Image B shows the same duck equipped with four colored circular electrodes.", "Also, a finger is shown hovering the duck and three distances, i.e. three circular arcs illustrated the finger's location at their intersection point."], "caption": "Figure 2: The principle of capacitive trilateration in 2D: The capacitance measured at a single electrode implies a fnger on the circular arc in its segment with distance di (B). For 2D, at least three electrodes need to be combined to estimate the position of the fnger at the intersection point of all circles.", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "An illustration of the steps of the fabrication pipeline: A) a black spherical electrode with a shield. B) a 3D rendering of a yellow duck containing brown spherical electrodes. C) The same duck now also with traces. D) A photo of a 3D printer that printed the duck and a close-up photo of the internal 3D-printed structures. E) Wires that are connected to the bottom of the 3D-printed duck. F) The 3D-printed duck on a black platform with a finger touching the duck. Also, a 3D visualization of the duck showing that a touch was recognized.", "levels": null, "corpus_id": 108395187, "sentences": ["An illustration of the steps of the fabrication pipeline: A) a black spherical electrode with a shield.", "B) a 3D rendering of a yellow duck containing brown spherical electrodes.", "C) The same duck now also with traces.", "D) A photo of a 3D printer that printed the duck and a close-up photo of the internal 3D-printed structures.", "E) Wires that are connected to the bottom of the 3D-printed duck.", "F) The 3D-printed duck on a black platform with a finger touching the duck. Also, a 3D visualization of the duck showing that a touch was recognized."], "caption": "", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "A visual illustration of the remove least utility algorithm: A triangle contains electrodes. A d4 distance is shown for four electrodes. This visualization is replicated for varying electrode configurations that, when summed up, result in the performance P.", "levels": null, "corpus_id": 108395187, "sentences": ["A visual illustration of the remove least utility algorithm: A triangle contains electrodes.", "A d4 distance is shown for four electrodes.", "This visualization is replicated for varying electrode configurations that, when summed up, result in the performance P."], "caption": "", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_005.png"], "annotated": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "A screenshot of the trilaterate object generator. Screenshot A shows a sidebar with UI elements at the left and a 3D view at the right that shows a pyramid object which contains electrodes and traces. Screenshot B shows the same user interface and pyramid but the surface of the pyramid is colored according to the expected sensing performance,", "levels": [[-1], [-1], [-1]], "corpus_id": 108395187, "sentences": ["A screenshot of the trilaterate object generator.", "Screenshot A shows a sidebar with UI elements at the left and a 3D view at the right that shows a pyramid object which contains electrodes and traces.", "Screenshot B shows the same user interface and pyramid but the surface of the pyramid is colored according to the expected sensing performance,"], "caption": "Figure 5: The Trilaterate object generator enables users to confgure properties (e.g. the width of sensing structures) of the autogeneration (A). Users can assess the expected sens- ing performance with respect to the surface of the object (B).", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "Image A shows a 3D rendering of the pyramid containing electrodes. Image B shows an image of the 3D-printed pyramid with the seven target positions highlighted.", "levels": null, "corpus_id": 108395187, "sentences": ["Image A shows a 3D rendering of the pyramid containing electrodes.", "Image B shows an image of the 3D-printed pyramid with the seven target positions highlighted."], "caption": "", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "The image shows a table of the positional errors per condition (2x2): For (near,high) = 13.8 mm (SD 3.54 mm), for (near,low) = 28.9 mm (SD 6.85 mm), for (far,high) = 44.7 mm (SD 15.1 mm), for (far,low) = 46.5 mm (SD 5.98 mm).", "levels": [[2, 1]], "corpus_id": 108395187, "sentences": ["The image shows a table of the positional errors per condition (2x2): For (near,high) = 13.8 mm (SD 3.54 mm), for (near,low) = 28.9 mm (SD 6.85 mm), for (far,high) = 44.7 mm (SD 15.1 mm), for (far,low) = 46.5 mm (SD 5.98 mm)."], "caption": "Figure 7: Positional 3D errors with standard deviations (SD) per target position (top) and condition (bottom).", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects", "pdf_hash": "3b592c4f68ad1c9cdb32fe825aae0c090c09f937", "year": 2019, "venue": "CHI", "alt_text": "Five example applications each showing a close-up of the 3D-printed object, a 3D rengering of the internal structures, and a photo of the interactive application described in the text.", "levels": null, "corpus_id": 108395187, "sentences": ["Five example applications each showing a close-up of the 3D-printed object, a 3D rengering of the internal structures, and a photo of the interactive application described in the text."], "caption": "", "local_uri": ["3b592c4f68ad1c9cdb32fe825aae0c090c09f937_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "'Wow! You're Wearing a Fitbit, You're a Young Boy Now!\": Socio-Technical Aspirations for Children with Autism in India", "pdf_hash": "108a29b3e76f0b2f375ddabb46b44b436bdee7b3", "year": 2018, "venue": "ASSETS", "alt_text": "This is a scatter plot of the Fitbit data for 6 participants with gender (color of point-box), totoal number of days the firbit worn in the x axis, and average steps per day in the y-axis. Data is as follows 1. Female, 4 days, apprx 9000 steps per day 2. Male, 5 days, a little less than 9000 steps a day 3. Male, 6 days, 2500 steps a day 4. Female, 7 days, 7500 steps a day 5. Male, 14 days, a littl emore than 6000 steps a day 6. Female, 16 days, a littl emore than 8000 steps a day", "levels": [[1], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 52942407, "sentences": ["This is a scatter plot of the Fitbit data for 6 participants with gender (color of point-box), totoal number of days the firbit worn in the x axis, and average steps per day in the y-axis.", "Data is as follows 1.", "Female, 4 days, apprx 9000 steps per day 2.", "Male, 5 days, a little less than 9000 steps a day 3.", "Male, 6 days, 2500 steps a day 4.", "Female, 7 days, 7500 steps a day 5.", "Male, 14 days, a littl emore than 6000 steps a day 6.", "Female, 16 days, a littl emore than 8000 steps a day"], "caption": "Figure 2: Fitbit data for the six participants", "local_uri": ["108a29b3e76f0b2f375ddabb46b44b436bdee7b3_Image_002.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The AT Effect: How Disability Affects the Perceived Social Acceptability of Head-Mounted Display Use", "pdf_hash": "dc5b77367c4ad08006c1cdd3f981595e88b42e4e", "year": 2016, "venue": "CHI", "alt_text": "Two part image (one left and one right) of a female actress against an outdoor background wearing the head-mounted display Google Glass. This first image (left) depicts the actress wearing just Google Glass. The second image (right) depicts the exact same image with the actress wearing Google Glass with sunglasses.", "levels": null, "corpus_id": 11139638, "sentences": ["Two part image (one left and one right) of a female actress against an outdoor background wearing the head-mounted display Google Glass. This first image (left) depicts the actress wearing just Google Glass.", "The second image (right) depicts the exact same image with the actress wearing Google Glass with sunglasses."], "caption": "", "local_uri": ["dc5b77367c4ad08006c1cdd3f981595e88b42e4e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The AT Effect: How Disability Affects the Perceived Social Acceptability of Head-Mounted Display Use", "pdf_hash": "dc5b77367c4ad08006c1cdd3f981595e88b42e4e", "year": 2016, "venue": "CHI", "alt_text": "Two part image (one left and one right) of a female actress against an outdoor background wearing the head-mounted display Google Glass. This second image (right) depicts the actress wearing Google Glass with sunglasses. The first image (left) depicts the exact same image with the actress wearing just Google Glass (without the sunglasses attachment(.", "levels": null, "corpus_id": 11139638, "sentences": ["Two part image (one left and one right) of a female actress against an outdoor background wearing the head-mounted display Google Glass. This second image (right) depicts the actress wearing Google Glass with sunglasses.", "The first image (left) depicts the exact same image with the actress wearing just Google Glass (without the sunglasses attachment(."], "caption": "Figure 1. Participants judged the social acceptability of a video scenario showing an actress using a head-mounted display in public. We varied information about the actress\u2019s disability by manipulating the video description and the actress\u2019s appearance. Left: Actress wearing Google Glass. Right: Actress wearing Google Glass with eye shades.", "local_uri": ["dc5b77367c4ad08006c1cdd3f981595e88b42e4e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The AT Effect: How Disability Affects the Perceived Social Acceptability of Head-Mounted Display Use", "pdf_hash": "dc5b77367c4ad08006c1cdd3f981595e88b42e4e", "year": 2016, "venue": "CHI", "alt_text": "Actress walking up to a bus stop while wearing Google Glass and eye shades, carrying a white cane.", "levels": null, "corpus_id": 11139638, "sentences": ["Actress walking up to a bus stop while wearing Google Glass and eye shades, carrying a white cane."], "caption": "Figure 2. In Video 1, the actress walks to a bus stop using a white cane and dark sunglasses to indicate that she has a disability. Video 2 presented an identical interaction scenario, but without the actress\u2019s white cane and sunglasses.", "local_uri": ["dc5b77367c4ad08006c1cdd3f981595e88b42e4e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites", "pdf_hash": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4", "year": 2017, "venue": "ASSETS", "alt_text": "a.) Faces are identified by each individual face detector. b.) Next the faces identified by a majority of the ensemble of the detectors are retained. c.) Foreground (FG) pixels are returned by the background subtraction. d.) Foreground is refined via morphological de-noising. e.) Regions of interest (ROI) defined for each face detected in the frame. f.) Computation of PMPs for a video frame based on polar representation of foreground relative to the face.", "levels": null, "corpus_id": 3108019, "sentences": ["a.) Faces are identified by each individual face detector.", "b.) Next the faces identified by a majority of the ensemble of the detectors are retained.", "c.) Foreground (FG) pixels are returned by the background subtraction.", "d.) Foreground is refined via morphological de-noising.", "e.) Regions of interest (ROI) defined for each face detected in the frame.", "f.) Computation of PMPs for a video frame based on polar representation of foreground relative to the face."], "caption": "Figure 1. (a) Faces identified by each face detector. (b) Faces from the ensemble of the detectors. (c) Foreground (FG) pixels returned by the background subtraction. (d) Refined FG after morphological de-noising. (e) Regions of interest (ROI) defined for each face detected in the frame. (f) Computation of PMPs for a video frame.", "local_uri": ["c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites", "pdf_hash": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4", "year": 2017, "venue": "ASSETS", "alt_text": "Graph shows that while the ensemble of face detectors works best, the alt and alt2 frontal face detectors perform similarly when there are at least 45 training samples for each class.", "levels": [[3, 2]], "corpus_id": 3108019, "sentences": ["Graph shows that while the ensemble of face detectors works best, the alt and alt2 frontal face detectors perform similarly when there are at least 45 training samples for each class."], "caption": "Figure 2. F1 scores for face detectors and training set sizes", "local_uri": ["c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites", "pdf_hash": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4", "year": 2017, "venue": "ASSETS", "alt_text": "Graph shows that performance is similar and there is a slow degradation in performance measured via F1 score as the sampling rate decreases (e.g. when there are more intermediate frames between those used for face detection) for both the Ensemble and Alt 2 face detectors. The performance is relatively table when frames are sampled at least every 20-25 frames for the 30 frame per second video.", "levels": [[3, 2], [3]], "corpus_id": 3108019, "sentences": ["Graph shows that performance is similar and there is a slow degradation in performance measured via F1 score as the sampling rate decreases (e.g. when there are more intermediate frames between those used for face detection) for both the Ensemble and Alt 2 face detectors.", "The performance is relatively table when frames are sampled at least every 20-25 frames for the 30 frame per second video."], "caption": "Figure 3. F1 scores for applying face detection to sampled frames. The erratic nature of the results are likely due to the deterministically sampled frames being more or less representative of the video as the starting frame was fixed for each iteration.", "local_uri": ["c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites", "pdf_hash": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4", "year": 2017, "venue": "ASSETS", "alt_text": "The ensemble face detection approach shows slightly better performance compared to individual face detectors for shorter video segments but the advantage is not very large with F1 scores differing by at most .04 between the ensemble and Alt 2 face detector.", "levels": null, "corpus_id": 3108019, "sentences": ["The ensemble face detection approach shows slightly better performance compared to individual face detectors for shorter video segments but the advantage is not very large with F1 scores differing by at most .04 between the ensemble and Alt 2 face detector."], "caption": "", "local_uri": ["c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites", "pdf_hash": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4", "year": 2017, "venue": "ASSETS", "alt_text": "The graph shows that the F1 score hovers around 71% once at least 10 frames are selected from a 60 second video segment while it takes more frames to achieve a lower performance (65%) when analyzing 30 second video segments.", "levels": [[3, 2, 1]], "corpus_id": 3108019, "sentences": ["The graph shows that the F1 score hovers around 71% once at least 10 frames are selected from a 60 second video segment while it takes more frames to achieve a lower performance (65%) when analyzing 30 second video segments."], "caption": "Figure 5. SL vs. Non-SL F1 score for 30 and 60 second videos.", "local_uri": ["c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Combining crowdsourcing and learning to improve engagement and performance", "pdf_hash": "0c3fb31211fcaba1f1cf7406813774c589acc83a", "year": 2014, "venue": "CHI", "alt_text": "The left side of this figure shows the panel inside of Photoshop. The right side of this figure shows originals and the improved versions submitted by our players.", "levels": null, "corpus_id": 8392767, "sentences": ["The left side of this figure shows the panel inside of Photoshop.", "The right side of this figure shows originals and the improved versions submitted by our players."], "caption": "", "local_uri": ["0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_001.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_002.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_003.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_004.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_005.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Combining crowdsourcing and learning to improve engagement and performance", "pdf_hash": "0c3fb31211fcaba1f1cf7406813774c589acc83a", "year": 2014, "venue": "CHI", "alt_text": "This figure shows an example of an interactive tutorial, the Brightness and Contrast tutorial.", "levels": null, "corpus_id": 8392767, "sentences": ["This figure shows an example of an interactive tutorial, the Brightness and Contrast tutorial."], "caption": "User:", "local_uri": ["0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_013.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_014.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_015.jpg"], "annotated": false, "compound": true}
{"title": "Combining crowdsourcing and learning to improve engagement and performance", "pdf_hash": "0c3fb31211fcaba1f1cf7406813774c589acc83a", "year": 2014, "venue": "CHI", "alt_text": "This figure shows the Challenge round. In the first challenge round the user gets two hints, to sharpen the image and adjust the brightness.", "levels": null, "corpus_id": 8392767, "sentences": ["This figure shows the Challenge round.", "In the first challenge round the user gets two hints, to sharpen the image and adjust the brightness."], "caption": "Adjust brightness (25 pts) User:", "local_uri": ["0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_016.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_017.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_018.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_019.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_020.jpg", "0c3fb31211fcaba1f1cf7406813774c589acc83a_Image_021.gif"], "annotated": false, "compound": true}
{"title": "VacuumTouch: attractive force feedback interface for haptic interactive surface using air suction", "pdf_hash": "b661e1e4bcc4e6d51f6cf0e30be641ed44a72889", "year": 2014, "venue": "CHI", "alt_text": "VacuumTouch: user can feel the attractive force to the surface without requiring to hold or attach an additional device. Here, user is feeling the attractive force from La Bocca della Verit\u00e0.", "levels": null, "corpus_id": 9681517, "sentences": ["VacuumTouch: user can feel the attractive force to the surface without requiring to hold or attach an additional device.", "Here, user is feeling the attractive force from La Bocca della Verit\u00e0."], "caption": "", "local_uri": ["b661e1e4bcc4e6d51f6cf0e30be641ed44a72889_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "VacuumTouch: attractive force feedback interface for haptic interactive surface using air suction", "pdf_hash": "b661e1e4bcc4e6d51f6cf0e30be641ed44a72889", "year": 2014, "venue": "CHI", "alt_text": "The setup to investigate the latency between the microcontroller\u2019s order and the generation of the suction force, typical duration is 10ms (#n=5 with a 500 fps).", "levels": null, "corpus_id": 9681517, "sentences": ["The setup to investigate the latency between the microcontroller\u2019s order and the generation of the suction force, typical duration is 10ms (#n=5 with a 500 fps)."], "caption": "Figure 3. The setup to investigate the latency between the microcontroller\u2019s order and the generation of the suction force. Typical duration is 10ms (#n=5 with a 500 fps).", "local_uri": ["b661e1e4bcc4e6d51f6cf0e30be641ed44a72889_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "VacuumTouch: attractive force feedback interface for haptic interactive surface using air suction", "pdf_hash": "b661e1e4bcc4e6d51f6cf0e30be641ed44a72889", "year": 2014, "venue": "CHI", "alt_text": "A VacuumTouch prototype surface with the workspace of 280 \u00d7 180mm and 5 \u00d7 5 holes.", "levels": null, "corpus_id": 9681517, "sentences": ["A VacuumTouch prototype surface with the workspace of 280 \u00d7 180mm and 5 \u00d7 5 holes."], "caption": "", "local_uri": ["b661e1e4bcc4e6d51f6cf0e30be641ed44a72889_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "VacuumTouch: attractive force feedback interface for haptic interactive surface using air suction", "pdf_hash": "b661e1e4bcc4e6d51f6cf0e30be641ed44a72889", "year": 2014, "venue": "CHI", "alt_text": "The experimental setup: the touch sensor (the IR photo reflector array and the induction-based touch detection unit) installed on the surface and the tact switch located on the right side of the surface. The IR sensor covered with the ABS resin and the thin black plastic sheet fixed along the touch detection unit.", "levels": null, "corpus_id": 9681517, "sentences": ["The experimental setup: the touch sensor (the IR photo reflector array and the induction-based touch detection unit) installed on the surface and the tact switch located on the right side of the surface.", "The IR sensor covered with the ABS resin and the thin black plastic sheet fixed along the touch detection unit."], "caption": "Figure 6. The experimental setup: the touch sensor (the IR photo reflector array and the induction-based touch detection unit) installed on the surface and the tact switch located on the right side of the surface. The IR sensor covered with the ABS resin and the thin, black plastic sheet fixed along the touch detection unit.", "local_uri": ["b661e1e4bcc4e6d51f6cf0e30be641ed44a72889_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "VacuumTouch: attractive force feedback interface for haptic interactive surface using air suction", "pdf_hash": "b661e1e4bcc4e6d51f6cf0e30be641ed44a72889", "year": 2014, "venue": "CHI", "alt_text": "Principle of an IR photo reflector array to detect one dimensional finger position.", "levels": null, "corpus_id": 9681517, "sentences": ["Principle of an IR photo reflector array to detect one dimensional finger position."], "caption": "Figure 7. Principle of an IR photo reflector array to detect one-dimensional finger position.", "local_uri": ["b661e1e4bcc4e6d51f6cf0e30be641ed44a72889_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "VacuumTouch: attractive force feedback interface for haptic interactive surface using air suction", "pdf_hash": "b661e1e4bcc4e6d51f6cf0e30be641ed44a72889", "year": 2014, "venue": "CHI", "alt_text": "Task times for the four feedback conditions  (+/- standard deviation of the mean).", "levels": [[1]], "corpus_id": 9681517, "sentences": ["Task times for the four feedback conditions  (+/- standard deviation of the mean)."], "caption": "Figure 9. Task times for the four feedback conditions (+/- standard deviation of the mean).", "local_uri": ["b661e1e4bcc4e6d51f6cf0e30be641ed44a72889_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Screenshot of the TextAlive system. There is a graphic with Japanese characters, labeled \"stage\", a long section labeled \"Timeline\" below it, and a sidebar labeled \"Editor\".", "levels": [[-1], [-1]], "corpus_id": 1386326, "sentences": ["Screenshot of the TextAlive system.", "There is a graphic with Japanese characters, labeled \"stage\", a long section labeled \"Timeline\" below it, and a sidebar labeled \"Editor\"."], "caption": "Figure 1. Overview of the TextAlive system.", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Timeline interace, including \"Range selection for A-B repeat\", \"Vocal volume visualization\", \"Chorus-part information\", \"Beat information\" highlighted.", "levels": null, "corpus_id": 1386326, "sentences": ["Timeline interace, including \"Range selection for A-B repeat\", \"Vocal volume visualization\", \"Chorus-part information\", \"Beat information\" highlighted."], "caption": "Figure 2. Timeline interface.", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_002.png"], "annotated": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Editor interface and integrated source code editor; the user can switch between them by clicking the buttons (f) \"Edit\", \"Save\", \"Close\".", "levels": null, "corpus_id": 1386326, "sentences": ["Editor interface and integrated source code editor; the user can switch between them by clicking the buttons (f) \"Edit\", \"Save\", \"Close\"."], "caption": "Figure 3. Editor interface and integrated source code editor; the user can switch between them by clicking the buttons (f).", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_003.png"], "annotated": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Example of \"Sliding\" with black Japanese characters on the right. Below that is an example of \"Sliding + Hopping\" with black Japanese characters on the right. And below that is an example of \"Sliding + Hopping + Karaoke\" with grey, red and black Japanese characters on the right.", "levels": [[-1], [-1], [-1]], "corpus_id": 1386326, "sentences": ["Example of \"Sliding\" with black Japanese characters on the right.", "Below that is an example of \"Sliding + Hopping\" with black Japanese characters on the right.", "And below that is an example of \"Sliding + Hopping + Karaoke\" with grey, red and black Japanese characters on the right."], "caption": "Figure 4. A simple example of kinetic typography effects generated from different sets of preset animation templates.", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Comments and GUI widgets generated for horizontalAlignment (left, ceter, and right with radio buttons), and Draw track, offset slide bar, color, and imagePath.", "levels": [[-1]], "corpus_id": 1386326, "sentences": ["Comments and GUI widgets generated for horizontalAlignment (left, ceter, and right with radio buttons), and Draw track, offset slide bar, color, and imagePath."], "caption": "", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Software architecture of the TextAlive system, including Chorus detection for song files (MP3, MS WAV), Automatic estimation of timing information with lyric or transcription. Then the two will be processed by automatic video composition. Then, together with animation templates written in Java by live programming, kinetic typography video can be generated by designers and casual users by interactive video editing.", "levels": [[-1], [-1], [-1]], "corpus_id": 1386326, "sentences": ["Software architecture of the TextAlive system, including Chorus detection for song files (MP3, MS WAV), Automatic estimation of timing information with lyric or transcription.", "Then the two will be processed by automatic video composition.", "Then, together with animation templates written in Java by live programming, kinetic typography video can be generated by designers and casual users by interactive video editing."], "caption": "Figure 5. Comments and GUI widgets generated from them.              Figure 6. Software architecture of the TextAlive system.\u200c", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Pseudocode for rendering each frame, with visualizations of the procedures on the right.", "levels": null, "corpus_id": 1386326, "sentences": ["Pseudocode for rendering each frame, with visualizations of the procedures on the right."], "caption": "Figure 8. Pseudocode for rendering each frame.", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Code examples for JSON-formatted text for saving the animation, including examples for Phase, Template, Word, Timing, Character, and Font.", "levels": null, "corpus_id": 1386326, "sentences": ["Code examples for JSON-formatted text for saving the animation, including examples for Phase, Template, Word, Timing, Character, and Font."], "caption": "Figure 9. JSON-formatted text for saving the animation.", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Setup of the user study. Left: a participant using our system. Right: a screenshot taken during the study of the system.", "levels": null, "corpus_id": 1386326, "sentences": ["Setup of the user study.", "Left: a participant using our system.", "Right: a screenshot taken during the study of the system."], "caption": "Figure 10. Setup of the user study. (a) A participant using our system. (b) A screenshot taken during the study.", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "TextAlive: Integrated Design Environment for Kinetic Typography", "pdf_hash": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "year": 2015, "venue": "CHI", "alt_text": "Screenshots of kinetic typography videos in various styles created by the participants (selected and trimmed.), showing various text, font, size, orientation, color, background, graphics.", "levels": [[-1], [-1]], "corpus_id": 1386326, "sentences": ["Screenshots of kinetic typography videos in various styles created by the participants (selected and trimmed.),", "showing various text, font, size, orientation, color, background, graphics."], "caption": "Figure 11. Screenshots of kinetic typography videos in various styles created by the participants (selected and trimmed.)", "local_uri": ["110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Dissimilarity-Consensus Approach to Agreement Analysis in Gesture Elicitation Studies", "pdf_hash": "ec53d5cef0968ba3a7edd0a55e60f634ba8b792c", "year": 2019, "venue": "CHI", "alt_text": "Fifteen charts arranged in a 3 by 5 matrix showing growth curves for each of the 15 referents considered in this paper.", "levels": [[1]], "corpus_id": 140341035, "sentences": ["Fifteen charts arranged in a 3 by 5 matrix showing growth curves for each of the 15 referents considered in this paper."], "caption": "", "local_uri": ["ec53d5cef0968ba3a7edd0a55e60f634ba8b792c_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Answering visual questions with conversational crowd assistants", "pdf_hash": "2be456d47795df387b4ca9273e17a785ee184e67", "year": 2013, "venue": "ASSETS", "alt_text": "A system diagram of Chorus:View. The user's phone sends video and audio to the server, which in turn sends it to multiple crowd workers' computers. The workers send back answers to the server, which are merged and sent back tothe users' phone.", "levels": null, "corpus_id": 436695, "sentences": ["A system diagram of Chorus:View.", "The user's phone sends video and audio to the server, which in turn sends it to multiple crowd workers' computers.", "The workers send back answers to the server, which are merged and sent back tothe users' phone."], "caption": "We can see a box on the desk.", "local_uri": ["2be456d47795df387b4ca9273e17a785ee184e67_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Answering visual questions with conversational crowd assistants", "pdf_hash": "2be456d47795df387b4ca9273e17a785ee184e67", "year": 2013, "venue": "ASSETS", "alt_text": "A series of 5 images taken of a frozen dinner (with some images in the series omitted). Each image is improperly framed, but different.", "levels": null, "corpus_id": 436695, "sentences": ["A series of 5 images taken of a frozen dinner (with some images in the series omitted).", "Each image is improperly framed, but different."], "caption": "\u2026", "local_uri": ["2be456d47795df387b4ca9273e17a785ee184e67_Image_004.jpg", "2be456d47795df387b4ca9273e17a785ee184e67_Image_005.jpg", "2be456d47795df387b4ca9273e17a785ee184e67_Image_006.jpg", "2be456d47795df387b4ca9273e17a785ee184e67_Image_007.jpg", "2be456d47795df387b4ca9273e17a785ee184e67_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Answering visual questions with conversational crowd assistants", "pdf_hash": "2be456d47795df387b4ca9273e17a785ee184e67", "year": 2013, "venue": "ASSETS", "alt_text": "Screen shot of the Chorus:View interface. On the left is a chat window with an audio player as one message, a proposd but not accepted message highlighted in red, and an accepted message in white visible. Below that is a score box reading '10475'. On the right is a video window with multiple screen shot icons displayed below for quick browsing.", "levels": null, "corpus_id": 436695, "sentences": ["Screen shot of the Chorus:View interface.", "On the left is a chat window with an audio player as one message, a proposd but not accepted message highlighted in red, and an accepted message in white visible.", "Below that is a score box reading '10475'.", "On the right is a video window with multiple screen shot icons displayed below for quick browsing."], "caption": "Figure 4. Chorus:View worker interface. Workers are shown a video streamed from the user\u2019s phone, and asked to reply to spoken queries using the chat interface. In this example, the recorded message asks workers \u201cHow do I cook this?\u201d a question that often involves multiple image-framing steps and was often challenging using VizWiz.", "local_uri": ["2be456d47795df387b4ca9273e17a785ee184e67_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists", "pdf_hash": "bcb3a46274165a64c4b951ae95fa118493593ddc", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 1 provides pictures of standardized crutch grip and a customized 3D printed crutch tip designed by PT professors to mitigate wrist tension. Person in photo is gripping each of the example grips with their hands. There are also two photos of the crutch grips attatched to the crutches while being layed on a flat surface for the photo.", "levels": null, "corpus_id": 17220969, "sentences": ["Figure 1 provides pictures of standardized crutch grip and a customized 3D printed crutch tip designed by PT professors to mitigate wrist tension.", "Person in photo is gripping each of the example grips with their hands.", "There are also two photos of the crutch grips attatched to the crutches while being layed on a flat surface for the photo."], "caption": "Figure 1. Standard crutch grip (left) and custom, ergonomic 3D-printed crutch grip co-designed with PT professors (right). Each is shown in use (above) and on its own (below).", "local_uri": ["bcb3a46274165a64c4b951ae95fa118493593ddc_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists", "pdf_hash": "bcb3a46274165a64c4b951ae95fa118493593ddc", "year": 2016, "venue": "ASSETS", "alt_text": "A black 3D printed crutch grip and the original 3D modeled grip made of red play-dough and molded by PTs professors. The black crutch grip was smoothed and enlarged in 3D modeling software.", "levels": null, "corpus_id": 17220969, "sentences": ["A black 3D printed crutch grip and the original 3D modeled grip made of red play-dough and molded by PTs professors.", "The black crutch grip was smoothed and enlarged in 3D modeling software."], "caption": "Figure 2. (Left) Black 3D printed crutch grip designed by scanning the smaller red play-dough prototype (Right) made by a PT professor.", "local_uri": ["bcb3a46274165a64c4b951ae95fa118493593ddc_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists", "pdf_hash": "bcb3a46274165a64c4b951ae95fa118493593ddc", "year": 2016, "venue": "ASSETS", "alt_text": "(Left) A 3D printed crutch tip installed onto a crutch. (Top Right) Different iterations of various 3D printed material to test crutch tips. (Bottom Right) A 3D printed crutch tip in comparison to original crutch tip.", "levels": null, "corpus_id": 17220969, "sentences": ["(Left) A 3D printed crutch tip installed onto a crutch. (Top Right) Different iterations of various 3D printed material to test crutch tips. (Bottom Right) A 3D printed crutch tip in comparison to original crutch tip."], "caption": "Figure 3. (Left) A 3D printed crutch tip installed onto a crutch. (Top Right) Different iterations of 3D printed material to test crutch tips. (Bottom Right) A 3D printed crutch tip in comparison to original crutch tip.", "local_uri": ["bcb3a46274165a64c4b951ae95fa118493593ddc_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists", "pdf_hash": "bcb3a46274165a64c4b951ae95fa118493593ddc", "year": 2016, "venue": "ASSETS", "alt_text": "Figure provides four images, two of which are sketches for grip augmentation designs for scenario two and four. The other two are images of the physical prototypes for this design. Scenario 2 image displays both a 3D printed and play-dough model of the augmented hand grip for a cane. Scenario 3 image displays just the play-dough version while it was being created by the students and placed on top of an actual cane.", "levels": null, "corpus_id": 17220969, "sentences": ["Figure provides four images, two of which are sketches for grip augmentation designs for scenario two and four.", "The other two are images of the physical prototypes for this design.", "Scenario 2 image displays both a 3D printed and play-dough model of the augmented hand grip for a cane.", "Scenario 3 image displays just the play-dough version while it was being created by the students and placed on top of an actual cane."], "caption": "Figure 4. Student designs for Scenario 2 (Top) and Scenario 4 (Bottom). Students working on Scenario 2 created an arm-rest grip augmentation to assist a patient with limited wrist mobility. Students working on Scenario 4 augmented a cane with a grip extension to alleviate stress to the scenario patient\u2019s elbow. 3D printed version of the Scenario 2 model is provided (Top-Right).", "local_uri": ["bcb3a46274165a64c4b951ae95fa118493593ddc_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists", "pdf_hash": "bcb3a46274165a64c4b951ae95fa118493593ddc", "year": 2016, "venue": "ASSETS", "alt_text": "Bar graph which demonstrates the increase level of knowledge gained by students after the 3D printing class sessions.", "levels": [[1]], "corpus_id": 17220969, "sentences": ["Bar graph which demonstrates the increase level of knowledge gained by students after the 3D printing class sessions."], "caption": "Figure 5. Bar graph of students\u2019 change in personal knowledge of 3D printing between Class One and Class Two.", "local_uri": ["bcb3a46274165a64c4b951ae95fa118493593ddc_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum", "pdf_hash": "d1d8b26974a672a00ac297c4910d279cb8f13a7c", "year": 2017, "venue": "ASSETS", "alt_text": "This image is an illustration of someone bowing a Chladni plate with a violin bow with a subsequent Cymatic shape formed on the surface.", "levels": null, "corpus_id": 40091765, "sentences": ["This image is an illustration of someone bowing a Chladni plate with a violin bow with a subsequent Cymatic shape formed on the surface."], "caption": "Figure 2: Chladni plate", "local_uri": ["d1d8b26974a672a00ac297c4910d279cb8f13a7c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum", "pdf_hash": "d1d8b26974a672a00ac297c4910d279cb8f13a7c", "year": 2017, "venue": "ASSETS", "alt_text": "This image shows a selection of 3D modelled Cymatic shapes created in Autodesk Maya software.", "levels": null, "corpus_id": 40091765, "sentences": ["This image shows a selection of 3D modelled Cymatic shapes created in Autodesk Maya software."], "caption": "Figure 4: 3D Cymatics shapes modelled in Autodesk Maya", "local_uri": ["d1d8b26974a672a00ac297c4910d279cb8f13a7c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum", "pdf_hash": "d1d8b26974a672a00ac297c4910d279cb8f13a7c", "year": 2017, "venue": "ASSETS", "alt_text": "This image shows a therapist and client on either side of the CymaSense interactive  table playing it with beaters.", "levels": null, "corpus_id": 40091765, "sentences": ["This image shows a therapist and client on either side of the CymaSense interactive  table playing it with beaters."], "caption": "Figure 5: CymaSense Interactive table", "local_uri": ["d1d8b26974a672a00ac297c4910d279cb8f13a7c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum", "pdf_hash": "d1d8b26974a672a00ac297c4910d279cb8f13a7c", "year": 2017, "venue": "ASSETS", "alt_text": "This line graph shows the trends for musical, non-musical and combined mean scores for all participants over the eight therapy sessions.", "levels": [[1]], "corpus_id": 40091765, "sentences": ["This line graph shows the trends for musical, non-musical and combined mean scores for all participants over the eight therapy sessions."], "caption": "Figure 7: One-to-one and group mean scores over 8 sessions without G1 included. Musical scores ranged from 23 to 77 including G1 data, 23 to 62 without, with mean scores of 35", "local_uri": ["d1d8b26974a672a00ac297c4910d279cb8f13a7c_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Invisible Work of Accessibility: How Blind Employees Manage Accessibility in Mixed-Ability Workplaces", "pdf_hash": "58103389e3be8c96d8dd04763acc9da05e79c34a", "year": 2015, "venue": "ASSETS", "alt_text": "Woman holding a printout copy of a research consent form. Aa sticky note has been added directly below the signature line to provide tactile feedback for placing a signature.", "levels": null, "corpus_id": 15285651, "sentences": ["Woman holding a printout copy of a research consent form.", "Aa sticky note has been added directly below the signature line to provide tactile feedback for placing a signature."], "caption": "Figure 1. Our blind office worker participants addressed accessibility challenges through a combination of assistive technology and collaboration with sighted users. Here, a participant shows a paper form with the signature line marked by a sticky note placed by her sighted colleague.", "local_uri": ["58103389e3be8c96d8dd04763acc9da05e79c34a_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The Invisible Work of Accessibility: How Blind Employees Manage Accessibility in Mixed-Ability Workplaces", "pdf_hash": "58103389e3be8c96d8dd04763acc9da05e79c34a", "year": 2015, "venue": "ASSETS", "alt_text": "Photographs of inaccessible office resources found during our study. There are eight items shown: a) accessible signage, b) decorations placed on a shelf, c) artwork, d) an informational poster, e) a snack vending machine, f) an office phone, g) an image from a magnetic resonance imaging scan viewer, and d) a computer without a properly configured screen reader.", "levels": [[-1], [-1]], "corpus_id": 15285651, "sentences": ["Photographs of inaccessible office resources found during our study.", "There are eight items shown: a) accessible signage, b) decorations placed on a shelf, c) artwork, d) an informational poster, e) a snack vending machine, f) an office phone, g) an image from a magnetic resonance imaging scan viewer, and d) a computer without a properly configured screen reader."], "caption": "Figure 2. Examples of the types of inaccessible resources participants identified through workplace walkthroughs and accessibility surveys. We explored four categories of inaccessible resources: environmental features (e.g., a refuge room, office decorations); print materials (e.g., photographs, posters); hardware and electronics (e.g., vending machine, desk phone); computer software (e.g., MRI scan viewer, screen reader without headphones).", "local_uri": ["58103389e3be8c96d8dd04763acc9da05e79c34a_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Invisible Work of Accessibility: How Blind Employees Manage Accessibility in Mixed-Ability Workplaces", "pdf_hash": "58103389e3be8c96d8dd04763acc9da05e79c34a", "year": 2015, "venue": "ASSETS", "alt_text": "A man is wearing two sets of headphones. The outer pair of over-the-ear headphones are connected to a PC screen reader, while a pair of earbuds underneath are connected to a smartphone.", "levels": null, "corpus_id": 15285651, "sentences": ["A man is wearing two sets of headphones.", "The outer pair of over-the-ear headphones are connected to a PC screen reader, while a pair of earbuds underneath are connected to a smartphone."], "caption": "Figure 3. Wallace has developed a creative solution for the need to simultaneously monitor multiple displays, a problem long addressed by HCI researchers for sighted users. He simply layers over-ear headphones atop his earbuds to receive updates from his computer and iPhone, respectively.", "local_uri": ["58103389e3be8c96d8dd04763acc9da05e79c34a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "An Explanation of Fitts' Law-like Performance in Gaze-Based Selection Tasks Using a Psychophysics Approach", "pdf_hash": "0d29f0ef7a18cfc6aae35381f163807c198c6ef6", "year": 2019, "venue": "CHI", "alt_text": "Three-part figure describing experimental setup and task. The top part shows an illustration of the experimental setup: on the left side, a participant sits at a desk with a keyboard in front of them. Their head rests on a chin rest and the participant is looking through a transparent mirror, above which the eye tracker is mounted. On the right side is a projection screen that the participant looks at, showing a gray background with a black circle. The middle part of the figure displays a 3 by 3 grid of potential starting locations (white crosses on black circles), with the grid centered on the middle of a noisy gray background image. Around the bottom-left starting location, dotted semicircles are drawn that indicate potential target positions during the experiment at 5 and 10 degrees distance from the starting location. The bottom part of the figure illustrates a time line: in each trial, a fixation cross was shown for 500-1250 ms, then the target was visible for 2000 ms, followed by a variable inter-stimulus interval of 1250-2000 ms.", "levels": [[1], [1], [1], [1], [1], [2, 1], [1]], "corpus_id": 140220677, "sentences": ["Three-part figure describing experimental setup and task.", "The top part shows an illustration of the experimental setup: on the left side, a participant sits at a desk with a keyboard in front of them.", "Their head rests on a chin rest and the participant is looking through a transparent mirror, above which the eye tracker is mounted.", "On the right side is a projection screen that the participant looks at, showing a gray background with a black circle.", "The middle part of the figure displays a 3 by 3 grid of potential starting locations (white crosses on black circles), with the grid centered on the middle of a noisy gray background image.", "Around the bottom-left starting location, dotted semicircles are drawn that indicate potential target positions during the experiment at 5 and 10 degrees distance from the starting location.", "The bottom part of the figure illustrates a time line: in each trial, a fixation cross was shown for 500-1250 ms, then the target was visible for 2000 ms, followed by a variable inter-stimulus interval of 1250-2000 ms."], "caption": "", "local_uri": ["0d29f0ef7a18cfc6aae35381f163807c198c6ef6_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Virtual Navigation for Blind People: Building Sequential Representations of the Real-World", "pdf_hash": "b073ce4197a929f926f4aa0601af3a2325f7568a", "year": 2017, "venue": "ASSETS", "alt_text": "It shows two images. In the first one, it depicts an hand suggesting movement by tilting down the phone. It shows the angle of such movement. The second image suggests the movement of rotating the phone to left.", "levels": null, "corpus_id": 1660920, "sentences": ["It shows two images.", "In the first one, it depicts an hand suggesting movement by tilting down the phone.", "It shows the angle of such movement.", "The second image suggests the movement of rotating the phone to left."], "caption": "", "local_uri": ["b073ce4197a929f926f4aa0601af3a2325f7568a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Navigation for Blind People: Building Sequential Representations of the Real-World", "pdf_hash": "b073ce4197a929f926f4aa0601af3a2325f7568a", "year": 2017, "venue": "ASSETS", "alt_text": "The correct representation of the two routes, using the lego blocks. The routes have 5 street blocks each, one additional intersection and 5 POIs along the route.", "levels": null, "corpus_id": 1660920, "sentences": ["The correct representation of the two routes, using the lego blocks.", "The routes have 5 street blocks each, one additional intersection and 5 POIs along the route."], "caption": "", "local_uri": ["b073ce4197a929f926f4aa0601af3a2325f7568a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Navigation for Blind People: Building Sequential Representations of the Real-World", "pdf_hash": "b073ce4197a929f926f4aa0601af3a2325f7568a", "year": 2017, "venue": "ASSETS", "alt_text": "The real-world route with the starting point, destination, POIs and turns. The sequence is the following. start, make a slight right, POI on the right, slight left.  2 POIs on the right, 1 POI on the left. Turn left. POI on the right. POI on the left. turn left, turn right. POI on the left., destination.", "levels": null, "corpus_id": 1660920, "sentences": ["The real-world route with the starting point, destination, POIs and turns.", "The sequence is the following. start, make a slight right, POI on the right, slight left.", "2 POIs on the right, 1 POI on the left.", "Turn left.", "POI on the right.", "POI on the left.", "turn left, turn right.", "POI on the left., destination."], "caption": "", "local_uri": ["b073ce4197a929f926f4aa0601af3a2325f7568a_Image_004.png"], "annotated": false, "compound": false}
{"title": "Virtual Navigation for Blind People: Building Sequential Representations of the Real-World", "pdf_hash": "b073ce4197a929f926f4aa0601af3a2325f7568a", "year": 2017, "venue": "ASSETS", "alt_text": "Summary:  Depicts all the metrics in the route reconstruction task for both VirtualLeap and VirtualWalk. Most results are similar between the two modalities, except the completion time, that is longer for Virtual Leap. Virtual Leap also shows more NumberElementErrors and FormElementErrors, but it is not statistically significant.    Data:     Completion Time    Virtual Leap Virtual Walk  min 238 178  q1 381 273  q2 445 370  q3 780 478  max 906 820     NumberElementsError  FormElementsError  PlacementErrors  POIsInBlockError  POISsOrderingError    Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0 0 0 0 0 1 0 0 0 0  q1 0 0 0 0 3 2 0 0 0 0  q2 0 0 0 0 3 3 2 1 1 1  q3 1 0 2 0 4 4 2 1 2 2  max 2 2 5 2 4 5 4 2 4 4     POIsDistanceError  POIsOnSideError    Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0.030555556 0.016666667 0 0  q1 0.088888889 0.064583333 0 0  q2 0.116666667 0.111111111 0.2 0.2  q3 0.169444444 0.216666667 0.2 0.4  max 0.326666667 0.275833333 0.67 0.75", "levels": [[1], [3], [2], [2]], "corpus_id": 1660920, "sentences": ["Summary:  Depicts all the metrics in the route reconstruction task for both VirtualLeap and VirtualWalk.", "Most results are similar between the two modalities, except the completion time, that is longer for Virtual Leap.", "Virtual Leap also shows more NumberElementErrors and FormElementErrors, but it is not statistically significant.", "Data:     Completion Time    Virtual Leap Virtual Walk  min 238 178  q1 381 273  q2 445 370  q3 780 478  max 906 820     NumberElementsError  FormElementsError  PlacementErrors  POIsInBlockError  POISsOrderingError    Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0 0 0 0 0 1 0 0 0 0  q1 0 0 0 0 3 2 0 0 0 0  q2 0 0 0 0 3 3 2 1 1 1  q3 1 0 2 0 4 4 2 1 2 2  max 2 2 5 2 4 5 4 2 4 4     POIsDistanceError  POIsOnSideError    Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0.030555556 0.016666667 0 0  q1 0.088888889 0.064583333 0 0  q2 0.116666667 0.111111111 0.2 0.2  q3 0.169444444 0.216666667 0.2 0.4  max 0.326666667 0.275833333 0.67 0.75"], "caption": "", "local_uri": ["b073ce4197a929f926f4aa0601af3a2325f7568a_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Virtual Navigation for Blind People: Building Sequential Representations of the Real-World", "pdf_hash": "b073ce4197a929f926f4aa0601af3a2325f7568a", "year": 2017, "venue": "ASSETS", "alt_text": "Summary:  It depicts all metriccs for the real-world exposure task for both Virtual Leap and Virtual Walk. Results are very similar between approaches, for all metrics.    Data:     Route Errors  Missing POIs    Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0 0 0 0  q1 0 0 0.75 1.5  q2 0 0 4 3  q3 0.75 1 5 3.5  max 3 2 5 7     Route Accuracy  POI Accuracy  Referred POIs Accuracy    Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0.65 0.55 0.3125 0.09375 0.833333333 0.5625  q1 0.7875 0.75 0.3125 0.34375 0.833333333 0.7  q2 0.9 0.8 0.4375 0.59375 0.854166667 0.833333333  q3 0.9375 0.975 0.796875 0.703125 0.89375 0.915178571  max 1 1 0.9375 0.9375 0.9375 0.95     Time (Real)  Time (Virtual)    Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 130 131 270 158  q1 163 153.5 315.75 406  q2 197.5 260 416 477  q3 268.75 352 485.5 482  max 334 397 499 507", "levels": [[1], [3], [2]], "corpus_id": 1660920, "sentences": ["Summary:  It depicts all metriccs for the real-world exposure task for both Virtual Leap and Virtual Walk.", "Results are very similar between approaches, for all metrics.", "Data:     Route Errors  Missing POIs    Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0 0 0 0  q1 0 0 0.75 1.5  q2 0 0 4 3  q3 0.75 1 5 3.5  max 3 2 5 7     Route Accuracy  POI Accuracy  Referred POIs Accuracy    Virtual Leap Virtual Walk Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 0.65 0.55 0.3125 0.09375 0.833333333 0.5625  q1 0.7875 0.75 0.3125 0.34375 0.833333333 0.7  q2 0.9 0.8 0.4375 0.59375 0.854166667 0.833333333  q3 0.9375 0.975 0.796875 0.703125 0.89375 0.915178571  max 1 1 0.9375 0.9375 0.9375 0.95     Time (Real)  Time (Virtual)    Virtual Leap Virtual Walk Virtual Leap Virtual Walk  min 130 131 270 158  q1 163 153.5 315.75 406  q2 197.5 260 416 477  q3 268.75 352 485.5 482  max 334 397 499 507"], "caption": "", "local_uri": ["b073ce4197a929f926f4aa0601af3a2325f7568a_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Safe walking technology for people with dementia: what do they want?", "pdf_hash": "2e980f90196496ed1d36f73ed02219d5d8f2a76a", "year": 2013, "venue": "ASSETS", "alt_text": "Figure illustrations 1. (a) Scenario representation with Lego figures on a map. (b) Idea generation props, with participant with dementia pointing to a prop.", "levels": null, "corpus_id": 16736421, "sentences": ["Figure illustrations 1. (a) Scenario representation with Lego figures on a map.", "(b) Idea generation props, with participant with dementia pointing to a prop."], "caption": "Figure 1. (a) Scenario representation. (b) Idea generation props.", "local_uri": ["2e980f90196496ed1d36f73ed02219d5d8f2a76a_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Safe walking technology for people with dementia: what do they want?", "pdf_hash": "2e980f90196496ed1d36f73ed02219d5d8f2a76a", "year": 2013, "venue": "ASSETS", "alt_text": "Mock-up resulting from co-design work,- including reminders, time, navigation, call home function and fall alarm.", "levels": null, "corpus_id": 16736421, "sentences": ["Mock-up resulting from co-design work,- including reminders, time, navigation, call home function and fall alarm."], "caption": "Figure 2. Mock-up resulting from co-design work.", "local_uri": ["2e980f90196496ed1d36f73ed02219d5d8f2a76a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Gesture-based Interaction for Individuals with Developmental Disabilities in India", "pdf_hash": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb", "year": 2016, "venue": "ASSETS", "alt_text": "The pictures drawn shows a dispaly on the right with a kinect placed right next to it. There is one bird's eye view of a participant figure with a head and shoulders facing the display and kinect, about 2 meters away. There are two moderators shown on the top (or left of the participant), facing the participant. The active area is a 1m x 1m square space about 1.5 meter away from the display.", "levels": null, "corpus_id": 18438345, "sentences": ["The pictures drawn shows a dispaly on the right with a kinect placed right next to it.", "There is one bird's eye view of a participant figure with a head and shoulders facing the display and kinect, about 2 meters away.", "There are two moderators shown on the top (or left of the participant), facing the participant.", "The active area is a 1m x 1m square space about 1.5 meter away from the display."], "caption": "Figure 2: Game Setup showing the interaction space", "local_uri": ["fc76954a4eca8adbdbec961d479c22e6d0d1a1cb_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Gesture-based Interaction for Individuals with Developmental Disabilities in India", "pdf_hash": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb", "year": 2016, "venue": "ASSETS", "alt_text": "This is an actual picture of a kirana store in India. The pictures shows two people (a women in a pink dress and a man in blue pants and bliue shirt) standing in front of the store facing the store. The store has wall-sized shelves with items placed on them. In front of the store is a waist table-top on top of shelves with items.", "levels": null, "corpus_id": 18438345, "sentences": ["This is an actual picture of a kirana store in India.", "The pictures shows two people (a women in a pink dress and a man in blue pants and bliue shirt) standing in front of the store facing the store.", "The store has wall-sized shelves with items placed on them.", "In front of the store is a waist table-top on top of shelves with items."], "caption": "Figure 5: A kirana store in India", "local_uri": ["fc76954a4eca8adbdbec961d479c22e6d0d1a1cb_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Gesture-based Interaction for Individuals with Developmental Disabilities in India", "pdf_hash": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb", "year": 2016, "venue": "ASSETS", "alt_text": "This is a screenshot of the virtual kirana store. On the left side is a thin light blue sidebar with two items and their price: biscuits for rupees 15 and choco for rupees 35. At the bottom of the side bar there is text that reads- bill rupees 50. In the center is the virtual store with two brown wooden shelves and a wooden table in front of them. The top shelf has eggs for rupees 20, milk for ruppes 20, sliced brown bread of rupees 15 and yogurt for rupees 15. The lower shelf only has a bag of chips for rupees 10 and butter for rupees 30. The table has two items on its left: one packet of chocolate biscuits for rupees 15 and one bar of chocolate for rupees 35.All the items, except eggs, are from brands commonly available in India. On the rigth side is a thin yellow sidebar which has Indian curreny / money placed on it. From the top, there is one 50 rupees note, 20 rupees note, 10 rupees note which is selected by an onscreen hand cursor so that this note is about 40% bigger in size than the other notes. Under the 10 rupees note is a 5 rupees note and finally a 5 rupee coin and 10 rupee coin. At the bottom of the side bar there is text that reads- total is rupees 100.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 18438345, "sentences": ["This is a screenshot of the virtual kirana store.", "On the left side is a thin light blue sidebar with two items and their price: biscuits for rupees 15 and choco for rupees 35.", "At the bottom of the side bar there is text that reads- bill rupees 50.", "In the center is the virtual store with two brown wooden shelves and a wooden table in front of them.", "The top shelf has eggs for rupees 20, milk for ruppes 20, sliced brown bread of rupees 15 and yogurt for rupees 15.", "The lower shelf only has a bag of chips for rupees 10 and butter for rupees 30.", "The table has two items on its left: one packet of chocolate biscuits for rupees 15 and one bar of chocolate for rupees 35.All the items, except eggs, are from brands commonly available in India.", "On the rigth side is a thin yellow sidebar which has Indian curreny / money placed on it.", "From the top, there is one 50 rupees note, 20 rupees note, 10 rupees note which is selected by an onscreen hand cursor so that this note is about 40% bigger in size than the other notes.", "Under the 10 rupees note is a 5 rupees note and finally a 5 rupee coin and 10 rupee coin.", "At the bottom of the side bar there is text that reads- total is rupees 100."], "caption": "Figure 6: Kirana- buying items on the table by paying using a 10 rupees note", "local_uri": ["fc76954a4eca8adbdbec961d479c22e6d0d1a1cb_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gesture-based Interaction for Individuals with Developmental Disabilities in India", "pdf_hash": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb", "year": 2016, "venue": "ASSETS", "alt_text": "The bar graph consists of the math scores from 0 to 10 on the y axis with participants on the x axis. For each participant, her pre and post test math score and post test math) is displayed. The solid blue bar is the pre test score and the lined blue bar is the post test math score. For P1 the pre test score is 9.5 and post test is 8, for P2, both pre and post scrores are 5, for P3 pre test score is 4.5 and post test is 6, for P4 pre test score is 8.5 and post is 7, for P5 and P6 pre test score is 6 and post 7, for P7 both pre test and post are 7, for P8 both pre and post test are 6, for P9 pre test is 6 and post 6.5, for P10 pre test is 3 and post is 8, for P11 pre test is 4.5 and post is 7.5, for P12 pre test is 3 and post is 6 , for P13 pre is 3 and post 5, for P14 pre is 8 and post is 7.5, for P15 pre is 8 and post is 6.5, for P16 pre is 5 and post is 6.5, for P17 pre is 8 and post is 9 and for P18, pre is 7.5 and post is 8.", "levels": [[1], [1], [1], [2]], "corpus_id": 18438345, "sentences": ["The bar graph consists of the math scores from 0 to 10 on the y axis with participants on the x axis.", "For each participant, her pre and post test math score and post test math) is displayed.", "The solid blue bar is the pre test score and the lined blue bar is the post test math score.", "For P1 the pre test score is 9.5 and post test is 8, for P2, both pre and post scrores are 5, for P3 pre test score is 4.5 and post test is 6, for P4 pre test score is 8.5 and post is 7, for P5 and P6 pre test score is 6 and post 7, for P7 both pre test and post are 7, for P8 both pre and post test are 6, for P9 pre test is 6 and post 6.5, for P10 pre test is 3 and post is 8, for P11 pre test is 4.5 and post is 7.5, for P12 pre test is 3 and post is 6 , for P13 pre is 3 and post 5, for P14 pre is 8 and post is 7.5, for P15 pre is 8 and post is 6.5, for P16 pre is 5 and post is 6.5, for P17 pre is 8 and post is 9 and for P18, pre is 7.5 and post is 8."], "caption": "Figure 7: Phase I mathematical test scores with improvements seen in Phase III for participants P1 \u2013 P18", "local_uri": ["fc76954a4eca8adbdbec961d479c22e6d0d1a1cb_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Predicting Cognitive Load in Future Code Puzzles", "pdf_hash": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb", "year": 2019, "venue": "CHI", "alt_text": "A screenshot of the Looking Glass Interface including A) a parts bin of statements and constructs to drag and drop, B) play correct and play mine buttons, C) the option to quit at any time, and D) a one question cognitive load survey displayed after a puzzle is completed.", "levels": null, "corpus_id": 140340306, "sentences": ["A screenshot of the Looking Glass Interface including A) a parts bin of statements and constructs to drag and drop, B) play correct and play mine buttons, C) the option to quit at any time, and D) a one question cognitive load survey displayed after a puzzle is completed."], "caption": "", "local_uri": ["7ce152841cc96d0d4c20e8a67c43692e1b742ddb_Image_005.png"], "annotated": false, "compound": false}
{"title": "Predicting Cognitive Load in Future Code Puzzles", "pdf_hash": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb", "year": 2019, "venue": "CHI", "alt_text": "A bar graph showing the feature importance by category. Germane was the most important category followed by Intrinsic, Participant's Characteristics, and Extraneous.", "levels": [[1], [2]], "corpus_id": 140340306, "sentences": ["A bar graph showing the feature importance by category.", "Germane was the most important category followed by Intrinsic, Participant's Characteristics, and Extraneous."], "caption": "", "local_uri": ["7ce152841cc96d0d4c20e8a67c43692e1b742ddb_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Predicting Cognitive Load in Future Code Puzzles", "pdf_hash": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb", "year": 2019, "venue": "CHI", "alt_text": "This graph displays the three most important features overall and for each model. Both transparency and height of the blocks represent the feature importance value.", "levels": [[1], [1]], "corpus_id": 140340306, "sentences": ["This graph displays the three most important features overall and for each model.", "Both transparency and height of the blocks represent the feature importance value."], "caption": "", "local_uri": ["7ce152841cc96d0d4c20e8a67c43692e1b742ddb_Image_012.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Power Struggles and Disciplined Designers - A Nexus Analytic Inquiry on Cross-Disciplinary Research and Design", "pdf_hash": "608b9995a1ddb2f3c89e57e17beb4906ee7857e5", "year": 2019, "venue": "CHI", "alt_text": "Usability guidelines, software engineering methods, technology developments, user insights, HCI methods, educational science theory", "levels": null, "corpus_id": 140465791, "sentences": ["Usability guidelines, software engineering methods, technology developments, user insights, HCI methods, educational science theory"], "caption": "", "local_uri": ["608b9995a1ddb2f3c89e57e17beb4906ee7857e5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "Line charts showing the transition in the number of completed or returned HITs (above) and active monthly users (bottom).", "levels": [[1]], "corpus_id": 5040507, "sentences": ["Line charts showing the transition in the number of completed or returned HITs (above) and active monthly users (bottom)."], "caption": "Figure 1. Line charts showing the transition in the number of active monthly users and HIT records.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "A histogram of the number of HIT performed by workers. More than 90% of workers completed less than 3000 HITs making the distribution long tail.", "levels": [[1], [3]], "corpus_id": 5040507, "sentences": ["A histogram of the number of HIT performed by workers.", "More than 90% of workers completed less than 3000 HITs making the distribution long tail."], "caption": "Figure 2. Histogram of performed HIT counts by workers.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "(A) A time line visualization of HIT intervals. (B) A demonstration of the tempral clustering method. (1) Each segment represents a HIT interval. (2) Segmetns of a same color represent HITs from the same group. (3) Two or more HIT intervals can overlap, showing workers perform multiple HITs concurrently. (4) Temporally close HIT intervals get clustered togeter, (5) whilte intevals with a gap larger than D does not get clustered.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 5040507, "sentences": ["(A) A time line visualization of HIT intervals. (", "B) A demonstration of the tempral clustering method. (", "1) Each segment represents a HIT interval. (", "2) Segmetns of a same color represent HITs from the same group. (", "3) Two or more HIT intervals can overlap, showing workers perform multiple HITs concurrently. (", "4) Temporally close HIT intervals get clustered togeter, (5) whilte intevals with a gap larger than D does not get clustered."], "caption": "Figure 3. Timeline visualization of HIT intervals and depiction of the temporal clustering method. The HIT interval data comes from one of the workers in our dataset.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "A line chart showing the change in the number of clusters formed as the paramter D (interval between HITs) changes. The change in the number of clusters diminishes after D=1min.", "levels": [[1], [3]], "corpus_id": 5040507, "sentences": ["A line chart showing the change in the number of clusters formed as the paramter D (interval between HITs) changes.", "The change in the number of clusters diminishes after D=1min."], "caption": "Figure 4. Line chart of the number of clusters formed. The change in the number becomes small after D=1min.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "Three kernel density estimation plots showing the distributions of per-HIT and per-cluster hourly wages. The blue and green lines indicate median and mean.", "levels": [[1], [1]], "corpus_id": 5040507, "sentences": ["Three kernel density estimation plots showing the distributions of per-HIT and per-cluster hourly wages.", "The blue and green lines indicate median and mean."], "caption": "Figure 5. Distributions of per-HIT and per-cluster hourly wages. The blue and green lines indicate median and mean.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "Three kernel density plots showing the distributions of per-worker hourly wages based on the interval-based and cluster-based methods.", "levels": [[1]], "corpus_id": 5040507, "sentences": ["Three kernel density plots showing the distributions of per-worker hourly wages based on the interval-based and cluster-based methods."], "caption": "Figure 6. Distributions of per-worker hourly wages based on the interval-based and cluster-based methods. The blue and green lines indicate median and mean.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "A scatter plot showing a relationship between HIT reward and hourly wage. A black dashed line represents the model that is fit to the data using an ordinary least square regression.", "levels": [[1], [1]], "corpus_id": 5040507, "sentences": ["A scatter plot showing a relationship between HIT reward and hourly wage.", "A black dashed line represents the model that is fit to the data using an ordinary least square regression."], "caption": "Figure 7. The scatter plot showing the relationship between the transformed reward and hourly wage. The line represents the model that we fit with ordinary linear regression.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "Points above the dashed line represent requesters who are paying above the minimum wage.", "levels": [[1]], "corpus_id": 5040507, "sentences": ["Points above the dashed line represent requesters who are paying above the minimum wage."], "caption": "Figure 9. A scatter plot of per-requester HIT count vs. per- requester hourly payment.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "Relatively high and less varying payment indicates that requesters who post high reward HITs treat workers fairly.", "levels": [[3]], "corpus_id": 5040507, "sentences": ["Relatively high and less varying payment indicates that requesters who post high reward HITs treat workers fairly."], "caption": "Figure 10. A scatter plot of median HIT reward paid by requesters vs. per-requester hourly payment.", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "pdf_hash": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "year": 2018, "venue": "CHI", "alt_text": "(1) High paying \"video evalution\" HITs push the hourly wage distribution up. (2) The CC distribution is skewed toward left by low hourly wages of \"transcribe data\" and \"transcribe image\" tasks.", "levels": [[4], [3]], "corpus_id": 5040507, "sentences": ["(1) High paying \"video evalution\" HITs push the hourly wage distribution up. (", "2) The CC distribution is skewed toward left by low hourly wages of \"transcribe data\" and \"transcribe image\" tasks."], "caption": "Figure 12. (a) Hourly wage distributions of seven HIT categories provided by Gadiraju et al. [25] (with an additional category", "local_uri": ["4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3, 4], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Basic senior personas: a representative design tool covering the spectrum of European older adults", "pdf_hash": "d42066767d9f97c2ee2e61fd45b4312e2fe010a3", "year": 2012, "venue": "ASSETS '12", "alt_text": "Example: final basic senior persona (age: 80 +, sex: male, region: northern Europe)", "levels": null, "corpus_id": 9319800, "sentences": ["Example: final basic senior persona (age: 80 +, sex: male, region: northern Europe)"], "caption": "Figure 6. Example: final basic senior persona (age: 80 +, sex: male, region: northern Europe)", "local_uri": ["d42066767d9f97c2ee2e61fd45b4312e2fe010a3_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models", "pdf_hash": "71c0d8ef7da98922939f7718c4caff79492c9db5", "year": 2018, "venue": "CHI", "alt_text": "An image of a pen holder from thingiverse. The pens are placed in cylindrical holes.", "levels": null, "corpus_id": 5040980, "sentences": ["An image of a pen holder from thingiverse.", "The pens are placed in cylindrical holes."], "caption": "", "local_uri": ["71c0d8ef7da98922939f7718c4caff79492c9db5_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models", "pdf_hash": "71c0d8ef7da98922939f7718c4caff79492c9db5", "year": 2018, "venue": "CHI", "alt_text": "An image of an iPhone case from Thingiverse. The case is molded around a model of an iPhone.", "levels": null, "corpus_id": 5040980, "sentences": ["An image of an iPhone case from Thingiverse.", "The case is molded around a model of an iPhone."], "caption": "", "local_uri": ["71c0d8ef7da98922939f7718c4caff79492c9db5_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models", "pdf_hash": "71c0d8ef7da98922939f7718c4caff79492c9db5", "year": 2018, "venue": "CHI", "alt_text": "An image of an earbud holder from Thingiverse. The holders is made with a path for the cable to wrap around.", "levels": null, "corpus_id": 5040980, "sentences": ["An image of an earbud holder from Thingiverse.", "The holders is made with a path for the cable to wrap around."], "caption": "", "local_uri": ["71c0d8ef7da98922939f7718c4caff79492c9db5_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models", "pdf_hash": "71c0d8ef7da98922939f7718c4caff79492c9db5", "year": 2018, "venue": "CHI", "alt_text": "An image of a pegboard tool organizer from Thingiverse. The organizer connects many similar screw drivers.", "levels": null, "corpus_id": 5040980, "sentences": ["An image of a pegboard tool organizer from Thingiverse.", "The organizer connects many similar screw drivers."], "caption": "", "local_uri": ["71c0d8ef7da98922939f7718c4caff79492c9db5_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models", "pdf_hash": "71c0d8ef7da98922939f7718c4caff79492c9db5", "year": 2018, "venue": "CHI", "alt_text": "An stl model from the original thingiverse design that does not match the image  in Figure 4D.", "levels": null, "corpus_id": 5040980, "sentences": ["An stl model from the original thingiverse design that does not match the image  in Figure 4D."], "caption": "", "local_uri": ["71c0d8ef7da98922939f7718c4caff79492c9db5_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models", "pdf_hash": "71c0d8ef7da98922939f7718c4caff79492c9db5", "year": 2018, "venue": "CHI", "alt_text": "Image of the printed and assembled PARTs CD lamp. Appears in same position of Figure 4E.", "levels": null, "corpus_id": 5040980, "sentences": ["Image of the printed and assembled PARTs CD lamp.", "Appears in same position of Figure 4E."], "caption": "Figure 4. Functional geometry contextualizes the modeled components among existing objects.", "local_uri": ["71c0d8ef7da98922939f7718c4caff79492c9db5_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "Who Should Have Access to my Pointing Data?: Privacy Tradeoffs of Adaptive Assistive Technologies", "pdf_hash": "ab98b531389db9db660a3538f68e45c19895d7d4", "year": 2018, "venue": "ASSETS", "alt_text": "This image shows a website with a series of links and a bubble cursor hovering over the links.", "levels": [[-1]], "corpus_id": 52942894, "sentences": ["This image shows a website with a series of links and a bubble cursor hovering over the links."], "caption": "Figure 1. The Adaptive Bubble Cursor\u2019s selection area grows in response to an increase in detected pointing errors with the larger size making it easier to select objects.", "local_uri": ["ab98b531389db9db660a3538f68e45c19895d7d4_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Who Should Have Access to my Pointing Data?: Privacy Tradeoffs of Adaptive Assistive Technologies", "pdf_hash": "ab98b531389db9db660a3538f68e45c19895d7d4", "year": 2018, "venue": "ASSETS", "alt_text": "This image shows two visualizations: a timeseries visualization showing when pointing errors occured, and a piechart visualization showing which websites had the most pointing errors.", "levels": [[1]], "corpus_id": 52942894, "sentences": ["This image shows two visualizations: a timeseries visualization showing when pointing errors occured, and a piechart visualization showing which websites had the most pointing errors."], "caption": "Figure 2. Pointing History Visualizations provide detected pointing error information to the user in two graphs: the Error Type Graph (top) shows the frequency of different kinds of error over time and the Website Graph (bottom) shows the most common websites where pointing errors have occurred.", "local_uri": ["ab98b531389db9db660a3538f68e45c19895d7d4_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Relating computer tasks to existing knowledge to improve accessibility for older adults", "pdf_hash": "d07cc441fe600f1b4f6590cfa82fcb979f23fe4d", "year": 2010, "venue": "ASSETS '10", "alt_text": "Mean number of times help was required, with standard deviations for each interface configuration", "levels": [[1]], "corpus_id": 40336143, "sentences": ["Mean number of times help was required, with standard deviations for each interface configuration"], "caption": "Figure 6: Mean number of times help was required, with standard deviations for each interface configuration", "local_uri": ["d07cc441fe600f1b4f6590cfa82fcb979f23fe4d_Image_006.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Relating computer tasks to existing knowledge to improve accessibility for older adults", "pdf_hash": "d07cc441fe600f1b4f6590cfa82fcb979f23fe4d", "year": 2010, "venue": "ASSETS '10", "alt_text": "Mean rank for each interface configuration (4 = most preferred, 1 = least preferred)", "levels": [[1]], "corpus_id": 40336143, "sentences": ["Mean rank for each interface configuration (4 = most preferred, 1 = least preferred)"], "caption": "Figure 9: Mean rank for each interface configuration (4 = most preferred, 1 = least preferred)", "local_uri": ["d07cc441fe600f1b4f6590cfa82fcb979f23fe4d_Image_009.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Experience design theatre: exploring the role of live theatre in scaffolding design dialogues", "pdf_hash": "347534b9133cf5996ab388fa8943da3822bff00a", "year": 2014, "venue": "CHI", "alt_text": "Photographs from the Experience Design Theatre workshops, showing the actors interacting with each other on stage and in front of participants.", "levels": [[-1]], "corpus_id": 2392576, "sentences": ["Photographs from the Experience Design Theatre workshops, showing the actors interacting with each other on stage and in front of participants."], "caption": "Figure 1. EDT performance workshops.", "local_uri": ["347534b9133cf5996ab388fa8943da3822bff00a_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Conversing with children: cartoon and video people elicit similar conversational behaviors", "pdf_hash": "433835b5b83bc38f6d7253d7d70d5a496374738b", "year": 2014, "venue": "CHI", "alt_text": "Example frames of confederate video tracking and corresponding character being animated with different levels of facial motion.  In this frame, the exaggeration/damping is most clearly shown in the mouth.", "levels": [[-1], [-1]], "corpus_id": 207210265, "sentences": ["Example frames of confederate video tracking and corresponding character being animated with different levels of facial motion.", "In this frame, the exaggeration/damping is most clearly shown in the mouth."], "caption": "Video with AAM tracking           (b) Normal cartoon                   (c) Damped cartoon               (d) Exaggerated cartoonFigure 1. Example frames of confederate video tracking and corresponding character being animated with different levels of facial motion. In this frame, the exaggeration/damping is most clearly shown in the mouth.that were counterbalanced so that conversation topics were not confounded with participants\u2019 interaction partner. For both tasks, the interactive partner was an adult confederate who appeared on-screen. In one task, she was shown through video; in the other task, she was shown as a cartoon char\u00ad acter. The confederate\u2019s cartoon character was human and was customized to her appearance. The confederate was blind to her appearance on the participant\u2019s screen. Also, we cre\u00ad ated questionnaires to determine the correspondence between subjective measures and participant behavior. Through this combination of control measures, we examined the precise effects of character appearance and motion on children\u2019s ex\u00ad periences. Our results indicate that children are resilient to the appearance and motion of their conversation partners, re\u00ad gardless of their personal preferences. Therefore, it should be possible to design conversational agents that elicit natural behavior from children.METHODIn this section, we explain the technique we used to animate a character with our confederate\u2019s facial motion in realtime. We also describe our equipment, study materials, participants, procedure, and measures.ApparatusOur goal is to create believable interactive animated charac\u00ad ters, with motion that mimics the pacing, style, and facial gestures of humans; to that end, we opted to have two con\u00ad federates \u201cpuppet\u201d the characters. To ensure that the confed\u00ad erates were blind to the study condition, they were animated using a markerless, computer vision method for face track\u00ad ing: active appearance models (AAMs) [13, 14, 27]. This method tracks a person\u2019s face in realtime, permitting it to be mapped onto a character\u2019s face without noticeable delay. We designed a desktop-like audiovisual telecommunications sys\u00ad tem so that research participants could interact with confed\u00ad erates who both appeared as themselves through video and as animated characters while using natural eye contact and speech.Active Appearance ModelsAAMs mathematically model the face shape and appearance of people and characters. To animate a character using human motion, an AAM can be customized to an individual, and a corresponding AAM can be created for the character. To cre\u00ad ate a custom AAM, images of an individual\u2019s face in differentposes are manually labeled so that a mesh of 79 vertices \ufb01t to the individual\u2019s face. Once the face and appearance space have been learned from the training images, the individual\u2019s face can be tracked from new video in realtime. Animating a character requires a mapping from the individual\u2019s AAM to the character\u2019s AAM. When the individual moves his/her face, the 79 mesh vertices change position. The change in position is mapped to the character\u2019s mesh, and then the char\u00ad acter\u2019s appearance is warped to illustrate movement (see Fig\u00ad ure 1). We purposefully created characters that resembled the confederates so that the remapped motion would be as accu\u00ad rate as possible. For further information on the creation and use of AAMs, please see [13, 14, 27].\u2212Given that each vertex changes position during movement, we followed previous procedures [5, 35] to multiply those changes in position by speci\ufb01c scale factors, thus exagger\u00ad ating or damping the spatial movements across all features of the face. We selected damped and exaggerated scale factors based on a previous perceptual study in which the adult threshold of facial motion level sensitivity was deter\u00ad mined [23]. We selected equally perceptible levels of damp\u00ad ing ( 20%) and exaggeration (+25%). For example images of exaggerated and damped character motion, see Figure 1.We used 2D AAMs for this research. Therefore, our charac\u00ad ters always faced forward. We added rigid points around the tops of the characters\u2019 heads and damped the face border and nose points by 50% to ensure that the character would not warp excessively if the confederate turned her head. Body motion was not tracked, so the torsos of the characters moved rigidly with respect to a pivot located at their mouths. The confederates practiced extensively with the characters prior to the experiment to ensure that they did not generate move\u00ad ments that appeared unnatural or otherwise distracting when presented on the characters.Telecommunications SystemOur audiovisual telecommunications system is diagrammed in Figure 2. It was designed to maximize natural interac\u00ad tions: both people appear life size and can make eye contact as they would in person. Two setups were positioned in sep\u00ad arate rooms with a control room in between. All video and audio data from each setup was relayed through the control room for video and audio processing before being presented to the other setup. Each setup consisted of a black box hous\u00adsualgetingRealtime Audiovi Processing:tracking and retarCamerasBeam SplitterMonitorsConfederate                                                                     Microphones                                    ParticipantFigure 2. Diagram of the telecommunications system.ing a monitor, camera, microphone, and beam splitter, and the box was positioned on a height-adjustable table to ensure that the camera was at eye level for the user. A speaker was also placed within the participant\u2019s setup, but headphones were used on the confederate\u2019s setup to avoid auditory feedback. We used a beam splitter made of re\ufb02ective material between the user and the camera so that the camera was hidden di\u00ad rectly in front of the user, allowing for eye contact between users, often an impossibility in audiovisual telecommunica\u00ad tions. The monitor was placed above the beam splitter in or\u00ad der to project visual information directly in front of the user. A shotgun microphone was mounted below the beam splitter to capture audio.A computer attached to each setup controlled the presentation of visual information. When animating a character, the partic\u00ad ipant\u2019s computer tracked the confederate, retargeted the mo\u00ad tion to the animated character, and displayed the character on the participant\u2019s monitor. If the system was displaying a video feed, the computer was used to add a small amount of delay to the presentation of the video in order to replicate the delay induced by tracking and animating the confederate character. A sound mixer in the control room was used to add delay to the audio and ensure that the audio and video/animation re\u00ad mained in sync. Our measurements indicated that the delay inherent in our system is 100 ms for video and 166 ms for animation. Because the confederate always saw video of the participant, the delay for the confederate was 100 ms; how\u00ad ever, because the participant saw both video and animated conditions we kept the participant\u2019s delay at 166 ms. Previ\u00ad ous research with a similar system validated that these delays have a negligible effect on conversation between adults [34].StorybookIn order to introduce the children to the task, we created sto\u00ad rybooks for each of the confederates and possible task or\u00ad ders. Each storybook starred one of two confederates and de\u00ad scribed how she enjoyed playing pretend, sometimes as a car\u00ad toon character. She particularly enjoyed pretending to own a bakery. Depending on the order of tasks assigned to a particu\u00ad lar participant, the storybook then described one of two tasks: designing a cake or an ice cream sundae. For the cake task, the children were told that the confederate had already cre\u00ad ated three cakes and needed the child\u2019s help to make a fourth, selecting the \ufb02avor of cake, the frosting, and a topping from alist. In the ice cream task, the children were asked to help de\u00ad sign a fourth sundae by selecting the ice cream \ufb02avor, a sauce, and a topping from a list. After the child completed the \ufb01rst task, the experimenter read the second part of the story that described the other task. Participants were randomly assigned a task order and confederate.Visual AidsWe used felt fabric to create facsimiles of all of the food com\u00ad ponents described in tasks so that the participants could see their creations at the end of each task and have a more con\u00ad crete representation of the items for imagination and recall.QuestionnairesTo obtain participant feedback, we used a modi\ufb01ed smiley\u00ad ometer with written labels. Smileyometers have been used frequently with children as they are understandable and re\u00ad liable; however, we included written labels because they are better for older children [6, 30]. The traditional smileyometer includes a neutral midpoint; however we removed the mid\u00ad point as prior work [7] found that four response options and no neutral midpoint obtained the most reliable responses from children. Our participants were all familiar with the various types of smileys as they were used in the other experiments that participants completed on the same day. Unfortunately, eliciting truthful responses from 4- to 5-year-olds can be very dif\ufb01cult as they are susceptible to satis\ufb01cing, and they will of\u00ad ten select the most positive response [38]. When presenting the rating scale, the experimenter verbally stated each option while pointing to the corresponding smiley. The experimenter also verbally con\ufb01rmed each of the participants\u2019 responses. After each task, the experimenter verbally asked the children four questions and offered possible answers while showing the questions and a rating scale:How much did you like talking to [name] just now? (Really liked, kind of liked, didn\u2019t really like, did not like)How much fun did you have talking to [name] just now? (Lots of fun, kind of fun, kind of bored, bored)How nice was [name] to you just now? (Very nice, kind of nice, kind of mean, mean)How much did you like talking to [name] about [topic]? (Really liked, kind of liked, didn\u2019t really like, did not like)After both tasks, the children answered an additional three questions. Again, participants were verbally read the ques\u00ad tions and possible responses as well as shown images of the responses. Participants were given response options in a ran\u00ad dom order:Did you like talking to [name] more when she looked like a real person or when she looked like a cartoon character? (photo of confederate and cartoon character)30BoysGirlsCount201004\u221256\u22128                          9\u221210Age Group (years)If you could speak to [name] again, would you want to see her as a real person or as a cartoon character? (photo of confederate and cartoon character)Did you like talking about ice cream or cake more? (image of ice cream sundae visual aid and cake visual aid)To measure participants\u2019 extroversion, we asked parents to answer a short, six-question survey. The six items were se\u00ad lected to represent the six facets of extroversion, as de\ufb01ned in the NEO-PI-R [15], a well-known and validated person\u00ad ality measure for adults. The items were selected from the M5-PS-35 [19, 32], a measure created and validated to assess preschool children\u2019s personality.ParticipantsChildren between the ages of four and ten years were re\u00ad cruited to take part in a series of short, unrelated experiments, including this study, that lasted a total of approximately 90 minutes. In total, 69 children (mean age = 7.17 years, stan\u00ad dard deviation = 2.02 years, 36 boys and 33 girls) participated successfully in this paradigm. Five additional children were excluded from analyses due to technical or behavioral issues. See Figure 3 for participant breakdown. Participants were recruited using email lists and advertisements in local gath\u00ad ering places, and they were compensated for their time. The research was approved by our Institutional Review Board.ProcedureUpon arrival at the experiment location, each child and his or her parent/guardian was met by one of the experimenter team. Parents/guardians completed the extroversion questionnaire in a separate room. Most parents remained in the separate room while their children completed the study; however, six\u00ad teen parents accompanied their children to the study room. They sat 12 feet to their children\u2019s left with the experimenter in between. Parents could not see the telecommunications screen, but they could hear the conversation. Parents of chil\u00ad dren who completed the study successfully sat silently during the study. The child was allowed to select one of many stick\u00ad ers to help the experimenter decorate the apparatus, in order to let him or her warm up and become accustomed to the ap\u00ad paratus and environment. Then, the experimenter seated the child facing the apparatus with the curtain still down and read the storybook to prepare the child to play a game of pretend with the confederate either as a video or an animated charac\u00ad ter. Upon completion of the \ufb01rst half of the storybook, the ex\u00ad perimenter asked if the child was ready and then whether the confederate was ready. After hearing agreement from both, she raised the curtain so that the participant and confederate were able to see each other.Figure 3. Participants by gender and age group.Before starting the \ufb01rst task, the confederate engaged the par\u00ad ticipant in unstructured small talk, typically about the child\u2019s summer activities, age, and favorite school subjects, in or\u00ad der to make the child comfortable with the confederate and the apparatus. When the confederate believed that the child was comfortable, she began the \ufb01rst task. While the child made selections of food, the experimenter assembled the \ufb01nal product. When the child made his/her second selection, the confederate challenged the choice and offered her own sug\u00ad gestion. Upon completion of the \ufb01rst task, the experimenter closed the curtain so that the child could no longer see the confederate, showed the child the design, and completed the questionnaire with the child. Then, the experimenter veri\u00ad \ufb01ed that the confederate was ready to begin the next task and raised the curtain. For the second task, there was no small talk; the confederate asked the child if he or she was ready and then began the task. The confederate challenged the child on his/her second selection just as she had done in the \ufb01rst con\u00ad versation. Again, the experimenter created the food design based on the child\u2019s selection, closed the curtain at the end of the task, showed the child the creation, and completed the questionnaire with the child. Finally, they completed the last three questions comparing both tasks and characters.MeasuresIndependent variables were divided into experimental and participant variables. Experimental variables included ap\u00ad pearance and motion manipulations. Participant variables in\u00ad cluded participant age, gender, and extroversion score. De\u00ad pendent variables were split into conversation, gaze, gesture, and self-report measures.We annotated the video recordings for child speech, gaze, and gesture using ELAN [28, 33], open-source software for anno\u00ad tating video and audio recordings. Each annotation has a start time, end time, and label or transcription. For each measure that involved annotated data, we had a primary annotator who annotated all of the data and a secondary annotator who an\u00ad notated one third of each child\u2019s data. To evaluate interrater reliability, we calculated the percentage of aligned annota\u00ad tions and then calculated Cohen\u2019s Kappa (\u03ba) for the aligned annotations. Percent alignment and \u03ba are given below.Experimental variablesOur main experimental variables included confederate ap\u00ad pearance and facial motion level. Other experimental vari\u00ad ables that we controlled in our analyses included conversa\u00ad tion topic, conversation order, and confederate. The within\u00adsubjects variables were confederate appearance with two lev\u00ad els (cartoon, video), conversation topic with two levels (cake, sundae), conversation order with two levels (\ufb01rst, second), and confederate with two levels (A, B). Facial motion level was a between-subjects variable with three levels (damped, unaltered, exaggerated). The experimental conditions were counterbalanced across participants.Participant variablesParticipant variables included gender, age group, and extro\u00ad version score. Participants were divided into three age groups (four to \ufb01ve years old, six to eight years old, nine to ten years old) based on theories of cognitive development [25, 37]. Par\u00ad ents completed a short questionnaire to assess their child\u2019s level of extroversion. The parents\u2019 responses were averaged across the six questionnaire items to create a single extrover\u00ad sion score (Cronbach\u2019s \u03b1 = .6712).Conversation measuresConversation measures included conversation length, number of utterances, number of words, and confederate in\ufb02uence. The primary annotator transcribed all participants\u2019 speech, and the secondary annotator marked the times when partic\u00ad ipants spoke. Pauses between utterances had to be at least 500 ms long. The annotators had 82% alignment on their annotations; we did not calculate Cohen\u2019s \u03ba because the sec\u00ad ondary annotator did not transcribe speech. From the tran\u00ad scriptions, we calculated the number of utterances and num\u00ad ber of words that each participant used. Conversation length was measured between when the experimenter asked the con\u00ad federate if she was ready and when the curtain covered the screen. Because the \ufb01rst conversation included unstructured small talk, the start of the \ufb01rst task occurred when the confed\u00ad erate asked if the child was ready to begin the task. For con\u00ad federate in\ufb02uence, we looked at when participants changed their selection based on the confederate\u2019s challenge. If the participant changed his/her original selection to the confeder\u00ad ate\u2019s selection, we scored the confederate as in\ufb02uential. If the participant stuck with his/her original selection, we scored the confederate as not in\ufb02uential.Gaze measuresGaze measures included the percentage of time participants spent looking at the screen, percent on-screen, and the av\u00ad erage length of each on-screen gaze segment, average gaze length. While percent on-screen gives a rough estimate of how much of the conversation participants watched the screen, the average gaze length gives an estimate of how long participants sustained their gaze at the screen. Annotators marked when participants were looking on- and off-screen. The annotators had 86% alignment of annotations, and they agreed on the annotation labels with perfect reliability (Co\u00ad hen\u2019s \u03ba = 1).Gesture measuresGesture measures included the total number of gestures and the number of nods, shakes, and shrugs each participant used. Gestures were only considered if they were communicative. For example, if a participant moved his/her head up-and\u00ad down to indicate \u201cyes,\u201d the motion was counted as a nod; however, if the participant was simply bouncing in his/herchair, the head movement was not considered a nod. Annota\u00ad tors marked and labeled the beginnings and ends of gestures. The two annotators had 83% alignment, and they agreed on the labels with perfect reliability (Cohen\u2019s \u03ba = 1).Self-report measuresSelf-report measures included conversation score, appear\u00ad ance preference, and topic preference. Participants were asked four questions after each task and another three ques\u00ad tions after completion of both tasks, as described in the Ques\u00ad tionnaires section of this paper. The \ufb01rst four questions asked participants to rate their conversations. The last three ques\u00ad tions asked participants to compare their conversations. The ratings from the \ufb01rst four questions were combined to cre\u00ad ate a conversation score with good reliability (Cronbach\u2019s \u03b1 = .6879). The two questions, asking participants to select between the cartoon and video confederate, were combined to create the measure of appearance preference with good re\u00ad liability (Cronbach\u2019s \u03b1 = .7178). The last question was used as an indication of participants\u2019 topic preference.RESULTSWe conducted several ANalyses Of VAriance (ANOVAs) to investigate possible effects from the independent variables. Due to a lack of variability in extroversion scores across par\u00ad ticipants and the fact that no relationship was found between extroversion score and the dependent measures, we excluded extroversion score from further analyses. We also excluded number of shakes and shrugs from our analyses, as the me\u00ad dian number of times these gestures occurred during conver\u00ad sation were 1 and 0, respectively. The number of nods and the total number of gestures were kept in the analyses. Although participants had strong preferences for appearance and topic, they did not alter their behavior to re\ufb02ect these preferences. Interestingly, confederate appearance only affected the num\u00ad ber of words children used.Preliminary analysisTo determine which control variables to include in our main analysis, we \ufb01rst conducted a repeated measures ANOVA with conversation topic and conversation order as within- subjects variables and confederate as a between-subjects vari\u00ad able. Only conversation length was affected by these vari\u00ad ables. Confederate B had signi\ufb01cantly longer conversations than Confederate A, F (1, 65) = 43.91, p < .0001. Be\u00ad cause the conversations were semi-structured and there were no signi\ufb01cant effects of confederate on the number of ut\u00ad terances or words, the difference in conversation length was likely due to a difference in the confederates\u2019 rates of speech. Conversation length was also affected by conversation order, F (1, 65) = 14.51, p = .0003; however, conversation order and confederate did not create a signi\ufb01cant interaction.To understand the difference in conversation length between the \ufb01rst and second conversation, we look at the signi\ufb01cant interaction of conversation order and topic, F (1, 65) = 5.35, p = .0239. Participants\u2019 second conversation was only longer than their \ufb01rst conversation if the second topic was ice cream sundae. From our analysis of self-report measures, we know that participants preferred the sundae topic to cake. We alsoNumber of Utterances252015Girls            Boys Gender(a)100Number of Words806040200Girls            Boys Gender5Conversation Score432GirlsGenderBoys100Number of Words806040200Cartoon         Video Confederate Appearance", "local_uri": ["433835b5b83bc38f6d7253d7d70d5a496374738b_Image_001.jpg", "433835b5b83bc38f6d7253d7d70d5a496374738b_Image_002.jpg", "433835b5b83bc38f6d7253d7d70d5a496374738b_Image_003.jpg", "433835b5b83bc38f6d7253d7d70d5a496374738b_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "You Watch, You Give, and You Engage: A Study of Live Streaming Practices in China", "pdf_hash": "1d8ad5af9bbcbf4266aa6d7c049084a4e6584373", "year": 2018, "venue": "CHI", "alt_text": "(a) Age distribution: 18 respondents were under 20, 70 between 20 to 25, 186 between 26 to 30, 137 between 31 to 35, 113 over 36, and 3 unkown. (b) Highest level of education: 16 respondents had high school degree, 150 had college degree, 269 had bachelor degree, and 92 had master or above. (c) Location distribution: Respondents were from most provinces of China, and a lot are from tier 1 cities. (d) Experience using live streaming: 52 had used less than 3 months, 106 had used 4 to 6 months, 140 had used 7 to 12 months, 169 had used 1 to 2 years, and 60 had used more than 2 years.", "levels": [[2], [2], [3], [2]], "corpus_id": 3953438, "sentences": ["(a) Age distribution: 18 respondents were under 20, 70 between 20 to 25, 186 between 26 to 30, 137 between 31 to 35, 113 over 36, and 3 unkown. (", "b) Highest level of education: 16 respondents had high school degree, 150 had college degree, 269 had bachelor degree, and 92 had master or above. (", "c) Location distribution: Respondents were from most provinces of China, and a lot are from tier 1 cities. (", "d) Experience using live streaming: 52 had used less than 3 months, 106 had used 4 to 6 months, 140 had used 7 to 12 months, 169 had used 1 to 2 years, and 60 had used more than 2 years."], "caption": "Figure 1. Distribution of age, highest level of education, location, and experience with live streaming.", "local_uri": ["1d8ad5af9bbcbf4266aa6d7c049084a4e6584373_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "You Watch, You Give, and You Engage: A Study of Live Streaming Practices in China", "pdf_hash": "1d8ad5af9bbcbf4266aa6d7c049084a4e6584373", "year": 2018, "venue": "CHI", "alt_text": "1) Being a guest: About 360 respondents reported using this at least once a month 2) Sending free gift: About 500 respondents reported using this at least once a month. It is one of the most frequently used features 3) Sending Paid gift: About 340 respondents reported using this at least once a month 4) Commenting for self expression: About 490 respondents reported using this at least once a month. It is one of the most frequently used features 5) Commenting for communication: About 480 respondents reported using this at least once a month. It is one of the most frequently used features 6) Buying goods from streamer: About 320 respondents reported using this at least once a month. 7) Participate in streamer's activities: About 430 respondents reported  using this at least once a month. 8) Sharing a stream:  About 440 respondents reported  using this at least once a month. 9) Joining a fan group: About 360 respondents reported  using this at least once a month. 10) Following a streamer: About 490 respondents reported  using this at least once a month. It is one of the most frequently used features. 11) Becoming a moderator of a streamer: About 310 respondents reported  using this at least once a month.  12) Watching archived video: About 410 respondents reported  using this at least once a month.", "levels": null, "corpus_id": 3953438, "sentences": ["1) Being a guest: About 360 respondents reported using this at least once a month 2) Sending free gift: About 500 respondents reported using this at least once a month. It is one of the most frequently used features 3) Sending Paid gift: About 340 respondents reported using this at least once a month 4) Commenting for self expression: About 490 respondents reported using this at least once a month. It is one of the most frequently used features 5) Commenting for communication: About 480 respondents reported using this at least once a month.", "It is one of the most frequently used features 6) Buying goods from streamer: About 320 respondents reported using this at least once a month.", "7) Participate in streamer's activities: About 430 respondents reported  using this at least once a month. 8) Sharing a stream:  About 440 respondents reported  using this at least once a month. 9) Joining a fan group: About 360 respondents reported  using this at least once a month.", "10) Following a streamer: About 490 respondents reported  using this at least once a month.", "It is one of the most frequently used features.", "11) Becoming a moderator of a streamer: About 310 respondents reported  using this at least once a month.", "12) Watching archived video: About 410 respondents reported  using this at least once a month."], "caption": "Figure 3. Respondents\u2019 reported use of live streaming features in the month preceding the survey.", "local_uri": ["1d8ad5af9bbcbf4266aa6d7c049084a4e6584373_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Getting Smartphones to Talkback: Understanding the Smartphone Adoption Process of Blind Users", "pdf_hash": "2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c", "year": 2015, "venue": "ASSETS", "alt_text": "Samsung S3 Mini with capacitive buttons at the bottom of the device. Where there isn't a physical border or cue between the touchscreen and the buttons or even non interactive areas of the display.", "levels": null, "corpus_id": 8279953, "sentences": ["Samsung S3 Mini with capacitive buttons at the bottom of the device. Where there isn't a physical border or cue between the touchscreen and the buttons or even non interactive areas of the display."], "caption": "Figure 1. Samsung S3 Mini with capacitive buttons at the bottom of the device.", "local_uri": ["2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Getting Smartphones to Talkback: Understanding the Smartphone Adoption Process of Blind Users", "pdf_hash": "2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c", "year": 2015, "venue": "ASSETS", "alt_text": "Graph with the number cumulative number of unique apps used per week. Where P4 is shown to be 40 unique apps used ahead of all the other participants by the end of the eight week.", "levels": [[1], [2]], "corpus_id": 8279953, "sentences": ["Graph with the number cumulative number of unique apps used per week.", "Where P4 is shown to be 40 unique apps used ahead of all the other participants by the end of the eight week."], "caption": "", "local_uri": ["2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Getting Smartphones to Talkback: Understanding the Smartphone Adoption Process of Blind Users", "pdf_hash": "2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c", "year": 2015, "venue": "ASSETS", "alt_text": "Graph with the cumulative interaction time during the eight weeks. Where we see partipant 5 as the one with the lowest amount throught every week. P4 was the one with the highest interaction time.", "levels": [[1], [2], [2]], "corpus_id": 8279953, "sentences": ["Graph with the cumulative interaction time during the eight weeks.", "Where we see partipant 5 as the one with the lowest amount throught every week.", "P4 was the one with the highest interaction time."], "caption": "Figure 2. Cumulative number of unique applications visit during the eight weeks by each participant.", "local_uri": ["2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Getting Smartphones to Talkback: Understanding the Smartphone Adoption Process of Blind Users", "pdf_hash": "2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c", "year": 2015, "venue": "ASSETS", "alt_text": "Cumulative number of focused items where P4 is shown to have more than double the amount of items focused with over 100000.", "levels": null, "corpus_id": 8279953, "sentences": ["Cumulative number of focused items where P4 is shown to have more than double the amount of items focused with over 100000."], "caption": "Figure 4. Cumulative number of items that were focused during exploration throughout the eight weeks per participant.", "local_uri": ["2ad9fb4a36bd91f26ad2b8d95fef54bf6a016e5c_Image_006.png"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A male sitting behind a desk looking into the camera with a blue background. There are white text on black background captions overlaid at the bottom of the picture. The caption is: \"which college career first they will be attending based on a cannibal corps\". Those words are decorated differently with a red color: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["A male sitting behind a desk looking into the camera with a blue background.", "There are white text on black background captions overlaid at the bottom of the picture.", "The caption is: \"which college career first they will be attending based on a cannibal corps\".", "Those words are decorated differently with a red color: career, first, they, will, cannibal, corps."], "caption": "Figure 1. The prototype tool examined in this work", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A male sitting behind a desk looking into the camera with a blue background. There are white text on black background captions overlaid at the bottom of the picture. The caption is: \"which college career first they will be attending based on a cannibal corps\".", "levels": null, "corpus_id": 37154277, "sentences": ["A male sitting behind a desk looking into the camera with a blue background.", "There are white text on black background captions overlaid at the bottom of the picture.", "The caption is: \"which college career first they will be attending based on a cannibal corps\"."], "caption": "Baseline condition: no markup(no_change)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a bold font: which, college, be, attending, based, on, a.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a bold font: which, college, be, attending, based, on, a."], "caption": "Bold on Confident (bold_c)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a bold font: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a bold font: career, first, they, will, cannibal, corps."], "caption": "Bold on Uncertain (bold_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a green color: which, college, be, attending, based, on, a.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a green color: which, college, be, attending, based, on, a."], "caption": "Green on Confident (color_c)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a red color: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a red color: career, first, they, will, cannibal, corps."], "caption": "Red on Uncertain (color_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a smaller size: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a smaller size: career, first, they, will, cannibal, corps."], "caption": "Small font size on Uncertain(size_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a dark gray color: career, first, they, will. Those words are decorated differently with a gray color: cannibal corps. Those words are decorated differently with a lighter gray color: which, college, be, attending.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a dark gray color: career, first, they, will.", "Those words are decorated differently with a gray color: cannibal corps.", "Those words are decorated differently with a lighter gray color: which, college, be, attending."], "caption": "Levels of gray color based on confidence (r_gray)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with a huge font size: based. Those words are decorated differently with a large font size: which, college, on, a. Those words are decorated differently with a small font size: cannibal, corps. Those words are decorated differently with a tiny font size: career, first, they, will.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with a huge font size: based.", "Those words are decorated differently with a large font size: which, college, on, a. Those words are decorated differently with a small font size: cannibal, corps.", "Those words are decorated differently with a tiny font size: career, first, they, will."], "caption": "Levels of font size based on confidence (r_size)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are missing and decorated differently with just underlines: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are missing and decorated differently with just underlines: career, first, they, will, cannibal, corps."], "caption": "Empty underline on Uncertain(del_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with an italic font: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with an italic font: career, first, they, will, cannibal, corps."], "caption": "Italics on Uncertain (it_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with underlines: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with underlines: career, first, they, will, cannibal, corps."], "caption": "Underline on Uncertain(ul_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "The same actor as in previous picture with captioning. Those words are decorated differently with underlines and a gray color: career, first, they, will, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["The same actor as in previous picture with captioning.", "Those words are decorated differently with underlines and a gray color: career, first, they, will, cannibal, corps."], "caption": "Underline and gray color on Uncertain (ul_gray_u)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A bar graph of participants' preferences of the markup styles. The x axis is the 12 markup styles: no_change, bold_c, bold_u, color_c, color_u, del_u, it_u, r_gray, r_size, size_u, ul_u, and ul_gray_u. The y axis is numeric, ranging from 0 to 1. The prominent feature is it_u being the most preferred with 0.5 and del_u having zero preference. Other styles ranged from 0.1 to 0.25 as their preference.", "levels": [[1], [1], [1], [2], [2]], "corpus_id": 37154277, "sentences": ["A bar graph of participants' preferences of the markup styles.", "The x axis is the 12 markup styles: no_change, bold_c, bold_u, color_c, color_u, del_u, it_u, r_gray, r_size, size_u, ul_u, and ul_gray_u.", "The y axis is numeric, ranging from 0 to 1.", "The prominent feature is it_u being the most preferred with 0.5 and del_u having zero preference.", "Other styles ranged from 0.1 to 0.25 as their preference."], "caption": "Figure 4. Pilot Study Preference Results", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A grid of 4 display styles that we chose for the expanded study. The pictures show the same actor as in previous pictures with captions. On the upper left, the picture is labelled No Change and corresponds to the no_change style used in the pilot study. On the upper right the picture is labelled Italics and corresponds to the it_u style. The bottom left picture is labelled Underline and corresponds to the ul_u style. The bottom right picture is labelled Yellow with those words differently styled with a yellow font: career, fair, they, will, attending, cannibal, corps.", "levels": null, "corpus_id": 37154277, "sentences": ["A grid of 4 display styles that we chose for the expanded study.", "The pictures show the same actor as in previous pictures with captions.", "On the upper left, the picture is labelled No Change and corresponds to the no_change style used in the pilot study.", "On the upper right the picture is labelled Italics and corresponds to the it_u style.", "The bottom left picture is labelled Underline and corresponds to the ul_u style.", "The bottom right picture is labelled Yellow with those words differently styled with a yellow font: career, fair, they, will, attending, cannibal, corps."], "caption": "Figure 5. Markup conditions in larger study (Section 2.2)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A vertical bar graph of participants' preferences of the markup styles. The x axis is the 4 markup styles: No Change, Yellow, Italics, and Underline. The y axis is numeric, ranging from 0 to 0.8. The prominent feature is No Change being the most preferred with 0.65. Other styles ranged from 0.3 to 0.45 as their preference with Italics being ahead of Yellow and Underline.", "levels": [[1], [1], [1], [2], [2, 1]], "corpus_id": 37154277, "sentences": ["A vertical bar graph of participants' preferences of the markup styles.", "The x axis is the 4 markup styles: No Change, Yellow, Italics, and Underline.", "The y axis is numeric, ranging from 0 to 0.8.", "The prominent feature is No Change being the most preferred with 0.65.", "Other styles ranged from 0.3 to 0.45 as their preference with Italics being ahead of Yellow and Underline."], "caption": "Figure 6. Larger Study: Preference Responses (binary)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_016.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A vertical bar graph of participants' perspective of the markup styles being distracting. The x axis is the 4 markup styles: No Change, Yellow, Italics, and Underline. The y axis is numeric, ranging from 0 to 0.8. The prominent feature is No Change being the least distracting with 0.4. Other styles ranged from 0.5 to 0.7 as their preference with Italics being less distracting than Yellow and Underline.", "levels": [[1], [1], [1], [3, 2], [3, 2]], "corpus_id": 37154277, "sentences": ["A vertical bar graph of participants' perspective of the markup styles being distracting.", "The x axis is the 4 markup styles: No Change, Yellow, Italics, and Underline.", "The y axis is numeric, ranging from 0 to 0.8.", "The prominent feature is No Change being the least distracting with 0.4.", "Other styles ranged from 0.5 to 0.7 as their preference with Italics being less distracting than Yellow and Underline."], "caption": "Figure 7. Larger Study: Distracting Responses (binary)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_017.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A horizontal stacked bar graph of participants' Likert responses to the helpful question. The x axis is numeric, ranging from 0 to 100%. The y axis is the 4 markup styles: Yellow, Underline, No Change, and Italics. All markup styles had similar percentages of their scales which ranged from Strongly Disagree, Disagree, Neither agree nor disagree, Agree, and Strongly Agree. Yellow had 13.1%, 15%, 27.1%, 32.7%, and 12.1% for the scales. Underline had 15.9%, 19.6%, 19.6%, 33.6%, and 11.2%. No Change had 7.5%, 14%, 25.2%, 39.3%, and 14%. Italics had 9.3%, 18.7%, 26.2%, 31.8%, and 14%.", "levels": [[1], [1], [1], [1], [2], [2], [2], [2]], "corpus_id": 37154277, "sentences": ["A horizontal stacked bar graph of participants' Likert responses to the helpful question.", "The x axis is numeric, ranging from 0 to 100%.", "The y axis is the 4 markup styles: Yellow, Underline, No Change, and Italics.", "All markup styles had similar percentages of their scales which ranged from Strongly Disagree, Disagree, Neither agree nor disagree, Agree, and Strongly Agree.", "Yellow had 13.1%, 15%, 27.1%, 32.7%, and 12.1% for the scales.", "Underline had 15.9%, 19.6%, 19.6%, 33.6%, and 11.2%.", "No Change had 7.5%, 14%, 25.2%, 39.3%, and 14%.", "Italics had 9.3%, 18.7%, 26.2%, 31.8%, and 14%."], "caption": "Figure 8. Larger Study: Helpful Responses (Likert)", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_018.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "pdf_hash": "471f9168db0fcb72d394222491966b97c098b1cd", "year": 2017, "venue": "ASSETS", "alt_text": "A horizontal stacked bar graph of participants' ranks of the markup styles. The x axis is numeric, ranging from 0 to 100%. The y axis is the 4 markup styles: Yellow, Underline, No Change, and Italics. The most prominent feature is No Change with the highest ranking. The ranks are Ranked #1, Ranked #2, Ranked #3, and Ranked #4. Yellow had 18.7%, 11.2%, 13.1%, and 57% for the ranks. Underline had 2.8%, 27.1%, 47.7%, and 22.4%. No Change had 66.4%, 12.1%, 8.4%, and 13.1%. Italics had 12.1%, 49.5%, 30.8%, and 7.5%.", "levels": [[1], [1], [1], [2], [2], [2], [2], [2], [2]], "corpus_id": 37154277, "sentences": ["A horizontal stacked bar graph of participants' ranks of the markup styles.", "The x axis is numeric, ranging from 0 to 100%.", "The y axis is the 4 markup styles: Yellow, Underline, No Change, and Italics.", "The most prominent feature is No Change with the highest ranking.", "The ranks are Ranked #1, Ranked #2, Ranked #3, and Ranked #4.", "Yellow had 18.7%, 11.2%, 13.1%, and 57% for the ranks.", "Underline had 2.8%, 27.1%, 47.7%, and 22.4%.", "No Change had 66.4%, 12.1%, 8.4%, and 13.1%.", "Italics had 12.1%, 49.5%, 30.8%, and 7.5%."], "caption": "Figure 9. Larger Study: Ranking of Markup Types", "local_uri": ["471f9168db0fcb72d394222491966b97c098b1cd_Image_019.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities", "pdf_hash": "9bca23885923c787dfc7f6e9fe3595fced207984", "year": 2019, "venue": "CHI", "alt_text": "The four stages of the DesignAbility framework are Learning requirements, Design for Engaged Learning, Prototyping and Evaluation", "levels": null, "corpus_id": 140313219, "sentences": ["The four stages of the DesignAbility framework are Learning requirements, Design for Engaged Learning, Prototyping and Evaluation"], "caption": "", "local_uri": ["9bca23885923c787dfc7f6e9fe3595fced207984_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities", "pdf_hash": "9bca23885923c787dfc7f6e9fe3595fced207984", "year": 2019, "venue": "CHI", "alt_text": "Learning Requirements is divided into Learning goals and Learning strategies. Design for Engaged Learning is divided into Storytelling and Collaborative Learning. Prototyping is divided into Low-fidelity and High fidelity prototypes. Evaluation is divided into Experts' review and Usability testing", "levels": null, "corpus_id": 140313219, "sentences": ["Learning Requirements is divided into Learning goals and Learning strategies.", "Design for Engaged Learning is divided into Storytelling and Collaborative Learning.", "Prototyping is divided into Low-fidelity and High fidelity prototypes.", "Evaluation is divided into Experts' review and Usability testing"], "caption": "", "local_uri": ["9bca23885923c787dfc7f6e9fe3595fced207984_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities", "pdf_hash": "9bca23885923c787dfc7f6e9fe3595fced207984", "year": 2019, "venue": "CHI", "alt_text": "88.5% consider that providing Learning Goals and Strategies in a framework reduces time spent during the design of an educational tool.   65.4% think that the proposed learning goals and strategies can be easy to implement in the design of an educational tool, while 23.1% answered maybe and 11.5% think they are not easy to implement.  76.9% think the Learning Requirements stage and its sub-stages are well designed and can help designers create educational tools.", "levels": null, "corpus_id": 140313219, "sentences": ["88.5% consider that providing Learning Goals and Strategies in a framework reduces time spent during the design of an educational tool.", "65.4% think that the proposed learning goals and strategies can be easy to implement in the design of an educational tool, while 23.1% answered maybe and 11.5% think they are not easy to implement.", "76.9% think the Learning Requirements stage and its sub-stages are well designed and can help designers create educational tools."], "caption": "", "local_uri": ["9bca23885923c787dfc7f6e9fe3595fced207984_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities", "pdf_hash": "9bca23885923c787dfc7f6e9fe3595fced207984", "year": 2019, "venue": "CHI", "alt_text": "73.1% think the GUI-DG is well structured and has the necessary guidelines to design a tool aimed at deaf children.  88.5% think the UX design sub-stage is clear and complete.  69.2% think this stage of the framework has the necessary elements to develop a prototype. 26.9% answered maybe.", "levels": null, "corpus_id": 140313219, "sentences": ["73.1% think the GUI-DG is well structured and has the necessary guidelines to design a tool aimed at deaf children.", "88.5% think the UX design sub-stage is clear and complete.", "69.2% think this stage of the framework has the necessary elements to develop a prototype.", "26.9% answered maybe."], "caption": "", "local_uri": ["9bca23885923c787dfc7f6e9fe3595fced207984_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities", "pdf_hash": "9bca23885923c787dfc7f6e9fe3595fced207984", "year": 2019, "venue": "CHI", "alt_text": "61.5% think the proposed heuristics and principles are enough for an experts' review. 34.6% answered maybe.  80.8% consider that the Usability Test Plan has the necessary elements to carry out and evaluation with users. 19.2% answered maybe.", "levels": null, "corpus_id": 140313219, "sentences": ["61.5% think the proposed heuristics and principles are enough for an experts' review.", "34.6% answered maybe.", "80.8% consider that the Usability Test Plan has the necessary elements to carry out and evaluation with users.", "19.2% answered maybe."], "caption": "", "local_uri": ["9bca23885923c787dfc7f6e9fe3595fced207984_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities", "pdf_hash": "9bca23885923c787dfc7f6e9fe3595fced207984", "year": 2019, "venue": "CHI", "alt_text": "73.1% think the three evaluation methods (direct observation, questionnaires/surveys, smileyometer) are good to be used with deaf children. 15.8% answered maybe.  80.8% think that the collaborative learning evaluation is well defined and has the necessary elements to evaluate collaboration. 19.2% answered maybe.", "levels": null, "corpus_id": 140313219, "sentences": ["73.1% think the three evaluation methods (direct observation, questionnaires/surveys, smileyometer) are good to be used with deaf children.", "15.8% answered maybe.", "80.8% think that the collaborative learning evaluation is well defined and has the necessary elements to evaluate collaboration.", "19.2% answered maybe."], "caption": "", "local_uri": ["9bca23885923c787dfc7f6e9fe3595fced207984_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Using accessible math textbooks with students who have learning disabilities", "pdf_hash": "d51ec8a4cae6a02dd5d0ba0befc2181769fc11e7", "year": 2010, "venue": "ASSETS '10", "alt_text": "Screenshot of math textbook page that contains a picture, text, and a math expressions.", "levels": [[-1]], "corpus_id": 15748989, "sentences": ["Screenshot of math textbook page that contains a picture, text, and a math expressions."], "caption": "Figure 2: Read & Write Gold tool bar & sample page from Say it with Symbols by Pearson Education", "local_uri": ["d51ec8a4cae6a02dd5d0ba0befc2181769fc11e7_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BreakSense: Combining Physiological and Location Sensing to Promote Mobility during Work-Breaks", "pdf_hash": "3f6d4e35b7f0507febc9708e0430859dbd71f973", "year": 2017, "venue": "CHI", "alt_text": "Figure 2 center: A medium size yellow star inside an orange circle. A grey cat inside the star.", "levels": null, "corpus_id": 12126527, "sentences": ["Figure 2 center: A medium size yellow star inside an orange circle.", "A grey cat inside the star."], "caption": "", "local_uri": ["3f6d4e35b7f0507febc9708e0430859dbd71f973_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "A web-based user survey for evaluating power saving strategies for deaf users of mobileASL", "pdf_hash": "7ca70d429ea7962f6801374b1016808c0a5c943a", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure two is a depiction of the variable frame rate (VFR) algorithm.   It consists of a series of frames from right to left, with each frame including a stick figure which represents a signer's upper torso and head.  The series has three distinct sections.  The first section is labeled, \"signing,\" and shows the frames close to each other.  The second section is labeled, \"just listening,\" and shows the frames a distance apart from each other, representing the decrease in frame rate when the signer is not-signing.  The third section is labeled \"signing,\" and shows the frames once again close to each other.  Underneath figure two, on the left, is the label, \"time,\" followed by an arrow pointing to the right along the length of the illustration.", "levels": null, "corpus_id": 10625558, "sentences": ["Figure two is a depiction of the variable frame rate (VFR) algorithm.", "It consists of a series of frames from right to left, with each frame including a stick figure which represents a signer's upper torso and head.", "The series has three distinct sections.", "The first section is labeled, \"signing,\" and shows the frames close to each other.", "The second section is labeled, \"just listening,\" and shows the frames a distance apart from each other, representing the decrease in frame rate when the signer is not-signing.", "The third section is labeled \"signing,\" and shows the frames once again close to each other.", "Underneath figure two, on the left, is the label, \"time,\" followed by an arrow pointing to the right along the length of the illustration."], "caption": "Figure 2: Depiction of variable frame rate algorithm. The frame rate decreases when the signer is not-signing, resulting in \u201cchoppy\u201d video quality.", "local_uri": ["7ca70d429ea7962f6801374b1016808c0a5c943a_Image_003.gif"], "annotated": false, "compound": false}
{"title": "A web-based user survey for evaluating power saving strategies for deaf users of mobileASL", "pdf_hash": "7ca70d429ea7962f6801374b1016808c0a5c943a", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure five is a screen shot of the ASL video interpretation of survey questions and 5-point Likert scale.    As in Figure one, the survey is in a grid format with questions one through four listed horizontally along the top, and a 5-point Likert scale listed vertically to the left.  Instead of having each question in English text, there are four video screens, each with a play button.  Above these questions is an icon labeled, \"Click this icon for the English text of questions 1 to 4.\"   To the left of the Likert scale, which remains in English, is a fifth video screens with a play button.", "levels": null, "corpus_id": 10625558, "sentences": ["Figure five is a screen shot of the ASL video interpretation of survey questions and 5-point Likert scale.", "As in Figure one, the survey is in a grid format with questions one through four listed horizontally along the top, and a 5-point Likert scale listed vertically to the left.", "Instead of having each question in English text, there are four video screens, each with a play button.", "Above these questions is an icon labeled, \"Click this icon for the English text of questions 1 to 4.\"   To the left of the Likert scale, which remains in English, is a fifth video screens with a play button."], "caption": "Figure 5: Screen shot of ASL video interpretation of survey questions and 5-point Likert scale.", "local_uri": ["7ca70d429ea7962f6801374b1016808c0a5c943a_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Causal interactions", "pdf_hash": "db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb", "year": 2014, "venue": "CHI", "alt_text": "The puzzle interface consists of eight circles in a 3-by-3 grid, two buttons and instructions. The bottom six circles each have a single distinctive feature (e.g. red fill color), the top right circle combines many of those features and the top left circle is blank. The two buttons show a selection arrow and an eraser.", "levels": null, "corpus_id": 9381142, "sentences": ["The puzzle interface consists of eight circles in a 3-by-3 grid, two buttons and instructions.", "The bottom six circles each have a single distinctive feature (e.g. red fill color), the top right circle combines many of those features and the top left circle is blank.", "The two buttons show a selection arrow and an eraser."], "caption": "", "local_uri": ["db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Causal interactions", "pdf_hash": "db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb", "year": 2014, "venue": "CHI", "alt_text": "Diagram of the order of actions in the causal order and non-causal order conditions. In both (a) and (b) the blank circle in the first frame has turned red in the second frame. In (a) the cursor is first shown over the red circle, then over the circle that was previously blank but turned red. In (b) the cursor is first shown over the blank circle, then over the red circle.", "levels": null, "corpus_id": 9381142, "sentences": ["Diagram of the order of actions in the causal order and non-causal order conditions.", "In both (a) and (b) the blank circle in the first frame has turned red in the second frame.", "In (a) the cursor is first shown over the red circle, then over the circle that was previously blank but turned red.", "In (b) the cursor is first shown over the blank circle, then over the red circle."], "caption": "", "local_uri": ["db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Causal interactions", "pdf_hash": "db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb", "year": 2014, "venue": "CHI", "alt_text": "Graph of how many participants solved the puzzle: 100% with causal order, 63% with non-causal order.", "levels": [[2, 1]], "corpus_id": 9381142, "sentences": ["Graph of how many participants solved the puzzle: 100% with causal order, 63% with non-causal order."], "caption": "", "local_uri": ["db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Causal interactions", "pdf_hash": "db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb", "year": 2014, "venue": "CHI", "alt_text": "Graph of solution times: mean of 6.35 minutes with causal order, mean of 10.63 minutes with non-causal order.", "levels": [[2, 1]], "corpus_id": 9381142, "sentences": ["Graph of solution times: mean of 6.35 minutes with causal order, mean of 10.63 minutes with non-causal order."], "caption": "", "local_uri": ["db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Causal interactions", "pdf_hash": "db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb", "year": 2014, "venue": "CHI", "alt_text": "Mean solution times of the Experiment 3 interfaces measured in number of clicks. 98.72 with continuous, 88.88 with follow and 113.87 with discontinuous.", "levels": [[1], [2]], "corpus_id": 9381142, "sentences": ["Mean solution times of the Experiment 3 interfaces measured in number of clicks.", "98.72 with continuous, 88.88 with follow and 113.87 with discontinuous."], "caption": "", "local_uri": ["db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Causal interactions", "pdf_hash": "db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb", "year": 2014, "venue": "CHI", "alt_text": "TurkGate web form interface, with inputs on the left side and results on the right side. The results appear to be on a page extending from behind the page of inputs.", "levels": null, "corpus_id": 9381142, "sentences": ["TurkGate web form interface, with inputs on the left side and results on the right side.", "The results appear to be on a page extending from behind the page of inputs."], "caption": "", "local_uri": ["db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People", "pdf_hash": "658db525407d63dbf4717f1bdfb1b1d579bf2303", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 2 shows a 3D printed ID code which is shaped like a fork (but the fork's lenght is shrunk to less than 2mm). The variable distances between the four tips contacting the touch display encode the individual IDs of the maps.", "levels": null, "corpus_id": 34975365, "sentences": ["Figure 2 shows a 3D printed ID code which is shaped like a fork (but the fork's lenght is shrunk to less than 2mm).", "The variable distances between the four tips contacting the touch display encode the individual IDs of the maps."], "caption": "Fig. 2: The ID code which is embedded in the tactile map: varying distances by variables n and m encode a unique identifier which can be recognized through touch-displays.", "local_uri": ["658db525407d63dbf4717f1bdfb1b1d579bf2303_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People", "pdf_hash": "658db525407d63dbf4717f1bdfb1b1d579bf2303", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 3 shows that the capacitive codes are integrated in the four corners of the tactile map. In contrast to the actual map they are printed by conductive filament.", "levels": null, "corpus_id": 34975365, "sentences": ["Figure 3 shows that the capacitive codes are integrated in the four corners of the tactile map.", "In contrast to the actual map they are printed by conductive filament."], "caption": "Fig. 3: Capacitive codes at tactile map\u2019s corners are printed with conductive filament. View from bottom side (right).", "local_uri": ["658db525407d63dbf4717f1bdfb1b1d579bf2303_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People", "pdf_hash": "658db525407d63dbf4717f1bdfb1b1d579bf2303", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 5 show a LucentMap attached to a tablet computer's display. Map features touch by the user are illuminated.", "levels": null, "corpus_id": 34975365, "sentences": ["Figure 5 show a LucentMap attached to a tablet computer's display.", "Map features touch by the user are illuminated."], "caption": "", "local_uri": ["658db525407d63dbf4717f1bdfb1b1d579bf2303_Image_006.jpg", "658db525407d63dbf4717f1bdfb1b1d579bf2303_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Improving calibration time and accuracy for situation-specific models of color differentiation", "pdf_hash": "6ce0ca40ea396cc6c0dcd6edd3bd00aebc267d04", "year": 2011, "venue": "ASSETS", "alt_text": "Formula for an ellipsoid. Mathematically desribes an ellipsoid in 3D Euclidean space.", "levels": null, "corpus_id": 2252631, "sentences": ["Formula for an ellipsoid.", "Mathematically desribes an ellipsoid in 3D Euclidean space."], "caption": "where a is the best-fit ellipse major axis half length, b is the best- fit ellipse minor axis half length, and c is the amount of luminance that is added or subtracted, as described above. This formula is used in ICD-2 to internally represent the discrimination ellipsoid.", "local_uri": ["6ce0ca40ea396cc6c0dcd6edd3bd00aebc267d04_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Improving calibration time and accuracy for situation-specific models of color differentiation", "pdf_hash": "6ce0ca40ea396cc6c0dcd6edd3bd00aebc267d04", "year": 2011, "venue": "ASSETS", "alt_text": "Formula for finding the major and minor axis length for the primary color's ellipse at the secondary color's luminance.", "levels": null, "corpus_id": 2252631, "sentences": ["Formula for finding the major and minor axis length for the primary color's ellipse at the secondary color's luminance."], "caption": "Figure 5. Formula for the resized half major axis length (left) and the formula for the resized half minor axis length (right).", "local_uri": ["6ce0ca40ea396cc6c0dcd6edd3bd00aebc267d04_Image_006.jpg", "6ce0ca40ea396cc6c0dcd6edd3bd00aebc267d04_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Voice Interfaces in Everyday Life", "pdf_hash": "4ace92c22a895d5e23e58de8d738df8e500d8d79", "year": 2018, "venue": "CHI", "alt_text": "The Conditional Voice Recorder (CVR) is a small (12cm by 12cm) black box with a white lid. On the lid is a conference microphone, two LEDs (one blue, one red; to indicate when the CVR is actively 'listening' for the hotword and when it is recording, respectively), and a button to turn on, or off, the the CVR. Pictured also is the bottom part of an Amazon Echo.", "levels": null, "corpus_id": 5047167, "sentences": ["The Conditional Voice Recorder (CVR) is a small (12cm by 12cm) black box with a white lid.", "On the lid is a conference microphone, two LEDs (one blue, one red; to indicate when the CVR is actively 'listening' for the hotword and when it is recording, respectively), and a button to turn on, or off, the the CVR.", "Pictured also is the bottom part of an Amazon Echo."], "caption": "Figure 1. Conditional Voice Recorder (CVR).", "local_uri": ["4ace92c22a895d5e23e58de8d738df8e500d8d79_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Philosophers Living with the Tilting Bowl", "pdf_hash": "8382ff3a2db6ce15e0650498162ec47b9c89a47b", "year": 2018, "venue": "CHI", "alt_text": "/Users/doenjaoogjes/Dropbox/Everyday Design Studio/A Projects/100 Ongoing/Tilting-Bowl/Deployment/Interviews/Data/05 Katharine/00 Self-Reporting/WhatsApp Image 2017-07-01 at 5.59.38 PM.jpeg", "levels": [[-1]], "corpus_id": 5041159, "sentences": ["/Users/doenjaoogjes/Dropbox/Everyday Design Studio/A Projects/100 Ongoing/Tilting-Bowl/Deployment/Interviews/Data/05 Katharine/00 Self-Reporting/WhatsApp Image 2017-07-01 at 5.59.38 PM.jpeg"], "caption": "", "local_uri": ["8382ff3a2db6ce15e0650498162ec47b9c89a47b_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "pdf_hash": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "year": 2016, "venue": "CHI", "alt_text": "The image shows a screenshot of the game THYFTHYF with the player character (small yellow cartoon bird) being located in a blue sky tile that features numerous collectable coins, a plane enemy a life indicator in the shape of eggs, a timer, a coin, score and altitude counter.", "levels": null, "corpus_id": 11459950, "sentences": ["The image shows a screenshot of the game THYFTHYF with the player character (small yellow cartoon bird) being located in a blue sky tile that features numerous collectable coins, a plane enemy a life indicator in the shape of eggs, a timer, a coin, score and altitude counter."], "caption": "", "local_uri": ["79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "pdf_hash": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "year": 2016, "venue": "CHI", "alt_text": "The image shows a screenshot of the game THYFTHYF with the player character (small yellow cartoon bird) being located in a dark blue / black sky tile that features numerous collectable coins, a sattelite enemy a life indicator in the shape of eggs, a timer, a coin, score and altitude counter.", "levels": null, "corpus_id": 11459950, "sentences": ["The image shows a screenshot of the game THYFTHYF with the player character (small yellow cartoon bird) being located in a dark blue / black sky tile that features numerous collectable coins, a sattelite enemy a life indicator in the shape of eggs, a timer, a coin, score and altitude counter."], "caption": "Figure 1. Two screenshots of the game THYFTHYF taken at different height (progress) levels show common game objects.", "local_uri": ["79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "pdf_hash": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "year": 2016, "venue": "CHI", "alt_text": "The study setup shows a main table with a large screen, Kinect device, speakers, mouse and keyboard, and a conductor screen that is hidden behind the large screen that faces the participant. A participant figure is located in front of the screen with 210 cm distance and min. 110 cm to move left and right. A gamepad stand holds a gamepad and a seat can be moved to the participant location for the sedentary conditions. Next to the large table a smaller table with two chairs holds a laptop with a microphone and further study materials for the interviews and the questionnaires. Behind the conductor seat a camera is located to record participants from the front whilst also observing the conductor screen which mirrors the player screen.", "levels": null, "corpus_id": 11459950, "sentences": ["The study setup shows a main table with a large screen, Kinect device, speakers, mouse and keyboard, and a conductor screen that is hidden behind the large screen that faces the participant.", "A participant figure is located in front of the screen with 210 cm distance and min.", "110 cm to move left and right.", "A gamepad stand holds a gamepad and a seat can be moved to the participant location for the sedentary conditions.", "Next to the large table a smaller table with two chairs holds a laptop with a microphone and further study materials for the interviews and the questionnaires.", "Behind the conductor seat a camera is located to record participants from the front whilst also observing the conductor screen which mirrors the player screen."], "caption": "Figure 3: The technical setup of the study.", "local_uri": ["79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "pdf_hash": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "year": 2016, "venue": "CHI", "alt_text": "Data table visualized as stacked ranks:  Rank 1 Rank 2 Rank 3 embedded11 6 10 menu 7 14 14 auto 7 11 10", "levels": [[1]], "corpus_id": 11459950, "sentences": ["Data table visualized as stacked ranks:  Rank 1 Rank 2 Rank 3 embedded11 6 10 menu 7 14 14 auto 7 11 10"], "caption": "Figure 4: Ranking by participants regarding preference in study 1 (with the game THYFTHYF).", "local_uri": ["79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "pdf_hash": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "year": 2016, "venue": "CHI", "alt_text": "White text in black ground reading \"Eat this food [little red food creature icon] to go one level down. And eat this food [little blue food creature icon] to go one level up.\"", "levels": null, "corpus_id": 11459950, "sentences": ["White text in black ground reading \"Eat this food [little red food creature icon] to go one level down. And eat this food [little blue food creature icon] to go one level up.\""], "caption": "Figure 5: Instructions for the condition embedded.", "local_uri": ["79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "pdf_hash": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "year": 2016, "venue": "CHI", "alt_text": "Stacked rank results chart for the following data: Rank 1 E 33 Rank 2 E 4 Rank 3 E 5 Rank 1 A 6 Rank 2 A 18 Rank 3 A 7 Rank 1 M 15 Rank 2 M 14 Rank 3 M 6", "levels": [[2, 1]], "corpus_id": 11459950, "sentences": ["Stacked rank results chart for the following data: Rank 1 E 33 Rank 2 E 4 Rank 3 E 5 Rank 1 A 6 Rank 2 A 18 Rank 3 A 7 Rank 1 M 15 Rank 2 M 14 Rank 3 M 6"], "caption": "Figure 7: Ranking by participants regarding preference in study 2 (with the game fl0w).", "local_uri": ["79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "pdf_hash": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "year": 2015, "venue": "ASSETS", "alt_text": "One of three 3D animated characters performing the sign HE (pointing at the left side of the audience).  In this image, the first character is male with the characteristics of a relatively young man and black hair. The animated character is in front of a dark blue background.", "levels": null, "corpus_id": 13346211, "sentences": ["One of three 3D animated characters performing the sign HE (pointing at the left side of the audience).", "In this image, the first character is male with the characteristics of a relatively young man and black hair.", "The animated character is in front of a dark blue background."], "caption": "", "local_uri": ["e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "pdf_hash": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "year": 2015, "venue": "ASSETS", "alt_text": "One of three 3D animated characters performing the sign HE (pointing at the left side of the audience).  The second character is female with the characteristics of a relatively young woman and brown hair. The animated character is in front of a light purple background.", "levels": null, "corpus_id": 13346211, "sentences": ["One of three 3D animated characters performing the sign HE (pointing at the left side of the audience).", "The second character is female with the characteristics of a relatively young woman and brown hair.", "The animated character is in front of a light purple background."], "caption": "", "local_uri": ["e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "pdf_hash": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "year": 2015, "venue": "ASSETS", "alt_text": "One of three 3D animated characters performing the sign HE (pointing at the left side of the audience).  The third character is male with the characteristics of a relatively young man and blonde hair. The animated character is in front of a light blue background.", "levels": null, "corpus_id": 13346211, "sentences": ["One of three 3D animated characters performing the sign HE (pointing at the left side of the audience).", "The third character is male with the characteristics of a relatively young man and blonde hair.", "The animated character is in front of a light blue background."], "caption": "Figure 1: Screenshots from the three avatars shown in the study: (a) EMBR, (b) JASigning, (c) VCom3D.", "local_uri": ["e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "pdf_hash": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "year": 2015, "venue": "ASSETS", "alt_text": "Two grouped bar charts displaying % of Accounted Variance (Adjusted squared R):\nComprehension Model 1: Demographic at 25.6%\nComprehension Model 2: Demographic and Technology at 38.2%\nSignificance code between the comprehension models: **\nSubjective Model 1: Demographic at 15.3%\nSubjective Model 2: Demographic and Technology at 33.5%\nSignificance code between the subjective models: ***", "levels": [[2, 1]], "corpus_id": 13346211, "sentences": ["Two grouped bar charts displaying % of Accounted Variance (Adjusted squared R):\nComprehension Model 1: Demographic at 25.6%\nComprehension Model 2: Demographic and Technology at 38.2%\nSignificance code between the comprehension models: **\nSubjective Model 1: Demographic at 15.3%\nSubjective Model 2: Demographic and Technology at 33.5%\nSignificance code between the subjective models: ***"], "caption": "Figure 2: Regression model comparison summary. (Significance codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01)", "local_uri": ["e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "pdf_hash": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "year": 2015, "venue": "ASSETS", "alt_text": "Two graphs: Comprehension and Subjective. \nComprehension\ny-axis: % of squared R and x-axis: ten bar charts with confidence intervals. The values for each of the bars are given as (percentage, lower, upper):\nSchoolType: 0.4756, 0.1884, 0.6030\nGameGroup: 0.1011, 0.0252, 0.2820\nGender: 0.0312, 0.0071, 0.1350\nDescribe: 0.0608, 0.0083, 0.2200\nWhenBecome: 0.0394, 0.0043, 0.1790\nHomeASL: 0.0520, 0.0054, 0.2090\nInternetSearch: 0.1240, 0.0196, 0.2930\nPositiveAttitudes: 0.0536, 0.0064, 0.1530\nASLChat: 0.0502, 0.0083, 0.1940\nSeenBefore: 0.0121, 0.0047, 0.1100\nSubjective\ny-axis: % of squared R and x-axis: six bar charts with confidence intervals. The values for each of the bars are given as (percentage, lower, upper):\nSchoolType: 0.1149, 0.0245, 0.3400\nWhenLearn: 0.0260, 0.0073, 0.1680\nHomeASL: 0.2561, 0.0430, 0.5210\nComputerComplex: 0.0478, 0.0031, 0.2330\nMediaSharingSubscale: 0.3158, 0.0595, 0.5490\nAnimationAttitude: 0.2394, 0.0270, 0.4690", "levels": [[1], [1], [2, 1], [2, 1]], "corpus_id": 13346211, "sentences": ["Two graphs: Comprehension and Subjective.", "Comprehension\ny-axis: % of squared R and x-axis: ten bar charts with confidence intervals.", "The values for each of the bars are given as (percentage, lower, upper):\nSchoolType: 0.4756, 0.1884, 0.6030\nGameGroup: 0.1011, 0.0252, 0.2820\nGender: 0.0312, 0.0071, 0.1350\nDescribe: 0.0608, 0.0083, 0.2200\nWhenBecome: 0.0394, 0.0043, 0.1790\nHomeASL: 0.0520, 0.0054, 0.2090\nInternetSearch: 0.1240, 0.0196, 0.2930\nPositiveAttitudes: 0.0536, 0.0064, 0.1530\nASLChat: 0.0502, 0.0083, 0.1940\nSeenBefore: 0.0121, 0.0047, 0.1100\nSubjective\ny-axis: % of squared R and x-axis: six bar charts with confidence intervals.", "The values for each of the bars are given as (percentage, lower, upper):\nSchoolType: 0.1149, 0.0245, 0.3400\nWhenLearn: 0.0260, 0.0073, 0.1680\nHomeASL: 0.2561, 0.0430, 0.5210\nComputerComplex: 0.0478, 0.0031, 0.2330\nMediaSharingSubscale: 0.3158, 0.0595, 0.5490\nAnimationAttitude: 0.2394, 0.0270, 0.4690"], "caption": "Figure 3: Relative importance (normalized to sum to 100%) of factors in Comprehension Model 2 and in Subjective Model 2, with 95% bootstrap confidence intervals.", "local_uri": ["e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_007.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones", "pdf_hash": "0b577d57553f72199f80c3036f8abb7782b9830b", "year": 2019, "venue": "CHI", "alt_text": "Illustration of main steps of PinchList operations. A shows two pinched-in fingers starts browsing a list. B shows pinching-out the fingers opens a sub-list. C shows moving the fingers up and down on the list. D shows flicking the fingers to move the view to the edges. E shows exploring the next two layers. F shows flicking the figures to resume previous layers.", "levels": null, "corpus_id": 140227051, "sentences": ["Illustration of main steps of PinchList operations.", "A shows two pinched-in fingers starts browsing a list.", "B shows pinching-out the fingers opens a sub-list.", "C shows moving the fingers up and down on the list.", "D shows flicking the fingers to move the view to the edges.", "E shows exploring the next two layers.", "F shows flicking the figures to resume previous layers."], "caption": "Figure 1. Steps of PinchList: (a) two pinched-in fingers starts browsing a list; (b) pinching-out the fingers reveals a sub-list;", "local_uri": ["0b577d57553f72199f80c3036f8abb7782b9830b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones", "pdf_hash": "0b577d57553f72199f80c3036f8abb7782b9830b", "year": 2019, "venue": "CHI", "alt_text": "Illustrations of using PinchList, Expand-and-Collapse, and View Switch techniques in Study 2.", "levels": null, "corpus_id": 140227051, "sentences": ["Illustrations of using PinchList, Expand-and-Collapse, and View Switch techniques in Study 2."], "caption": "Figure 6: The three techniques used in Study 2.", "local_uri": ["0b577d57553f72199f80c3036f8abb7782b9830b_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones", "pdf_hash": "0b577d57553f72199f80c3036f8abb7782b9830b", "year": 2019, "venue": "CHI", "alt_text": "Screenshots showing using pinch gestures to select volume command and adjust the value.", "levels": null, "corpus_id": 140227051, "sentences": ["Screenshots showing using pinch gestures to select volume command and adjust the value."], "caption": "Figure 11: Using pinch gestures to select volume command and adjust the value.", "local_uri": ["0b577d57553f72199f80c3036f8abb7782b9830b_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Optimizing Touchscreen Keyboards for Gesture Typing", "pdf_hash": "13edf7dd51106ede13ea81ad69cd311aff29b05e", "year": 2015, "venue": "CHI", "alt_text": "A 3D convex hull of points plotted in a 3D Cartesian space, where the axes represent clarity, speed, and Qwerty similarity and range from 0 to 1. The convex hull faces almost perfectly away from the origin at a 45 degree diagonal fashion. The hull is relatively symmetric, and the points near the middle of the whole appear to have coordinates of roughly (0.7, 0.7, 0.7).", "levels": [[1], [3, 2], [3, 2]], "corpus_id": 17598436, "sentences": ["A 3D convex hull of points plotted in a 3D Cartesian space, where the axes represent clarity, speed, and Qwerty similarity and range from 0 to 1.", "The convex hull faces almost perfectly away from the origin at a 45 degree diagonal fashion.", "The hull is relatively symmetric, and the points near the middle of the whole appear to have coordinates of roughly (0.7, 0.7, 0.7)."], "caption": "", "local_uri": ["13edf7dd51106ede13ea81ad69cd311aff29b05e_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Optimizing Touchscreen Keyboards for Gesture Typing", "pdf_hash": "13edf7dd51106ede13ea81ad69cd311aff29b05e", "year": 2015, "venue": "CHI", "alt_text": "The GK-C keyboard has three rows of keys. From left to right, the first row's keys are R, Q, A, J, T, U, Y, I, P, and H. Second row: W, O, Z, V, F, X, K, G, and N. Third row: E, B, S, M, D, C, and L.", "levels": null, "corpus_id": 17598436, "sentences": ["The GK-C keyboard has three rows of keys.", "From left to right, the first row's keys are R, Q, A, J, T, U, Y, I, P, and H. Second row: W, O, Z, V, F, X, K, G, and N. Third row: E, B, S, M, D, C, and L."], "caption": "", "local_uri": ["13edf7dd51106ede13ea81ad69cd311aff29b05e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Optimizing Touchscreen Keyboards for Gesture Typing", "pdf_hash": "13edf7dd51106ede13ea81ad69cd311aff29b05e", "year": 2015, "venue": "CHI", "alt_text": "The GK-S keyboard has three rows of keys. From left to right, the first row's keys are K, G, D, N, I, L, Y, B, J, and Z. Second row: V, C, A, E, R, O, M, F, and Q. Third row: W, H, T, S, U, P, and X.", "levels": null, "corpus_id": 17598436, "sentences": ["The GK-S keyboard has three rows of keys.", "From left to right, the first row's keys are K, G, D, N, I, L, Y, B, J, and Z. Second row: V, C, A, E, R, O, M, F, and Q. Third row: W, H, T, S, U, P, and X."], "caption": "Figure 3. 3D Pareto front. The keyboard layouts with lighter colors are farther from the origin.", "local_uri": ["13edf7dd51106ede13ea81ad69cd311aff29b05e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Sculpting a Mobile Musical Soundtrack", "pdf_hash": "49580e7c298126e0712b00680ae93bc2a782bb32", "year": 2015, "venue": "CHI", "alt_text": "A cross-section of the YSP map. This images shows a cross section of the YSP map, which highlights musical regions, location of exhibits and harmonic key centres", "levels": null, "corpus_id": 15001615, "sentences": ["A cross-section of the YSP map.", "This images shows a cross section of the YSP map, which highlights musical regions, location of exhibits and harmonic key centres"], "caption": "Figure 1: A cross-section of the YSP map", "local_uri": ["49580e7c298126e0712b00680ae93bc2a782bb32_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Sculpting a Mobile Musical Soundtrack", "pdf_hash": "49580e7c298126e0712b00680ae93bc2a782bb32", "year": 2015, "venue": "CHI", "alt_text": "Musical trajectory through an exhibit. This image illustrates the 'Trajectory through exhibits' design method.", "levels": null, "corpus_id": 15001615, "sentences": ["Musical trajectory through an exhibit.", "This image illustrates the 'Trajectory through exhibits' design method."], "caption": "Figure 2: Musical trajectory through an exhibit", "local_uri": ["49580e7c298126e0712b00680ae93bc2a782bb32_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing step-by-step information extraction to enhance existing how-to videos", "pdf_hash": "674bd5ba8aa35ccba6691788dbb394665e669128", "year": 2014, "venue": "CHI", "alt_text": "a three-stage crowdsourcing workflow for extracting step-by-step information from how-to videos", "levels": null, "corpus_id": 1470846, "sentences": ["a three-stage crowdsourcing workflow for extracting step-by-step information from how-to videos"], "caption": "Figure 1. Our crowdsourcing work\ufb02ow extracts step-by-step informa\u00ad tion from a how-to video with their descriptions and before/after images. It features the Find-Verify-Expand design pattern, time-based cluster\u00ad ing, and text/visual analysis techniques. Extracted step information can be used to help learners navigate how-to videos with higher interactivity.", "local_uri": ["674bd5ba8aa35ccba6691788dbb394665e669128_Image_005.png"], "annotated": false, "compound": false}
{"title": "Investigating Cursor-based Interactions to Support Non-Visual Exploration in the Real World", "pdf_hash": "4da6e24670af3ebb6c637c68233741f981120d04", "year": 2018, "venue": "ASSETS", "alt_text": "This figure has 4 components, a, b, c, and d, respectively.  In a: It shows a table with many boxes covered with white paper showing text such as glasses, butter, jam, etc. A user is holding and targeting his phone at one object, while touching the object with the other hand. In b: It shows a user is using his phone to scan the wall full of text labels, showing animal names. In c: It shows a user is aiming his phone at a prototype microwave control panel, while pointing his finger at one of the cells on the panel. In d: It shows a user is aiming his phone at a document on the wall, while pointing his finger on the touchscreen of the phone to read the text.", "levels": null, "corpus_id": 52937080, "sentences": ["This figure has 4 components, a, b, c, and d, respectively.", "In a: It shows a table with many boxes covered with white paper showing text such as glasses, butter, jam, etc.", "A user is holding and targeting his phone at one object, while touching the object with the other hand.", "In b: It shows a user is using his phone to scan the wall full of text labels, showing animal names.", "In c: It shows a user is aiming his phone at a prototype microwave control panel, while pointing his finger at one of the cells on the panel.", "In d: It shows a user is aiming his phone at a document on the wall, while pointing his finger on the touchscreen of the phone to read the text."], "caption": "Figure 1. We de\ufb01ne and compare cursor-based interactions that support non-visual attention to items within a complex visual scene: (a) window cursor, in which the user moves the device itself to scan the scene and receives information about what is in the center of the image; (b) vertical window cursor, a variation of window cursor that sacri\ufb01ces the granularity on the vertical axis, but potentially facilitates locating the direction of a speci\ufb01c object; (c) \ufb01nger cursor, in which the user moves their \ufb01nger on the real world object they want to access and receives information about details near (or under) their \ufb01ngertip; and (d) touch cursor, in which the visual scene is brought onto the device screen and the user moves their \ufb01nger on the touchscreen to receive information about what they touch on the live camera image.", "local_uri": ["4da6e24670af3ebb6c637c68233741f981120d04_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Investigating Cursor-based Interactions to Support Non-Visual Exploration in the Real World", "pdf_hash": "4da6e24670af3ebb6c637c68233741f981120d04", "year": 2018, "venue": "ASSETS", "alt_text": "This figure has 4 components, a, b, c, and d, respectively.  In a: It shows how window cursor works. Two entities are extracted from an image, the window cursor is overlapping with the left one, which is highlighted. In b: it shows how vertical window cursor works. Two entities are extracted from an image, the vertical window cursor is overlapping with the left one, which is highlighted. In c: it shows how finger cursor works. Two entities are extracted from an image. A finger in the scene is on the left entity, and the finger cursor is overlapping with the left one, which is highlighted. In d: it shows how touch cursor works. Two entities are extracted from an image. A finger on the touchscreen is on the left entity, and the touch cursor is overlapping with the left one, which is highlighted.", "levels": null, "corpus_id": 52937080, "sentences": ["This figure has 4 components, a, b, c, and d, respectively.", "In a: It shows how window cursor works.", "Two entities are extracted from an image, the window cursor is overlapping with the left one, which is highlighted.", "In b: it shows how vertical window cursor works.", "Two entities are extracted from an image, the vertical window cursor is overlapping with the left one, which is highlighted.", "In c: it shows how finger cursor works.", "Two entities are extracted from an image.", "A finger in the scene is on the left entity, and the finger cursor is overlapping with the left one, which is highlighted.", "In d: it shows how touch cursor works.", "Two entities are extracted from an image.", "A finger on the touchscreen is on the left entity, and the touch cursor is overlapping with the left one, which is highlighted."], "caption": "Figure 2. Illustrations of cursor-based interactions: (a) window cursor, (b) vertical window cursor, (c) \ufb01nger cursor, and (d) touch cursor.", "local_uri": ["4da6e24670af3ebb6c637c68233741f981120d04_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Investigating Cursor-based Interactions to Support Non-Visual Exploration in the Real World", "pdf_hash": "4da6e24670af3ebb6c637c68233741f981120d04", "year": 2018, "venue": "ASSETS", "alt_text": "This figure has 4 components, a, b, c, and d, respectively. Each one has a transparent region showing the cursor region, and the background is darker. In a: it shows a screenshot of the phone running window cursor. There are multiple bounding boxes on top of labels on the table. In b: it shows a screenshot of the phone running vertical window cursor. There are multiple bounding boxes on top of labels on the wall. In c: it shows a screenshot of the phone running finger cursor. There are multiple bounding boxes on top of labels on a prototype appliance control panel. In d: it shows a screenshot of the phone running touch cursor. There are multiple bounding boxes on top of texts on a document.", "levels": null, "corpus_id": 52937080, "sentences": ["This figure has 4 components, a, b, c, and d, respectively.", "Each one has a transparent region showing the cursor region, and the background is darker.", "In a: it shows a screenshot of the phone running window cursor.", "There are multiple bounding boxes on top of labels on the table.", "In b: it shows a screenshot of the phone running vertical window cursor.", "There are multiple bounding boxes on top of labels on the wall.", "In c: it shows a screenshot of the phone running finger cursor.", "There are multiple bounding boxes on top of labels on a prototype appliance control panel.", "In d: it shows a screenshot of the phone running touch cursor.", "There are multiple bounding boxes on top of texts on a document."], "caption": "Figure 3. Screenshots of cursor-based interactions in different use cases: (a) window cursor, (b) vertical window cursor, (c) \ufb01nger cursor, and (d) touch cursor. Colored bounding boxes show the recognized OCR results, the cursor region is drawn as a transparent overlay, while the rest of the image is covered with a semi-transparent dark overlay.", "local_uri": ["4da6e24670af3ebb6c637c68233741f981120d04_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Investigating Cursor-based Interactions to Support Non-Visual Exploration in the Real World", "pdf_hash": "4da6e24670af3ebb6c637c68233741f981120d04", "year": 2018, "venue": "ASSETS", "alt_text": "This figure has 4 components, a, b, c, and d, respectively.  In a, several boxes of different sizes are placed on the table. The boxes are labeled with texts. In b, several flyers containing texts are hanging on the wall. In c, it shows a prototype appliance control panel, with a table of numbers. The prototype control panel is placed on the table. In d, several text labels eaching containing 1-3 words are hanging on the walls.", "levels": null, "corpus_id": 52937080, "sentences": ["This figure has 4 components, a, b, c, and d, respectively.", "In a, several boxes of different sizes are placed on the table.", "The boxes are labeled with texts.", "In b, several flyers containing texts are hanging on the wall.", "In c, it shows a prototype appliance control panel, with a table of numbers.", "The prototype control panel is placed on the table.", "In d, several text labels eaching containing 1-3 words are hanging on the walls."], "caption": "Figure 4. Study setup comparing the cursor techniques across a series of tasks representative of blind people\u2019s daily routines, including (a) locating an object in the environment, (b) interpreting documents and signs, (c) manipulating an appliance interface, and (d) learning about their surroundings.", "local_uri": ["4da6e24670af3ebb6c637c68233741f981120d04_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "pdf_hash": "8bba1845a85370618cd5c400ec8be42208554549", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 1 - System diagram showing example interaction setup. A deaf or hard of hearing user interacts with an intelligent agent such as an Echo Show which uses speech recognition to process the input and register a command or otherwise nonverbal output.", "levels": null, "corpus_id": 51809667, "sentences": ["Figure 1 - System diagram showing example interaction setup.", "A deaf or hard of hearing user interacts with an intelligent agent such as an Echo Show which uses speech recognition to process the input and register a command or otherwise nonverbal output."], "caption": "Figure 1. Example interaction setup for our work. Here, a deaf or hard- of-hearing (DHH) user interacts via speech with an intelligent agent (e.g. on a smartphone or Amazon Echo Show device). Based on the output of a speech recognition process, the system either performs an action (e.g. turns on a light) or provides on-screen feedback. Because most au- tomatic speech recognition systems are trained on speech from hearing users, DHH users are often unable to use these devices effectively due to a \"deaf speech\" accent. Our work explores the viability of using current automatic and human-powered approaches to bridge this accessibility gap, and suggests directions and insights for future work to create more powerful and robust speech-based interfaces for DHH users.", "local_uri": ["8bba1845a85370618cd5c400ec8be42208554549_Image_001.png"], "annotated": false, "compound": false}
{"title": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "pdf_hash": "8bba1845a85370618cd5c400ec8be42208554549", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 2 - Histogram showing the distribution of word error rate of transcriptions generated by individual transcribers. Clips at intelligibility level 50 tended to have low word error rate, those at intelligibility level 30 tended to have high word error rate, and those at intelligibility level 40 had more evenly distributed word error rate.    Figure 3 - Bar graph showing average word error rates at the three intelligibility levels (30, 40, 50), for both automated and individual crowd worker approaches. Word error rate increased as clip intelligibility decreased. Individual transcribers outperformed the automated approach at each intelligibility level.", "levels": [[1], [3, 2], [-1], [-1], [-1]], "corpus_id": 51809667, "sentences": ["Figure 2 - Histogram showing the distribution of word error rate of transcriptions generated by individual transcribers.", "Clips at intelligibility level 50 tended to have low word error rate, those at intelligibility level 30 tended to have high word error rate, and those at intelligibility level 40 had more evenly distributed word error rate.", "Figure 3 - Bar graph showing average word error rates at the three intelligibility levels (30, 40, 50), for both automated and individual crowd worker approaches.", "Word error rate increased as clip intelligibility decreased.", "Individual transcribers outperformed the automated approach at each intelligibility level."], "caption": "Figure 2. Word error rate (WER) distribution of transcriptions gen- erated by individual crowd workers, separated by three levels of clip intelligibility. A lower WER is better and indicates a more accu- rate transcription. More intelligible clips tended to have lower WER, while less intelligible clips tended to have higher WER.", "local_uri": ["8bba1845a85370618cd5c400ec8be42208554549_Image_002.jpg", "8bba1845a85370618cd5c400ec8be42208554549_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": true}
{"title": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "pdf_hash": "8bba1845a85370618cd5c400ec8be42208554549", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 4 - Scatter plot plotting the word error rate of one randomly sampled crowd worker transcription on the X axis, and the average word error rate of the rest of the transcriptions they submitted on the Y axis. There was a positive correlation between the two groups across the collected data.", "levels": [[1], [3]], "corpus_id": 51809667, "sentences": ["Figure 4 - Scatter plot plotting the word error rate of one randomly sampled crowd worker transcription on the X axis, and the average word error rate of the rest of the transcriptions they submitted on the Y axis.", "There was a positive correlation between the two groups across the collected data."], "caption": "Figure 4. There was a positive correlation between the quality of one randomly sampled transcription generated by a unique crowd worker (sample), and the quality of the remainder of their transcriptions (outer). This suggests that fltering based on worker performance on one clip could be used to improve average transcription quality.", "local_uri": ["8bba1845a85370618cd5c400ec8be42208554549_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "pdf_hash": "8bba1845a85370618cd5c400ec8be42208554549", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 5 - Three line graphs showing word error rate of worker transcriptions in a 10-step iterative workflow, with step number on the X axis and word error rate on the Y axis. The first graph (intel 30) shows iteration fails to improve word error rate The second graph (intel 40) shows iteration produces transcriptions with significantly lower word error rate after 10 steps. The third graph (intel 50) shows iteration is able to produce transcriptions with average word error rate below 0.1, significantly outperforming automated and individual human approaches.", "levels": [[1], [3, 1], [3, 1]], "corpus_id": 51809667, "sentences": ["Figure 5 - Three line graphs showing word error rate of worker transcriptions in a 10-step iterative workflow, with step number on the X axis and word error rate on the Y axis.", "The first graph (intel 30) shows iteration fails to improve word error rate The second graph (intel 40) shows iteration produces transcriptions with significantly lower word error rate after 10 steps.", "The third graph (intel 50) shows iteration is able to produce transcriptions with average word error rate below 0.1, significantly outperforming automated and individual human approaches."], "caption": "Figure 5. WER of transcriptions from iterative versus baseline approaches: intel 30 (left), intel 40 (center), intel 50 (right). The quality of iteratively- generated transcriptions quickly surpassed those from automated and individual crowd worker approaches for intelligibility levels 40 and 50. However the iterative approach failed to produce better transcription at intelligibility level 30.", "local_uri": ["8bba1845a85370618cd5c400ec8be42208554549_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "pdf_hash": "8bba1845a85370618cd5c400ec8be42208554549", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 6 - Line graph with iteration step on the X axis and cosine similarity on the Y axis, with one line for each of three intelligibility levels (30, 40, 50). Worker transcriptions tended to converge resulting in overall increasing cosine similarity with each iteration step.    Figure 7 - Line graph showing word error rate of worker transcriptions in a 10-step iterative workflow for the Alexa commands dataset. ASR transcription word error rate is constant across all 10 steps (at about 0.84) and average individual transcription word error rate is constant across all 10 steps (at about 0.4). Both are represented by horizontal lines. The iterative approach produced significantly lower word error rates than both other approaches, with a downwards trending line as the number of iteration steps increased.", "levels": [[1], [3], [-1], [-1], [-1], [-1]], "corpus_id": 51809667, "sentences": ["Figure 6 - Line graph with iteration step on the X axis and cosine similarity on the Y axis, with one line for each of three intelligibility levels (30, 40, 50).", "Worker transcriptions tended to converge resulting in overall increasing cosine similarity with each iteration step.", "Figure 7 - Line graph showing word error rate of worker transcriptions in a 10-step iterative workflow for the Alexa commands dataset.", "ASR transcription word error rate is constant across all 10 steps (at about 0.84) and average individual transcription word error rate is constant across all 10 steps (at about 0.4).", "Both are represented by horizontal lines.", "The iterative approach produced significantly lower word error rates than both other approaches, with a downwards trending line as the number of iteration steps increased."], "caption": "", "local_uri": ["8bba1845a85370618cd5c400ec8be42208554549_Image_009.jpg", "8bba1845a85370618cd5c400ec8be42208554549_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "PaperPulse: An Integrated Approach for Embedding Electronics in Paper Designs", "pdf_hash": "0903c31166aebba9cbd0cc9959ac32b15ceb449a", "year": 2015, "venue": "CHI", "alt_text": "The three families of PaperPulse widgets: (a) Off-the-shelf slider; (b) Paper-membrane slider; (c) Pull-chain slider.", "levels": null, "corpus_id": 12220958, "sentences": ["The three families of PaperPulse widgets: (a) Off-the-shelf slider; (b) Paper-membrane slider; (c) Pull-chain slider."], "caption": "", "local_uri": ["0903c31166aebba9cbd0cc9959ac32b15ceb449a_Image_008.png"], "annotated": false, "compound": false}
{"title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities", "pdf_hash": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Types of Questions Week 1 - 31 Visual/Objective, 14 Fashion/Subjective, 6 Both Objective and Subjective Week 2 - 18 Visual, 3 Fashion, 4 Both", "levels": null, "corpus_id": 5995953, "sentences": ["Types of Questions Week 1 - 31 Visual/Objective, 14 Fashion/Subjective, 6 Both Objective and Subjective Week 2 - 18 Visual, 3 Fashion, 4 Both"], "caption": "Figure 2 - Most questions during the study asked for objective information such as identifying color", "local_uri": ["6539bd0b923c54c7c79281ab1821d09c65c92c6e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities", "pdf_hash": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 3A: Photo of shirt with question \"Could you describe the colors of this polo shirt to me please?\"", "levels": null, "corpus_id": 5995953, "sentences": ["Figure 3A: Photo of shirt with question \"Could you describe the colors of this polo shirt to me please?\""], "caption": "", "local_uri": ["6539bd0b923c54c7c79281ab1821d09c65c92c6e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities", "pdf_hash": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 3B: Photo of shirt and pants with question \"Does this polo shirt go with these trousers? And if you also wan to suggest waht color shoes would go with this particular shade of - well it's olive.\"", "levels": null, "corpus_id": 5995953, "sentences": ["Figure 3B: Photo of shirt and pants with question \"Does this polo shirt go with these trousers? And if you also wan to suggest waht color shoes would go with this particular shade of - well it's olive.\""], "caption": "Figure 3 \u2013 Example of basic question asked early in the study", "local_uri": ["6539bd0b923c54c7c79281ab1821d09c65c92c6e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities", "pdf_hash": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Photo of shirt with question \"What is the color of my grandson's shirt? He is turning 10 today and I want to make sure I can take him our in public.\"", "levels": null, "corpus_id": 5995953, "sentences": ["Photo of shirt with question \"What is the color of my grandson's shirt? He is turning 10 today and I want to make sure I can take him our in public.\""], "caption": "Figure 4 - A grandfather asks the volunteers about the appropriateness of his grandson\u2019s shirt", "local_uri": ["6539bd0b923c54c7c79281ab1821d09c65c92c6e_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities", "pdf_hash": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 5A: Photo of jeans with question \"Hi, please could you describe the color of these jeans to me, please?\"", "levels": null, "corpus_id": 5995953, "sentences": ["Figure 5A: Photo of jeans with question \"Hi, please could you describe the color of these jeans to me, please?\""], "caption": "", "local_uri": ["6539bd0b923c54c7c79281ab1821d09c65c92c6e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities", "pdf_hash": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 5B: Photo of object that looks like a blanket with question \"What colors are in this material?\"", "levels": null, "corpus_id": 5995953, "sentences": ["Figure 5B: Photo of object that looks like a blanket with question \"What colors are in this material?\""], "caption": "Figure 5 \u2013 Color questions misidentified by volunteers due to lighting, answering gray when blue was expected (A) and conflicting responses of red and dark orange (B)", "local_uri": ["6539bd0b923c54c7c79281ab1821d09c65c92c6e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 2. Different configurations of 39 targets; (A) shows a 10x10 interface before the Metropolis algorithm has run, in which 39 hexes have been randomly assigned a phoneme (blue) and the rest are unassigned (grey) (B) shows an interface with high efficiency after running the Metropolis algorithm, which has tightly clustered the targets (max efficiency noted: 39.655 WPM via Suggested AAC corpus). (C) has the 39 targets arranged in a consistent layout both before and after the optimization (max efficiency noted: 39.608 WPM via Suggested AAC corpus).", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 2.", "Different configurations of 39 targets; (A) shows a 10x10 interface before the Metropolis algorithm has run, in which 39 hexes have been randomly assigned a phoneme (blue) and the rest are unassigned (grey) (B) shows an interface with high efficiency after running the Metropolis algorithm, which has tightly clustered the targets (max efficiency noted: 39.655 WPM via Suggested AAC corpus). (C) has the 39 targets arranged in a consistent layout both before and after the optimization (max efficiency noted: 39.608 WPM via Suggested AAC corpus)."], "caption": "", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 1. Width (Wj) and distance (Dij) calculations for the interfaces developed and evaluated in this study. Starting phoneme i outlined in green, with target phoneme j outlined in red. Width is calculated as the distance between the two intersection points of the ideal path from the center of the starting phoneme through the center of the target phoneme (blue dots).", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 21745660, "sentences": ["Figure 1.", "Width (Wj) and distance (Dij) calculations for the interfaces developed and evaluated in this study.", "Starting phoneme i outlined in green, with target phoneme j outlined in red.", "Width is calculated as the distance between the two intersection points of the ideal path from the center of the starting phoneme through the center of the target phoneme (blue dots)."], "caption": "Figure 1. Width (Wj) and distance (Dij) calculations for the interfaces developed and evaluated in this study. Starting phoneme i outlined in green, with target phoneme j outlined in red. Width is calculated as the distance between the two intersection points of the ideal path from the center of the starting phoneme through the center of the target phoneme (blue dots).", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 3. Each panel shows the random walk through the interface space via the Metropolis algorithm with a different value for scalar k with T (arbitrarily) at 10. Each data point represents the WPM for an interface arrangement with a higher efficiency than the \u201ccurrent\u201d arrangement.", "levels": [[0], [1], [3]], "corpus_id": 21745660, "sentences": ["Figure 3.", "Each panel shows the random walk through the interface space via the Metropolis algorithm with a different value for scalar k with T (arbitrarily) at 10.", "Each data point represents the WPM for an interface arrangement with a higher efficiency than the \u201ccurrent\u201d arrangement."], "caption": "Figure 3. Each panel shows the random walk through the interface space via the Metropolis algorithm with a different value for scalar k with T (arbitrarily) at 10. Each data point represents the WPM for an interface arrangement with a", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 4. Metropolis algorithm. Top panel shows one iteration of the algorithm, consisting of 8 million random swaps and annealing system temperature. Panel B shows the typical process at the beginning of the algorithm, in which the efficiency quickly rises to the neighborhood of the final \u201coptimized\u201d version. Panel C shows how the annealing process (system temperature in green raising and lowering over time) allows the system to come out of local maxima in order to then approach the optimal solution (red dots).", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 4.", "Metropolis algorithm.", "Top panel shows one iteration of the algorithm, consisting of 8 million random swaps and annealing system temperature.", "Panel B shows the typical process at the beginning of the algorithm, in which the efficiency quickly rises to the neighborhood of the final \u201coptimized\u201d version.", "Panel C shows how the annealing process (system temperature in green raising and lowering over time) allows the system to come out of local maxima in order to then approach the optimal solution (red dots)."], "caption": "Figure 4. Metropolis algorithm. Top panel shows one iteration of the algorithm, consisting of 8 million random swaps and annealing system temperature. Panel B shows the typical process at the beginning of the algorithm, in which the efficiency quickly rises to the neighborhood of the final \u201coptimized\u201d version. Panel C shows how the annealing process (system temperature in green raising and lowering over time) allows the system to come out of local maxima in order to then approach the optimal solution (red dots).", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 5. Visual comparison of results of Metropolis algorithm. Left shows a random organization of phonemes; right shows an optimized interface. Width of lines between two targets represents the likelihood of transitioning between them in the AAC conversational phrases corpus.", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 5.", "Visual comparison of results of Metropolis algorithm.", "Left shows a random organization of phonemes; right shows an optimized interface.", "Width of lines between two targets represents the likelihood of transitioning between them in the AAC conversational phrases corpus."], "caption": "", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 6. Interfaces developed and evaluated in this study. Target colors show rough groupings of phonemes: simple vowels in green, complex vowels (diphthongs, r-colored vowels) in purple, fricatives and affricates in orange, stops in red, and liquids, nasals, and semivowels in blue.", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 6.", "Interfaces developed and evaluated in this study.", "Target colors show rough groupings of phonemes: simple vowels in green, complex vowels (diphthongs, r-colored vowels) in purple, fricatives and affricates in orange, stops in red, and liquids, nasals, and semivowels in blue."], "caption": "Figure 6. Interfaces developed and evaluated in this study. Target colors show rough groupings of phonemes: simple vowels in green, complex vowels (diphthongs, r-colored vowels) in purple, fricatives and affricates in orange, stops in red, and liquids, nasals, and semivowels in blue.", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 7. Similarity between phoneme-to-phoneme transition probabilities from different corpora (color represents r-value of Pearson\u2019s correlation between all diphone probabilities)", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 7.", "Similarity between phoneme-to-phoneme transition probabilities from different corpora (color represents r-value of Pearson\u2019s correlation between all diphone probabilities)"], "caption": "Figure 7. Similarity between phoneme-to-phoneme transition probabilities from different corpora (color represents r-value of Pearson\u2019s correlation between all diphone probabilities)", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 8. Efficiency in words per minute (WPM) for each interface tested with each corpus. Highlights on the diagonal show when the interface is being tested against the same corpus used to optimize its layout.", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 8.", "Efficiency in words per minute (WPM) for each interface tested with each corpus.", "Highlights on the diagonal show when the interface is being tested against the same corpus used to optimize its layout."], "caption": "Figure 8. Efficiency in words per minute (WPM) for each interface tested with each corpus. Highlights on the diagonal show when the interface is being tested against the same corpus used to optimize its layout.", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "pdf_hash": "a92be5bddb46da574738c6452582c9a5939958c3", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 9. Percent difference in efficiency compared to a random arrangement of phonemes.", "levels": null, "corpus_id": 21745660, "sentences": ["Figure 9. Percent difference in efficiency compared to a random arrangement of phonemes."], "caption": "Figure 9. Percent difference in efficiency compared to a random arrangement of phonemes.", "local_uri": ["a92be5bddb46da574738c6452582c9a5939958c3_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Interviews and Observation of Blind Software Developers at Work to Understand Code Navigation Challenges", "pdf_hash": "64ff9d92a6f9f6d411c5b2b718139dd7ac4f819b", "year": 2017, "venue": "ASSETS", "alt_text": "An image of a participant who is using JAWS with an 80-cell Brilliant Braille Display, while programming in Java using the Eclipse Integrated Development Environment (IDE). The image shows his programming environment including tools that he use to write software code.", "levels": null, "corpus_id": 10889054, "sentences": ["An image of a participant who is using JAWS with an 80-cell Brilliant Braille Display, while programming in Java using the Eclipse Integrated Development Environment (IDE).", "The image shows his programming environment including tools that he use to write software code."], "caption": "Figure 1. A participant using JAWS with an 80-cell Brilliant Braille Display, while programming in Java using the Eclipse Integrated Development Environment (IDE).", "local_uri": ["64ff9d92a6f9f6d411c5b2b718139dd7ac4f819b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Wearables and chairables: inclusive design of mobile input and output techniques for power wheelchair users", "pdf_hash": "063ca9ac50d63715a8795ba214d2f0290a100ab8", "year": 2014, "venue": "CHI", "alt_text": "diagram of a wheelchair user with highlighted areas representing potential locations for wearable and chairable devices.", "levels": null, "corpus_id": 6568979, "sentences": ["diagram of a wheelchair user with highlighted areas representing potential locations for wearable and chairable devices."], "caption": "Figure 1. We collaboratively designed new input and output techniques for and with power wheelchair users. Participants explored two primary form factors: wearable technology embedded in clothing or on skin, and chairable input and output devices installed on and around the wheelchair itself. These form factors maintained the original wheelchair dimensions, minimized obstructions, and were less noticeable.", "local_uri": ["063ca9ac50d63715a8795ba214d2f0290a100ab8_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Wearables and chairables: inclusive design of mobile input and output techniques for power wheelchair users", "pdf_hash": "063ca9ac50d63715a8795ba214d2f0290a100ab8", "year": 2014, "venue": "CHI", "alt_text": "Workflow diagram describing our process: 9 formative interviews with wheelchair users, followed by 4 focus groups with 30 cliniscal staff, then 7 design interviews with wheelchair users.", "levels": null, "corpus_id": 6568979, "sentences": ["Workflow diagram describing our process: 9 formative interviews with wheelchair users, followed by 4 focus groups with 30 cliniscal staff, then 7 design interviews with wheelchair users."], "caption": "Figure 2. Overview of our study procedures. We conducted formative interviews with power wheelchair users; held a series of design-oriented focus group sessions with clinical staff; and validated design recommendations from the focus groups with one-on-one design interviews.", "local_uri": ["063ca9ac50d63715a8795ba214d2f0290a100ab8_Image_002.png"], "annotated": false, "compound": false}
{"title": "Wearables and chairables: inclusive design of mobile input and output techniques for power wheelchair users", "pdf_hash": "063ca9ac50d63715a8795ba214d2f0290a100ab8", "year": 2014, "venue": "CHI", "alt_text": "Makey-Makey microcontroller board with several attachments, including play-doh, two types of buttons, and a metal bolt.", "levels": null, "corpus_id": 6568979, "sentences": ["Makey-Makey microcontroller board with several attachments, including play-doh, two types of buttons, and a metal bolt."], "caption": "Figure 3. Medium-fidelity prototype created with the MaKey MaKey microcontroller board and various button materials.", "local_uri": ["063ca9ac50d63715a8795ba214d2f0290a100ab8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Wearables and chairables: inclusive design of mobile input and output techniques for power wheelchair users", "pdf_hash": "063ca9ac50d63715a8795ba214d2f0290a100ab8", "year": 2014, "venue": "CHI", "alt_text": "Sketch of a quilt with different sections representing different types of controls for devices. Devices include games, TV, Phone, and home appliances.", "levels": null, "corpus_id": 6568979, "sentences": ["Sketch of a quilt with different sections representing different types of controls for devices.", "Devices include games, TV, Phone, and home appliances."], "caption": "Figure 4. New input devices proposed by our participants. (Left) P7, who is paralyzed below the neck, designed extra inputs around the straw of his sip and puff control. (Right) P9 designed a wearable quilt, containing conductive fabric controls for devices and applications at school and home.", "local_uri": ["063ca9ac50d63715a8795ba214d2f0290a100ab8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Wearables and chairables: inclusive design of mobile input and output techniques for power wheelchair users", "pdf_hash": "063ca9ac50d63715a8795ba214d2f0290a100ab8", "year": 2014, "venue": "CHI", "alt_text": "Six focus group participants working on a possible layout for wheelchair interface.", "levels": null, "corpus_id": 6568979, "sentences": ["Six focus group participants working on a possible layout for wheelchair interface."], "caption": "Figure 5. We conducted focus groups with therapists and rehabilitation technicians regarding possible layouts and functionality for chairable interfaces. Participants used sticky notes to indicate the location, size, and type of inputs.", "local_uri": ["063ca9ac50d63715a8795ba214d2f0290a100ab8_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Motor-impaired touchscreen interactions in the wild", "pdf_hash": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "year": 2014, "venue": "ASSETS", "alt_text": "screenshots take from the sudoku stimulus application, overlaying the possible errors and methods of refining the user intent classifications", "levels": [[-1]], "corpus_id": 21808388, "sentences": ["screenshots take from the sudoku stimulus application, overlaying the possible errors and methods of refining the user intent classifications"], "caption": "Figure 1 The Sudoku Game model refining the target intent for a wrong target error (left) and refining the gesture type of an unrecognized gesture error (right).", "local_uri": ["c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Motor-impaired touchscreen interactions in the wild", "pdf_hash": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "year": 2014, "venue": "ASSETS", "alt_text": "four boxplot charts showing how the tap xoffset, yoffset, duration and movement characteristics differed between participants", "levels": [[1]], "corpus_id": 21808388, "sentences": ["four boxplot charts showing how the tap xoffset, yoffset, duration and movement characteristics differed between participants"], "caption": "Figure 2 Boxplots of the overall tap x-offset, y-offset, duration and movement of each participant.", "local_uri": ["c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Motor-impaired touchscreen interactions in the wild", "pdf_hash": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "year": 2014, "venue": "ASSETS", "alt_text": "individual line graphs for each participant, tracing the variances of their daily average tap xoffset characteristics", "levels": [[1]], "corpus_id": 21808388, "sentences": ["individual line graphs for each participant, tracing the variances of their daily average tap xoffset characteristics"], "caption": "Figure 3 Line graphs showing the daily average tap x-offset of each participant.", "local_uri": ["c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Motor-impaired touchscreen interactions in the wild", "pdf_hash": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "year": 2014, "venue": "ASSETS", "alt_text": "bar graph showing the three gesture recognisers accuracies; baseline 85%, user specific 79.7% and session specific 95.1%", "levels": [[2, 1]], "corpus_id": 21808388, "sentences": ["bar graph showing the three gesture recognisers accuracies; baseline 85%, user specific 79.7% and session specific 95.1%"], "caption": "Figure 4 Classification accuracy of gesture recognizers for touch model conditions", "local_uri": ["c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Motor-impaired touchscreen interactions in the wild", "pdf_hash": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "year": 2014, "venue": "ASSETS", "alt_text": "bar graph showing the effect of the subject used to train the models, user specific group 59%, individual, 82.6%; session specific group 97%, individual 93.6% recognition accuracy", "levels": [[2, 1]], "corpus_id": 21808388, "sentences": ["bar graph showing the effect of the subject used to train the models, user specific group 59%, individual, 82.6%; session specific group 97%, individual 93.6% recognition accuracy"], "caption": "Figure 5 Classification accuracy of tap gesture recognizers for touch models by subject condition", "local_uri": ["c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Pictorial System Usability Scale (P-SUS): Developing an Instrument for Measuring Perceived Usability", "pdf_hash": "a6124d7836ac8b54f40f6e590b7535f6aa8277d6", "year": 2019, "venue": "CHI", "alt_text": "Figure 1. A pictorial version of the ten verbal SUS items with positive and negative extreme points and seven-point scale.", "levels": [[-1], [-1]], "corpus_id": 140238027, "sentences": ["Figure 1.", "A pictorial version of the ten verbal SUS items with positive and negative extreme points and seven-point scale."], "caption": "", "local_uri": ["a6124d7836ac8b54f40f6e590b7535f6aa8277d6_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences", "pdf_hash": "2439e7d6c24848b5304b860ec03e63414b95e286", "year": 2019, "venue": "CHI", "alt_text": "A grid of images showing a) a smart speaker equipped with a LIDAR at the base, b) a set of example signals as seen from the LIDAR, c) objects placed on top of a table with the system properly recognizing what they are, d) user touching the tabble with the system detecing what they are and where the touches are located, and e) a user in front of a table while the system estimates which direction the user is facing.", "levels": null, "corpus_id": 140321054, "sentences": ["A grid of images showing a) a smart speaker equipped with a LIDAR at the base, b) a set of example signals as seen from the LIDAR, c) objects placed on top of a table with the system properly recognizing what they are, d) user touching the tabble with the system detecing what they are and where the touches are located, and e) a user in front of a table while the system estimates which direction the user is facing."], "caption": "Figure 1. SurfaceSight enriches Internet-of-Things (IoT) experiences with touch, user, and object sensing. This is achieved by adding LIDAR to devices such as smart speakers (A). Next, we perform clustering and tracking (B), which unlocks novel interactive capabilities such as object recognition (C), touch input (D), and person tracking (E).", "local_uri": ["2439e7d6c24848b5304b860ec03e63414b95e286_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences", "pdf_hash": "2439e7d6c24848b5304b860ec03e63414b95e286", "year": 2019, "venue": "CHI", "alt_text": "Two image, top and bottom. The top image shows the raw point clouds, and the bottom image shows a normalized and interpolated point cloud.", "levels": [[-1], [-1]], "corpus_id": 140321054, "sentences": ["Two image, top and bottom.", "The top image shows the raw point clouds, and the bottom image shows a normalized and interpolated point cloud."], "caption": "Figure 3. For each cluster, we transform all points into a lo- cal coordinate system, rotate, and resample them for feature extraction.", "local_uri": ["2439e7d6c24848b5304b860ec03e63414b95e286_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences", "pdf_hash": "2439e7d6c24848b5304b860ec03e63414b95e286", "year": 2019, "venue": "CHI", "alt_text": "A grid of images depicting an example appication for a smart speaker (in a kitchen) equipped with SurfaceSight sensing capabilities. The system can proactively detect what objects are within the proximity, which can be used to assist the user for e.g., a recipe application.", "levels": null, "corpus_id": 140321054, "sentences": ["A grid of images depicting an example appication for a smart speaker (in a kitchen) equipped with SurfaceSight sensing capabilities.", "The system can proactively detect what objects are within the proximity, which can be used to assist the user for e.g., a recipe application."], "caption": "Figure 9. Recipe helper demo application. When a recipe is loaded, the system asks the user to retrieve necessary items (A), moving to the next step automatically upon detection (B). Contextual questions such as \u201chow many ounces are in this\u201d after putting down a measuring cup are possible (C). Swipe gestures move between recipe steps (D & E). When a user is finished with a step (e.g., mortar and pestle lifted from surface), the recipe can automatically advance (F).", "local_uri": ["2439e7d6c24848b5304b860ec03e63414b95e286_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Understanding quantified-selfers' practices in collecting and exploring personal data", "pdf_hash": "12bc05ca02519f15117f08e93d961138a805cc0d", "year": 2014, "venue": "CHI", "alt_text": "Figure 1. QS Video posts per year. Our dataset is colored in orange with vertical stripes.", "levels": [[0], [1], [1]], "corpus_id": 16116223, "sentences": ["Figure 1.", "QS Video posts per year.", "Our dataset is colored in orange with vertical stripes."], "caption": "Figure 1. QS Video posts per year. Our dataset is colored in orange with vertical stripes.", "local_uri": ["12bc05ca02519f15117f08e93d961138a805cc0d_Image_001.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "The image shows a cropped photo of a conference room table covered in small pieces of paper. White pieces with words are organized into columns with colored post-it notes as headers. Each level of the hierarchy is represented by its own color.", "levels": null, "corpus_id": 5046189, "sentences": ["The image shows a cropped photo of a conference room table covered in small pieces of paper.", "White pieces with words are organized into columns with colored post-it notes as headers.", "Each level of the hierarchy is represented by its own color."], "caption": "Figure 1. Affinity diagram with hierarchical clusters.", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "This image shows a cropped photo with a child on the left and a parent sitting opposite on the right. Puzzle pieces are spread on the floor between them.", "levels": null, "corpus_id": 5046189, "sentences": ["This image shows a cropped photo with a child on the left and a parent sitting opposite on the right.", "Puzzle pieces are spread on the floor between them."], "caption": "", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "This image shows a cropped photo with a child on the left and a parent sitting opposite on the right. Lego bricks are spread on the floor between them.", "levels": null, "corpus_id": 5046189, "sentences": ["This image shows a cropped photo with a child on the left and a parent sitting opposite on the right.", "Lego bricks are spread on the floor between them."], "caption": "", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "The image shows a cropped photo with a child sitting on the floor hunched over a tablet on his lap. Parent sits nearby and looks on.", "levels": null, "corpus_id": 5046189, "sentences": ["The image shows a cropped photo with a child sitting on the floor hunched over a tablet on his lap.", "Parent sits nearby and looks on."], "caption": "", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_004.jpg", "df49cdcba85d585e30c3798d1273b6439cde0345_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "The image shows a cropped photo with a child sitting on the floor hunched over a tablet on her lap. Parent sits nearby and looks on.", "levels": null, "corpus_id": 5046189, "sentences": ["The image shows a cropped photo with a child sitting on the floor hunched over a tablet on her lap.", "Parent sits nearby and looks on."], "caption": "", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_005.jpg", "df49cdcba85d585e30c3798d1273b6439cde0345_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "This image shows a cropped photo with a child on the right and a parent sitting opposite on the left. Pieces of paper with drawings lie on the floor between them.", "levels": null, "corpus_id": 5046189, "sentences": ["This image shows a cropped photo with a child on the right and a parent sitting opposite on the left.", "Pieces of paper with drawings lie on the floor between them."], "caption": "", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "This image shows a cropped photo with an aerial view of a child on one side and a parent sitting opposite. A board game board is open on the floor between them.", "levels": null, "corpus_id": 5046189, "sentences": ["This image shows a cropped photo with an aerial view of a child on one side and a parent sitting opposite.", "A board game board is open on the floor between them."], "caption": "", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Let's Play!: Digital and Analog Play between Preschoolers and Parents", "pdf_hash": "df49cdcba85d585e30c3798d1273b6439cde0345", "year": 2018, "venue": "CHI", "alt_text": "The image shows a cropped photo with a parent on the left and a child on the right holding a tablet on her lap. The child has ownership of the tablet, and wraps her left hand around one side while tapping the screen with her right hand. The parent sits opposite and reaches across the top of the screen to tap it from an upside-down orientation.", "levels": null, "corpus_id": 5046189, "sentences": ["The image shows a cropped photo with a parent on the left and a child on the right holding a tablet on her lap.", "The child has ownership of the tablet, and wraps her left hand around one side while tapping the screen with her right hand.", "The parent sits opposite and reaches across the top of the screen to tap it from an upside-down orientation."], "caption": "Figure 4. Mom and child each participate in an exploratory game in which multiple items on screen can be moved at once, such that they can each engage with the screen simultaneously. However, mom struggles to participate from an upside-down orientation and eventually stops playing.", "local_uri": ["df49cdcba85d585e30c3798d1273b6439cde0345_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Selecting an effective niche: an ecological view of the success of online communities", "pdf_hash": "dc3ec04644d37f6d54893db29992e15f9a7f38a8", "year": 2014, "venue": "CHI", "alt_text": "The figure shows a curvilinear relationship between the topic overlap of a given community with other communities and the activity level of this community. Low topic overlap and high topic overlap results in low activity level, while moderate topic overlap results in highest activity level.", "levels": [[3, 1], [3]], "corpus_id": 14759197, "sentences": ["The figure shows a curvilinear relationship between the topic overlap of a given community with other communities and the activity level of this community.", "Low topic overlap and high topic overlap results in low activity level, while moderate topic overlap results in highest activity level."], "caption": "Figure 1. Relationship between topic overlap and activity. We showed the quadratic prediction plots with 95% confident interval as well as the box plots.", "local_uri": ["dc3ec04644d37f6d54893db29992e15f9a7f38a8_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Selecting an effective niche: an ecological view of the success of online communities", "pdf_hash": "dc3ec04644d37f6d54893db29992e15f9a7f38a8", "year": 2014, "venue": "CHI", "alt_text": "The figure shows that the effects of topic overlap are stronger for communities that share members than for communities that do not share members.", "levels": null, "corpus_id": 14759197, "sentences": ["The figure shows that the effects of topic overlap are stronger for communities that share members than for communities that do not share members."], "caption": "", "local_uri": ["dc3ec04644d37f6d54893db29992e15f9a7f38a8_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Selecting an effective niche: an ecological view of the success of online communities", "pdf_hash": "dc3ec04644d37f6d54893db29992e15f9a7f38a8", "year": 2014, "venue": "CHI", "alt_text": "The figure shows that the effects of topic overlap are stronger with linked communities than with non-linked communities.", "levels": null, "corpus_id": 14759197, "sentences": ["The figure shows that the effects of topic overlap are stronger with linked communities than with non-linked communities."], "caption": "Figure 2. Moderating effects of shared members.             Figure 3. Moderating effects of content linking.", "local_uri": ["dc3ec04644d37f6d54893db29992e15f9a7f38a8_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Selecting an effective niche: an ecological view of the success of online communities", "pdf_hash": "dc3ec04644d37f6d54893db29992e15f9a7f38a8", "year": 2014, "venue": "CHI", "alt_text": "The figure shows that topic overlap with communities that do not share offline organizational affiliation has a curvilinear effect on activity level. However, topic overlap with communities that do share offline organizational affiliation, has a negative effect on activity level.", "levels": null, "corpus_id": 14759197, "sentences": ["The figure shows that topic overlap with communities that do not share offline organizational affiliation has a curvilinear effect on activity level.", "However, topic overlap with communities that do share offline organizational affiliation, has a negative effect on activity level."], "caption": "Figure 4. Moderating effects of offline organizational affiliation.", "local_uri": ["dc3ec04644d37f6d54893db29992e15f9a7f38a8_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "\"Collective Wisdom\": Inquiring into Collective Homes as a Site for HCI Design", "pdf_hash": "fa6740d9b0907cb38618f7fbfee222e86c439f33", "year": 2019, "venue": "CHI", "alt_text": "Three separate images of each participating collective home. From left to right, first image is an external picture of the Union Collective, the second image is of Club 16's shared living area, and the final image if an external night time photo of the back of Mountainview Collective.", "levels": null, "corpus_id": 140469461, "sentences": ["Three separate images of each participating collective home.", "From left to right, first image is an external picture of the Union Collective, the second image is of Club 16's shared living area, and the final image if an external night time photo of the back of Mountainview Collective."], "caption": "", "local_uri": ["fa6740d9b0907cb38618f7fbfee222e86c439f33_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating importance of facial expression in american sign language and pidgin signed english animations", "pdf_hash": "d85d3db46b485963a5a4279acd6a465a85f2b79e", "year": 2011, "venue": "ASSETS", "alt_text": "Image of the animated signing character used in the Pidgin Signed English animations (young woman of Asian descent). Close-up view of eight facial expressions used during the study: neutral face, continuing face (looking up and to the left), contrastive emphasis (head tilted forward and to the side, eye brows raised slightly), incredulous emphasis (head tilted to the side, eyes rolled upward), anger (angry emotional face), sadness (sad emotional face), yes / no question (head tilted forward and to the side with eye brows raised), and W. H. question face (head tilted forward with brows downward, looks a little angry).", "levels": null, "corpus_id": 13414696, "sentences": ["Image of the animated signing character used in the Pidgin Signed English animations (young woman of Asian descent).", "Close-up view of eight facial expressions used during the study: neutral face, continuing face (looking up and to the left), contrastive emphasis (head tilted forward and to the side, eye brows raised slightly), incredulous emphasis (head tilted to the side, eyes rolled upward), anger (angry emotional face), sadness (sad emotional face), yes / no question (head tilted forward and to the side with eye brows raised), and W. H. question face (head tilted forward with brows downward, looks a little angry)."], "caption": "", "local_uri": ["d85d3db46b485963a5a4279acd6a465a85f2b79e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating importance of facial expression in american sign language and pidgin signed english animations", "pdf_hash": "d85d3db46b485963a5a4279acd6a465a85f2b79e", "year": 2011, "venue": "ASSETS", "alt_text": "Image of the animated signing character used in the American Sign Language animations (young man with blond hair). Close-up view of eight facial expressions used during the study: neutral face, continuing face (looking up and to the right, lips pursed), contrastive emphasis (head tilted forward and to the side, eye brows downward slightly), incredulous emphasis (head tilted to the side, eyes rolled upward), anger (angry emotional face), sadness (sad emotional face), yes / no question (head tilted forward and to the side with eye brows raised), and W. H. question face (head tilted forward with brows downward).", "levels": null, "corpus_id": 13414696, "sentences": ["Image of the animated signing character used in the American Sign Language animations (young man with blond hair).", "Close-up view of eight facial expressions used during the study: neutral face, continuing face (looking up and to the right, lips pursed), contrastive emphasis (head tilted forward and to the side, eye brows downward slightly), incredulous emphasis (head tilted to the side, eyes rolled upward), anger (angry emotional face), sadness (sad emotional face), yes / no question (head tilted forward and to the side with eye brows raised), and W. H. question face (head tilted forward with brows downward)."], "caption": "", "local_uri": ["d85d3db46b485963a5a4279acd6a465a85f2b79e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "A Large Inclusive Study of Human Listening Rates", "pdf_hash": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "year": 2018, "venue": "CHI", "alt_text": "Line graph of VoiceOver speed (x-axis) vs. Words per Minute (y-axis), with two segmented curves: one for rhyme test questions, and one for transcription and yes/no questions. The exact values, in CSV format, are:  VoiceOver speed, Words per Minute - Rhyme test questions, Words per minute - Transcription and yes/no questions  14, 101, 97  29, 139, 132  43, 233, 208  57, 357, 311  71, 501, 402  86, 651, 462  100, 766, 506", "levels": [[1], [2]], "corpus_id": 5040904, "sentences": ["Line graph of VoiceOver speed (x-axis) vs. Words per Minute (y-axis), with two segmented curves: one for rhyme test questions, and one for transcription and yes/no questions.", "The exact values, in CSV format, are:  VoiceOver speed, Words per Minute - Rhyme test questions, Words per minute - Transcription and yes/no questions  14, 101, 97  29, 139, 132  43, 233, 208  57, 357, 311  71, 501, 402  86, 651, 462  100, 766, 506"], "caption": "Figure 2: VoiceOver speeds translated into words per minute, for the rhyme test questions (words) and for the transcription and yes/no questions (sentences). Typical human speaking rate 120-180 WPM corresponds to VoiceOver range 24-38.", "local_uri": ["90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "A Large Inclusive Study of Human Listening Rates", "pdf_hash": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "year": 2018, "venue": "CHI", "alt_text": "Histogram of Listening Rates for all participants. The exact values, in CSV format, are:  Listening Rate, % Participants  0-14, 5.74  14-29, 8.17  29-43, 8.17  43-57, 22.96  57-71, 28.48  71-86, 18.54  86-100, 6.40  100-114, 1.55", "levels": null, "corpus_id": 5040904, "sentences": ["Histogram of Listening Rates for all participants.", "The exact values, in CSV format, are:  Listening Rate, % Participants  0-14, 5.74  14-29, 8.17  29-43, 8.17  43-57, 22.96  57-71, 28.48  71-86, 18.54  86-100, 6.40  100-114, 1.55"], "caption": "Figure 3: Histogram of Listening Rates for all participants.\u200c", "local_uri": ["90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "A Large Inclusive Study of Human Listening Rates", "pdf_hash": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "year": 2018, "venue": "CHI", "alt_text": "Histogram of Listening Rates for all participants, separated into sighted and visually impaired groups. The exact values, in CSV format, are:  Listening Rate, % Participants - sighted, % Participants - visually impaired  0-14, 3.87, 9.79  14-29, 9.03, 6.29  29-43, 7.42, 9.79  43-56, 27.74, 12.59  57-71, 33.87, 16.78  71-86, 14.19, 27.97  86-100, 3.55, 12.59  >100, 0.32, 4.20", "levels": null, "corpus_id": 5040904, "sentences": ["Histogram of Listening Rates for all participants, separated into sighted and visually impaired groups.", "The exact values, in CSV format, are:  Listening Rate, % Participants - sighted, % Participants - visually impaired  0-14, 3.87, 9.79  14-29, 9.03, 6.29  29-43, 7.42, 9.79  43-56, 27.74, 12.59  57-71, 33.87, 16.78  71-86, 14.19, 27.97  86-100, 3.55, 12.59  >100, 0.32, 4.20"], "caption": "Figure 4: Histogram of Listening Rates, separated into visually impaired and sighted participant groups.", "local_uri": ["90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "A Large Inclusive Study of Human Listening Rates", "pdf_hash": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "year": 2018, "venue": "CHI", "alt_text": "Line plot of Age (x-axis) vs. Listening Rate (y-axis) with two segmented curves: one for sighted participants, and one for visually impaired participants. The exact values, in CSV format, are:  Age (years), Listening Rate - sighted, Listening Rate - visually impaired  0-15, 49.86 (std err 7.04), 72.16 (std err 5.25)  15-30, 56.15 (std err 1.46), 70.97 (std err 3.36)  30-45, 59.04 (std err 2.39), 66.78 (std err 3.68)  45-60, 48.97 (std err 3.08), 38.51 (std err 4.70)  60-75, 55.19 (std err 3.38), 29.02 (std err 5.30)", "levels": [[1], [2]], "corpus_id": 5040904, "sentences": ["Line plot of Age (x-axis) vs. Listening Rate (y-axis) with two segmented curves: one for sighted participants, and one for visually impaired participants.", "The exact values, in CSV format, are:  Age (years), Listening Rate - sighted, Listening Rate - visually impaired  0-15, 49.86 (std err 7.04), 72.16 (std err 5.25)  15-30, 56.15 (std err 1.46), 70.97 (std err 3.36)  30-45, 59.04 (std err 2.39), 66.78 (std err 3.68)  45-60, 48.97 (std err 3.08), 38.51 (std err 4.70)  60-75, 55.19 (std err 3.38), 29.02 (std err 5.30)"], "caption": "Figure 5: Plot of age vs. Listening Rate, for visually impaired and sighted groups.", "local_uri": ["90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "A Large Inclusive Study of Human Listening Rates", "pdf_hash": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "year": 2018, "venue": "CHI", "alt_text": "Scatterplot of Age (years) vs. Screen Reader Adoption Age (years). Each dot represents one of our 123 visually impaired participants. Dots are colored by Listening Rate on a grayscale, with white being 0 and black being 100. Dots appear clustered along a line with slope just under 1. Darker dots are clustered at the bottom-left (low age and low screen reader adoption age), and become lighter moving along the line to the upper-right (high age and high screen reader adoption age). The upper-left  (low age and high adoption age) and lower-right areas (high age and low adoption age) are empty.", "levels": [[1], [1], [1], [3, 2], [3, 1], [2]], "corpus_id": 5040904, "sentences": ["Scatterplot of Age (years) vs. Screen Reader Adoption Age (years).", "Each dot represents one of our 123 visually impaired participants.", "Dots are colored by Listening Rate on a grayscale, with white being 0 and black being 100.", "Dots appear clustered along a line with slope just under 1.", "Darker dots are clustered at the bottom-left (low age and low screen reader adoption age), and become lighter moving along the line to the upper-right (high age and high screen reader adoption age).", "The upper-left  (low age and high adoption age) and lower-right areas (high age and low adoption age) are empty."], "caption": "\u2212", "local_uri": ["90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Designing Interactions for 3D Printed Models with Blind People", "pdf_hash": "76a0bb408094431abf668e6d1a12bb1944e8ccb6", "year": 2017, "venue": "ASSETS", "alt_text": "The four identified hand postures. From left to right: Grabbing, Stabilizing, Diverging, and Converging.", "levels": null, "corpus_id": 7584191, "sentences": ["The four identified hand postures.", "From left to right: Grabbing, Stabilizing, Diverging, and Converging."], "caption": "Figure 4. The four identified hand postures. From left to right: Grabbing, Stabilizing, Diverging, and Converging.", "local_uri": ["76a0bb408094431abf668e6d1a12bb1944e8ccb6_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Designing Interactions for 3D Printed Models with Blind People", "pdf_hash": "76a0bb408094431abf668e6d1a12bb1944e8ccb6", "year": 2017, "venue": "ASSETS", "alt_text": "The four identified single-finger gestures. From left to right: Pointing, Striking, Index Scanning, and Thumb Scanning. The movements of dynamic gestures (Striking, Index Scanning and Thumb Scanning) are marked with arrows.", "levels": null, "corpus_id": 7584191, "sentences": ["The four identified single-finger gestures.", "From left to right: Pointing, Striking, Index Scanning, and Thumb Scanning.", "The movements of dynamic gestures (Striking, Index Scanning and Thumb Scanning) are marked with arrows."], "caption": "", "local_uri": ["76a0bb408094431abf668e6d1a12bb1944e8ccb6_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Designing Interactions for 3D Printed Models with Blind People", "pdf_hash": "76a0bb408094431abf668e6d1a12bb1944e8ccb6", "year": 2017, "venue": "ASSETS", "alt_text": "The four identified multi-finger gestures. From left to right: Pinching, Hovering, Following, and Rubbing. The movements of dynamic gestures (Following and Rubbing) are marked with arrows.", "levels": null, "corpus_id": 7584191, "sentences": ["The four identified multi-finger gestures.", "From left to right: Pinching, Hovering, Following, and Rubbing.", "The movements of dynamic gestures (Following and Rubbing) are marked with arrows."], "caption": "", "local_uri": ["76a0bb408094431abf668e6d1a12bb1944e8ccb6_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Designing Interactions for 3D Printed Models with Blind People", "pdf_hash": "76a0bb408094431abf668e6d1a12bb1944e8ccb6", "year": 2017, "venue": "ASSETS", "alt_text": "Two figures. The left figure shows a long stick on a globe model. In the right figure, a participant used the long stick as a handle to stabilize and manipulate the model.", "levels": null, "corpus_id": 7584191, "sentences": ["Two figures.", "The left figure shows a long stick on a globe model.", "In the right figure, a participant used the long stick as a handle to stabilize and manipulate the model."], "caption": "Figure 7. Participants took advantage of (a) the long stick we added on each model. For example, (b) P3 used it as a handle to stabilize and manipulate the model.", "local_uri": ["76a0bb408094431abf668e6d1a12bb1944e8ccb6_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Rewire: Interface Design Assistance from Examples", "pdf_hash": "d2db89e9215d37867996fc85efa5bf0890c502ed", "year": 2018, "venue": "CHI", "alt_text": "This figure shows the interface design screenshots that designers re-created for our study. The screenshots all have the components of a login and signup mobile app screen. The top left of the figure shows the original screenshot with a Lighthouse header, a slider component with a lighthouse logo, a text label below it, a Login and Sign Up button, and a widget below for moving the slider. To the right are 3 variations of the Lighthouse app, all with different themes, colors, and modified layout, but with the same number and approximately the same types of shapes. Below the original and variations, the figure shows the design specifications for the original and variations. The design specifications are the same visually but have modified text labels so that the designers will know that they have to change the text in their final output.", "levels": null, "corpus_id": 3396901, "sentences": ["This figure shows the interface design screenshots that designers re-created for our study.", "The screenshots all have the components of a login and signup mobile app screen.", "The top left of the figure shows the original screenshot with a Lighthouse header, a slider component with a lighthouse logo, a text label below it, a Login and Sign Up button, and a widget below for moving the slider.", "To the right are 3 variations of the Lighthouse app, all with different themes, colors, and modified layout, but with the same number and approximately the same types of shapes.", "Below the original and variations, the figure shows the design specifications for the original and variations.", "The design specifications are the same visually but have modified text labels so that the designers will know that they have to change the text in their final output."], "caption": "(b) Variations\u221a", "local_uri": ["d2db89e9215d37867996fc85efa5bf0890c502ed_Image_036.jpg", "d2db89e9215d37867996fc85efa5bf0890c502ed_Image_037.jpg"], "annotated": false, "compound": true}
{"title": "Tracked Speech-To-Text Display: Enhancing Accessibility and Readability of Real-Time Speech-To-Text", "pdf_hash": "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 1: A snapshot of a classroom, in which there is a laptop showing the speech-to-text display, and visuals. A trace of a student's gaze path shifting between the speech-to-text display and lecture visuals (slides) is shown.  Figure 2: A snapshot of a classroom with multiple visuals - student, teacher, slides, captions and whiteboard.", "levels": null, "corpus_id": 17553133, "sentences": ["Figure 1: A snapshot of a classroom, in which there is a laptop showing the speech-to-text display, and visuals.", "A trace of a student's gaze path shifting between the speech-to-text display and lecture visuals (slides) is shown.", "Figure 2: A snapshot of a classroom with multiple visuals - student, teacher, slides, captions and whiteboard."], "caption": "", "local_uri": ["4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_001.jpg", "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_002.jpg"], "annotated": false, "compound": true}
{"title": "Tracked Speech-To-Text Display: Enhancing Accessibility and Readability of Real-Time Speech-To-Text", "pdf_hash": "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 3: A snapshot of a TSD system - projector, bag, tripod and Kinect.  Figure 4: A snapshot of the TSD system in a class where the teacher is standing in front of a whiteboard and explaining a diagram. The  captions are projected above the teacher.", "levels": null, "corpus_id": 17553133, "sentences": ["Figure 3: A snapshot of a TSD system - projector, bag, tripod and Kinect.  Figure 4: A snapshot of the TSD system in a class where the teacher is standing in front of a whiteboard and explaining a diagram.", "The  captions are projected above the teacher."], "caption": "", "local_uri": ["4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_003.jpg", "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Tracked Speech-To-Text Display: Enhancing Accessibility and Readability of Real-Time Speech-To-Text", "pdf_hash": "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 5: A screenshot of captions - showing slides and the captions next to it.  Figure 6: A screenshot of the TSD program interface .. showing the variable number of lines and control features.", "levels": null, "corpus_id": 17553133, "sentences": ["Figure 5: A screenshot of captions - showing slides and the captions next to it.  Figure 6: A screenshot of the TSD program interface .. showing the variable number of lines and control features."], "caption": "", "local_uri": ["4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Tracked Speech-To-Text Display: Enhancing Accessibility and Readability of Real-Time Speech-To-Text", "pdf_hash": "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9", "year": 2015, "venue": "ASSETS", "alt_text": "A screenshot of the TSD interface showing the variable number of lines and other features.", "levels": null, "corpus_id": 17553133, "sentences": ["A screenshot of the TSD interface showing the variable number of lines and other features."], "caption": "Figure 6: A snapshot of the application interface that can adjust the display location and size.", "local_uri": ["4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Tracked Speech-To-Text Display: Enhancing Accessibility and Readability of Real-Time Speech-To-Text", "pdf_hash": "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9", "year": 2015, "venue": "ASSETS", "alt_text": "On the left, a diagram showing the static display, and on the right, a diagram showing the dynamic display.", "levels": null, "corpus_id": 17553133, "sentences": ["On the left, a diagram showing the static display, and on the right, a diagram showing the dynamic display."], "caption": "", "local_uri": ["4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_007.jpg", "4f83aa86bd97ef91cc24ff47c07d7b0fea9453b9_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Laughing is Scary, but Farting is Cute: A Conceptual Model of Children's Perspectives of Creepy Technologies", "pdf_hash": "c503544647b2e95d1bc2da68b4b1b5c4d67a9816", "year": 2019, "venue": "CHI", "alt_text": "Figure 1 shows 7 toys and technologies we used in the participatory design sessions. 1A is K-2SO, a dark, slim robot with small eyes. 1B is Pusheen, a stuffed and soft cat toy. 1C is Pepper, a humanoid robot about the size of a small child. 1D is Anki Cozmo, a small robot with wheel and an LCD screen as a face. 1E is Woobo, a soft stuffed toy with an LCD touchscreen as a face. 1F is Luvabella, a doll that reacts to touch. 1G is Maslo, a diary app represented by a large black dot.", "levels": null, "corpus_id": 140241448, "sentences": ["Figure 1 shows 7 toys and technologies we used in the participatory design sessions.", "1A is K-2SO, a dark, slim robot with small eyes.", "1B is Pusheen, a stuffed and soft cat toy.", "1C is Pepper, a humanoid robot about the size of a small child.", "1D is Anki Cozmo, a small robot with wheel and an LCD screen as a face.", "1E is Woobo, a soft stuffed toy with an LCD touchscreen as a face.", "1F is Luvabella, a doll that reacts to touch.", "1G is Maslo, a diary app represented by a large black dot."], "caption": "", "local_uri": ["c503544647b2e95d1bc2da68b4b1b5c4d67a9816_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Laughing is Scary, but Farting is Cute: A Conceptual Model of Children's Perspectives of Creepy Technologies", "pdf_hash": "c503544647b2e95d1bc2da68b4b1b5c4d67a9816", "year": 2019, "venue": "CHI", "alt_text": "Figure 2 is our conceptual model of children's perceptions of creepy technologies. A left box is labelled \"Creepy signals from technologies\". There are five signals: mimicry, lack of control, unpredictability, deception vs. transparency, and ominous physical appearance. The right box is labeled \"Children's responses to creepy technologies\". Two responses are shown: loss of attachment and physical fears. A mediator connects the left and right boxes. The mediator is \"parents\".", "levels": null, "corpus_id": 140241448, "sentences": ["Figure 2 is our conceptual model of children's perceptions of creepy technologies.", "A left box is labelled \"Creepy signals from technologies\".", "There are five signals: mimicry, lack of control, unpredictability, deception vs. transparency, and ominous physical appearance.", "The right box is labeled \"Children's responses to creepy technologies\".", "Two responses are shown: loss of attachment and physical fears.", "A mediator connects the left and right boxes.", "The mediator is \"parents\"."], "caption": "", "local_uri": ["c503544647b2e95d1bc2da68b4b1b5c4d67a9816_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Design and evaluation of classifier for identifying sign language videos in video sharing sites", "pdf_hash": "522efe48374fc8347598dc6126969b6989a5b865", "year": 2012, "venue": "ASSETS '12", "alt_text": "Frames from videos returned from the query \"sign language\" that are not in sign language.", "levels": null, "corpus_id": 14666925, "sentences": ["Frames from videos returned from the query \"sign language\" that are not in sign language."], "caption": "Figure 2. Videos returned on the first page of results for query \u201csign language\u201d that are not in sign language. Two are for songs with \u201csign language\u201d in the title, one is on sign language recognition research, and one refers to the language in signs.", "local_uri": ["522efe48374fc8347598dc6126969b6989a5b865_Image_002.png"], "annotated": false, "compound": false}
{"title": "Design and evaluation of classifier for identifying sign language videos in video sharing sites", "pdf_hash": "522efe48374fc8347598dc6126969b6989a5b865", "year": 2012, "venue": "ASSETS '12", "alt_text": "Image showing different stages of video processing. The first shows the frame from the video with a box indicating the results of face detection. The second shows the background model generated for the video at this point. The third shows the intermediate foreground, which is computed as the difference between the current frame and the background model. The fourth frame shows the final foreground which has removed portions of the foreground too small to be hands and/or arms.", "levels": null, "corpus_id": 14666925, "sentences": ["Image showing different stages of video processing.", "The first shows the frame from the video with a box indicating the results of face detection.", "The second shows the background model generated for the video at this point.", "The third shows the intermediate foreground, which is computed as the difference between the current frame and the background model.", "The fourth frame shows the final foreground which has removed portions of the foreground too small to be hands and/or arms."], "caption": "Figure 3. (a) the incoming frame of the video, (b) the final foreground image,", "local_uri": ["522efe48374fc8347598dc6126969b6989a5b865_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Design and evaluation of classifier for identifying sign language videos in video sharing sites", "pdf_hash": "522efe48374fc8347598dc6126969b6989a5b865", "year": 2012, "venue": "ASSETS '12", "alt_text": "Frames from videos that were selected because they are visually similar to sign language video.", "levels": null, "corpus_id": 14666925, "sentences": ["Frames from videos that were selected because they are visually similar to sign language video."], "caption": "Figure 5. Examples of non-Sign Language videos that are visually similar to sign language videos and thus likely false- positives for the classifier", "local_uri": ["522efe48374fc8347598dc6126969b6989a5b865_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality", "pdf_hash": "1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8", "year": 2018, "venue": "CHI", "alt_text": "The left shows a screenshot from Steep, a snowboarding game. The path of the snowboarder is shown with a white line with dotted spheres, and checkpoints that are red dotted arches. The right shows a screenshot from a Lowe's In-Store Navigation concept video, where the path to a product is highlighted as a yellow arrow on the ground, and ends with a white placard with more information about the product.", "levels": [[-1], [-1], [-1]], "corpus_id": 3330180, "sentences": ["The left shows a screenshot from Steep, a snowboarding game.", "The path of the snowboarder is shown with a white line with dotted spheres, and checkpoints that are red dotted arches.", "The right shows a screenshot from a Lowe's In-Store Navigation concept video, where the path to a product is highlighted as a yellow arrow on the ground, and ends with a white placard with more information about the product."], "caption": "Figure 1. These Go interaction cues provide navigation guidance along a path. Steep (left) [L15] displays a dotted line in the course; Lowe\u2019s In-Store Navigation, a mobile AR app (right) [14], uses a bold yellow line.", "local_uri": ["1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality", "pdf_hash": "1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8", "year": 2018, "venue": "CHI", "alt_text": "A 3x4 table of example images from video games. The columns represent each of the first dimension of visual cues represents task/purpose (discover, look, go); the rows represent each of the markedness, the second dimension (subtle, emphasized, integrated, overlaid).", "levels": null, "corpus_id": 3330180, "sentences": ["A 3x4 table of example images from video games.", "The columns represent each of the first dimension of visual cues represents task/purpose (discover, look, go); the rows represent each of the markedness, the second dimension (subtle, emphasized, integrated, overlaid)."], "caption": "Figure 2. Screenshots from some of the games from our sample set: (a) [L10], (b) [L12], (c) [L13], (d) [L3], (e) [L11], (f) [L6], (g) [L14], (h) [L16], (i) [L15], (j) [L4], (k) [L5], (l) [L2].", "local_uri": ["1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality", "pdf_hash": "1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8", "year": 2018, "venue": "CHI", "alt_text": "Ten different images of possible AR interventions for guiding people through a door at an intersection where there are three doors.", "levels": null, "corpus_id": 3330180, "sentences": ["Ten different images of possible AR interventions for guiding people through a door at an intersection where there are three doors."], "caption": "Figure 4. Variations on an imaginary AR interface that provides a Go cue to the door on the left.", "local_uri": ["1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality", "pdf_hash": "1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8", "year": 2018, "venue": "CHI", "alt_text": "Two images from Reitmayr and Schmalstieg's AR tour guide system. On the left, a pillar represents a waypoint, with yellow paths on the ground representing where to go. On the right, an information placard is presented next to a landmark, and the landmark is circled.", "levels": null, "corpus_id": 3330180, "sentences": ["Two images from Reitmayr and Schmalstieg's AR tour guide system.", "On the left, a pillar represents a waypoint, with yellow paths on the ground representing where to go.", "On the right, an information placard is presented next to a landmark, and the landmark is circled."], "caption": "Figure 5. Reitmayr and Schmalstieg\u2019s AR tour guide system.", "local_uri": ["1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality", "pdf_hash": "1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8", "year": 2018, "venue": "CHI", "alt_text": "A screenshot from the Lumin Project showing some interaction cues placed atop the mummy.", "levels": null, "corpus_id": 3330180, "sentences": ["A screenshot from the Lumin Project showing some interaction cues placed atop the mummy."], "caption": "Figure 6. The Lumin Project gives museum-goers an AR experience for navigating exhibits and learning about artefacts. The AR view of artefacts provides Integrated Discover cues for more information.", "local_uri": ["1fbb3172d3d5d26a20a2dfeecc57fb230af90dc8_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "In this figure, there are five images in a row.  From left to right, the first image shows a picture of a user taking a photo of an aspirin bottle in proximity to his left hand. The second image shows the user input, which is the actual photo the user takes. The third image shows the output of the hand segmentation; the output is overlaid with the green color. The fourth image shows the output of the object localization model; this output is overlaid with the blue color and has a bounding box located around the object according to the localization output.  The last image shows the image of the aspirin bottle cropped by our object localization model.", "levels": null, "corpus_id": 78087743, "sentences": ["In this figure, there are five images in a row.", "From left to right, the first image shows a picture of a user taking a photo of an aspirin bottle in proximity to his left hand.", "The second image shows the user input, which is the actual photo the user takes.", "The third image shows the output of the hand segmentation; the output is overlaid with the green color.", "The fourth image shows the output of the object localization model; this output is overlaid with the blue color and has a bounding box located around the object according to the localization output.", "The last image shows the image of the aspirin bottle cropped by our object localization model."], "caption": "College of Information Studies University of Maryland College Park, MD, USA hernisa@umd.edu", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_001.png"], "annotated": false, "compound": false}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "On the left of the figure, there are an example image, its hand mask, and the output of our hand segmentation model.  The output of the hand recognition is overlaid with the green color and shows that the model correctly segments the hand from the input image. The right side of the figure lists an example image, its heatmap annotation for the object center, and the output of our object localization model.  The output is overlaid with the blue color and shows that the model appropriately estimates the center locations of the object.", "levels": null, "corpus_id": 78087743, "sentences": ["On the left of the figure, there are an example image, its hand mask, and the output of our hand segmentation model.", "The output of the hand recognition is overlaid with the green color and shows that the model correctly segments the hand from the input image.", "The right side of the figure lists an example image, its heatmap annotation for the object center, and the output of our object localization model.", "The output is overlaid with the blue color and shows that the model appropriately estimates the center locations of the object."], "caption": "", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_006.jpg", "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "This figure illustrates the architecture of our hand-guided object recognizer that consists of an object localization model and an object classification model.  To build our system, we first develop a hand segmentation convolutional neural network that is not yet part of our system (shown in the top of the figure).  From this hand segmentation model, we then build the object localization convolutional neural network that is located below the hand segmentation model.  The object classification model, which is located next to the localization model, takes as input the output of the localization model to predict a label of an object.", "levels": null, "corpus_id": 78087743, "sentences": ["This figure illustrates the architecture of our hand-guided object recognizer that consists of an object localization model and an object classification model.", "To build our system, we first develop a hand segmentation convolutional neural network that is not yet part of our system (shown in the top of the figure).", "From this hand segmentation model, we then build the object localization convolutional neural network that is located below the hand segmentation model.", "The object classification model, which is located next to the localization model, takes as input the output of the localization model to predict a label of an object."], "caption": "Figure 2: In our approach, a hand segmentation model (Step I) is fne-tuned to estimate the center of the object in prox- imity to the hand (Step II). A bounding box, placed in that center is used to isolate the object and crop the image, which is then passed to the object classifcation model (Step III).", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_008.png"], "annotated": false, "compound": false}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "In this figure, we select eight examples that include an object of interest in proximity to the user's hand from the GTEA, Intel Egocentric Vision, EgoHands, Glassense-Vision, VizWiz, and our benchmark egocentric datasets.", "levels": [[-1]], "corpus_id": 78087743, "sentences": ["In this figure, we select eight examples that include an object of interest in proximity to the user's hand from the GTEA, Intel Egocentric Vision, EgoHands, Glassense-Vision, VizWiz, and our benchmark egocentric datasets."], "caption": "Figure 4: Examples from each dataset. Glassense-Vision, VizWiz, and our benchmark examples are selected to include hands.", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_080.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "In this figure, we list the nineteen objects used for our data collection.  Following is the list of the objects: oregano spice, grill salt, can of mandarin oranges, can of chicken soup, dr pepper can, mountain dew can, sprite bottle, regular coke bottle, diet coke bottle, baked cheetos, baked lays chip, great grains cereal, cheerios cereal, extra dry skin moisturizer, spf55 sunscreen, aspirin bottle, airborne gummies bottle, hand soap, hand sanitizer.", "levels": [[-1], [-1]], "corpus_id": 78087743, "sentences": ["In this figure, we list the nineteen objects used for our data collection.", "Following is the list of the objects: oregano spice, grill salt, can of mandarin oranges, can of chicken soup, dr pepper can, mountain dew can, sprite bottle, regular coke bottle, diet coke bottle, baked cheetos, baked lays chip, great grains cereal, cheerios cereal, extra dry skin moisturizer, spf55 sunscreen, aspirin bottle, airborne gummies bottle, hand soap, hand sanitizer."], "caption": "", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_082.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "For both of the Glassense-Vision and VizWiz datasets, two positive and two negative examples are shown in this figure.  In the positive examples, the object localization model correctly crops the object of interest from the input images.  In contrast, the negative examples show the case when the model fails to localize the object of interest.", "levels": [[-1], [-1], [-1]], "corpus_id": 78087743, "sentences": ["For both of the Glassense-Vision and VizWiz datasets, two positive and two negative examples are shown in this figure.", "In the positive examples, the object localization model correctly crops the object of interest from the input images.", "In contrast, the negative examples show the case when the model fails to localize the object of interest."], "caption": "Glassense-Vision                           (b) VizWizFigure 6: Positive and negative outputs of our object local- ization model on the Glassense-Vision and VizWiz datasets.user\u2019s hand, we run our object segmentation and localization models on the VizWiz data and detect a total of 1, 548 images that include a user\u2019s hand for the object of interest. While still an estimate, this result indicates that, in at least 18% of images regarding object identifcation, real-world end users use their hands as a natural interface to indicate the object of interest. Figure 6(b) depicts examples of our localization model\u2019s positive and negative results on VizWiz. By visually inspecting 6, 000 random samples not detected by our hand model, we identify only 36 images (<1%) where users\u2019 hands were present. The majority (24) includes just the fngertips, and one image includes a previously unseen hand shape. Some images were blurry (9), and some had the camera fash on (9) \u2014 a lighting condition not present in any of the training sets. When visible, objects\u2019 shapes were: relatively fat (e.g., paper, currency, gift card, newspaper, DVD cover), roughly cylindrical (e.g., pen, cigarette, cans, bottles, mug), and nearly rectangular (e.g., smartphones, laptop, keyboard).Analysis on Our New Benchmark Dataset TEgOFor our analysis, we frst separate the S data from the B data in TEgO, then divide their data into three subsets based on the method applied to the training and testing images \u2014 cropped-object (CO), hand-object (HO), and object (O). The HO and O methods include original images taken by S andB. HO contains images where S and B are holding the object, while O contains the rest. CO consists of HO images cropped by the object localization model; that is, the CO images are extracted from the HO images.Model Performance. We explore the potential of the hand- guided recognition approach in the context of teachable ob- ject recognizers by comparing recognition performance of TORs trained on the CO images to those trained on the orig- inal HO and O images, respectively. Each model is trained on around 30 images per object at a given condition (e.g., environment, button), and its accuracy is calculated on fve testing images per object (total of 95) in the same condition.The average accuracy across models in HO, CO, and O is reported in Fig. 7, with error bars denoting standard error across multiple sets of testing results for S and B \u2014 12 sets for S and 24 sets for B (due to the button settings).As expected, the models from S, serving as an upper base- line and trained on images taken by a sighted machine learn- ing expert, outperform those of B. They also indicate that this 19-way classifcation task is challenging, with CO achieving an average accuracy of 92% and an improvement of 5% and 6% on average over HO and O, respectively. Models from B follow a similar pattern, with CO achieving an average accu- racy of 71% and an improvement of 6% and 9% on average over HO and O, respectively. However, overlapping error bars indicate that these diferences may not be signifcant.Efect of Environment. Fig. 8 shows a breakdown of the models\u2019 performance across vanilla and wild environments. As expected, overall model performance is lower in the wild, where cluttered backgrounds tend to be present. In the vanilla environment, we observe that, in general, models trained on images where S and B hold the objects (HO and CO) perform better than those where they don\u2019t (O). How- ever, the utility of our approach seems more pertinent to the wild environment, where CO achieves, on average, 67% ac- curacy for B and improvement of 12% and 14% over HO and O, respectively. These results highlight the potential of our approach, given that photos of objects taken by people with visual impairments in the real world tend to include cluttered environments, as illustrated with the VizWiz dataset (Sec. 4).Efect of Sample Size. We explore the potential of our ap- proach for training with highly limited sample sizes of 1 and 5 with k-shot learning in Fig. 9. Similar to Kacorri et al. [36], we observe that, on average, the model performance increases with the sample size. More importantly, we note that CO tends to outperform HO and O consistently across sample sizes.Teachable vs. Generic. We explore whether the positive efect of the CO method over the HO and O methods, ob- served in teachable object recognizers (TORs), carries on to a generic object recognizer (GOR). As discussed in Sec. 3, we use as our GOR the Google\u2019s Inception V3 model [64]. Since a GOR is not trained on the labels of TEgO, its accuracy score should not be compared with those of TORs, directly. As shown in Fig. 10, to allow for the comparison, we use V-measure [57] by comparing desirable properties of the two, such as consistency of their predictions given images of the same object, and ability to distinguish between two diferent objects. Similar to Kacorri et al. [36], we observe that a TOR achieves higher V-scores than a GOR. This is not surprising since a TOR model is fne-tuned to the users, their environments, and the number of objects. However, we didFigure 7: Our hand-guided object recognition method (CO) tends to improve recognition accuracy on average for S and B compared to the original HO and O methods.Figure 8: Accuracy gain of our method (CO) over HO and O is more pertinent in cluttered backgrounds (wild).Figure 9: On average CO outperforms HO and O consistently across training sample sizes k = 1, 5, 20.Figure 10: Presence of hands (HO and CO) seems to have a diferent efect for generic vs. teachable models.not anticipate CO and HO underperforming O in the GOR case. We suspect that the presence of hands in the HO and CO images, as well as close-up cropped images in the CO, may have induced some confusion in the GOR.Error Analysis. Interested on how to improve our method, we focus on hand segmentation and localization errors (Fig. 11) as well as object features afecting recognition (Fig. 12).Hands out of frame. While S and B held objects in all CO images, the B\u2019s hand is not present in nine images; but, the objects are. These were taken in the wild environment and included relatively large objects such as soda bottles (6) and cereal boxes (3). The localization model correctly identifes object centers on seven of them and failed to localize any on the other two, which partially include the object.Hand segmentation errors. Among 855 testing images col- lected in the vanilla environment, hands are partially seg- mented on 11 images for S and 9 for B. Even with partial segmentation, the localization model successfully infers the object centers. In 18 images from S and 29 from B, the seg- mentation model misclassifes small non-hand parts of the image in addition to correctly segmenting the hand. The localization model successfully infers the object centers in all but 2 images from B, where the misclassifes non-hand parts are in proximity to another object. We observe similar patterns for the simulated wild environment. Among 855 testing images in the wild, hands are partially segmented on 54 images for S and 15 images for B. However, the localiza- tion model is afected only on one image for S and two for B. In other 30 images from S and 42 from B, the segmentation model classifes small non-hand parts of the image as a hand while correctly segmenting the hand. Again, the localization model successfully infers the object centers for all but two images from B, where the misclassifed non-hand parts are in proximity to another object.Object localization errors. There are few instances in the vanilla environment where the localization model fails even though most of the hand is detected. Specifcally, on two images from B, the model fails to localize the object, which is partially included. We observe 11 similar instances from B in the simulated wild environment. However, the most common localization error, observed in one image from S and 20 images from B in the wild, is misidentifying another object close to the object of interest. We suspect that some of these errors could be mitigated with additional training examples including such ambiguities.Discriminative features of objects. The stimuli objects in TEgO were engineered to be diverse in terms of shape and function while sharing similarities to make recognition chal- lenging. We illustrate aggregated results on misclassifed testing images as a confusion heatmap (Fig. 12). We observe that some of the highest confusions are among objects within the same category that are cylindrical or share visual fea- tures such as soup can and mandarin can, salt and oregano, cheetos and lays. It indicates room for better performance in the recognition models (TORs) independent of the quality of cropped photos fed by our localization model.Figure 11: Positive and negative results on TEgO, with out- of-frame hands for some of the negative examples.Figure 12: Confusion matrix for the CO models showing that misclassifcation occurs within objects of similar shape. Cans and bottles are indicated as \u201c-c\u201d and \u201c-b\u201d, respectively.", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_084.png", "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_085.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "This figure has two graphs that show the average accuracy depending on the background environment of objects: vanilla and wild.  The graph for the vanilla environment is shown on left and the graph for the wild environment is shown on right.  In the vanilla environment, the model achieves 0.754 (std: 0.086), 0.747 (std: 0.102), and 0.704 (std: 0.028) on the HO, CO, and O method with the B data, respectively, and 0.937 (std: 0.011), 0.933 (std: 0.037), and 0.863 (std: 0.028) on the HO, CO, and O method with the S data, respectively.  In the wild environment, the model achieves 0.542 (std: 0.047), 0.667 (std: 0.040), and 0.532 (std: 0.065) on the HO, CO, and O method with the B data, respectively, and 0.796 (std: 0.095), 0.905 (std: 0.053), and 0.853 (std: 0.046) on the HO, CO, and O method with the S data, respectively.", "levels": [[1], [1], [2], [2]], "corpus_id": 78087743, "sentences": ["This figure has two graphs that show the average accuracy depending on the background environment of objects: vanilla and wild.", "The graph for the vanilla environment is shown on left and the graph for the wild environment is shown on right.", "In the vanilla environment, the model achieves 0.754 (std: 0.086), 0.747 (std: 0.102), and 0.704 (std: 0.028) on the HO, CO, and O method with the B data, respectively, and 0.937 (std: 0.011), 0.933 (std: 0.037), and 0.863 (std: 0.028) on the HO, CO, and O method with the S data, respectively.", "In the wild environment, the model achieves 0.542 (std: 0.047), 0.667 (std: 0.040), and 0.532 (std: 0.065) on the HO, CO, and O method with the B data, respectively, and 0.796 (std: 0.095), 0.905 (std: 0.053), and 0.853 (std: 0.046) on the HO, CO, and O method with the S data, respectively."], "caption": "", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_087.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Hands Holding Clues for Object Recognition in Teachable Machines", "pdf_hash": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "year": 2019, "venue": "CHI", "alt_text": "The figure consists of three graphs that show the average accuracy depending on the sample size during training, referred to as k-shot where k is the number of images used for training --- three sample sizes (20, 5, 1).  When using 20 images per object for training, the model achieves 0.618 (std: 0.114), 0.705 (std: 0.075), and 0.608 (std: 0.098) on the HO, CO, and O methods with the B data, respectively, and 0.855 (std: 0.087), 0.888 (std: 0.064), and 0.859 (std: 0.039) on the HO, CO, and O methods with the S data, respectively.  When using 5 images per object for training, the model achieves 0.539 (std: 0.099), 0.595 (std: 0.112), and 0.550 (std: 0.098) on the HO, CO, and O methods with the B data, respectively, and 0.788 (std: 0.098), 0.818 (std: 0.058), and 0.811 (std: 0.052) on the HO, CO, and O methods with the S data, respectively.  When using 1 image per object for training, the model achieves 0.400 (std: 0.068), 0.450 (std: 0.090), and 0.424 (std: 0.053) on the HO, CO, and O methods with the B data, respectively, and 0.587 (std: 0.056), 0.614 (std: 0.094), and 0.611 (std: 0.056) on the HO, CO, and O methods with the S data, respectively.", "levels": [[1], [2], [2], [2]], "corpus_id": 78087743, "sentences": ["The figure consists of three graphs that show the average accuracy depending on the sample size during training, referred to as k-shot where k is the number of images used for training --- three sample sizes (20, 5, 1).", "When using 20 images per object for training, the model achieves 0.618 (std: 0.114), 0.705 (std: 0.075), and 0.608 (std: 0.098) on the HO, CO, and O methods with the B data, respectively, and 0.855 (std: 0.087), 0.888 (std: 0.064), and 0.859 (std: 0.039) on the HO, CO, and O methods with the S data, respectively.", "When using 5 images per object for training, the model achieves 0.539 (std: 0.099), 0.595 (std: 0.112), and 0.550 (std: 0.098) on the HO, CO, and O methods with the B data, respectively, and 0.788 (std: 0.098), 0.818 (std: 0.058), and 0.811 (std: 0.052) on the HO, CO, and O methods with the S data, respectively.", "When using 1 image per object for training, the model achieves 0.400 (std: 0.068), 0.450 (std: 0.090), and 0.424 (std: 0.053) on the HO, CO, and O methods with the B data, respectively, and 0.587 (std: 0.056), 0.614 (std: 0.094), and 0.611 (std: 0.056) on the HO, CO, and O methods with the S data, respectively."], "caption": "", "local_uri": ["de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_088.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "pdf_hash": "35941a4414b58e76d1f92b495c3c8d90a2593315", "year": 2017, "venue": "CHI", "alt_text": "The Group Spinner interface showing the radar chart for a group of students for the current and previous sessions. It also shows the indicators for the selected group so the teacher can update the graph based on the indicator values.", "levels": [[1], [1]], "corpus_id": 12790972, "sentences": ["The Group Spinner interface showing the radar chart for a group of students for the current and previous sessions.", "It also shows the indicators for the selected group so the teacher can update the graph based on the indicator values."], "caption": "Figure 1. Annotated crop of Group Spinner\u2019s interface showing current and previous session graphs along with some indicators.", "local_uri": ["35941a4414b58e76d1f92b495c3c8d90a2593315_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "pdf_hash": "35941a4414b58e76d1f92b495c3c8d90a2593315", "year": 2017, "venue": "CHI", "alt_text": "An empty radar chart with 10 axis: Outcome, Collaboration, Organization process, Classroom dynamics, Confidence, Behaviour, Motivation, Language, Skillfullness, and Thinking skills.", "levels": [[1]], "corpus_id": 12790972, "sentences": ["An empty radar chart with 10 axis: Outcome, Collaboration, Organization process, Classroom dynamics, Confidence, Behaviour, Motivation, Language, Skillfullness, and Thinking skills."], "caption": "Figure 3. First version radar chart; superset of all possible axes as inspired by Activity Theory\u2019s activity triangle.", "local_uri": ["35941a4414b58e76d1f92b495c3c8d90a2593315_Image_004.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "pdf_hash": "35941a4414b58e76d1f92b495c3c8d90a2593315", "year": 2017, "venue": "CHI", "alt_text": "The graph mode allows for manipulating the graph of any of the selected groups through moving the buble marks for each of the axes. It also shows the graph for the previous session for comparison. The user can show/hide the graph for the previous session and the graph of the students' self assessment.", "levels": [[-1], [-1], [-1]], "corpus_id": 12790972, "sentences": ["The graph mode allows for manipulating the graph of any of the selected groups through moving the buble marks for each of the axes.", "It also shows the graph for the previous session for comparison.", "The user can show/hide the graph for the previous session and the graph of the students' self assessment."], "caption": "", "local_uri": ["35941a4414b58e76d1f92b495c3c8d90a2593315_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "pdf_hash": "35941a4414b58e76d1f92b495c3c8d90a2593315", "year": 2017, "venue": "CHI", "alt_text": "In the indicators mode, it is possible to show a description of any of the indicators, to scroll to any of the indicators using the bookmarks toolbar and to increase/decrease the value of any of the indicators.", "levels": [[-1]], "corpus_id": 12790972, "sentences": ["In the indicators mode, it is possible to show a description of any of the indicators, to scroll to any of the indicators using the bookmarks toolbar and to increase/decrease the value of any of the indicators."], "caption": "teachers, except for T6 (a member of the research team) who provided her feedback in written form. Interviews were audio recorded and transcribed for analysis.", "local_uri": ["35941a4414b58e76d1f92b495c3c8d90a2593315_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "pdf_hash": "35941a4414b58e76d1f92b495c3c8d90a2593315", "year": 2017, "venue": "CHI", "alt_text": "The radar chart from T1 shows plots from 6 previous sessions as well as the yet unmodified plot for the 7th session. It shows how the teachers' focus may change from session to session.", "levels": [[1], [1]], "corpus_id": 12790972, "sentences": ["The radar chart from T1 shows plots from 6 previous sessions as well as the yet unmodified plot for the 7th session.", "It shows how the teachers' focus may change from session to session."], "caption": "Figure 6. Graphs of 6 sessions by T1 with the yet unmodified graph of session 7.", "local_uri": ["35941a4414b58e76d1f92b495c3c8d90a2593315_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "The layout of the comic showing abstraction levels and visual structure prior to interaction.", "levels": null, "corpus_id": 14921094, "sentences": ["The layout of the comic showing abstraction levels and visual structure prior to interaction."], "caption": "Figure 1: Abstraction levels 1 & 2 of the comic, and a possible application of Cohn\u2019s [9] visual structure (Est. = Establisher, Init. = Initial, Pr. = Prolongation, Pk. = Peak, Re. = Release)", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_001.png"], "annotated": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "The layout of the comic after interaction has exposed the lower levels of abstraction.", "levels": null, "corpus_id": 14921094, "sentences": ["The layout of the comic after interaction has exposed the lower levels of abstraction."], "caption": "Figure 2: One of the sixteen possible variations of the comic.", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_002.png"], "annotated": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "A diagrammatic interpretation of the comic's layout for the purposes of rendering heat maps.", "levels": null, "corpus_id": 14921094, "sentences": ["A diagrammatic interpretation of the comic's layout for the purposes of rendering heat maps."], "caption": "Figure 3: Diagram of the comic\u2019s content with containers for all possible branches", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_003.png"], "annotated": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "Heat map of responses to the 'summary' question from those reading the digital version of the comic.", "levels": null, "corpus_id": 14921094, "sentences": ["Heat map of responses to the 'summary' question from those reading the digital version of the comic."], "caption": "", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_004.png"], "annotated": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "Heat map of responses to the 'favorite' question from those reading the digital version of the comic.", "levels": null, "corpus_id": 14921094, "sentences": ["Heat map of responses to the 'favorite' question from those reading the digital version of the comic."], "caption": "", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_005.png"], "annotated": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "Heat map of responses to the 'summary' question from those reading the paper version of the comic.", "levels": null, "corpus_id": 14921094, "sentences": ["Heat map of responses to the 'summary' question from those reading the paper version of the comic."], "caption": "", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_007.png"], "annotated": false, "compound": false}
{"title": "Visualizing interactive narratives: employing a branching comic to tell a story and show its readings", "pdf_hash": "833c1f421c54b839dbcef5920f91da7f4644fa42", "year": 2014, "venue": "CHI", "alt_text": "Heat map of responses to the 'Favorite' question from those reading the paper version of the comic.", "levels": null, "corpus_id": 14921094, "sentences": ["Heat map of responses to the 'Favorite' question from those reading the paper version of the comic."], "caption": "Figure 4: Heat maps of participants\u2019 responses to questions about the comics\u2019 content", "local_uri": ["833c1f421c54b839dbcef5920f91da7f4644fa42_Image_008.png"], "annotated": false, "compound": false}
{"title": "On the selection of 2D objects using external labeling", "pdf_hash": "4cc487ea1ee9713baeb4afed3c3986c12abb9542", "year": 2014, "venue": "CHI", "alt_text": "The configurations of UAVs and their asociated labels  for Fixed (subfigure a) and Dynamic/LabelFreeze (subfigure b) are depicted.", "levels": null, "corpus_id": 10039265, "sentences": ["The configurations of UAVs and their asociated labels  for Fixed (subfigure a) and Dynamic/LabelFreeze (subfigure b) are depicted."], "caption": "(b)\u200c", "local_uri": ["4cc487ea1ee9713baeb4afed3c3986c12abb9542_Image_001.jpg", "4cc487ea1ee9713baeb4afed3c3986c12abb9542_Image_002.jpg"], "annotated": false, "compound": true}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A male sitting behind a desk looking into the camera with a blue background. There are white text on black background captions overlaid at the bottom of the picture. The caption is: \"they warm what their role will bean during the internship experience requirements\".", "levels": null, "corpus_id": 5046516, "sentences": ["A male sitting behind a desk looking into the camera with a blue background.", "There are white text on black background captions overlaid at the bottom of the picture.", "The caption is: \"they warm what their role will bean during the internship experience requirements\"."], "caption": "Figure 1. The research prototype tool examined in this work.", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A bar graph titled Figure 2: H1-a for Noticed Errors (Boolean). The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0 to 1, with ticks at increments of 0.1. On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human. In the WRAT-L facet, all pairwise comparisons are not significant, and the means are approximately: 0.55 for Desktop, 0.45 for Cloud, and 0.4 for Human. In the WRAT-M facet, only the Desktop-Human pairwise comparison was significant with two stars, and the means are approximately: 0.7 for Desktop, 0.65 for Cloud, and 0.55 for Human. In the WRAT-H facet, all pairwise comparisons are not significant, and the means are approximately: 0.95 for Desktop, 0.9 for Cloud, and 0.85 for Human.", "levels": [[1], [1], [1], [2], [2], [2]], "corpus_id": 5046516, "sentences": ["A bar graph titled Figure 2: H1-a for Noticed Errors (Boolean).", "The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0 to 1, with ticks at increments of 0.1.", "On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human.", "In the WRAT-L facet, all pairwise comparisons are not significant, and the means are approximately: 0.55 for Desktop, 0.45 for Cloud, and 0.4 for Human.", "In the WRAT-M facet, only the Desktop-Human pairwise comparison was significant with two stars, and the means are approximately: 0.7 for Desktop, 0.65 for Cloud, and 0.55 for Human.", "In the WRAT-H facet, all pairwise comparisons are not significant, and the means are approximately: 0.95 for Desktop, 0.9 for Cloud, and 0.85 for Human."], "caption": "Figure 2. H1-a for Noticed Errors (Boolean)", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A stacked bar graph titled Figure 4: H1-c for ASR Did a Good Job (Likert). The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0% to 100%, with ticks at 10% increments. On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human. The bar graph displays the percentage of answers for each choice: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree. In the WRAT-L facet, all pairwise comparison lines are not significant and the overall percentages are: 15% Strongly Disagree, 15% Disagree, 25% Neither agree nor disagree, 25% Agree, 10% Strongly Agree. In the WRAT-M facet, the Desktop-Cloud pairwise comparison was significant with two stars, the Desktop-Human pairwise comparison was significant with three stars, and the overall percentages are: 15% Strongly Disagree, 15% Disagree, 15% Neither agree nor disagree, 15% Agree, 25% Strongly Agree. In the WRAT-H facet, both Desktop-Cloud and Desktop-Human had significant pairwise comparisons with three stars, and the overall percentages are: 25% Strongly Disagree, 20% Disagree, 10% Neither agree nor disagree, 10% Agree, 20% Strongly Agree.", "levels": [[1], [1], [1], [1], [2], [2], [2]], "corpus_id": 5046516, "sentences": ["A stacked bar graph titled Figure 4: H1-c for ASR Did a Good Job (Likert).", "The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0% to 100%, with ticks at 10% increments.", "On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human.", "The bar graph displays the percentage of answers for each choice: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree.", "In the WRAT-L facet, all pairwise comparison lines are not significant and the overall percentages are: 15% Strongly Disagree, 15% Disagree, 25% Neither agree nor disagree, 25% Agree, 10% Strongly Agree.", "In the WRAT-M facet, the Desktop-Cloud pairwise comparison was significant with two stars, the Desktop-Human pairwise comparison was significant with three stars, and the overall percentages are: 15% Strongly Disagree, 15% Disagree, 15% Neither agree nor disagree, 15% Agree, 25% Strongly Agree.", "In the WRAT-H facet, both Desktop-Cloud and Desktop-Human had significant pairwise comparisons with three stars, and the overall percentages are: 25% Strongly Disagree, 20% Disagree, 10% Neither agree nor disagree, 10% Agree, 20% Strongly Agree."], "caption": "Figure 4. H1-c for ASR Did a Good Job (Likert)", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A bar graph titled Figure 6: H1-e for Evaluation of Accuracy (Numeric). The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0 to 100, with ticks at increments of 10. On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human. In the WRAT-L facet, all pairwise comparisons are not significant, and the means are approximately: 55 for Desktop, 60 for Cloud, and 65 for Human. In the WRAT-M facet, the Desktop-Cloud and Desktop-Human pairwise comparison was significant with three stars, the Cloud-Human pairwise comparison had one star, and the means are approximately: 55 for Desktop, 65 for Cloud, and 70 for Human. In the WRAT-H facet, the Desktop-Cloud and Desktop-Human pairwise comparisons were significant at three stars, and the means are approximately: 50 for Desktop, 65 for Cloud, and 75 for Human.", "levels": [[1], [1], [1], [2], [2], [2]], "corpus_id": 5046516, "sentences": ["A bar graph titled Figure 6: H1-e for Evaluation of Accuracy (Numeric).", "The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0 to 100, with ticks at increments of 10.", "On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human.", "In the WRAT-L facet, all pairwise comparisons are not significant, and the means are approximately: 55 for Desktop, 60 for Cloud, and 65 for Human.", "In the WRAT-M facet, the Desktop-Cloud and Desktop-Human pairwise comparison was significant with three stars, the Cloud-Human pairwise comparison had one star, and the means are approximately: 55 for Desktop, 65 for Cloud, and 70 for Human.", "In the WRAT-H facet, the Desktop-Cloud and Desktop-Human pairwise comparisons were significant at three stars, and the means are approximately: 50 for Desktop, 65 for Cloud, and 75 for Human."], "caption": "Figure 6. H1-e for Evaluation of Accuracy (Numeric)", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A boxplot titled Figure 12: H2-d for Accuracy of Evaluation (Ordinal Scale). The vertical axis has ticks representing the answer choices: Zero, Low, Medium, High, Perfect. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. Only the WRAT-M/WRAT-H pairwise comparison was significant with one star. Please refer the subsection on the current page for the median and modes for the WRAT levels.", "levels": [[1], [1], [2, 1], [0]], "corpus_id": 5046516, "sentences": ["A boxplot titled Figure 12: H2-d for Accuracy of Evaluation (Ordinal Scale).", "The vertical axis has ticks representing the answer choices: Zero, Low, Medium, High, Perfect.", "On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. Only the WRAT-M/WRAT-H pairwise comparison was significant with one star.", "Please refer the subsection on the current page for the median and modes for the WRAT levels."], "caption": "Figure 9. H2-a for Noticed Errors in Captions (Boolean)", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A boxplot titled Figure 10: H2-b for Errors Prevented Understanding (Likert). The vertical axis has ticks representing the answer choices: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were not significant. Please refer the subsection on the previous page for the median and modes for the WRAT levels.", "levels": [[1], [1], [2, 1], [0]], "corpus_id": 5046516, "sentences": ["A boxplot titled Figure 10: H2-b for Errors Prevented Understanding (Likert).", "The vertical axis has ticks representing the answer choices: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree.", "On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were not significant.", "Please refer the subsection on the previous page for the median and modes for the WRAT levels."], "caption": "", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A boxplot titled Figure 11: H2-c for ASR Did a Good Job (Likert). The vertical axis has ticks representing the answer choices: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. Both WRAT-M/WRAT-H and WRAT-L/WRAT-H pairwise comparisons were significant with two stars. Please refer the subsection on the previous page for the median and modes for the WRAT levels.", "levels": [[1], [1], [2, 1], [0]], "corpus_id": 5046516, "sentences": ["A boxplot titled Figure 11: H2-c for ASR Did a Good Job (Likert).", "The vertical axis has ticks representing the answer choices: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree.", "On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. Both WRAT-M/WRAT-H and WRAT-L/WRAT-H pairwise comparisons were significant with two stars.", "Please refer the subsection on the previous page for the median and modes for the WRAT levels."], "caption": "", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A bar graph titled Figure 13: H2-e for Evaluation of Accuracy (Numeric). The vertical axis ranges from 0 to 100, with ticks at increments of 10. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were not significant. Please refer the subsection on the current page for the means and standard errors for the WRAT levels.", "levels": [[1], [1], [2], [0]], "corpus_id": 5046516, "sentences": ["A bar graph titled Figure 13: H2-e for Evaluation of Accuracy (Numeric).", "The vertical axis ranges from 0 to 100, with ticks at increments of 10.", "On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were not significant.", "Please refer the subsection on the current page for the means and standard errors for the WRAT levels."], "caption": "Figure 13. H2-e for Evaluation of Accuracy (Numeric)", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "pdf_hash": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "year": 2018, "venue": "CHI", "alt_text": "A bar graph titled Figure 14: H2-f for Comprehension Quiz Success (Mult. Choice). The vertical axis ranges from 0% to 100%, with ticks at increments of 10%. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were significant at three stars. Please refer the subsection on the current page for the means and standard errors for the WRAT levels.", "levels": [[1], [1], [1], [2, 1], [0]], "corpus_id": 5046516, "sentences": ["A bar graph titled Figure 14: H2-f for Comprehension Quiz Success (Mult.", "Choice).", "The vertical axis ranges from 0% to 100%, with ticks at increments of 10%.", "On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were significant at three stars.", "Please refer the subsection on the current page for the means and standard errors for the WRAT levels."], "caption": "Figure 14. H2-f for Comprehension Quiz Success (Mult. Choice)", "local_uri": ["a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Mobi3DSketch: 3D Sketching in Mobile AR", "pdf_hash": "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1", "year": 2019, "venue": "CHI", "alt_text": "Small-sized mobile screens result in a much narrower field of view (FOV) than VR HMDs. 2D/3D input and display are coupled in our case.", "levels": null, "corpus_id": 140326172, "sentences": ["Small-sized mobile screens result in a much narrower field of view (FOV) than VR HMDs.", "2D/3D input and display are coupled in our case."], "caption": "", "local_uri": ["3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_003.gif"], "annotated": false, "compound": false}
{"title": "Mobi3DSketch: 3D Sketching in Mobile AR", "pdf_hash": "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1", "year": 2019, "venue": "CHI", "alt_text": "(Left) Lack of context due to narrow FOV. (Right) Users can step back to perform relative drawing while seeing the whole context.", "levels": null, "corpus_id": 140326172, "sentences": ["(Left) Lack of context due to narrow FOV. (Right) Users can step back to perform relative drawing while seeing the whole context."], "caption": "Figure 3: (Left) Lack of context due to narrow FOV. (Right) Users can step back to perform relative drawing while seeing the whole context.\u200c", "local_uri": ["3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_005.gif", "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_006.gif"], "annotated": false, "compound": true}
{"title": "Mobi3DSketch: 3D Sketching in Mobile AR", "pdf_hash": "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1", "year": 2019, "venue": "CHI", "alt_text": "Snapping suggestions to connect the endpoints of a new stroke to the existing ones.", "levels": [[-1]], "corpus_id": 140326172, "sentences": ["Snapping suggestions to connect the endpoints of a new stroke to the existing ones."], "caption": "Before snapping               (b) After snappingFigure 4: Snapping suggestions (in white) to connect the end- points of a new stroke (in green) to the existing ones (in red).of the mobile devices are not stereoscopic. This makes users difcult to perceive the depth of 3D strokes on the screen, re- sulting in drawings with wrong depth. 3) Small-sized mobile screens result in an extremely narrower feld of view (FOV) compared to immersive VR displays (Figure 2), and a small operation area for multi-touch interaction. This would easily make users draw poorly without seeing the whole context, as illustrated in Figure 3. 4) The output and 2D/3D input of mobile device are coupled. This coupling makes users dif- cult to check whether the depth of a stroke being currently drawn is reasonable or not by examining it from another viewpoint. Steadily holding the device in mid-air easily leads to fatigue or errors otherwise due to device shaking.", "local_uri": ["3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_007.gif", "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_008.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Mobi3DSketch: 3D Sketching in Mobile AR", "pdf_hash": "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1", "year": 2019, "venue": "CHI", "alt_text": "The user interface of Mobi3DSketch. The functions of the action button change with the selected tools.", "levels": null, "corpus_id": 140326172, "sentences": ["The user interface of Mobi3DSketch.", "The functions of the action button change with the selected tools."], "caption": "OperationInputDescriptionSelectiontouchSingle-fnger tap for selecting a stroke or snap-ping point; two-fnger tap for selecting a proxy. A proxy can also be selected by single-fnger tapping its center. Tap on any empty space for deselection.TranslationMoT,touchA selected object moves with the device inspace when the \u201cGrab\u201d action button being pressed. An optional constraint for vertical translation is also provided.RotateIMU,touchA selected object rotates according to the ori-entation of the device (Figure 6 (d)) when the \u201cRotate\u201d action button being pressed.ScalingtouchTwo-fnger pinch gesture for scaling up/downa selected object.Zoom-in/outIMU,touchTwo-fnger pinch gesture for moving a selectedobject along the device\u2019s viewing direction when the \u201cGrab\u201d action button being pressed.CloningMoT,touchCopy a selected object, and place it in frontof the device (Figure 6 (e)) when the \u201cPaste\u201d action button is tapped.DeletiontouchLong-press a selected stroke, proxy, or snap-ping point and then choose \u201cDel\u201d in the pop-up fan menu.", "local_uri": ["3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_013.gif"], "annotated": false, "compound": false}
{"title": "Mobi3DSketch: 3D Sketching in Mobile AR", "pdf_hash": "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1", "year": 2019, "venue": "CHI", "alt_text": "Relative drawing from explicitly specified snapping points on a selected stroke or proxy.", "levels": [[-1]], "corpus_id": 140326172, "sentences": ["Relative drawing from explicitly specified snapping points on a selected stroke or proxy."], "caption": "", "local_uri": ["3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_014.gif", "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_015.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Mobi3DSketch: 3D Sketching in Mobile AR", "pdf_hash": "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1", "year": 2019, "venue": "CHI", "alt_text": "Draw in the 2D parameter domain to avoid fatigue during drawing while pointing to a proxy. The remote proxy is brought closer to the user and make it parallel to the screen.", "levels": null, "corpus_id": 140326172, "sentences": ["Draw in the 2D parameter domain to avoid fatigue during drawing while pointing to a proxy.", "The remote proxy is brought closer to the user and make it parallel to the screen."], "caption": "(a) Normal mode (b) \u201c2D\u201d mode Figure 12: Draw in the 2D pa- rameter domain (b) to avoid", "local_uri": ["3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_018.gif", "3437b7f020e5f6e014d4f6ca6703a0055d60d2a1_Image_019.gif"], "annotated": false, "compound": true}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "The figure shows a slide with a house, tree, and one cloud. It seems to be an instructional slide. The cloud is abouve freezing level and there are some airdrafts represented. The interface also shows some presenter notes.", "levels": null, "corpus_id": 573753, "sentences": ["The figure shows a slide with a house, tree, and one cloud.", "It seems to be an instructional slide.", "The cloud is abouve freezing level and there are some airdrafts represented.", "The interface also shows some presenter notes."], "caption": "Figure 1. SlidePacer \u2013 presenter interface: a) current slide, b) presenter notes, c) illustration representing whether the instructor should wait to start speaking again; d) slideshow controls; and e) notification area.", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "The figure is composed of four images. The first two represent a state where the instructor changed slide and is waiting for the interpreter to finish. The presenter view is grayed out and shows a red progress bar. The mobile app shows a big red button. The remaining two images represent a state where the interpreter tapped on the mobile screen indicating that s/he is finished with the interpretation. The presenter view is grayed out and shows an orange progress bar. The mobile app show a gray button.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 573753, "sentences": ["The figure is composed of four images.", "The first two represent a state where the instructor changed slide and is waiting for the interpreter to finish.", "The presenter view is grayed out and shows a red progress bar.", "The mobile app shows a big red button.", "The remaining two images represent a state where the interpreter tapped on the mobile screen indicating that s/he is finished with the interpretation.", "The presenter view is grayed out and shows an orange progress bar.", "The mobile app show a gray button."], "caption": "Figure 2. From left to right: presenter view waiting for interpreter to finish; mobile app is waiting for interpreter input to signal that interpretation is finished; presenter view waiting for students to read slide content; mobile app is inactive.", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_003.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "https://lh4.googleusercontent.com/dtBwYHen2YpfIwZCfISFiYlh9xQBKizsOO364D6To7T1VnFuFQgLik4slATPCDriDWYckTvOvHUU2Qk9vGALoOUgWCaGMg_LEUj_mCLneQ1M52aescjXxTaIDY5_IGrrZM1m0g8q", "levels": null, "corpus_id": 573753, "sentences": ["https://lh4.googleusercontent.com/dtBwYHen2YpfIwZCfISFiYlh9xQBKizsOO364D6To7T1VnFuFQgLik4slATPCDriDWYckTvOvHUU2Qk9vGALoOUgWCaGMg_LEUj_mCLneQ1M52aescjXxTaIDY5_IGrrZM1m0g8q"], "caption": "", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "https://lh5.googleusercontent.com/2GIficOP_Vj6AWYiXgptMUnj8rdQyJmwvv8bpv7V2qQp6lphrVoaf0Rai4N_anDcIGpixeyObcS2Xayo_7pTxPUUul4It6YcwUOWtc-TR15pIl5wKQ-zTT5-fuiQJk2qPzSnw1wK", "levels": [[-1]], "corpus_id": 573753, "sentences": ["https://lh5.googleusercontent.com/2GIficOP_Vj6AWYiXgptMUnj8rdQyJmwvv8bpv7V2qQp6lphrVoaf0Rai4N_anDcIGpixeyObcS2Xayo_7pTxPUUul4It6YcwUOWtc-TR15pIl5wKQ-zTT5-fuiQJk2qPzSnw1wK"], "caption": "Before starting the lecture, participants were asked to fill a pre- questionnaire about demographic information, fluency in ASL, and previous knowledge of lightning formation [20]. They were asked to fill in a 5-point Likert scale ranging from very little to very much, to the questions: 1) I regularly read the weather maps in the newspaper / online; 2) I can distinguish cumulus and nimbus clouds; 3) I know what a low pressure system is; 4) I can explain what makes the wind blow; 5) I know what this symbol means . 6) I know what this symbol means .", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "Retention scores in a bar chart. For both user groups, differences between conditons are small and non-significant.", "levels": [[1], [3]], "corpus_id": 573753, "sentences": ["Retention scores in a bar chart.", "For both user groups, differences between conditons are small and non-significant."], "caption": "", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_006.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "Transfer scores in a bar chart. For both user groups, differences between conditons are small and non-significant.", "levels": [[1], [3]], "corpus_id": 573753, "sentences": ["Transfer scores in a bar chart.", "For both user groups, differences between conditons are small and non-significant."], "caption": "Figure 3. Mean retention score for both user groups and experimental conditions.", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "Bar chart with average time on each visual sources for DHH participants: slides, ASL, and instructor. Differences between control and SlidePacer conditions are significant for all visual sources.", "levels": [[1], [3]], "corpus_id": 573753, "sentences": ["Bar chart with average time on each visual sources for DHH participants: slides, ASL, and instructor.", "Differences between control and SlidePacer conditions are significant for all visual sources."], "caption": "Figure 5. Average time DHH participants spent looking at each visual source.", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "Bar chart with perceived pace. SlidePacer has an effect on perceived pace from approapriate to slightly slow.", "levels": [[1], [3]], "corpus_id": 573753, "sentences": ["Bar chart with perceived pace.", "SlidePacer has an effect on perceived pace from approapriate to slightly slow."], "caption": "", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "Bar chart with average time on each visual sources for hearing participants: slides, ASL, and instructor. Differences between control and SlidePacer conditions are significant for all visual sources.", "levels": [[1], [3]], "corpus_id": 573753, "sentences": ["Bar chart with average time on each visual sources for hearing participants: slides, ASL, and instructor.", "Differences between control and SlidePacer conditions are significant for all visual sources."], "caption": "Figure 6. Average time hearing participants spent looking at each visual source.", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_010.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "pdf_hash": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "year": 2016, "venue": "ASSETS", "alt_text": "Bar chart with perceived dificulty for both user groups and conditions. There are no significant differences beetween control and SlidePacer conditions.", "levels": [[1], [2]], "corpus_id": 573753, "sentences": ["Bar chart with perceived dificulty for both user groups and conditions.", "There are no significant differences beetween control and SlidePacer conditions."], "caption": "Figure 7. Perceived lecture difficulty for both user groups and conditions.", "local_uri": ["505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_011.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "DreamGigs: Designing a Tool to Empower Low-Resource Job Seekers", "pdf_hash": "c385e994b9917cbdc14351060a44d8de21bb763d", "year": 2019, "venue": "CHI", "alt_text": "The implementation of DreamGigs consists of three sprints: (1) Sprint 1: finding the right gigs, (2) Sprint 2: categorizing skills and addressing low-digital literacy, and (3) Sprint 3: final evaluation. Each sprint consists of three stages: design, development, and evaluation. In Sprint 1, we ran 5 small-scale usability tests with social work graduate students, and they suggested having practical, reliable, and flexible gigs. In Sprint 2, we conducted 10 semi-structured interviews with low-resource job seekers. Our participants liked the intermediate occupation list as well as the formal job postings and volunteer opportunities nearby. We found that our job seekers had difficulties navigating the identified skills and uncertainties about how to proceed within the tool. In Sprint 3, we conducted 10 semi-structured interviews with low-resource job seekers, including 5 returning participants from Sprint 2 and 5 new participants. Overall, our participants liked the new classification of knowledge, skills, and abilities. Our final evaluation revealed a process of personal empowerment.", "levels": null, "corpus_id": 92990382, "sentences": ["The implementation of DreamGigs consists of three sprints: (1) Sprint 1: finding the right gigs, (2) Sprint 2: categorizing skills and addressing low-digital literacy, and (3) Sprint 3: final evaluation.", "Each sprint consists of three stages: design, development, and evaluation.", "In Sprint 1, we ran 5 small-scale usability tests with social work graduate students, and they suggested having practical, reliable, and flexible gigs.", "In Sprint 2, we conducted 10 semi-structured interviews with low-resource job seekers.", "Our participants liked the intermediate occupation list as well as the formal job postings and volunteer opportunities nearby.", "We found that our job seekers had difficulties navigating the identified skills and uncertainties about how to proceed within the tool.", "In Sprint 3, we conducted 10 semi-structured interviews with low-resource job seekers, including 5 returning participants from Sprint 2 and 5 new participants.", "Overall, our participants liked the new classification of knowledge, skills, and abilities.", "Our final evaluation revealed a process of personal empowerment."], "caption": "Figure 2: Iterative development and evaluation results: This fgure summarizes the result of each sprint; the evaluation setting; and feedback received and/or updates made as a result of the feedback.", "local_uri": ["c385e994b9917cbdc14351060a44d8de21bb763d_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Understanding digital and material social communications for older adults", "pdf_hash": "7a0dd13f2ea6834298750950b56f49b7371f855a", "year": 2014, "venue": "CHI", "alt_text": "Figure 2 Caption: Crochet newsletter created by one participant Figure 2 Description: Two photocopied pages from a hand-made crochet newsletter are laying on the floor. One page shows a pattern for a Japanese design piece, and the other page talks about the history of the crochet design.", "levels": null, "corpus_id": 9090043, "sentences": ["Figure 2 Caption: Crochet newsletter created by one participant Figure 2 Description: Two photocopied pages from a hand-made crochet newsletter are laying on the floor.", "One page shows a pattern for a Japanese design piece, and the other page talks about the history of the crochet design."], "caption": "Figure 2. Crochet newsletter created by one participant.", "local_uri": ["7a0dd13f2ea6834298750950b56f49b7371f855a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "pdf_hash": "09f444597b50aebe550b5efd1368a55953d8ddd1", "year": 2016, "venue": "ASSETS", "alt_text": "Spherical \"Magic 8-ball\" speakers were used as targets during the experiment, and an \"Owl\" shaped speaker that was attached to the wrist. Both are spherical in shape, but the Owl has pointed conical \"ears\" on top and range feet at the bottom.", "levels": null, "corpus_id": 15422662, "sentences": ["Spherical \"Magic 8-ball\" speakers were used as targets during the experiment, and an \"Owl\" shaped speaker that was attached to the wrist.", "Both are spherical in shape, but the Owl has pointed conical \"ears\" on top and range feet at the bottom."], "caption": "Figure 2: Speakers used as targets (\u201c8 Ball\u201d, left) and placed on the wrist (\u201cOwl\u201d, right). Images from [18].", "local_uri": ["09f444597b50aebe550b5efd1368a55953d8ddd1_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "pdf_hash": "09f444597b50aebe550b5efd1368a55953d8ddd1", "year": 2016, "venue": "ASSETS", "alt_text": "The bar chart shows the average selection error (y-axis, in percent) for all feedback conditions, and contains 10 vertical bars, one for each condition. The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control. There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations. The Control Design only has a single bar (for Object). The values, from left to right, are as follows: Geiger-Wrist = 17.6%, Geiger-Object = 12%, Geiger-Both = 6.5%; Pitch-Wrist = 13%, Pitch-Object = 9.2%, Pitch-Both = 15.7%; Constant-Wrist = 25%, Constant-Object = 25.9%, Constant-Both = 21.3%; Control-Object = 36.1%.", "levels": [[1], [1], [1], [1], [2]], "corpus_id": 15422662, "sentences": ["The bar chart shows the average selection error (y-axis, in percent) for all feedback conditions, and contains 10 vertical bars, one for each condition.", "The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control.", "There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations.", "The Control Design only has a single bar (for Object).", "The values, from left to right, are as follows: Geiger-Wrist = 17.6%, Geiger-Object = 12%, Geiger-Both = 6.5%; Pitch-Wrist = 13%, Pitch-Object = 9.2%, Pitch-Both = 15.7%; Constant-Wrist = 25%, Constant-Object = 25.9%, Constant-Both = 21.3%; Control-Object = 36.1%."], "caption": "Figure 5: Mean Study 1 target selection error for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "local_uri": ["09f444597b50aebe550b5efd1368a55953d8ddd1_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "pdf_hash": "09f444597b50aebe550b5efd1368a55953d8ddd1", "year": 2016, "venue": "ASSETS", "alt_text": "The bar chart shows the average selection time (y-axis, in milliseconds) for all feedback conditions, and contains 10 vertical bars, one for each condition. The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control. There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations. The Control Design only has a single bar (for Object). The values, from left to right, are as follows: Geiger-Wrist = 4881ms, Geiger-Object = 4686ms, Geiger-Both = 4020ms; Pitch-Wrist = 3938ms, Pitch-Object = 3107ms, Pitch-Both = 4268ms; Constant-Wrist = 3195%, Constant-Object = 3568ms, Constant-Both = 3972ms; Control-Object = 2666ms.", "levels": [[1], [1], [1], [1], [2]], "corpus_id": 15422662, "sentences": ["The bar chart shows the average selection time (y-axis, in milliseconds) for all feedback conditions, and contains 10 vertical bars, one for each condition.", "The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control.", "There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations.", "The Control Design only has a single bar (for Object).", "The values, from left to right, are as follows: Geiger-Wrist = 4881ms, Geiger-Object = 4686ms, Geiger-Both = 4020ms; Pitch-Wrist = 3938ms, Pitch-Object = 3107ms, Pitch-Both = 4268ms; Constant-Wrist = 3195%, Constant-Object = 3568ms, Constant-Both = 3972ms; Control-Object = 2666ms."], "caption": "Figure 6: Mean Study 1 target selection times for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "local_uri": ["09f444597b50aebe550b5efd1368a55953d8ddd1_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "pdf_hash": "09f444597b50aebe550b5efd1368a55953d8ddd1", "year": 2016, "venue": "ASSETS", "alt_text": "The bar chart shows the average selection error (y-axis, in percent) for all feedback conditions in Study 2, and contains 10 vertical bars, one for each condition. The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control. There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations. The Control Design only has a single bar (for Object). The values, from left to right, are as follows: Geiger-Wrist = 35.4%, Geiger-Object = 22.9%, Geiger-Both = 14.6%; Pitch-Wrist = 35.4%, Pitch-Object = 27.1%, Pitch-Both = 45.8%; Constant-Wrist = 37.5%, Constant-Object = 20.8%, Constant-Both = 33.3%; Control-Object = 54.2%.", "levels": [[1], [1], [1], [1], [2]], "corpus_id": 15422662, "sentences": ["The bar chart shows the average selection error (y-axis, in percent) for all feedback conditions in Study 2, and contains 10 vertical bars, one for each condition.", "The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control.", "There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations.", "The Control Design only has a single bar (for Object).", "The values, from left to right, are as follows: Geiger-Wrist = 35.4%, Geiger-Object = 22.9%, Geiger-Both = 14.6%; Pitch-Wrist = 35.4%, Pitch-Object = 27.1%, Pitch-Both = 45.8%; Constant-Wrist = 37.5%, Constant-Object = 20.8%, Constant-Both = 33.3%; Control-Object = 54.2%."], "caption": "Figure 7: Mean Study 2 target selection error for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "local_uri": ["09f444597b50aebe550b5efd1368a55953d8ddd1_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Designing for Digital Playing Out", "pdf_hash": "eee0090bbe7dab47f202cdd639bb64272c5be45d", "year": 2019, "venue": "CHI", "alt_text": "A hand-drawn picture largely in black felt-tip pen featuring two houses. The house has two children in each window who appear to be stuck - either because of the rain drawn in the picture or angry parents.", "levels": null, "corpus_id": 140220347, "sentences": ["A hand-drawn picture largely in black felt-tip pen featuring two houses.", "The house has two children in each window who appear to be stuck - either because of the rain drawn in the picture or angry parents."], "caption": "", "local_uri": ["eee0090bbe7dab47f202cdd639bb64272c5be45d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Designing for Digital Playing Out", "pdf_hash": "eee0090bbe7dab47f202cdd639bb64272c5be45d", "year": 2019, "venue": "CHI", "alt_text": "A map of Meadow Well with different colours indicated how a child uses their neightbour.", "levels": null, "corpus_id": 140220347, "sentences": ["A map of Meadow Well with different colours indicated how a child uses their neightbour."], "caption": "", "local_uri": ["eee0090bbe7dab47f202cdd639bb64272c5be45d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Designing for Digital Playing Out", "pdf_hash": "eee0090bbe7dab47f202cdd639bb64272c5be45d", "year": 2019, "venue": "CHI", "alt_text": "A child has drawn a picture of themselves and a friend playing 'Catch'. The card is position above a map of Meadow Well.", "levels": null, "corpus_id": 140220347, "sentences": ["A child has drawn a picture of themselves and a friend playing 'Catch'.", "The card is position above a map of Meadow Well."], "caption": "", "local_uri": ["eee0090bbe7dab47f202cdd639bb64272c5be45d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Designing for Digital Playing Out", "pdf_hash": "eee0090bbe7dab47f202cdd639bb64272c5be45d", "year": 2019, "venue": "CHI", "alt_text": "This is a photo that depicts how a child has decorated a map and placed this on a tree in the community.", "levels": null, "corpus_id": 140220347, "sentences": ["This is a photo that depicts how a child has decorated a map and placed this on a tree in the community."], "caption": "Figure 1. From left to right: (a) Poppy\u2019s drawing of play on the local estate; (b) Map activity (c) Card describing play (d)", "local_uri": ["eee0090bbe7dab47f202cdd639bb64272c5be45d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Investigating the feasibility of extracting tool demonstrations from in-situ video content", "pdf_hash": "9c3dff78167c8fa4d89107bcf0921c6f46ddb569", "year": 2014, "venue": "CHI", "alt_text": "Architecture for remotely gathering synchronized screen recording and log data. A web server running on Amazon S3/EC2 serves a web-based image editor. As the user uses the image editor, log events are sent to a screen recording and logging utility installed on the user's computer. This utility uploads the recorded data back to us using Amazon web services.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 15193243, "sentences": ["Architecture for remotely gathering synchronized screen recording and log data.", "A web server running on Amazon S3/EC2 serves a web-based image editor.", "As the user uses the image editor, log events are sent to a screen recording and logging utility installed on the user's computer.", "This utility uploads the recorded data back to us using Amazon web services."], "caption": "Figure 1. Architecture for remotely gathering synchronized screen recording and log data.", "local_uri": ["9c3dff78167c8fa4d89107bcf0921c6f46ddb569_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating the feasibility of extracting tool demonstrations from in-situ video content", "pdf_hash": "9c3dff78167c8fa4d89107bcf0921c6f46ddb569", "year": 2014, "venue": "CHI", "alt_text": "Diagram showing three timelines to demonstrate (a) the naive approach of segmenting each tool invocation, (b) segmenting to include tool selection and merge contiguous invocations of the tool, and (c) our clip segmentation approach.", "levels": null, "corpus_id": 15193243, "sentences": ["Diagram showing three timelines to demonstrate (a) the naive approach of segmenting each tool invocation, (b) segmenting to include tool selection and merge contiguous invocations of the tool, and (c) our clip segmentation approach."], "caption": "Figure 3. Three clip segmentation approaches. (a) Segment each invocation, (b) Include tool selection and merge contiguous invocations, (c) Our segmentation algorithm.", "local_uri": ["9c3dff78167c8fa4d89107bcf0921c6f46ddb569_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Investigating the feasibility of extracting tool demonstrations from in-situ video content", "pdf_hash": "9c3dff78167c8fa4d89107bcf0921c6f46ddb569", "year": 2014, "venue": "CHI", "alt_text": "The results of a qualtiative analysis of participants\u2019 rationales for rating clips. Blue bars extending to the right of the central axis indicate attributes cited as positive. Red bars extending to the left of the central axis indicate attributes cited as negative.", "levels": [[1], [1], [1]], "corpus_id": 15193243, "sentences": ["The results of a qualtiative analysis of participants\u2019 rationales for rating clips.", "Blue bars extending to the right of the central axis indicate attributes cited as positive.", "Red bars extending to the left of the central axis indicate attributes cited as negative."], "caption": "", "local_uri": ["9c3dff78167c8fa4d89107bcf0921c6f46ddb569_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating the feasibility of extracting tool demonstrations from in-situ video content", "pdf_hash": "9c3dff78167c8fa4d89107bcf0921c6f46ddb569", "year": 2014, "venue": "CHI", "alt_text": "A design sketch showing a tooltip that includes an interface for playing a short video clip. The tooltip also includes up and down arrow buttons for voting on whether the clip is good or not. There is also an area that displays thumbnails of Additional Clips.", "levels": null, "corpus_id": 15193243, "sentences": ["A design sketch showing a tooltip that includes an interface for playing a short video clip.", "The tooltip also includes up and down arrow buttons for voting on whether the clip is good or not.", "There is also an area that displays thumbnails of Additional Clips."], "caption": "Figure 7. Design sketch showing how tool clips could be collaboratively filtered, and how multiple tool clips could be presented to users.", "local_uri": ["9c3dff78167c8fa4d89107bcf0921c6f46ddb569_Image_009.png"], "annotated": false, "compound": false}
{"title": "Projective Windows: Bringing Windows in Space to the Fingertip", "pdf_hash": "499f9bc80501599f8cd9a77febcb750dd1363be4", "year": 2018, "venue": "CHI", "alt_text": "https://lh4.googleusercontent.com/gX0zeBg6PAU2Pz2vBF0ereCYYFldvrywdN3dJ1w6Kl_0E3SuOyqlN7Y9-cjzkLtmFiyDEHfNccWSlBslzB_DK9LRJcYP2KGCH_AoLD_atKk713_J_MYk52xZXtSYZD-uSKezoXEf", "levels": null, "corpus_id": 5046739, "sentences": ["https://lh4.googleusercontent.com/gX0zeBg6PAU2Pz2vBF0ereCYYFldvrywdN3dJ1w6Kl_0E3SuOyqlN7Y9-cjzkLtmFiyDEHfNccWSlBslzB_DK9LRJcYP2KGCH_AoLD_atKk713_J_MYk52xZXtSYZD-uSKezoXEf"], "caption": "Typical values for D are 1 m for a wall just out of reach, and 4 m for a distant wall in a conference room. For d, they are", "local_uri": ["499f9bc80501599f8cd9a77febcb750dd1363be4_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Editing Spatial Layouts through Tactile Templates for People with Visual Impairments", "pdf_hash": "f3483b976f018c80e472fc35b0f89c12b24b608e", "year": 2019, "venue": "CHI", "alt_text": "An iPad with a sheet of capsule paper overlaid on the screen. The capsule paper shows a website layout. A user is pointing to an element on the layout, and the iPad has a speech bubble which says, \"Element: Heading, level 2. Content: Project 3 Title. Text align: left.\"", "levels": null, "corpus_id": 140238918, "sentences": ["An iPad with a sheet of capsule paper overlaid on the screen.", "The capsule paper shows a website layout.", "A user is pointing to an element on the layout, and the iPad has a speech bubble which says, \"Element: Heading, level 2.", "Content: Project 3 Title.", "Text align: left.\""], "caption": "Figure 1: Our interface leverages interactive tactile sheets to convey web page structure and content. Here, a user double taps on a page element to hear more information.", "local_uri": ["f3483b976f018c80e472fc35b0f89c12b24b608e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Editing Spatial Layouts through Tactile Templates for People with Visual Impairments", "pdf_hash": "f3483b976f018c80e472fc35b0f89c12b24b608e", "year": 2019, "venue": "CHI", "alt_text": "Four images (labeled A through D) showing different representations of a sample HTML template. From left to right: (A) shows a sample HTML template which includes a navigation menu, a header that reads 'Hi, I'm Your Name' next to a placeholder for a header image, and placeholders for two images with subtitles and descriptions. (B) shows the same template with a capsule paper overlay of the tool's automatically generated pixel representation. The paper has solid raised lines defining the boundaries of each of the content HTML elements. (C) shows the same template with a capsule paper overlay of the inflated representation. In addition to solid raised lines defining HTML content elements, it has dashed raised lines defining the rows and column elements of the HTML template (D) shows a capsule paper of a sample small multiples representation. This representation has three rows defined by raised dashed line rectangular boxes; each has a Braille label that reads Margin 0, Margin 2, and Margin 4. Inside the dashed boxes are two solid line rectangular box elements.", "levels": null, "corpus_id": 140238918, "sentences": ["Four images (labeled A through D) showing different representations of a sample HTML template.", "From left to right: (A) shows a sample HTML template which includes a navigation menu, a header that reads 'Hi, I'm Your Name' next to a placeholder for a header image, and placeholders for two images with subtitles and descriptions.", "(B) shows the same template with a capsule paper overlay of the tool's automatically generated pixel representation.", "The paper has solid raised lines defining the boundaries of each of the content HTML elements. (C) shows the same template with a capsule paper overlay of the inflated representation.", "In addition to solid raised lines defining HTML content elements, it has dashed raised lines defining the rows and column elements of the HTML template (D) shows a capsule paper of a sample small multiples representation.", "This representation has three rows defined by raised dashed line rectangular boxes; each has a Braille label that reads Margin 0, Margin 2, and Margin 4.", "Inside the dashed boxes are two solid line rectangular box elements."], "caption": "", "local_uri": ["f3483b976f018c80e472fc35b0f89c12b24b608e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Editing Spatial Layouts through Tactile Templates for People with Visual Impairments", "pdf_hash": "f3483b976f018c80e472fc35b0f89c12b24b608e", "year": 2019, "venue": "CHI", "alt_text": "A menu printed in capsule paper has 6 options represented by icons (from left to right): 1) the Select option has a check mark icon, 2) the Back option has an  arrow pointing left, 3) the Next option has an arrow pointing right, 4) the Edit option has a capital letter E, 5) the Print option has a capital letter P, and 6) the Cancel button has a capital letter X.", "levels": null, "corpus_id": 140238918, "sentences": ["A menu printed in capsule paper has 6 options represented by icons (from left to right): 1) the Select option has a check mark icon, 2) the Back option has an  arrow pointing left, 3) the Next option has an arrow pointing right, 4) the Edit option has a capital letter E, 5) the Print option has a capital letter P, and 6) the Cancel button has a capital letter X."], "caption": "the heading, and then selects the heading and uses the Copy style action to propagate the changes to the other two project headings. Finally, Ben prints a pixel representation of his layout to verify how", "local_uri": ["f3483b976f018c80e472fc35b0f89c12b24b608e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Editing Spatial Layouts through Tactile Templates for People with Visual Impairments", "pdf_hash": "f3483b976f018c80e472fc35b0f89c12b24b608e", "year": 2019, "venue": "CHI", "alt_text": "A side-by-side before-and-after image of two layout representations. On the left side, the \"Before\" image shows several elements labeled (A) through (D). (A) shows large dotted-line box spanning the entire width and height of the before photo, representing a row container. (B) shows a pair of smaller dot-dashed boxes dividing the row container into two columns. (C) shows a dashed-line box within the column container, about half its height. (D) shows a thin solid-line box representing content within (C). Also in the \"Before\" image are two pointing-hand icons, labeled (1) and (2), with a shaded region under the index finger to indicate touch. (1) shows the hand touching the right column container, and (2) shows the hand touching the row container. A right-arrow in the center points to the \"After\" image. The \"After\" image shows the same layout but with three columns, equally resized within the same row container.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 140238918, "sentences": ["A side-by-side before-and-after image of two layout representations.", "On the left side, the \"Before\" image shows several elements labeled (A) through (D). (", "A) shows large dotted-line box spanning the entire width and height of the before photo, representing a row container. (", "B) shows a pair of smaller dot-dashed boxes dividing the row container into two columns. (", "C) shows a dashed-line box within the column container, about half its height. (", "D) shows a thin solid-line box representing content within (C).", "Also in the \"Before\" image are two pointing-hand icons, labeled (1) and (2), with a shaded region under the index finger to indicate touch. (", "1) shows the hand touching the right column container, and (2) shows the hand touching the row container.", "A right-arrow in the center points to the \"After\" image.", "The \"After\" image shows the same layout but with three columns, equally resized within the same row container."], "caption": "", "local_uri": ["f3483b976f018c80e472fc35b0f89c12b24b608e_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Editing Spatial Layouts through Tactile Templates for People with Visual Impairments", "pdf_hash": "f3483b976f018c80e472fc35b0f89c12b24b608e", "year": 2019, "venue": "CHI", "alt_text": "A diagram representation of the tool's workflow. From left to right: Three icons representing various parts of the tool workflow, an HTML template, tactile sheets and small multiples, and an interactive design tool. Arrows connect these icons showing the workflow path. From the HTML  template, an outgoing arrow to tactile sheets and small multiples that is labeled with the action \"Parse DOM.\" From the tactile sheets, an arrow pointing to the design tool labeled \"Fabricate sheets.\" From the design tool, three outgoing arrows: one a loop to itself labeled \"make edits\", one to the tactile sheets labeled \"regenerate sheets\", and one back to the HTML template labeled \"export changes.\"", "levels": null, "corpus_id": 140238918, "sentences": ["A diagram representation of the tool's workflow. From left to right: Three icons representing various parts of the tool workflow, an HTML template, tactile sheets and small multiples, and an interactive design tool.", "Arrows connect these icons showing the workflow path.", "From the HTML  template, an outgoing arrow to tactile sheets and small multiples that is labeled with the action \"Parse DOM.\" From the tactile sheets, an arrow pointing to the design tool labeled \"Fabricate sheets.\" From the design tool, three outgoing arrows: one a loop to itself labeled \"make edits\", one to the tactile sheets labeled \"regenerate sheets\", and one back to the HTML template labeled \"export changes.\""], "caption": "Participants underwent one hour of training with our tool and one hour of a design task. During training, the proctor guided them through all of the editing features provided. For the design task, participants frst explored and chose from four Bootstrap templates for personal portfolio websites, and then flled in content and modifed the layout.", "local_uri": ["f3483b976f018c80e472fc35b0f89c12b24b608e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Online News Videos: The UX of Subtitle Position", "pdf_hash": "10ef2dd7830c95f2b9b56c22298a0eb8879ea393", "year": 2015, "venue": "ASSETS", "alt_text": "Diagram showing 4 layouts for subtitles positioned in a web page. Layout 1 shows a video half the size of the screen with subtitles over the video. Layout 2 shows a video half the size of the screen with subtitles under the video. Layout 3 shows a video two thirds the size of the screen with subtitles over the video. Layout 4 shows a video two thirds the size of the screen with subtitles under the video.", "levels": null, "corpus_id": 4141371, "sentences": ["Diagram showing 4 layouts for subtitles positioned in a web page.", "Layout 1 shows a video half the size of the screen with subtitles over the video.", "Layout 2 shows a video half the size of the screen with subtitles under the video.", "Layout 3 shows a video two thirds the size of the screen with subtitles over the video.", "Layout 4 shows a video two thirds the size of the screen with subtitles under the video."], "caption": "Figure 1 - Experiment Changes in Video Size and Subtitle Positioning", "local_uri": ["10ef2dd7830c95f2b9b56c22298a0eb8879ea393_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Programming, Problem Solving, and Self-Awareness: Effects of Explicit Guidance", "pdf_hash": "5f9eb9cc8e57ad370594a581d1a6edc421bbbcc3", "year": 2016, "venue": "CHI", "alt_text": "The Idea Garden Panel in Cloud9 and the Idea Garden icon in Cloud9's code editor", "levels": null, "corpus_id": 16626608, "sentences": ["The Idea Garden Panel in Cloud9 and the Idea Garden icon in Cloud9's code editor"], "caption": "", "local_uri": ["5f9eb9cc8e57ad370594a581d1a6edc421bbbcc3_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "VelociWatch: Designing and Evaluating a Virtual Keyboard for the Input of Challenging Text", "pdf_hash": "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405", "year": 2019, "venue": "CHI", "alt_text": "The left image shows VelociWatch after typing: it was the best of tum. The top half of the smartwatch screen contains the written text and prediction slots while the bottom half shows a QWERTY keyboard. The highlighted best slot is rum. The top left slot is tum, the top right slot is time, the bottom left is times, and the bottom right is timing. A backspace button is to the left of the text area. The right image shows the simpler keyboard after typing: it was the age of W I A F. A row above the QWERTY keyboard has three options. The left option is wisdom, the middle option is W I A F, and the right option is a backspace button.", "levels": null, "corpus_id": 140239443, "sentences": ["The left image shows VelociWatch after typing: it was the best of tum.", "The top half of the smartwatch screen contains the written text and prediction slots while the bottom half shows a QWERTY keyboard.", "The highlighted best slot is rum.", "The top left slot is tum, the top right slot is time, the bottom left is times, and the bottom right is timing.", "A backspace button is to the left of the text area.", "The right image shows the simpler keyboard after typing: it was the age of W I A F. A row above the QWERTY keyboard has three options.", "The left option is wisdom, the middle option is W I A F, and the right option is a backspace button."], "caption": "Figure 1: Entering text on the VelociWatch keyboard (left) and on a simpler version of the keyboard (right).", "local_uri": ["4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_002.jpg", "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "VelociWatch: Designing and Evaluating a Virtual Keyboard for the Input of Challenging Text", "pdf_hash": "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405", "year": 2019, "venue": "CHI", "alt_text": "A user progressively typing using their index finger on a Sony SmartWatch 3. The interface has the text written thus far in the top half. The lower half is a QWERTY keyboard. When the user's finger is touching the screen, the nearest letter is show in a large font that is alpha channeled over the text area of the interface.", "levels": null, "corpus_id": 140239443, "sentences": ["A user progressively typing using their index finger on a Sony SmartWatch 3.", "The interface has the text written thus far in the top half.", "The lower half is a QWERTY keyboard.", "When the user's finger is touching the screen, the nearest letter is show in a large font that is alpha channeled over the text area of the interface."], "caption": "", "local_uri": ["4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_009.jpg", "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_010.jpg", "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_011.jpg", "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_012.jpg", "4e25e97e0f4bd9e4eaa2349ab8be2ca26a441405_Image_013.jpg"], "annotated": false, "compound": true}
{"title": "Exploring Accessible Smartwatch Interactions for People with Upper Body Motor Impairments", "pdf_hash": "c1aba78b941676a9ae044baaa64018bb79180ed0", "year": 2018, "venue": "CHI", "alt_text": "This figure shows an example of a trial in Study 1. For example, the figure on the left is shown to the participants for the right-edge swipe. The figure on the right shows a successful attempt.", "levels": [[-1], [-1], [-1]], "corpus_id": 5039068, "sentences": ["This figure shows an example of a trial in Study 1.", "For example, the figure on the left is shown to the participants for the right-edge swipe.", "The figure on the right shows a successful attempt."], "caption": "Figure 1. The figure shows screens for the right-edge swipe trial before (left) and after a successful attempt (right).", "local_uri": ["c1aba78b941676a9ae044baaa64018bb79180ed0_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploring Accessible Smartwatch Interactions for People with Upper Body Motor Impairments", "pdf_hash": "c1aba78b941676a9ae044baaa64018bb79180ed0", "year": 2018, "venue": "CHI", "alt_text": "This image shows all the areas touched by all participants in the three tasks in Study 2. The numbers in each of the three figures represent the number of times that area was touched.", "levels": null, "corpus_id": 5039068, "sentences": ["This image shows all the areas touched by all participants in the three tasks in Study 2.", "The numbers in each of the three figures represent the number of times that area was touched."], "caption": "Figure 2. Areas touched during the three tasks for all 16 actions for all participants. In the mixed task, no participants chose to touch the skin locations. (Darkercolors represent a higher frequency of gestures).", "local_uri": ["c1aba78b941676a9ae044baaa64018bb79180ed0_Image_002.png"], "annotated": false, "compound": false}
{"title": "The gest-rest: a pressure-sensitive chairable input pad for power wheelchair armrests", "pdf_hash": "11a244495c5e0cfac72e44947c2601db68ef4740", "year": 2014, "venue": "ASSETS", "alt_text": "A four part figure showing four power wheelchair users with four different hand poses interacting with the Gest-Rest", "levels": null, "corpus_id": 15909535, "sentences": ["A four part figure showing four power wheelchair users with four different hand poses interacting with the Gest-Rest"], "caption": "Figure 1. The Gest-Rest is a prototype, pressure sensitive interaction platform for power wheelchair users. Each image shows a power wheelchair user with a different hand pose performing a swipe gesture.", "local_uri": ["11a244495c5e0cfac72e44947c2601db68ef4740_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The gest-rest: a pressure-sensitive chairable input pad for power wheelchair armrests", "pdf_hash": "11a244495c5e0cfac72e44947c2601db68ef4740", "year": 2014, "venue": "ASSETS", "alt_text": "The Gest-Rest sensing surface shown alongside arduino hardware and a breadboard.", "levels": null, "corpus_id": 15909535, "sentences": ["The Gest-Rest sensing surface shown alongside arduino hardware and a breadboard."], "caption": "Figure 2. Prototype Hardware. 12 Flexiforce FSRs arranged in a grid attached to the wheelchair armrest. (left) The sensors are connected to an Arduino (right), which outputs information via USB. When assembled, the electronics fit under the armrest, it has been disassembled for this photo.", "local_uri": ["11a244495c5e0cfac72e44947c2601db68ef4740_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "I did that! Measuring users' experience of agency in their own actions", "pdf_hash": "5bcc09ffdb0c0f75a8542f220d486b6bbb3bdf18", "year": 2012, "venue": "CHI", "alt_text": "http://t3.gstatic.com/images?q=tbn:ANd9GcSYZYGGi88xCHIsX2nuIqM0kr-vAthoS9uq1j8WxdIjoGdvc0DeUQ", "levels": null, "corpus_id": 14089793, "sentences": ["http://t3.gstatic.com/images?q=tbn:ANd9GcSYZYGGi88xCHIsX2nuIqM0kr-vAthoS9uq1j8WxdIjoGdvc0DeUQ"], "caption": "", "local_uri": ["5bcc09ffdb0c0f75a8542f220d486b6bbb3bdf18_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "I did that! Measuring users' experience of agency in their own actions", "pdf_hash": "5bcc09ffdb0c0f75a8542f220d486b6bbb3bdf18", "year": 2012, "venue": "CHI", "alt_text": "http://assets1.bigthink.com/system/idea_thumbnails/49163/headline/Depositphotos_11988845_xs.jpg?1359383173", "levels": null, "corpus_id": 14089793, "sentences": ["http://assets1.bigthink.com/system/idea_thumbnails/49163/headline/Depositphotos_11988845_xs.jpg?1359383173"], "caption": "Media Agency     Intelligent Agent", "local_uri": ["5bcc09ffdb0c0f75a8542f220d486b6bbb3bdf18_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication", "pdf_hash": "5e2b43efb44af88bae17ed1e7932d3df1506dfb6", "year": 2018, "venue": "CHI", "alt_text": "The top image shows the Expressive Keyboard. The keyboard contains four rows. The top row has contains the 8 emoji and the remaining four rows contain a QWERTY keyboard layout. The left-most column contains the four punctuation marks supported in the keyboard: a period, a comma, a question mark, and an exclamation point. At the top of the app, above the actual keyboard, is a text box where input text is displayed. To the right of this text box are a button with a pen icon and a button with an ear icon. These buttons open the Voicesetting Editor and the Active Listening Mode, respectively.", "levels": null, "corpus_id": 5047113, "sentences": ["The top image shows the Expressive Keyboard.", "The keyboard contains four rows.", "The top row has contains the 8 emoji and the remaining four rows contain a QWERTY keyboard layout.", "The left-most column contains the four punctuation marks supported in the keyboard: a period, a comma, a question mark, and an exclamation point.", "At the top of the app, above the actual keyboard, is a text box where input text is displayed.", "To the right of this text box are a button with a pen icon and a button with an ear icon.", "These buttons open the Voicesetting Editor and the Active Listening Mode, respectively."], "caption": "", "local_uri": ["5e2b43efb44af88bae17ed1e7932d3df1506dfb6_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication", "pdf_hash": "5e2b43efb44af88bae17ed1e7932d3df1506dfb6", "year": 2018, "venue": "CHI", "alt_text": "This table contains the eight emoji operators from the expressive keyboard and a description of their functionality. The following list describes each: 1. Crying Emoji: Sets the emotional tone of the voice to sad. 2. Sleepy Emoji: Sets the emotional tone of the voice to calm. 3. Smiling Emoji: Sets the emotional tone of the voice to happy. 4. Laughing Emoji: Sets the emotional tone of the voice to happy and prepends the laughter sound effect. 5. Winking Sarcastic Emoji: Sets the emotional tone of the voice to happy and prepends the sarcastic scoff sound effect. 6. Surprised Emoji: Prepends the sharp intake of breath sound effect. 7. Irritated Emoji: Sets the emotional tone of the voice to cross (i.e., angry). 8. Angry Emoji: Sets the emotional tone of the voice to cross and prepends an angry argh sound effect.", "levels": null, "corpus_id": 5047113, "sentences": ["This table contains the eight emoji operators from the expressive keyboard and a description of their functionality.", "The following list describes each: 1.", "Crying Emoji: Sets the emotional tone of the voice to sad. 2. Sleepy Emoji: Sets the emotional tone of the voice to calm. 3. Smiling Emoji: Sets the emotional tone of the voice to happy. 4.", "Laughing Emoji: Sets the emotional tone of the voice to happy and prepends the laughter sound effect. 5.", "Winking Sarcastic Emoji: Sets the emotional tone of the voice to happy and prepends the sarcastic scoff sound effect.", "6. Surprised Emoji: Prepends the sharp intake of breath sound effect.", "7. Irritated Emoji: Sets the emotional tone of the voice to cross (i.e., angry). 8.", "Angry Emoji: Sets the emotional tone of the voice to cross and prepends an angry argh sound effect."], "caption": "", "local_uri": ["5e2b43efb44af88bae17ed1e7932d3df1506dfb6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication", "pdf_hash": "5e2b43efb44af88bae17ed1e7932d3df1506dfb6", "year": 2018, "venue": "CHI", "alt_text": "When a user clicks the ear icon button in the Expressive Keyboard, the keyboard is greyed out, and the Active Listening Mode buttons are displayed on top of the greyed out background. The buttons are circular and contain text descrbing the vocal sound effect each one plays. For instance, there is a button with the word \"laugh\" in it, which plays laughter when clicked. There is a close button for returning to the Expressive Keyboard.", "levels": null, "corpus_id": 5047113, "sentences": ["When a user clicks the ear icon button in the Expressive Keyboard, the keyboard is greyed out, and the Active Listening Mode buttons are displayed on top of the greyed out background.", "The buttons are circular and contain text descrbing the vocal sound effect each one plays.", "For instance, there is a button with the word \"laugh\" in it, which plays laughter when clicked.", "There is a close button for returning to the Expressive Keyboard."], "caption": "Figure 1. Emoji (top, b) and punctuation marks (top, a) act as operators over text typed in the Expressive Keyboard (top), changing the synthetic speech output. The ALM (bottom) is initiated by selecting the \u201cear\u201d key in the Expressive Keyboard (c); it is displayed as a set of buttons overlaid on the keyboard, each corresponding to a different vocal sound effect.", "local_uri": ["5e2b43efb44af88bae17ed1e7932d3df1506dfb6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication", "pdf_hash": "5e2b43efb44af88bae17ed1e7932d3df1506dfb6", "year": 2018, "venue": "CHI", "alt_text": "This figure includes six images. The first image shows a high level overview of the editor. At the top of the editor, there is a set of buttons allowing the user to return to the keyboard, pause the eye gaze input, reset, undo, and redo changes, and play the current or original speech. The second image shows a radial menu opened around a word token in the editor. This radial menu contains options to insert a sound effect, preview the token, or change the tone, emphasis, pitch, volume, or rate of speech for the token. The third image shows the horizontal linear menu that opens when a user makes a range selection. It contains options to edit the tone, pitch, rate of speech, and volume for the range of tokens. The fourth image shows the interface for editing a property value of a single token. There are buttons for increasing and decreasing the property value and a button to preview the speech for the token. The fifth image shows how volume, rate of speech, and emotional tones are all displayed using lines above the tokens in the editor. The sixth image shows how boldness and baseline height of text in tokens changes to display the value of the emphasis and pitch properties for thos tokens.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 5047113, "sentences": ["This figure includes six images.", "The first image shows a high level overview of the editor.", "At the top of the editor, there is a set of buttons allowing the user to return to the keyboard, pause the eye gaze input, reset, undo, and redo changes, and play the current or original speech.", "The second image shows a radial menu opened around a word token in the editor.", "This radial menu contains options to insert a sound effect, preview the token, or change the tone, emphasis, pitch, volume, or rate of speech for the token.", "The third image shows the horizontal linear menu that opens when a user makes a range selection.", "It contains options to edit the tone, pitch, rate of speech, and volume for the range of tokens.", "The fourth image shows the interface for editing a property value of a single token.", "There are buttons for increasing and decreasing the property value and a button to preview the speech for the token.", "The fifth image shows how volume, rate of speech, and emotional tones are all displayed using lines above the tokens in the editor.", "The sixth image shows how boldness and baseline height of text in tokens changes to display the value of the emphasis and pitch properties for thos tokens."], "caption": "Figure 2. The Voicesetting Editor interface parses and displays input text as word, punctuation, and vocal sound effect tokens (a, e). Speech property values can be edited on a single token basis (b) or over a range of tokens (c). Speech property values are only allowed to take on values from a set of predefined levels with semantically meaningful names (d). Speech property values are displayed using visual properties in the editor (e, f, g). Emotional tone, volume, and rate of speech are displayed as lines over token buttons with value descriptions like \u201cLoud\u201d at the outset of the line (e). Voice pitch is displayed using baseline height (f, g - higher baseline height corresponds to higher pitch). Emphasis is displayed using font boldness (f, g \u2013 bolder font corresponds to heavier emphasis). Oval width corresponds to pause length associated with inter-word or punctuation tokens (f depicts a longer pause than g).", "local_uri": ["5e2b43efb44af88bae17ed1e7932d3df1506dfb6_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication", "pdf_hash": "5e2b43efb44af88bae17ed1e7932d3df1506dfb6", "year": 2018, "venue": "CHI", "alt_text": "This table contains one row for each of the nine prompts showing the statistical analysis results of question responses for those prompts in the online questionnaire data. There are two parts: one analyzing responses in part Q1 and one analyzing responses in part Q2. For part Q1, Fisher tests found significant differences between participant's responses for the three conditions for 7 prompts (prompts 2, 3, 4, 5, 6, 8, and 9). Prompt 7 was not included in Q1. For Q2, Friedman tests found significant differences between the three conditions for prompts 2, 4, 6, 7, 8, and 9.", "levels": [[1], [1], [3], [1], [3]], "corpus_id": 5047113, "sentences": ["This table contains one row for each of the nine prompts showing the statistical analysis results of question responses for those prompts in the online questionnaire data.", "There are two parts: one analyzing responses in part Q1 and one analyzing responses in part Q2.", "For part Q1, Fisher tests found significant differences between participant's responses for the three conditions for 7 prompts (prompts 2, 3, 4, 5, 6, 8, and 9).", "Prompt 7 was not included in Q1.", "For Q2, Friedman tests found significant differences between the three conditions for prompts 2, 4, 6, 7, 8, and 9."], "caption": "Table 2. Online questionnaire results. Prompt IDs correspond to those in Supplemental Table 1. The abbreviations U, K, and E correspond to the Unedited, Keyboard, and Editor conditions, respectively. Q2 responses are on a scale from \u201cVery Different\u201d (1) to \u201cVery Similar\u201d (6). For all p value results, bold font indicates \ud835\udc91 < \ud835\udfce. \ud835\udfce\ud835\udfcf, while bold and underlined font indicates \ud835\udc91 < \ud835\udfce. \ud835\udfce\ud835\udfce\ud835\udfcf. Green cells indicate that the most frequent response matched the intended emotion. Post-hoc Wilcoxon tests (with Bonferonni correction, \ud835\udc91 < \ud835\udfce. \ud835\udfce\ud835\udfcf\ud835\udfd5) were only performed in Q2 for prompts that had a significant p-value from Friedman\u2019s Test (\ud835\udc91 < \ud835\udfce. \ud835\udfce\ud835\udfcf).", "local_uri": ["5e2b43efb44af88bae17ed1e7932d3df1506dfb6_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Usable gestures for blind people: understanding preference and performance", "pdf_hash": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e", "year": 2011, "venue": "CHI", "alt_text": "Two images of triangle gestures. The left gesture was created by a blind study participant, while the right gesture was created by a sighted participant.", "levels": null, "corpus_id": 1681652, "sentences": ["Two images of triangle gestures.", "The left gesture was created by a blind study participant, while the right gesture was created by a sighted participant."], "caption": "Figure 1. Two representative versions of a triangle gesture produced by a blind person (left) and a sighted person (right).", "local_uri": ["9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Usable gestures for blind people: understanding preference and performance", "pdf_hash": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e", "year": 2011, "venue": "CHI", "alt_text": "Photograph of a touch screen tablet PC placed on a table. A study participant performs a gesture on the screen.", "levels": [[-1], [-1]], "corpus_id": 1681652, "sentences": ["Photograph of a touch screen tablet PC placed on a table.", "A study participant performs a gesture on the screen."], "caption": "not creating gestures for any specific operating system or application. The shapes on the screen provided both visual and audio feedback: for example, touching a square would cause the program to speak the word \u201csquare,\u201d and a white noise loop played while the participant held his or her finger over the shape.", "local_uri": ["9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Usable gestures for blind people: understanding preference and performance", "pdf_hash": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e", "year": 2011, "venue": "CHI", "alt_text": "Bar chart of ratings by gesture category, for blind and sighted participants.  Values on a scale from 1 to 7: Tap gestures: Rated 6.1 by blind participants, 6.3 by sighted participants Flick gestures: Rated 6.5 by blind participants, 6.3 by sighted participants Multi-touch gestures: Rated 5.7  by blind participants, 4.3 by sighted participants Shape gestures: Rated 4.9 by blind participants, 5.7  by sighted participants Symbol gestures: Rated 5.4 by blind participants, 5.7 by sighted participants", "levels": [[1], [2, 1]], "corpus_id": 1681652, "sentences": ["Bar chart of ratings by gesture category, for blind and sighted participants.", "Values on a scale from 1 to 7: Tap gestures: Rated 6.1 by blind participants, 6.3 by sighted participants Flick gestures: Rated 6.5 by blind participants, 6.3 by sighted participants Multi-touch gestures: Rated 5.7  by blind participants, 4.3 by sighted participants Shape gestures: Rated 4.9 by blind participants, 5.7  by sighted participants Symbol gestures: Rated 5.4 by blind participants, 5.7 by sighted participants"], "caption": "Figure 4. Participants\u2019 easiness ratings by gesture category.", "local_uri": ["9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Usable gestures for blind people: understanding preference and performance", "pdf_hash": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e", "year": 2011, "venue": "CHI", "alt_text": "Two circle gestures drawn by a blind participant. In both gestures, there is a large distance between the start and end points of the circle.", "levels": [[-1], [-1]], "corpus_id": 1681652, "sentences": ["Two circle gestures drawn by a blind participant.", "In both gestures, there is a large distance between the start and end points of the circle."], "caption": "Figure 5. Two circle gestures drawn by a blind participant with high form closure distance, i.e., a large distance between the start and end points.", "local_uri": ["9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Usable gestures for blind people: understanding preference and performance", "pdf_hash": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e", "year": 2011, "venue": "CHI", "alt_text": "Two square gestures. The left square has wavy lines, while the right square has straighter lines.", "levels": null, "corpus_id": 1681652, "sentences": ["Two square gestures.", "The left square has wavy lines, while the right square has straighter lines."], "caption": "Figure 6. Lines drawn by blind participants tended to be less steady (left) than lines drawn by sighted participants (right).", "local_uri": ["9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e_Image_008.gif"], "annotated": false, "compound": false}
{"title": "Audio-visual speech understanding in simulated telephony applications by individuals with hearing loss", "pdf_hash": "020621edc59be97e9a2d086b5133a48f6c62cc9c", "year": 2013, "venue": "ASSETS", "alt_text": "Experimental setup showing the speaker setup (one speaker to the left and right of the flat screen monitor) and the screen with the cardboard mask.   The mask contains a photo of a simulated iPhone, with the stimulus video showing through a cutout.", "levels": null, "corpus_id": 8024642, "sentences": ["Experimental setup showing the speaker setup (one speaker to the left and right of the flat screen monitor) and the screen with the cardboard mask.", "The mask contains a photo of a simulated iPhone, with the stimulus video showing through a cutout."], "caption": "", "local_uri": ["020621edc59be97e9a2d086b5133a48f6c62cc9c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Audio-visual speech understanding in simulated telephony applications by individuals with hearing loss", "pdf_hash": "020621edc59be97e9a2d086b5133a48f6c62cc9c", "year": 2013, "venue": "ASSETS", "alt_text": "Graph showing Audio Only vs. Audio + Video at 95% confidence interval. Y axis is difference in words correct. X axis indicates the 7 conditions. Mean difference and 95% confidence intervals around the differences of words correct out of 102 vs. audio only.  The only interval that crosses zero belongs to condition 3 and is not significantly different from 0.", "levels": [[1], [1], [1], [1], [3, 2]], "corpus_id": 8024642, "sentences": ["Graph showing Audio Only vs. Audio + Video at 95% confidence interval.", "Y axis is difference in words correct.", "X axis indicates the 7 conditions.", "Mean difference and 95% confidence intervals around the differences of words correct out of 102 vs. audio only.", "The only interval that crosses zero belongs to condition 3 and is not significantly different from 0."], "caption": "", "local_uri": ["020621edc59be97e9a2d086b5133a48f6c62cc9c_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Audio-visual speech understanding in simulated telephony applications by individuals with hearing loss", "pdf_hash": "020621edc59be97e9a2d086b5133a48f6c62cc9c", "year": 2013, "venue": "ASSETS", "alt_text": "Graph showing differences in words correct for 7.5 vs. 15 vs. 30 fps at 95% confidence level.  The X axis indicates frame rates 30-15, 15-7.5, and 30-7.5).  The Y axis indicates differences in words correct. Mean difference and 95% confidence intervals around the differences of words correct out of 102 across frame rates with no AV delay. One interval that crosses zero (dotted line) is not significantly different from 0. Overall, increasing the frame rate from 7.5 fps to 15 fps or from 7.5 fps to 30 fps results in significant improvements in words correct. The difference between 15 fps and 30 fps is not significant.", "levels": [[1], [1], [1], [2], [2], [3], [2]], "corpus_id": 8024642, "sentences": ["Graph showing differences in words correct for 7.5 vs. 15 vs. 30 fps at 95% confidence level.", "The X axis indicates frame rates 30-15, 15-7.5, and 30-7.5).", "The Y axis indicates differences in words correct.", "Mean difference and 95% confidence intervals around the differences of words correct out of 102 across frame rates with no AV delay.", "One interval that crosses zero (dotted line) is not significantly different from 0.", "Overall, increasing the frame rate from 7.5 fps to 15 fps or from 7.5 fps to 30 fps results in significant improvements in words correct.", "The difference between 15 fps and 30 fps is not significant."], "caption": "", "local_uri": ["020621edc59be97e9a2d086b5133a48f6c62cc9c_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Audio-visual speech understanding in simulated telephony applications by individuals with hearing loss", "pdf_hash": "020621edc59be97e9a2d086b5133a48f6c62cc9c", "year": 2013, "venue": "ASSETS", "alt_text": "Differences in words correct at 15 fps for varying perceived AV delay at 95% confidence interval.  X axis indicates ms: -100 vs 0, 0 vs +100, and +100 vs -100.  Y axis indicates difference in words correct ranging from -20 to 20. One interval that crosses zero is not significantly different from 0. Overall, perceived audio delays of 0 ms or 100 ms versus the video results in significant improvements over audio that is perceived 100 ms early. The difference between 0 ms and 100 ms perceived delay is not significant.", "levels": null, "corpus_id": 8024642, "sentences": ["Differences in words correct at 15 fps for varying perceived AV delay at 95% confidence interval.", "X axis indicates ms: -100 vs 0, 0 vs +100, and +100 vs -100.", "Y axis indicates difference in words correct ranging from -20 to 20.", "One interval that crosses zero is not significantly different from 0.", "Overall, perceived audio delays of 0 ms or 100 ms versus the video results in significant improvements over audio that is perceived 100 ms early.", "The difference between 0 ms and 100 ms perceived delay is not significant."], "caption": "", "local_uri": ["020621edc59be97e9a2d086b5133a48f6c62cc9c_Image_004.png"], "annotated": false, "compound": false}
{"title": "Audio-visual speech understanding in simulated telephony applications by individuals with hearing loss", "pdf_hash": "020621edc59be97e9a2d086b5133a48f6c62cc9c", "year": 2013, "venue": "ASSETS", "alt_text": "Differences in words correct at 7.5 fps for varying perceived AV delay at 95% confidence interval.  X axis indicates ms: -100 vs 0, 0 vs +100, and +100 vs -100.  Y axis indicates difference in words correct ranging from -20 to 20. One interval that crosses zero is not significantly different from 0. Overall, perceived audio delays of 0 ms or 100 ms versus the video results in significant improvements over audio that is perceived 100 ms early. The difference between 0 ms and 100 ms perceived delay is not significant.", "levels": null, "corpus_id": 8024642, "sentences": ["Differences in words correct at 7.5 fps for varying perceived AV delay at 95% confidence interval.", "X axis indicates ms: -100 vs 0, 0 vs +100, and +100 vs -100.", "Y axis indicates difference in words correct ranging from -20 to 20.", "One interval that crosses zero is not significantly different from 0.", "Overall, perceived audio delays of 0 ms or 100 ms versus the video results in significant improvements over audio that is perceived 100 ms early.", "The difference between 0 ms and 100 ms perceived delay is not significant."], "caption": "", "local_uri": ["020621edc59be97e9a2d086b5133a48f6c62cc9c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "The Space of Possibilities: Political Economies of Technology Innovation in Sub-Saharan Africa", "pdf_hash": "72fafaeea23a2671bde71d788171ea70c82a29d6", "year": 2018, "venue": "CHI", "alt_text": "https://lh6.googleusercontent.com/gX6y6DwZwTyCS-AW_ceyValpzamGA-BkG0blTuxRaK-FfC1QNwKNczaDkmHnHyZUagqm8z8v3XVq4LWn-0OOkyPORx5BckbFlY8_vApwvh8e3aG5dNmtm2oSqqgRbhFX2XSwCL8o", "levels": null, "corpus_id": 5068650, "sentences": ["https://lh6.googleusercontent.com/gX6y6DwZwTyCS-AW_ceyValpzamGA-BkG0blTuxRaK-FfC1QNwKNczaDkmHnHyZUagqm8z8v3XVq4LWn-0OOkyPORx5BckbFlY8_vApwvh8e3aG5dNmtm2oSqqgRbhFX2XSwCL8o"], "caption": "Figure 2: Interviewees by country", "local_uri": ["72fafaeea23a2671bde71d788171ea70c82a29d6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Should I Trust It When I Cannot See It?: Credibility Assessment for Blind Web Users", "pdf_hash": "16d0ce00f8bad9a8777fa9f930bf5d365097cd65", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 1: Grinning Page. The page has a green colored background which fades towards the right-hand side.  A single column format is used. This page was not found to be visually appealing by sighted participants.", "levels": null, "corpus_id": 17613435, "sentences": ["Figure 1: Grinning Page.", "The page has a green colored background which fades towards the right-hand side.", "A single column format is used.", "This page was not found to be visually appealing by sighted participants."], "caption": "", "local_uri": ["16d0ce00f8bad9a8777fa9f930bf5d365097cd65_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Should I Trust It When I Cannot See It?: Credibility Assessment for Blind Web Users", "pdf_hash": "16d0ce00f8bad9a8777fa9f930bf5d365097cd65", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 2: FactCheck Page.  Black colored text presented on a white background. The page was found to lack headings. Though images were labeled, these labels were not found to be meaningful, which led to difficulties for blind users to gain an appropriate overview of the content.", "levels": null, "corpus_id": 17613435, "sentences": ["Figure 2: FactCheck Page.  Black colored text presented on a white background.", "The page was found to lack headings.", "Though images were labeled, these labels were not found to be meaningful, which led to difficulties for blind users to gain an appropriate overview of the content."], "caption": "", "local_uri": ["16d0ce00f8bad9a8777fa9f930bf5d365097cd65_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Social Influences on Executive Functioning in Autism: Design of a Mobile Gaming Platform", "pdf_hash": "27fdb4d7799ec3314c613033217efcfa68c7f474", "year": 2018, "venue": "CHI", "alt_text": "https://lh6.googleusercontent.com/cH9dVHnK-E_sHIkzKn_P2L7ax37v-rg2Vry0bWohKwdOqD7b5gakIT1SrPNQl_b6L554P0IIm7Dgq6n5Rud8gGQhxxAxImNfPcqr0dLXru2fIwDr_Ha6Yuw5UHhORYGXqQ5O7UpU", "levels": null, "corpus_id": 5040578, "sentences": ["https://lh6.googleusercontent.com/cH9dVHnK-E_sHIkzKn_P2L7ax37v-rg2Vry0bWohKwdOqD7b5gakIT1SrPNQl_b6L554P0IIm7Dgq6n5Rud8gGQhxxAxImNfPcqr0dLXru2fIwDr_Ha6Yuw5UHhORYGXqQ5O7UpU"], "caption": "Figure 2. Participant Playing the Game in Lab Setting with the Experimenter", "local_uri": ["27fdb4d7799ec3314c613033217efcfa68c7f474_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction", "pdf_hash": "09da6da7d4ca5d2126e5e4cc9d4b7a65b914c92a", "year": 2014, "venue": "CHI", "alt_text": "A user controlling a computer through the movements of his body, while sitting on a flexible office chair.", "levels": null, "corpus_id": 130981, "sentences": ["A user controlling a computer through the movements of his body, while sitting on a flexible office chair."], "caption": "Figure 1. A user controlling a computer through the move- ments of his body, while sitting on a flexible office chair.", "local_uri": ["09da6da7d4ca5d2126e5e4cc9d4b7a65b914c92a_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction", "pdf_hash": "09da6da7d4ca5d2126e5e4cc9d4b7a65b914c92a", "year": 2014, "venue": "CHI", "alt_text": "A user performing a simple navigation task through chair-based web browser control.", "levels": null, "corpus_id": 130981, "sentences": ["A user performing a simple navigation task through chair-based web browser control."], "caption": "", "local_uri": ["09da6da7d4ca5d2126e5e4cc9d4b7a65b914c92a_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction", "pdf_hash": "09da6da7d4ca5d2126e5e4cc9d4b7a65b914c92a", "year": 2014, "venue": "CHI", "alt_text": "A user performing a simple navigation task through chair-based web browser control (tilt left).", "levels": null, "corpus_id": 130981, "sentences": ["A user performing a simple navigation task through chair-based web browser control (tilt left)."], "caption": "Figure 5. A user performing a simple navigation task through chair-based web browser control.", "local_uri": ["09da6da7d4ca5d2126e5e4cc9d4b7a65b914c92a_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Showdown: An Accessible Virtual Reality Game with Scaffolds for Youth with Visual Impairments", "pdf_hash": "d1a6409bd2e308b4a1c900c858eab7c6d089efc3", "year": 2019, "venue": "CHI", "alt_text": "We explain the trajectories and endpoints of the balls. There are three starting points on the opponent's side and five ending points on the player's side. We describe the trajectories based on the participant's right and left sides, always moving from opponent to participant.\n\nStarting at Participant's left side: paths 1-5\nStarting at Paritcipant's center: paths 6-10\nStarting at Participant's right side: paths 11-15\n\nFor each starting ball, it will end in one of five locations on the Participant's side.\nEnding at Participant's far left side: Paths 1, 6, 11\nEnding at Pariticipant's center left side: Paths 2, 7, 12\nEnding at Participant's center: Paths 3, 8, 13\nEnding at Participant's centter right side: Paths 4, 9, 14\nEnding at Participant's right side: Paths 5, 10, 15", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 140241274, "sentences": ["We explain the trajectories and endpoints of the balls.", "There are three starting points on the opponent's side and five ending points on the player's side.", "We describe the trajectories based on the participant's right and left sides, always moving from opponent to participant.", "Starting at Participant's left side: paths 1-5\nStarting at Paritcipant's center: paths 6-10\nStarting at Participant's right side: paths 11-15\n\nFor each starting ball, it will end in one of five locations on the Participant's side.", "Ending at Participant's far left side: Paths 1, 6, 11\nEnding at Pariticipant's center left side: Paths 2, 7, 12\nEnding at Participant's center: Paths 3, 8, 13\nEnding at Participant's centter right side: Paths 4, 9, 14\nEnding at Participant's right side: Paths 5, 10, 15"], "caption": "Figure 5. Illustration of the trajectories and endpoints. Two examples: 1) Trajectory 1 starts in the upper left corner and finishes in the lower left corner; 2) Endpoint 5 is located in the lower right corner.", "local_uri": ["d1a6409bd2e308b4a1c900c858eab7c6d089efc3_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The usability of CommandMaps in realistic tasks", "pdf_hash": "dcb972a43968970817f949416a649b5459b91cb9", "year": 2014, "venue": "CHI", "alt_text": "Microsoft Word with the CommandMap tab visible and controls removed from the Ribbon.", "levels": null, "corpus_id": 164017, "sentences": ["Microsoft Word with the CommandMap tab visible and controls removed from the Ribbon."], "caption": "", "local_uri": ["dcb972a43968970817f949416a649b5459b91cb9_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Taking things apart: reaching common ground and shared material understanding", "pdf_hash": "2d307a8d909e50ea0f9c18a12c35819a7abd139e", "year": 2014, "venue": "CHI", "alt_text": "Photograph of the \"mystery device\". A palm-size plastic casing, with a keyring attached.", "levels": [[-1], [-1]], "corpus_id": 6585582, "sentences": ["Photograph of the \"mystery device\".", "A palm-size plastic casing, with a keyring attached."], "caption": "Figure 1: \u201cThe Thing\u201d \u2013 an unknown device", "local_uri": ["2d307a8d909e50ea0f9c18a12c35819a7abd139e_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Taking things apart: reaching common ground and shared material understanding", "pdf_hash": "2d307a8d909e50ea0f9c18a12c35819a7abd139e", "year": 2014, "venue": "CHI", "alt_text": "The mystery device, opened up, showing its internal components: a small PCB, a loudspeaker, a battery compartement, a small ball-based movement sensor as well as the according wiring.", "levels": null, "corpus_id": 6585582, "sentences": ["The mystery device, opened up, showing its internal components: a small PCB, a loudspeaker, a battery compartement, a small ball-based movement sensor as well as the according wiring."], "caption": "", "local_uri": ["2d307a8d909e50ea0f9c18a12c35819a7abd139e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Taking things apart: reaching common ground and shared material understanding", "pdf_hash": "2d307a8d909e50ea0f9c18a12c35819a7abd139e", "year": 2014, "venue": "CHI", "alt_text": "A scan of colored pen sketches of different patterns on a PCB causing different sensor behaviour.", "levels": null, "corpus_id": 6585582, "sentences": ["A scan of colored pen sketches of different patterns on a PCB causing different sensor behaviour."], "caption": "", "local_uri": ["2d307a8d909e50ea0f9c18a12c35819a7abd139e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "A series of three drawings that show lines rising and falling, representing events in participants life.", "levels": null, "corpus_id": 140224972, "sentences": ["A series of three drawings that show lines rising and falling, representing events in participants life."], "caption": "Figure 1: Examples of output from the line drawing activity.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "A series of drawings by six different participants that represent their experiences with bipolar disorder, annotated with labels and explanatory text.", "levels": null, "corpus_id": 140224972, "sentences": ["A series of drawings by six different participants that represent their experiences with bipolar disorder, annotated with labels and explanatory text."], "caption": "Figure 2: Examples of timelines. \u00a9Authors. Image credit: De-identifed participants.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "A series of drawings by four different participants showing their timelines drawings with symbol icons added to indicated significant passages of time.", "levels": null, "corpus_id": 140224972, "sentences": ["A series of drawings by four different participants showing their timelines drawings with symbol icons added to indicated significant passages of time."], "caption": "Figure 3: Examples of annotated timelines. \u00a9Authors. Image credit: De-identifed participants.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "Three photographs: a stormy sea, a man riding a surf board on aqua colored ocean wave, and a woman's face submerged just below the surface of water.", "levels": [[-1]], "corpus_id": 140224972, "sentences": ["Three photographs: a stormy sea, a man riding a surf board on aqua colored ocean wave, and a woman's face submerged just below the surface of water."], "caption": "Figure 4: Visual motif: Water. Image credits: Fair use. See supplemental material for source URLs.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_016.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "Three photographs: a piece of frayed rope with a person on either side pulling in opposite directions, a historic scientific drawing of bowels, and a stack of rounded stones carefully balanced one on top of another, with a hand adding a stone to the top of the stack.", "levels": [[-1]], "corpus_id": 140224972, "sentences": ["Three photographs: a piece of frayed rope with a person on either side pulling in opposite directions, a historic scientific drawing of bowels, and a stack of rounded stones carefully balanced one on top of another, with a hand adding a stone to the top of the stack."], "caption": "Figure 5: Visual motif: Tension and balance. Image credits: Fair use. See supplemental material for source URLs.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_017.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "Three images: a pair of sneakers with the toes aligned with a yellow strip on the ground, with the words \"Do not cross\"; a map of the southeast portion of the US showing the predicted path of a hurricane along the Atlantic coast; a pile of about none cardboard boxes piled haphazardly in front of a door, all labeled with Amazon.com logo.", "levels": null, "corpus_id": 140224972, "sentences": ["Three images: a pair of sneakers with the toes aligned with a yellow strip on the ground, with the words \"Do not cross\"; a map of the southeast portion of the US showing the predicted path of a hurricane along the Atlantic coast; a pile of about none cardboard boxes piled haphazardly in front of a door, all labeled with Amazon.com logo."], "caption": "Figure 7: Sense-making challenges: Baselines. Image credits: Fair use. See supplemental material for source URLs.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_019.jpg"], "annotated": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "Three photographs: One showing a man, wincing with his fingers at his temples and drawn marks and squiggles hovering over his head; two pencils in a glass of water; a clear wine glass in front of a back and white striped background.", "levels": [[-1]], "corpus_id": 140224972, "sentences": ["Three photographs: One showing a man, wincing with his fingers at his temples and drawn marks and squiggles hovering over his head; two pencils in a glass of water; a clear wine glass in front of a back and white striped background."], "caption": "Figure 8: Sense-making challenges: Distortions. Image cred- its: Fair use. See supplemental material for source URLs.", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_020.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visually Encoding the Lived Experience of Bipolar Disorder", "pdf_hash": "27398fe169d310f04a93dec90a7013f7eb3e8a75", "year": 2019, "venue": "CHI", "alt_text": "Three images: A road with fields on either side that splits into two paths in the middle distance; a photo of a city street with the foreground in sharp focus but getting blurrier as the background recedes; a diagram showing a convex curve labled with the grieving process along side of the same drawing with a dense scribble of lines in the convex curve, indicating the process is not so neat and tidy.", "levels": null, "corpus_id": 140224972, "sentences": ["Three images: A road with fields on either side that splits into two paths in the middle distance; a photo of a city street with the foreground in sharp focus but getting blurrier as the background recedes; a diagram showing a convex curve labled with the grieving process along side of the same drawing with a dense scribble of lines in the convex curve, indicating the process is not so neat and tidy."], "caption": "Figure 9: Sense-making challenges: Non-linearity. Image credits: Fair use. See supplemental material for source URLs", "local_uri": ["27398fe169d310f04a93dec90a7013f7eb3e8a75_Image_021.jpg"], "annotated": false, "compound": false}
{"title": "Life-Affirming Biosensing in Public: Sounding Heartbeats on a Red Bench", "pdf_hash": "905e52910e8811a63cd41271a7a792429ed9ac2a", "year": 2019, "venue": "CHI", "alt_text": "Two women sit on a red bench. They hold stethoscopes to their chests and listen to the sounds of their hearts beating with the environment. The bench is in a public space.", "levels": null, "corpus_id": 140295632, "sentences": ["Two women sit on a red bench.", "They hold stethoscopes to their chests and listen to the sounds of their hearts beating with the environment.", "The bench is in a public space."], "caption": "Figure 1: The Heart Sounds Bench amplifes the heart sounds of bench-sitters, inviting a moment of calm yet vi- brant life-afrmation.", "local_uri": ["905e52910e8811a63cd41271a7a792429ed9ac2a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Life-Affirming Biosensing in Public: Sounding Heartbeats on a Red Bench", "pdf_hash": "905e52910e8811a63cd41271a7a792429ed9ac2a", "year": 2019, "venue": "CHI", "alt_text": "System diagram of the electronics. The stethoscope produces sound waves in air in a tube. This tube connects to a microscope which pics up on the sound waves and converts them to electrical signals in a wire. The wire leads to an amplifier that amplifies the electrical signal. Then this goes to speakers to play the heart sounds.", "levels": null, "corpus_id": 140295632, "sentences": ["System diagram of the electronics.", "The stethoscope produces sound waves in air in a tube.", "This tube connects to a microscope which pics up on the sound waves and converts them to electrical signals in a wire.", "The wire leads to an amplifier that amplifies the electrical signal.", "Then this goes to speakers to play the heart sounds."], "caption": "", "local_uri": ["905e52910e8811a63cd41271a7a792429ed9ac2a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Life-Affirming Biosensing in Public: Sounding Heartbeats on a Red Bench", "pdf_hash": "905e52910e8811a63cd41271a7a792429ed9ac2a", "year": 2019, "venue": "CHI", "alt_text": "Three images of pairs sitting on the bench. The left pair is calmly listening to their heart sounds. The middle pair is calmly listening to their heart sounds. The right pair is laughing.", "levels": null, "corpus_id": 140295632, "sentences": ["Three images of pairs sitting on the bench.", "The left pair is calmly listening to their heart sounds.", "The middle pair is calmly listening to their heart sounds.", "The right pair is laughing."], "caption": "", "local_uri": ["905e52910e8811a63cd41271a7a792429ed9ac2a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "Interaction in Auto-switch: (a) Scenario after an application switch; (b) User enters a word; (c) User enters the space key.", "levels": null, "corpus_id": 140247020, "sentences": ["Interaction in Auto-switch: (a) Scenario after an application switch; (b) User enters a word; (c) User enters the space key."], "caption": "Figure 3: Interaction in Auto-switch: (a) Scenario after an ap- plication switch; (b) User enters a word; (c) User enters the space key.", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "Interaction in Preview: (A) Case where the input field is empty in the Korean input mode; (B) Case where some text already exists in the input field in the Korean input mode; (C) Case where the input field is empty in the English input mode; (D) Case where some text already exists in the input field in the English input mode.", "levels": null, "corpus_id": 140247020, "sentences": ["Interaction in Preview: (A) Case where the input field is empty in the Korean input mode; (B) Case where some text already exists in the input field in the Korean input mode; (C) Case where the input field is empty in the English input mode; (D) Case where some text already exists in the input field in the English input mode."], "caption": "Our choice may be called \u2018space-based\u2019 auto-switching, and this option is in fact chosen by all auto-switching func- tions or applications available for the Korean-English dual- language environment. We speculate that their developers must have undergone the same considerations and simula- tions as we did.", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "Interaction in Smart-toggle: (a) Scenario after an application switch; (b) User is typing; (c) User enters the toggle key.", "levels": null, "corpus_id": 140247020, "sentences": ["Interaction in Smart-toggle: (a) Scenario after an application switch; (b) User is typing; (c) User enters the toggle key."], "caption": "Figure 6: Interaction in Smart-toggle: (a) Scenario after an application switch; (b) User is typing; (c) User enters the tog- gle key.", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "Trend of the AdoptRate of Auto-switch in Experiment 2. The error bars in the graph indicate standard deviation.", "levels": [[1], [1]], "corpus_id": 140247020, "sentences": ["Trend of the AdoptRate of Auto-switch in Experiment 2.", "The error bars in the graph indicate standard deviation."], "caption": "Figure 7: Trend of the AdoptRate of Auto-switch in Experi- ment 2. The error bars in the graph indicate standard devia- tion.", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "Frequency of the participants' attempts to delete the visual feedback characters throughout Experiment 3. The error bars in the graph indicate standard deviation.", "levels": [[1], [1]], "corpus_id": 140247020, "sentences": ["Frequency of the participants' attempts to delete the visual feedback characters throughout Experiment 3.", "The error bars in the graph indicate standard deviation."], "caption": "", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_021.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "Trend of the AdoptRate of Smart-toggle in Experiment 4. The error bars in the graph indicate standard deviation.", "levels": [[1], [1]], "corpus_id": 140247020, "sentences": ["Trend of the AdoptRate of Smart-toggle in Experiment 4.", "The error bars in the graph indicate standard deviation."], "caption": "", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_022.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "pdf_hash": "408a505662902bbaf20ef32ac6deaf7a78e52650", "year": 2019, "venue": "CHI", "alt_text": "The proportions of the mode errors corrected manually or by Smart-toggle for different mode error lengths (in the number of keystrokes) in Experiment 4.", "levels": [[1]], "corpus_id": 140247020, "sentences": ["The proportions of the mode errors corrected manually or by Smart-toggle for different mode error lengths (in the number of keystrokes) in Experiment 4."], "caption": "", "local_uri": ["408a505662902bbaf20ef32ac6deaf7a78e52650_Image_023.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An assistive robotic table for older and post-stroke adults: results from participatory design and evaluation activities with clinical staff", "pdf_hash": "c7b00311313d5ed6e7fbbf72a31dab397c0016b2", "year": 2014, "venue": "CHI", "alt_text": "Mean Ratings of Like, Need, and Easy to Use between over-the-bed table and Prototypes E, F, G", "levels": [[1]], "corpus_id": 16703644, "sentences": ["Mean Ratings of Like, Need, and Easy to Use between over-the-bed table and Prototypes E, F, G"], "caption": "", "local_uri": ["c7b00311313d5ed6e7fbbf72a31dab397c0016b2_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visual recognition in museum guide apps: do visitors want it?", "pdf_hash": "a8ce25225fdfc6a4081b16cb82f714d0fc01a25a", "year": 2014, "venue": "CHI", "alt_text": "Figure 1. Museum visitors taking photos of artworks in the Van Gogh Museum, Amsterdam. \n\n\u00a9 Liset van der Laan, http://blog.n8.nl/2013/05/16/kunst-op-%e2%2580%2598t-kiekje/", "levels": null, "corpus_id": 14437537, "sentences": ["Figure 1.", "Museum visitors taking photos of artworks in the Van Gogh Museum, Amsterdam.", "\u00a9 Liset van der Laan, http://blog.n8.nl/2013/05/16/kunst-op-%e2%2580%2598t-kiekje/"], "caption": "Figure 1. Museum visitors taking photos of artworks in the Van Gogh Museum, Amsterdam1", "local_uri": ["a8ce25225fdfc6a4081b16cb82f714d0fc01a25a_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Visual recognition in museum guide apps: do visitors want it?", "pdf_hash": "a8ce25225fdfc6a4081b16cb82f714d0fc01a25a", "year": 2014, "venue": "CHI", "alt_text": "Figure 4. VisRec is preferred by a majority of participants when asked about their explicit preference", "levels": null, "corpus_id": 14437537, "sentences": ["Figure 4.", "VisRec is preferred by a majority of participants when asked about their explicit preference"], "caption": "", "local_uri": ["a8ce25225fdfc6a4081b16cb82f714d0fc01a25a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "RIMES: Embedding Interactive Multimedia Exercises in Lecture Videos", "pdf_hash": "bc886d48846c6b99bd54c90e8a56be6bfe5bcf7f", "year": 2015, "venue": "CHI", "alt_text": "A gallery with 6 responses to RIMES exercises, with sorting and filtering options at the top", "levels": [[-1]], "corpus_id": 9177161, "sentences": ["A gallery with 6 responses to RIMES exercises, with sorting and filtering options at the top"], "caption": "Background image: an optional background image that students can reference or mark on. The teacher can add a link to any web image, or upload their own.The teacher can click on a \u201cPreview\u201d button within  the RIMES slide to check what the recording view will look like for students. The teacher can move between the preview and edit modes to update the exercise. Once a video including a RIMES exercise is published, the RIMES exercise is part of the video playback sequence. The video automatically pauses when the play control reaches the slide with the RIMES exercise. The Office Mix tool already supports the video publication and playback environment.RecordingWhile watching a video, students are prompted to record their own response using video, audio, and/or drawing when they encounter a RIMES slide (Figure 2). They can explain their solution using multiple modalities inside a web browser without having to install any plugins.The recording interface starts capturing any of the enabled modalities (video, audio, drawing) once the student clicks the \u201cRecord\u201d button. When inking is enabled, the sidebar shows a palette for changing the pen size and color. The student can use touch or a stylus if available, or a mouse if not. When video is enabled, the student sees a visual preview in real-time. When the student is done recording, she clicks \u201cStop.\u201d She is given options to either re-record or submit. Once she submits, the submission dialog box opens, asking her to rate how confident she is about her response and how helpful the RIMES exercise was. The teacher can refer to these scores when reviewing students\u2019 responses, as research suggests that self-efficacy and confidence are effective indicators of learning [2].Figure 3. The RIMES reviewing interface shows a gallery of all responses to a RIMES exercise. Various sorting and filtering options help the teacher with the reviewing process.Once a recording is submitted, the RIMES backend post- processes each recording for improved navigation  in the gallery. It detects responses with silent audio or no pen input, and labels them as \u201cno audio\u201d or \u201cno inking,\u201d respectively.ReviewingThe gallery for each RIMES exercise (Figure 3) is automatically populated as the students submit their responses. The gallery renders each response in a card-style layout, and displays thumbnails of the final drawing canvas or video recording if available, student name, helpfulness and confidence ratings, recorded length, and captured input modes. The gallery is designed to support the following use cases: 1) get a quick overview of all student responses, 2) easily identify model answers and misconceptions, and 3) replay a particular student\u2019s response to better understand his thought process. The specific gallery features are:", "local_uri": ["bc886d48846c6b99bd54c90e8a56be6bfe5bcf7f_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "RIMES: Embedding Interactive Multimedia Exercises in Lecture Videos", "pdf_hash": "bc886d48846c6b99bd54c90e8a56be6bfe5bcf7f", "year": 2015, "venue": "CHI", "alt_text": "One formula and three equations to solve.  Formula:  f(x) = x + 4.  The equations to solve:  f(3) = ?   f(2) = ?  f(-10) = ?", "levels": null, "corpus_id": 9177161, "sentences": ["One formula and three equations to solve.", "Formula:  f(x) = x + 4.", "The equations to solve:  f(3) = ?   f(2) = ?  f(-10) = ?"], "caption": "the teacher played back a response with incorrect answers (6, 4, and -20 instead of 7, 6, and -6, shown below).", "local_uri": ["bc886d48846c6b99bd54c90e8a56be6bfe5bcf7f_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "RIMES: Embedding Interactive Multimedia Exercises in Lecture Videos", "pdf_hash": "bc886d48846c6b99bd54c90e8a56be6bfe5bcf7f", "year": 2015, "venue": "CHI", "alt_text": "Two other response to the three equations with wrong answers to all three.  Formula:  f(x) = x + 4.  The incorrectly solved equations for the first student:  f(3) = 6   f(2) = 4 f(-10) = -2. The incorrectly solved equations for the second student:  f(3) = 3 x 3   f(2) = 2 x 2 f(-10) = x", "levels": null, "corpus_id": 9177161, "sentences": ["Two other response to the three equations with wrong answers to all three.  Formula:  f(x) = x + 4.", "The incorrectly solved equations for the first student:  f(3) = 6   f(2) = 4 f(-10) = -2.", "The incorrectly solved equations for the second student:  f(3) = 3 x 3   f(2) = 2 x 2 f(-10) = x"], "caption": "T5 described the benefit of the playback, \u201cI believe that by listening to the responses of each student we truly get a realistic view of student understanding of concepts presented. It will definitely help teachers to know which students need assistance in understanding the concept.\u201d", "local_uri": ["bc886d48846c6b99bd54c90e8a56be6bfe5bcf7f_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Gaze gestures and haptic feedback in mobile devices", "pdf_hash": "f1c65f5ae00874891111e9750eba23ad03421f5e", "year": 2014, "venue": "CHI", "alt_text": "Four haptic feedback conditions for the up gesture. In the no condition no feedback is given, in the full condition the feedback is given when gesture is ready, in the out condition the feedback is given when gaze has went out of device, in the both condition the feedback is given both when the gaze has moved out of the device and when the gesture is ready.", "levels": null, "corpus_id": 17667227, "sentences": ["Four haptic feedback conditions for the up gesture.", "In the no condition no feedback is given, in the full condition the feedback is given when gesture is ready, in the out condition the feedback is given when gaze has went out of device, in the both condition the feedback is given both when the gaze has moved out of the device and when the gesture is ready."], "caption": "", "local_uri": ["f1c65f5ae00874891111e9750eba23ad03421f5e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Gaze gestures and haptic feedback in mobile devices", "pdf_hash": "f1c65f5ae00874891111e9750eba23ad03421f5e", "year": 2014, "venue": "CHI", "alt_text": "Boxplot view of the block completion times. The median times for no and full conditions are clearly longer than the median times for out and both conditions.", "levels": [[1], [2]], "corpus_id": 17667227, "sentences": ["Boxplot view of the block completion times.", "The median times for no and full conditions are clearly longer than the median times for out and both conditions."], "caption": "", "local_uri": ["f1c65f5ae00874891111e9750eba23ad03421f5e_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Autonomous navigation through the city for the blind", "pdf_hash": "6008748c4606e8f97afd88da7c5850f61a565037", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 3. Audio-based GPS software interface. Button 1. Enter the destination, Button 2. Ask the system for information, and Button 3. Change the starting point", "levels": null, "corpus_id": 14379311, "sentences": ["Figure 3.", "Audio-based GPS software interface.", "Button 1.", "Enter the destination, Button 2.", "Ask the system for information, and Button 3.", "Change the starting point"], "caption": "Figure 3. Audio-based GPS software interface. Button 1.", "local_uri": ["6008748c4606e8f97afd88da7c5850f61a565037_Image_003.gif"], "annotated": false, "compound": false}
{"title": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "pdf_hash": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 1: Three bar charts presenting the percentage of participants who are interested in knowing about particular sounds (a) at home, (b) at work, and (c) while mobile. Each chart contains pairs of bars: one representing deaf participants, and the other hard-of-hearing participants.  (a) A bar chart of the percentage of participants interested in particular sounds at home. The sounds, sorted in decreasing order of deaf participants are: appliances running, emergency alarms, appliance alerts, intruders, knocking on door, doorbell, sounds outside of the house, wake-up alarms, people knocking things over, dog barking, phone ringing, baby crying, people shouting, people laughing, children fighting, children playing, and other. The percent of deaf participants who selected each sound decreases consistently from about 70% to about 10%. Hard-of-hearing participants followed a similar trend, but about 15% more selected knocking on door, wake-up alarms, and phone ringing than deaf participants. (b) A bar chart of the percentage of participants interested in particular sounds at work. The sounds, sorted in decreasing order of deaf participants are: emergency alarms, presence of co-workers, co-workers calling attention, surrounding conversations, knocking on door, announcements, co-worker activity, gun shots, phone ringing, other, and faxes. Deaf percentages decreased from around 50% to 10%. Hard-of-hearing participants followed a similar trend, with about 20% more interest in co-workers calling attention, surrounding conversations, announcements, and phone ringing. (c) A bar chart of the percentage of participants interested in particular sounds while mobile. The sounds, sorted in decreasing order of deaf participants are: sirens, bikes or people behind, if you are in the way, honking, announcements, vehicles driving by, sounds in nature, dogs barking, airplanes or helicopters, and other. Deaf percentages decrease from about 70% to 5%.", "levels": [[1], [1], [1], [2], [3, 2], [3, 2], [1], [2], [3, 2], [2], [1], [2], [2]], "corpus_id": 14909116, "sentences": ["Figure 1: Three bar charts presenting the percentage of participants who are interested in knowing about particular sounds (a) at home, (b) at work, and (c) while mobile.", "Each chart contains pairs of bars: one representing deaf participants, and the other hard-of-hearing participants.", "(a) A bar chart of the percentage of participants interested in particular sounds at home.", "The sounds, sorted in decreasing order of deaf participants are: appliances running, emergency alarms, appliance alerts, intruders, knocking on door, doorbell, sounds outside of the house, wake-up alarms, people knocking things over, dog barking, phone ringing, baby crying, people shouting, people laughing, children fighting, children playing, and other.", "The percent of deaf participants who selected each sound decreases consistently from about 70% to about 10%.", "Hard-of-hearing participants followed a similar trend, but about 15% more selected knocking on door, wake-up alarms, and phone ringing than deaf participants. (", "b) A bar chart of the percentage of participants interested in particular sounds at work.", "The sounds, sorted in decreasing order of deaf participants are: emergency alarms, presence of co-workers, co-workers calling attention, surrounding conversations, knocking on door, announcements, co-worker activity, gun shots, phone ringing, other, and faxes.", "Deaf percentages decreased from around 50% to 10%.", "Hard-of-hearing participants followed a similar trend, with about 20% more interest in co-workers calling attention, surrounding conversations, announcements, and phone ringing. (", "c) A bar chart of the percentage of participants interested in particular sounds while mobile.", "The sounds, sorted in decreasing order of deaf participants are: sirens, bikes or people behind, if you are in the way, honking, announcements, vehicles driving by, sounds in nature, dogs barking, airplanes or helicopters, and other.", "Deaf percentages decrease from about 70% to 5%."], "caption": "Sounds of interest at home                        (b) Sounds of interest at work                      (c) Sounds of interest while mobileFigure 1: Sounds of interest to deaf and hard-of-hearing participants (a) at home, (b) at work, and (c) while mobile. (a) How often sounds missed at home           (b) How often sounds missed at work         (c) How often sounds missed while mobileFigure 2: Frequency of missed sounds (a) at home, (b) at work, and (c) while mobile.Various onset detection methods have been developed and used to boost audio event detection accuracy (e.g., [5]). We explored the performance of a sliding window GMM sound detection algorithm using training data gathered in our user study. The state-of-the-art in sound detection is advanc\u00ad ing, and we expect future work applying these algorithms to yield more accurate results.", "local_uri": ["28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "pdf_hash": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 2: Three bar charts of missed sounds (a) at home, (b) at work, and (c) while mobile. Each chart displays the percentage of participants who reported missing sounds never, once/month, once/week, once/day, and more than once/day. There are one bar for deaf participants, and one for hard-of-hearing participants in each condition. (a) At home, about 30% of deaf participants never missed sounds and 50% missed sounds more than once per day. About 25% of hard-of-hearing participants missed sounds once/week, and about 50% more than once per day. Other responses were all under about 12%. (b) At work, the bar chart takes a U-shape for both deaf and hard-of-hearing participants. About 35% of deaf participants never missed sounds, and about 40% missed sounds more than once per day, with all other answers under 15%. Hard-of-hearing participants followed a similar curve. (c) While mobile, the bar chart takes a U-shape for both deaf and hard-of-hearing participants. About 35% of deaf participants never missed sounds, and about 50% missed more than once per day. The other responses were under about 10%. Hard-of-hearing participants followed a similar trend, but less extreme.", "levels": [[1], [1], [1], [2], [2], [2], [3], [2], [2], [3], [2], [2], [2]], "corpus_id": 14909116, "sentences": ["Figure 2: Three bar charts of missed sounds (a) at home, (b) at work, and (c) while mobile.", "Each chart displays the percentage of participants who reported missing sounds never, once/month, once/week, once/day, and more than once/day.", "There are one bar for deaf participants, and one for hard-of-hearing participants in each condition. (", "a) At home, about 30% of deaf participants never missed sounds and 50% missed sounds more than once per day.", "About 25% of hard-of-hearing participants missed sounds once/week, and about 50% more than once per day.", "Other responses were all under about 12%. (", "b) At work, the bar chart takes a U-shape for both deaf and hard-of-hearing participants.", "About 35% of deaf participants never missed sounds, and about 40% missed sounds more than once per day, with all other answers under 15%.", "Hard-of-hearing participants followed a similar curve. (", "c) While mobile, the bar chart takes a U-shape for both deaf and hard-of-hearing participants.", "About 35% of deaf participants never missed sounds, and about 50% missed more than once per day.", "The other responses were under about 10%.", "Hard-of-hearing participants followed a similar trend, but less extreme."], "caption": "(a) How often sounds missed at home           (b) How often sounds missed at work         (c) How often sounds missed while mobile", "local_uri": ["28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "pdf_hash": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 3: Desired information for app notifications. Pairs of bars represent deaf and hard-of-hearing (HH) participants. The following types of information are included: identity, location, volume, length, pitch, urgency, and confidence. Possible participant responses were: absolutely essential, very important, moderately important, of little importance, and not important at all. A stacked bar chart shows user responses for each type of information. Most participants responded that identity, location, urgency, and confidence were at least moderately important, while fewer participants gave volume, length, and pitch the same importance. Deaf and hard-of-hearing responses were very similar.", "levels": [[1], [1], [1], [1], [1], [2], [2]], "corpus_id": 14909116, "sentences": ["Figure 3: Desired information for app notifications.", "Pairs of bars represent deaf and hard-of-hearing (HH) participants.", "The following types of information are included: identity, location, volume, length, pitch, urgency, and confidence.", "Possible participant responses were: absolutely essential, very important, moderately important, of little importance, and not important at all.", "A stacked bar chart shows user responses for each type of information.", "Most participants responded that identity, location, urgency, and confidence were at least moderately important, while fewer participants gave volume, length, and pitch the same importance.", "Deaf and hard-of-hearing responses were very similar."], "caption": "Figure 3: Desired information for app noti\ufb01cations. Pairs of bars represent deaf and hard-of-hearing (HH) participants.", "local_uri": ["28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "pdf_hash": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 4: Screen shots of five of the app interface's screens. (a) The main screen. The text at the top reads \"Sound Detector.\" Below that, it says \"Listener\", and provides a sliding ON/OFF button. Below that, there are two side-by-side buttons, quick record and disply. Below that, the sound types are listed vertically: Uncategorized, Alarm, Dishwasher, and Laundry. To the left of each sound type is a green check box, and to the right is a delete button. A button with a + on it is located at the top-right of the list of sound types.  (b) Recording list. The text at the top reads \"Alarm Recordings.\" The recording names are listed vertically below: Alarm 1, Alarm 2, and Alarm 3. To the right of each recording name are edit and delete buttons. At the top-right is a button with a + on it. (c) Recording screen. A dialog is overlaid on top of the main menu screen. At the top it says \"Add Recording\". Below is a drop-down for sound type, and a text input box for name of recording. Below that is a waveform visualization of the sound. At the bottom is a button labeled \"stop\". (d) Editing screen. A dialog is overlaid on top of the recordings screen. Text at the top reads \"Sound Type: Alarm\". Below that, it says Name of Recording: Alarm 1. Below that there is a waveform visualization. Two sliders, with the area between highlighted in blue rest at the left and right side of the visualization. Below the visualization there are buttons that say Play, Save, and Cancel. (e) Real-time display. Bright green bars of varying hight extend up from the bottom of the screen into a black background. A button labeled \"Quick Record\" is at the very bottom.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 14909116, "sentences": ["Figure 4: Screen shots of five of the app interface's screens. (a) The main screen.", "The text at the top reads \"Sound Detector.\" Below that, it says \"Listener\", and provides a sliding ON/OFF button.", "Below that, there are two side-by-side buttons, quick record and disply.", "Below that, the sound types are listed vertically: Uncategorized, Alarm, Dishwasher, and Laundry.", "To the left of each sound type is a green check box, and to the right is a delete button.", "A button with a + on it is located at the top-right of the list of sound types.", "(b) Recording list.", "The text at the top reads \"Alarm Recordings.\" The recording names are listed vertically below: Alarm 1, Alarm 2, and Alarm 3.", "To the right of each recording name are edit and delete buttons.", "At the top-right is a button with a + on it. (c) Recording screen.", "A dialog is overlaid on top of the main menu screen.", "At the top it says \"Add Recording\".", "Below is a drop-down for sound type, and a text input box for name of recording.", "Below that is a waveform visualization of the sound.", "At the bottom is a button labeled \"stop\". (d) Editing screen.", "A dialog is overlaid on top of the recordings screen.", "Text at the top reads \"Sound Type: Alarm\".", "Below that, it says Name of Recording: Alarm 1.", "Below that there is a waveform visualization.", "Two sliders, with the area between highlighted in blue rest at the left and right side of the visualization.", "Below the visualization there are buttons that say Play, Save, and Cancel. (e) Real-time display.", "Bright green bars of varying hight extend up from the bottom of the screen into a black background.", "A button labeled \"Quick Record\" is at the very bottom."], "caption": "", "local_uri": ["28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_006.jpg", "28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_007.jpg", "28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Facade: Auto-generating Tactile Interfaces to Appliances", "pdf_hash": "73b87fd845a2f7169ef173a210b26ec3a0f3ac21", "year": 2017, "venue": "CHI", "alt_text": "Facade is a crowdsourced fabrication pipeline that enables blind people to make flat physical interfaces accessible by independently producing a 3D-printed overlay of tactile buttons. From left to right, we demonstrate example applications including microwave, refrigerator door, copier, and another microwave. Insets shows close views of individual embossed buttons, including augmentations with Braille and embossed letters.", "levels": null, "corpus_id": 31445486, "sentences": ["Facade is a crowdsourced fabrication pipeline that enables blind people to make flat physical interfaces accessible by independently producing a 3D-printed overlay of tactile buttons.", "From left to right, we demonstrate example applications including microwave, refrigerator door, copier, and another microwave.", "Insets shows close views of individual embossed buttons, including augmentations with Braille and embossed letters."], "caption": "Figure 1. Facade is a crowdsourced fabrication pipeline that enables blind people to make \ufb02at physical interfaces accessible by independently producing a 3D-printed overlay of tactile buttons. From left to right, we demonstrate example applications including microwave, refrigerator door, copier, and another microwave. Insets shows close views of individual embossed buttons.", "local_uri": ["73b87fd845a2f7169ef173a210b26ec3a0f3ac21_Image_001.png"], "annotated": false, "compound": false}
{"title": "Facade: Auto-generating Tactile Interfaces to Appliances", "pdf_hash": "73b87fd845a2f7169ef173a210b26ec3a0f3ac21", "year": 2017, "venue": "CHI", "alt_text": "Facade users capture a photo of an interface they would like to use with a fiducial marker attached to it (we use a dollar bill). Using perspective transformation, the interface image is warped to the front view and absolute measurements are calculated. Then this image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface. Blind users can then customize settings of the labeling strategy, and these labels and preferences are used to generate the 3D models of a tactile layer matching the original controls. Finally, an off-the-shelf 3D printer fabricates the layer, which is then attached to the interface using adhesives.", "levels": null, "corpus_id": 31445486, "sentences": ["Facade users capture a photo of an interface they would like to use with a fiducial marker attached to it (we use a dollar bill).", "Using perspective transformation, the interface image is warped to the front view and absolute measurements are calculated.", "Then this image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface.", "Blind users can then customize settings of the labeling strategy, and these labels and preferences are used to generate the 3D models of a tactile layer matching the original controls.", "Finally, an off-the-shelf 3D printer fabricates the layer, which is then attached to the interface using adhesives."], "caption": "Figure 2. Facade users capture a photo of an interface they would like to use with a \ufb01ducial marker attached to it (we use a dollar bill). Using perspective transformation, the interface image is warped to the front view and absolute measurements are calculated. Then this image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface. Blind users can then customize settings of the labeling strategy, and these labels and preferences are used to generate the 3D models of a tactile layer matching the original controls. Finally, an off-the-shelf 3D printer fabricates the layer, which is then attached to the interface using adhesives.", "local_uri": ["73b87fd845a2f7169ef173a210b26ec3a0f3ac21_Image_002.png"], "annotated": false, "compound": false}
{"title": "Facade: Auto-generating Tactile Interfaces to Appliances", "pdf_hash": "73b87fd845a2f7169ef173a210b26ec3a0f3ac21", "year": 2017, "venue": "CHI", "alt_text": "Shapes inform users of different functionalities. For example, half spherical buttons without Braille label indicates number buttons (a), while rectangular buttons with Braille labels indicate function buttons (b). Users are also able to change settings to use symbols (c), Braille (d), or embossed letters (e) for buttons labels such as plus and minus.", "levels": null, "corpus_id": 31445486, "sentences": ["Shapes inform users of different functionalities.", "For example, half spherical buttons without Braille label indicates number buttons (a), while rectangular buttons with Braille labels indicate function buttons (b).", "Users are also able to change settings to use symbols (c), Braille (d), or embossed letters (e) for buttons labels such as plus and minus."], "caption": "Figure 3. Shapes inform users of different functionalities. For example, half spherical buttons without Braille label indicates number buttons (a), while rectangular buttons with Braille labels indicate function buttons (b). Users are also able to change settings to use symbols (c), Braille (d), or embossed letters (e) for buttons labels such as plus and minus.", "local_uri": ["73b87fd845a2f7169ef173a210b26ec3a0f3ac21_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Facade: Auto-generating Tactile Interfaces to Appliances", "pdf_hash": "73b87fd845a2f7169ef173a210b26ec3a0f3ac21", "year": 2017, "venue": "CHI", "alt_text": "A design probe tested with 6 blind participants. An augmented button set with Braille labels (a) is attached to the microwave (c), and buttons are connected with thin bridges to facilitate pressing (b).", "levels": null, "corpus_id": 31445486, "sentences": ["A design probe tested with 6 blind participants.", "An augmented button set with Braille labels (a) is attached to the microwave (c), and buttons are connected with thin bridges to facilitate pressing (b)."], "caption": "Figure 4. A design probe tested with 6 blind participants. An augmented button set with Braille labels (a) is attached to the microwave (c), and buttons are connected with thin bridges to facilitate pressing (b).", "local_uri": ["73b87fd845a2f7169ef173a210b26ec3a0f3ac21_Image_005.png"], "annotated": false, "compound": false}
{"title": "Facade: Auto-generating Tactile Interfaces to Appliances", "pdf_hash": "73b87fd845a2f7169ef173a210b26ec3a0f3ac21", "year": 2017, "venue": "CHI", "alt_text": "Example printed overlays and legends generated by Facade. (a)-(d) demonstrate the different material combinations we tested in the design iterations (NinjaFlex with Braille, Flex+PLA Braille label, Flex+PLA Braille cover, and Flex+PLA embossed letter cover). Facade users can choose to print a legend for the abbreviations (e). If a user does not have a 3D printer at home, models can also be printed through commercial printing services and mail-ordered. (f) and (g) show two example prints ordered from 3D Hubs using PolyFlex and SemiFlex materials.", "levels": null, "corpus_id": 31445486, "sentences": ["Example printed overlays and legends generated by Facade. (a)-(d) demonstrate the different material combinations we tested in the design iterations (NinjaFlex with Braille, Flex+PLA Braille label, Flex+PLA Braille cover, and Flex+PLA embossed letter cover).", "Facade users can choose to print a legend for the abbreviations (e).", "If a user does not have a 3D printer at home, models can also be printed through commercial printing services and mail-ordered.", "(f) and (g) show two example prints ordered from 3D Hubs using PolyFlex and SemiFlex materials."], "caption": "Figure 5. Example printed overlays and legends generated by Facade. (a)-(d) demonstrate the different material combinations we tested in the design iterations (NinjaFlex with Braille, Flex+PLA Braille label, Flex+PLA Braille cover, and Flex+PLA embossed letter cover). Facade users can choose to print a legend for the abbreviations (e). If a user does not have a 3D printer at home, models can also be printed through commercial printing services and mail-ordered. (f) and (g) show two example prints ordered from 3D Hubs using PolyFlex and SemiFlex materials.", "local_uri": ["73b87fd845a2f7169ef173a210b26ec3a0f3ac21_Image_007.png"], "annotated": false, "compound": false}
{"title": "Facade: Auto-generating Tactile Interfaces to Appliances", "pdf_hash": "73b87fd845a2f7169ef173a210b26ec3a0f3ac21", "year": 2017, "venue": "CHI", "alt_text": "Examples for image localization and warping on photos taken by the participants using a dollar bill as the fiducial marker. Boxes show in green represents warping results that were good enough for generating a usable tactile overlay model, while those shown in orange and red represents failure cases.", "levels": null, "corpus_id": 31445486, "sentences": ["Examples for image localization and warping on photos taken by the participants using a dollar bill as the fiducial marker.", "Boxes show in green represents warping results that were good enough for generating a usable tactile overlay model, while those shown in orange and red represents failure cases."], "caption": "Figure 7. Examples for image localization and warping on photos taken by the participants using a dollar bill as the \ufb01ducial marker. Boxes show in green represents warping results that were good enough for generat\u00ad ing a usable tactile overlay model, while those shown in orange and red represents failure cases.", "local_uri": ["73b87fd845a2f7169ef173a210b26ec3a0f3ac21_Image_009.png"], "annotated": false, "compound": false}
{"title": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "pdf_hash": "9bd21b56644843a04823a05de227853db6ee5f07", "year": 2019, "venue": "CHI", "alt_text": "A person is lying on a bed fitted with and EEG and a VR headset. A vibrant fiery abstract graphic is being projected on the walls around them in response to their EEG activity.", "levels": [[-1], [-1]], "corpus_id": 140290976, "sentences": ["A person is lying on a bed fitted with and EEG and a VR headset.", "A vibrant fiery abstract graphic is being projected on the walls around them in response to their EEG activity."], "caption": "Figure 1: The Inter-Dream system. Participants manipulate graphical imagery through EEG, which projected around the room and in VR while resting in an interactive bed.", "local_uri": ["9bd21b56644843a04823a05de227853db6ee5f07_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "pdf_hash": "9bd21b56644843a04823a05de227853db6ee5f07", "year": 2019, "venue": "CHI", "alt_text": "The left image shows the EEG headset, which is small, and light weight,  reminiscent of a tiara. The right image shows a close up of a participant lying in the bed, with subtly oscillating luminescent spheres orbiting the projection space around them in response to EEG activity.", "levels": null, "corpus_id": 140290976, "sentences": ["The left image shows the EEG headset, which is small, and light weight,  reminiscent of a tiara.", "The right image shows a close up of a participant lying in the bed, with subtly oscillating luminescent spheres orbiting the projection space around them in response to EEG activity."], "caption": "Figure 2: The EEG headset (left) and the active Inter-Dream system (right).", "local_uri": ["9bd21b56644843a04823a05de227853db6ee5f07_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "pdf_hash": "9bd21b56644843a04823a05de227853db6ee5f07", "year": 2019, "venue": "CHI", "alt_text": "This image shows a participant interfaced with the Inter-Dream system. The EEG associated projection shows a violently swirling maelstrom of spheres warping together into an amorphous mass.", "levels": null, "corpus_id": 140290976, "sentences": ["This image shows a participant interfaced with the Inter-Dream system.", "The EEG associated projection shows a violently swirling maelstrom of spheres warping together into an amorphous mass."], "caption": "Figure 3: The surrounding pattern is generated by the participant, providing creative affordance.", "local_uri": ["9bd21b56644843a04823a05de227853db6ee5f07_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "pdf_hash": "9bd21b56644843a04823a05de227853db6ee5f07", "year": 2019, "venue": "CHI", "alt_text": "This image shows a participant interfaced with the Inter-Dream system. The EEG associated projection appears as a warm, cradling hammock of spheres, encapsulating the participant.", "levels": null, "corpus_id": 140290976, "sentences": ["This image shows a participant interfaced with the Inter-Dream system.", "The EEG associated projection appears as a warm, cradling hammock of spheres, encapsulating the participant."], "caption": "Figure 4: The Inter-Dream system attempts to build towards a holistic acknowledgement of all proposed factors of pre-sleep.", "local_uri": ["9bd21b56644843a04823a05de227853db6ee5f07_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "pdf_hash": "9bd21b56644843a04823a05de227853db6ee5f07", "year": 2019, "venue": "CHI", "alt_text": "In this image is a bed (centre) with the EEG and VR headset placed on top. Behind the bed is the projection wall with a static sphere composed of smaller spheres. In front and behind the bed are two lights which are turned off once the experience begins.", "levels": [[-1], [-1], [-1]], "corpus_id": 140290976, "sentences": ["In this image is a bed (centre) with the EEG and VR headset placed on top.", "Behind the bed is the projection wall with a static sphere composed of smaller spheres.", "In front and behind the bed are two lights which are turned off once the experience begins."], "caption": "Figure 5: This image demonstrates the Inter-Dream system in an idle state. Note the projected graphic is static without EEG input.", "local_uri": ["9bd21b56644843a04823a05de227853db6ee5f07_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "pdf_hash": "9bd21b56644843a04823a05de227853db6ee5f07", "year": 2019, "venue": "CHI", "alt_text": "In this graph, all frequency bandwidth decrease in activity across the time sample. Frequencies in order of highest to lowest activity is delta, alpha, beta, theta, ad gamma.", "levels": [[3], [2]], "corpus_id": 140290976, "sentences": ["In this graph, all frequency bandwidth decrease in activity across the time sample.", "Frequencies in order of highest to lowest activity is delta, alpha, beta, theta, ad gamma."], "caption": "Figure 7: Time series of between-subjects (N=11) mean absolute power for frequency bandwidths delta [red], theta [purple], alpha [blue], beta [green] and gamma [orange]", "local_uri": ["9bd21b56644843a04823a05de227853db6ee5f07_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Time to Scale: Generalizable Affect Detection for Tens of Thousands of Students across An Entire School Year", "pdf_hash": "7eb3c24fa108aa1bbc4e615f33568e74bcfeefd1", "year": 2019, "venue": "CHI", "alt_text": "Image shows a screenshot from a algebra nation video. In the video, the tutor is working through the process of solving a quadratic equation using the quadratic forumla", "levels": null, "corpus_id": 140240896, "sentences": ["Image shows a screenshot from a algebra nation video.", "In the video, the tutor is working through the process of solving a quadratic equation using the quadratic forumla"], "caption": "Figure 1. Sample Algebra Nation video lecture.", "local_uri": ["7eb3c24fa108aa1bbc4e615f33568e74bcfeefd1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Finding dependencies between actions using the crowd", "pdf_hash": "86fffe172d05cd3222cd16401caa561950c71d7b", "year": 2014, "venue": "CHI", "alt_text": "The ARchitect worker interface is shown with a video on the right side, and a question and Yes and No buttons on the left.", "levels": null, "corpus_id": 13567640, "sentences": ["The ARchitect worker interface is shown with a video on the right side, and a question and Yes and No buttons on the left."], "caption": "ABSTRACT", "local_uri": ["86fffe172d05cd3222cd16401caa561950c71d7b_Image_001.png"], "annotated": false, "compound": false}
{"title": "Finding dependencies between actions using the crowd", "pdf_hash": "86fffe172d05cd3222cd16401caa561950c71d7b", "year": 2014, "venue": "CHI", "alt_text": "Dependency graphs for our three trials. Each shows all of the actions that composing making tea, in a the structure that shows which must be performed before others.", "levels": [[-1], [-1]], "corpus_id": 13567640, "sentences": ["Dependency graphs for our three trials.", "Each shows all of the actions that composing making tea, in a the structure that shows which must be performed before others."], "caption": "Figure 3. The dependency graphs output by our 3 tri- als. An average of 22.3 workers were asked each question. Thresholds of 50%, 50%, and 40% were used, respectively.", "local_uri": ["86fffe172d05cd3222cd16401caa561950c71d7b_Image_030.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Finding dependencies between actions using the crowd", "pdf_hash": "86fffe172d05cd3222cd16401caa561950c71d7b", "year": 2014, "venue": "CHI", "alt_text": "Three plots of precision versus recall in the filtering process. All three show curve that indicate recall does not drop until precision is at or near 100 percent.", "levels": [[1], [3]], "corpus_id": 13567640, "sentences": ["Three plots of precision versus recall in the filtering process.", "All three show curve that indicate recall does not drop until precision is at or near 100 percent."], "caption": "Figure 5. Precision and recall of edges plotted over increasing thresholds for our videos. Precision increases with required agreement at first as invalid constraint edges are removed. Recall eventually decreases as even valid edges are removed.", "local_uri": ["86fffe172d05cd3222cd16401caa561950c71d7b_Image_034.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Assessing Virtual Assistant Capabilities with Italian Dysarthric Speech", "pdf_hash": "44d33f99a5a9f23387691f99abba5bfc8942f2fb", "year": 2018, "venue": "ASSETS", "alt_text": "A box plot showing the WER distribution for Google Assistant, Cortana and Siri. As reported in the text, Google Assistant and Cortana exhibit a lower WER than Siri, with median values of 0, 0.4, and 0.89 respectively.", "levels": null, "corpus_id": 52936115, "sentences": ["A box plot showing the WER distribution for Google Assistant, Cortana and Siri.", "As reported in the text, Google Assistant and Cortana exhibit a lower WER than Siri, with median values of 0, 0.4, and 0.89 respectively."], "caption": "Figure 1. WER distribution for each assistant", "local_uri": ["44d33f99a5a9f23387691f99abba5bfc8942f2fb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Development of a Checklist for the Prevention of Intradialytic Hypotension in Hemodialysis Care: Design Considerations Based on Activity Theory", "pdf_hash": "66f933941bf622e3ad257266e42fff292f59bc11", "year": 2019, "venue": "CHI", "alt_text": "Overlapping Triangles with Subject, Object, Tools, Rules, Community, and Division of Labor/Workflow at tips", "levels": null, "corpus_id": 140229945, "sentences": ["Overlapping Triangles with Subject, Object, Tools, Rules, Community, and Division of Labor/Workflow at tips"], "caption": "Figure 1: Application of Activity Theory to Fluid Assessment Activity System in Hemodialysis Facilities. (bolded elements show expansion to include IDH prevention via the checklist).", "local_uri": ["66f933941bf622e3ad257266e42fff292f59bc11_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Development of a Checklist for the Prevention of Intradialytic Hypotension in Hemodialysis Care: Design Considerations Based on Activity Theory", "pdf_hash": "66f933941bf622e3ad257266e42fff292f59bc11", "year": 2019, "venue": "CHI", "alt_text": "Five Stakeholder Groups: Investigators, Steering Committee, Advisory Committee, Clinician and Patient Interviewees, Nurses at two facilities", "levels": null, "corpus_id": 140229945, "sentences": ["Five Stakeholder Groups: Investigators, Steering Committee, Advisory Committee, Clinician and Patient Interviewees, Nurses at two facilities"], "caption": "Figure 2: Checklist Design Process by Stakeholder Group.", "local_uri": ["66f933941bf622e3ad257266e42fff292f59bc11_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Interstices: Sustained Spatial Relationships between Hands and Surfaces Reveal Anticipated Action", "pdf_hash": "b9541ca756117004984e6b48bcad4d9b5f2b1589", "year": 2019, "venue": "CHI", "alt_text": "A series of six images, labeled a to f. Each image shows the positions of a participant's hands from overhead. Each image represents a temporal step in a sequence.", "levels": null, "corpus_id": 111379646, "sentences": ["A series of six images, labeled a to f. Each image shows the positions of a participant's hands from overhead.", "Each image represents a temporal step in a sequence."], "caption": "Figure 1. Hovering interstice sequence: (a) LH rests on the desk (Rested); (b) LH lifts and performs a gesture (activate ink properties tool); (c) LH is Hovering near the surface without contact, RH draws ink strokes; (d) LH and RH bimanually zoom and pan to situate interaction; (e) RH draws additional ink strokes, LH is Hovering; (f) LH swipes on the Hotpad to undo an erroneous ink stroke.", "local_uri": ["b9541ca756117004984e6b48bcad4d9b5f2b1589_Image_002.png"], "annotated": false, "compound": false}
{"title": "Interstices: Sustained Spatial Relationships between Hands and Surfaces Reveal Anticipated Action", "pdf_hash": "b9541ca756117004984e6b48bcad4d9b5f2b1589", "year": 2019, "venue": "CHI", "alt_text": "A series of five images, labeled a to e. Each image shows the positions of a participant's hands from overhead. Each image represents a temporal step in a sequence.", "levels": null, "corpus_id": 111379646, "sentences": ["A series of five images, labeled a to e. Each image shows the positions of a participant's hands from overhead.", "Each image represents a temporal step in a sequence."], "caption": "Figure 2. Away interstice sequence: (a) LH activates ink property tool; (b) RH selects ink style while LH remains in-air; (c) both hands are Away; (d) both hands manipulate ink properties; (e) RH sketches, while LH remains Away.", "local_uri": ["b9541ca756117004984e6b48bcad4d9b5f2b1589_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Interstices: Sustained Spatial Relationships between Hands and Surfaces Reveal Anticipated Action", "pdf_hash": "b9541ca756117004984e6b48bcad4d9b5f2b1589", "year": 2019, "venue": "CHI", "alt_text": "A series of four images, labeled a to d. Each image shows the positions of a participant's hands from overhead. Each image represents a temporal step in a sequence.", "levels": null, "corpus_id": 111379646, "sentences": ["A series of four images, labeled a to d. Each image shows the positions of a participant's hands from overhead.", "Each image represents a temporal step in a sequence."], "caption": "Figure 3. Rested interstice sequence: (a) LH rests lightly on the desk with tips of fngers; (b) LH lifts of the desk to perform a gesture (activate ink properties tool); (c) LH returns to her lap, RH sketches with new ink style; (d) LH continues to rest in her lap while RH performs additional actions.", "local_uri": ["b9541ca756117004984e6b48bcad4d9b5f2b1589_Image_004.png", "b9541ca756117004984e6b48bcad4d9b5f2b1589_Image_016.jpg"], "annotated": false, "compound": true}
{"title": "Interstices: Sustained Spatial Relationships between Hands and Surfaces Reveal Anticipated Action", "pdf_hash": "b9541ca756117004984e6b48bcad4d9b5f2b1589", "year": 2019, "venue": "CHI", "alt_text": "A series of three images, labeled a to c. Each image shows a hand holding a pen.", "levels": null, "corpus_id": 111379646, "sentences": ["A series of three images, labeled a to c. Each image shows a hand holding a pen."], "caption": "Figure 5. Examples of pen grips: (a) palm grip; (b) tuck grip;", "local_uri": ["b9541ca756117004984e6b48bcad4d9b5f2b1589_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "A web-based intelligibility evaluation of sign language video transmitted at low frame rates and bitrates", "pdf_hash": "0b90241062e335452fb92388c974af146c7d34e2", "year": 2013, "venue": "ASSETS", "alt_text": "This is a picture of the vertical Likert scale used in the survey. The question asks \u201chow easy was the video to understand?\u201d Underneath the question is 7 radio buttons, where 1-very easy, 2-easy, 3-somewhat easy, 4-neutral, 5- somewhat difficult, 6-difficult, 7-very difficult.", "levels": null, "corpus_id": 6138270, "sentences": ["This is a picture of the vertical Likert scale used in the survey.", "The question asks \u201chow easy was the video to understand?\u201d Underneath the question is 7 radio buttons, where 1-very easy, 2-easy, 3-somewhat easy, 4-neutral, 5- somewhat difficult, 6-difficult, 7-very difficult."], "caption": "Figure 1: Screen short of one video from web survey evaluating intelligibility of sign language video displayed at 15 frames per second at 30 kilobits per second.", "local_uri": ["0b90241062e335452fb92388c974af146c7d34e2_Image_002.jpg", "0b90241062e335452fb92388c974af146c7d34e2_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "A web-based intelligibility evaluation of sign language video transmitted at low frame rates and bitrates", "pdf_hash": "0b90241062e335452fb92388c974af146c7d34e2", "year": 2013, "venue": "ASSETS", "alt_text": "Block diagram starting from the lower left, image of a person's head with a label 'sender's mind', arrow point up from that picture points to next box labeled 'articulation.' Inside the articulation box is a hand and a picture of an open mouth to indicate sound coming out. Pointing up from the articulation box is an arrow to a picture of a cell phone with the label 'information source.' The arrow pointing from the articulation box to the information source box goes through an oval labeled 'environment.' From the information source box, these is an arrow pointing right to a box labeled 'transmitter (encoder).' This arrow has a lavel called x sub n with a label 'message.' From the transmitter (encoder) box an arrow points from left to right labeled u sub n. There is a box called 'noise source' which points up to the signal u sub n. Output from these two signals is labeled u sub n hat. Ths u sub n hat labels a right pointing arrow which points to the 'receiver (decoder)'  square block. There is an arrow that points from the receiver to the destination box with a picture of a phons is labeled  x sub n hat. There is a down arrow that points from the destination block to a block called 'perception.' The perception block has a picture of an eye and an ear. The arrow pointing from the destination to the perception block goes through an oval labeled 'environment'. There are two 'environment' labels which are symmetrically placed in the diagram. Finally there is a downward pointing arrow from the perception box to the the 'receiver's mind' box with a picture of a head. There is a cloud in-between the sender's mind and receiver's mind blocks that is labeled 'knowledge store.' There are two-dashed arrows pointing outward to the sender's mind and receiver's mind blocks. The dashed lines have question marks underneath. The sender's mind and articulation block diagrams are grouped together and labeled 'sender'. The perception and receiver's mind blocks are grouped together and labeled receiver. There is red dashed line labeled 'signal intelligiblity' which includes all the blocks mentioned EXCEPT the sender's mind, knowledge store, and the receiver's mind. There is a purple dashed line labeled 'signal comprehension' which includes all the blocks circled by signal intelligibility and the receiver's mind block.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 6138270, "sentences": ["Block diagram starting from the lower left, image of a person's head with a label 'sender's mind', arrow point up from that picture points to next box labeled 'articulation.'", "Inside the articulation box is a hand and a picture of an open mouth to indicate sound coming out.", "Pointing up from the articulation box is an arrow to a picture of a cell phone with the label 'information source.'", "The arrow pointing from the articulation box to the information source box goes through an oval labeled 'environment.'", "From the information source box, these is an arrow pointing right to a box labeled 'transmitter (encoder).'", "This arrow has a lavel called x sub n with a label 'message.'", "From the transmitter (encoder) box an arrow points from left to right labeled u sub n. There is a box called 'noise source' which points up to the signal u sub n. Output from these two signals is labeled u sub n hat.", "Ths u sub n hat labels a right pointing arrow which points to the 'receiver (decoder)'  square block.", "There is an arrow that points from the receiver to the destination box with a picture of a phons is labeled  x sub n hat.", "There is a down arrow that points from the destination block to a block called 'perception.'", "The perception block has a picture of an eye and an ear.", "The arrow pointing from the destination to the perception block goes through an oval labeled 'environment'.", "There are two 'environment' labels which are symmetrically placed in the diagram.", "Finally there is a downward pointing arrow from the perception box to the the 'receiver's mind' box with a picture of a head.", "There is a cloud in-between the sender's mind and receiver's mind blocks that is labeled 'knowledge store.'", "There are two-dashed arrows pointing outward to the sender's mind and receiver's mind blocks.", "The dashed lines have question marks underneath.", "The sender's mind and articulation block diagrams are grouped together and labeled 'sender'.", "The perception and receiver's mind blocks are grouped together and labeled receiver.", "There is red dashed line labeled 'signal intelligiblity' which includes all the blocks mentioned EXCEPT the sender's mind, knowledge store, and the receiver's mind.", "There is a purple dashed line labeled 'signal comprehension' which includes all the blocks circled by signal intelligibility and the receiver's mind block."], "caption": "Figure 2: Block diagram of the Human Signal Intelligibility Model. Note that the components comprising signal intelligibility are a subset of signal comprehension, which is signal intelligibility plus the receiver\u2019s mind.", "local_uri": ["0b90241062e335452fb92388c974af146c7d34e2_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A web-based intelligibility evaluation of sign language video transmitted at low frame rates and bitrates", "pdf_hash": "0b90241062e335452fb92388c974af146c7d34e2", "year": 2013, "venue": "ASSETS", "alt_text": "This is an example of the comprehension question asked about the video content shown. This example comprehension questions asks \u201cHow does Stephanie get to school?\u201d Below the question are four multiple choice answers displayed horizontally and each answer is accompanied by a picture. In this example, from left to right is a picture of a green car, yellow bus, a person walking, and a bicycle.", "levels": null, "corpus_id": 6138270, "sentences": ["This is an example of the comprehension question asked about the video content shown.", "This example comprehension questions asks \u201cHow does Stephanie get to school?\u201d Below the question are four multiple choice answers displayed horizontally and each answer is accompanied by a picture.", "In this example, from left to right is a picture of a green car, yellow bus, a person walking, and a bicycle."], "caption": "Figure 4: Multiple choice comprehension question example.", "local_uri": ["0b90241062e335452fb92388c974af146c7d34e2_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Cognitive Aids in Acute Care: Investigating How Cognitive Aids Affect and Support In-hospital Emergency Teams", "pdf_hash": "c041c62f3709cd0df15bc34c88a00df4f83cd230", "year": 2019, "venue": "CHI", "alt_text": "Photo of tablet with the running application. In detail: Large buttons to document defibrillation, adrenalin administration, i.v. access, i.o. access and intubation. Toggle button for chest compressions or spontaneous circulation. Timer and counter for defibrillation and adrenalin.", "levels": null, "corpus_id": 140222645, "sentences": ["Photo of tablet with the running application.", "In detail: Large buttons to document defibrillation, adrenalin administration, i.v. access, i.o. access and intubation.", "Toggle button for chest compressions or spontaneous circulation.", "Timer and counter for defibrillation and adrenalin."], "caption": "", "local_uri": ["c041c62f3709cd0df15bc34c88a00df4f83cd230_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Cognitive Aids in Acute Care: Investigating How Cognitive Aids Affect and Support In-hospital Emergency Teams", "pdf_hash": "c041c62f3709cd0df15bc34c88a00df4f83cd230", "year": 2019, "venue": "CHI", "alt_text": "The paper-based prototypes show pieces of paper such as buttons that could be moved around. There are also visible annotations that were made during the design sessions. The Android-based prototypes show implemented paper-based designs ideas.", "levels": null, "corpus_id": 140222645, "sentences": ["The paper-based prototypes show pieces of paper such as buttons that could be moved around.", "There are also visible annotations that were made during the design sessions.", "The Android-based prototypes show implemented paper-based designs ideas."], "caption": "", "local_uri": ["c041c62f3709cd0df15bc34c88a00df4f83cd230_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Cognitive Aids in Acute Care: Investigating How Cognitive Aids Affect and Support In-hospital Emergency Teams", "pdf_hash": "c041c62f3709cd0df15bc34c88a00df4f83cd230", "year": 2019, "venue": "CHI", "alt_text": "Sketch of a resuscitation team (team leader and four co-workers) at a patient. The team leader is holding the tablet. A red error indicates that there is a one-way flow of information from the tablet to the head of the team leader.", "levels": [[-1], [-1], [-1]], "corpus_id": 140222645, "sentences": ["Sketch of a resuscitation team (team leader and four co-workers) at a patient.", "The team leader is holding the tablet.", "A red error indicates that there is a one-way flow of information from the tablet to the head of the team leader."], "caption": "", "local_uri": ["c041c62f3709cd0df15bc34c88a00df4f83cd230_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Cognitive Aids in Acute Care: Investigating How Cognitive Aids Affect and Support In-hospital Emergency Teams", "pdf_hash": "c041c62f3709cd0df15bc34c88a00df4f83cd230", "year": 2019, "venue": "CHI", "alt_text": "Same sketch of a resuscitation team (team leader and four co-workers) at a patient as in figure 6. Several red error indicate that information is transmitted between human and non-human agents. Several red dots in the head/center of the agents indicates data transformation.", "levels": [[-1], [-1], [-1]], "corpus_id": 140222645, "sentences": ["Same sketch of a resuscitation team (team leader and four co-workers) at a patient as in figure 6.", "Several red error indicate that information is transmitted between human and non-human agents.", "Several red dots in the head/center of the agents indicates data transformation."], "caption": "", "local_uri": ["c041c62f3709cd0df15bc34c88a00df4f83cd230_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Cognitive Aids in Acute Care: Investigating How Cognitive Aids Affect and Support In-hospital Emergency Teams", "pdf_hash": "c041c62f3709cd0df15bc34c88a00df4f83cd230", "year": 2019, "venue": "CHI", "alt_text": "Same sketch of a resuscitation team (team leader and four co-workers) at a patient as in figure 6 and 7. Red errors between individuals illustrate the social effects of artefacts.", "levels": [[-1], [-1]], "corpus_id": 140222645, "sentences": ["Same sketch of a resuscitation team (team leader and four co-workers) at a patient as in figure 6 and 7.", "Red errors between individuals illustrate the social effects of artefacts."], "caption": "", "local_uri": ["c041c62f3709cd0df15bc34c88a00df4f83cd230_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Cognitive Aids in Acute Care: Investigating How Cognitive Aids Affect and Support In-hospital Emergency Teams", "pdf_hash": "c041c62f3709cd0df15bc34c88a00df4f83cd230", "year": 2019, "venue": "CHI", "alt_text": "Same sketch of a resuscitation team (team leader and four co-workers) at a patient as in figure 6,7, and 8. Figure illustrates how sense making is happening overtime while interacting with the environment.", "levels": null, "corpus_id": 140222645, "sentences": ["Same sketch of a resuscitation team (team leader and four co-workers) at a patient as in figure 6,7, and 8.", "Figure illustrates how sense making is happening overtime while interacting with the environment."], "caption": "", "local_uri": ["c041c62f3709cd0df15bc34c88a00df4f83cd230_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Participatory Design of VR Scenarios for Exposure Therapy", "pdf_hash": "28acc8424d98002689d4444f3c5084812e0fe283", "year": 2019, "venue": "CHI", "alt_text": "Students are sitting in a circle looking at the camera, which in the context of a use setting would imply that the students are looking directly at the viewer.", "levels": null, "corpus_id": 140223724, "sentences": ["Students are sitting in a circle looking at the camera, which in the context of a use setting would imply that the students are looking directly at the viewer."], "caption": "", "local_uri": ["28acc8424d98002689d4444f3c5084812e0fe283_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 1. A duet of interaction between a handheld and a wrist worn device (a): the watch is used as a tool palette when annotating text on the phone (b); a simultaneous pinch-to-close swipe gesture on both devices mute their notifications (c); the watch\u2019s orientation indicates which hand part causes a touch, thus enabling a seamless transition between modes: for example, writing with the pad of the finger (d), scrolling with side of the finger (e), and text selection with the knuckle (f).", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 1.", "A duet of interaction between a handheld and a wrist worn device (a): the watch is used as a tool palette when annotating text on the phone (b); a simultaneous pinch-to-close swipe gesture on both devices mute their notifications (c); the watch\u2019s orientation indicates which hand part causes a touch, thus enabling a seamless transition between modes: for example, writing with the pad of the finger (d), scrolling with side of the finger (e), and text selection with the knuckle (f)."], "caption": "Figure 1. A duet of interaction between a handheld and a wrist worn device (a): the watch is used as a tool palette when annotating text on the phone (b); a simultaneous pinch-to-close swipe gesture on both devices mute their notifications (c); the watch\u2019s orientation indicates which hand part causes a touch, thus enabling a seamless transition between modes: for example, writing with the pad of the finger (d), scrolling with side of the finger (e), and text selection with the knuckle (f).", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 2. Devices\u2019 two spatial configurations: watch worn on the a) dorsal side or (b) ventral side of the wrist. c) An expansion watchband allows easy switching between the two.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 2.", "Devices\u2019 two spatial configurations: watch worn on the a) dorsal side or (b) ventral side of the wrist.", "c) An expansion watchband allows easy switching between the two."], "caption": "Figure 2. Devices\u2019 two spatial configurations: watch worn on the a) dorsal side or (b) ventral side of the wrist. c) An expansion watchband allows easy switching between the two.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_003.jpg", "0a0f2973339372b705031dbacbe3ac3341867054_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 3. The core gesture and sensing techniques on a joint interactive platform of a phone and a watch: a) Double bump, b) Multi-device gestures (four different swipes: pinch-open, pinch-close, phone-to-watch, and phone-to-watch, c) Flip and tap, d) Hold and flip, e) Finger posture recognition (the pad, side or knuckle of the finger), and f) Handedness recognition (left- vs. right- hand touch).", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 3.", "The core gesture and sensing techniques on a joint interactive platform of a phone and a watch: a) Double bump, b) Multi-device gestures (four different swipes: pinch-open, pinch-close, phone-to-watch, and phone-to-watch, c) Flip and tap, d) Hold and flip, e) Finger posture recognition (the pad, side or knuckle of the finger), and f) Handedness recognition (left- vs. right- hand touch)."], "caption": "Figure 3. The core gesture and sensing techniques on a joint interactive platform of a phone and a watch: a) Double bump,", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 4. \u2018Hold and flip\u2019 unlocks the phone and registers the devices\u2019 spatial configuration (in this case, the watch is worn on the dorsal side of the wrist, also see Figure 2).", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 4. \u2018Hold and flip\u2019 unlocks the phone and registers the devices\u2019 spatial configuration (in this case, the watch is worn on the dorsal side of the wrist, also see Figure 2)."], "caption": "Figure 4. Hold and flip unlocks the phone and registers the devices\u2019 spatial configuration (in this case, the watch is worn on the dorsal side of the wrist, also see Figure 2).", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 5. On the home screen, a, b) A knuckle-touch repositions app icons on the phone. c) The watch can be used to switch apps on the phone.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 5.", "On the home screen, a, b) A knuckle-touch repositions app icons on the phone.", "c) The watch can be used to switch apps on the phone."], "caption": "Figure 5. On the home screen, a, b) A knuckle-touch repositions app icons on the phone. c) The watch can be used to switch between apps on the phone.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 6. In Email, Multi-device gestures are used to manage new email notifications on both devices.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 6. In Email, Multi-device gestures are used to manage new email notifications on both devices."], "caption": "Figure 6. In Email, Multi-device gestures are used to manage new email notifications on both devices.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 7. In a one-handed scenario, double bumping the phone on the watch creates a gestural shortcut to zoom out the map.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 7.", "In a one-handed scenario, double bumping the phone on the watch creates a gestural shortcut to zoom out the map."], "caption": "Figure 7. In a one-handed scenario, double bumping the phone on the watch creates a gestural shortcut to zoom out the map.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 8. Using the watch to zoom in and select small map location markers on the phone.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 8.", "Using the watch to zoom in and select small map location markers on the phone."], "caption": "Figure 8. Using the watch to zoom in and select small map location markers on the phone.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 9. Swiping on the watch\u2019s screen (b) switches map views \u2013 in this case, from normal (a) to satellite (c).", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 9.", "Swiping on the watch\u2019s screen (b) switches map views \u2013 in this case, from normal (a) to satellite (c)."], "caption": "Figure 9. Swiping on the watch\u2019s screen (b) switches map views \u2013 in this case, from normal (a) to satellite (c).", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 10. A tap accesses a basic menu (a), while a flip-and-tap accesses an advanced menu in the Reader app (b,c).", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 10.", "A tap accesses a basic menu (a), while a flip-and-tap accesses an advanced menu in the Reader app (b,c)."], "caption": "Figure 10. A tap accesses a basic menu (a), while a flip-and-tap accesses an advanced menu in the Reader app (bc).", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 11. Using the watch as an additional way to copy and paste from a clipboard of text. Selected text (a) is added to the clipboard with a swipe to the right (b), which then displays all the selected text (c). Swiping down goes through the text (c).", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 11.", "Using the watch as an additional way to copy and paste from a clipboard of text.", "Selected text (a) is added to the clipboard with a swipe to the right (b), which then displays all the selected text (c).", "Swiping down goes through the text (c)."], "caption": "Figure 11. Using the watch as an additional way to copy and paste from a clipboard of text. Selected text (a) is added to the clipboard with a swipe to the right (b), which then displays all the selected text (c). Swiping down goes through the text (c).", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 12. Enabling basic app access on the watch while using making a phone call.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 12.", "Enabling basic app access on the watch while using making a phone call."], "caption": "Figure 12. Enabling basic app access on the watch while using making a phone call.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Duet: exploring joint interactions on a smart phone and a smart watch", "pdf_hash": "0a0f2973339372b705031dbacbe3ac3341867054", "year": 2014, "venue": "CHI", "alt_text": "Figure 14. Phrasing between the phone and the watch in a target selection task on a map.", "levels": null, "corpus_id": 16747993, "sentences": ["Figure 14.", "Phrasing between the phone and the watch in a target selection task on a map."], "caption": "Figure 14. Phrasing between the phone and the watch in a target selection task on a map.", "local_uri": ["0a0f2973339372b705031dbacbe3ac3341867054_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "Swire: Sketch-based User Interface Retrieval", "pdf_hash": "f02ce7cbb5421c78f59535acba471cd5e32eae65", "year": 2019, "venue": "CHI", "alt_text": "This figure shows the query results of complete sketches by Swire. From the top to bottom, we show six rows of side-by-side the sketch and the top three retrieved results. The first row presents a sketch of a menu and the retrieved variations of the menus. The following rows respectively showed pop-up windows, lists in settings, image windows on top of large blocks of texts, login screens, and full lists of items for selection with similar variations.", "levels": null, "corpus_id": 140226268, "sentences": ["This figure shows the query results of complete sketches by Swire. From the top to bottom, we show six rows of side-by-side the sketch and the top three retrieved results.", "The first row presents a sketch of a menu and the retrieved variations of the menus.", "The following rows respectively showed pop-up windows, lists in settings, image windows on top of large blocks of texts, login screens, and full lists of items for selection with similar variations."], "caption": "TechniqueTop-1Top-10QueryResults (Ranked 1, 2, 3)(Chance)0.362%3.62%BoW-HOG flters15.6%38.8%Swire15.9%60.9%", "local_uri": ["f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_009.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_012.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_013.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_015.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_017.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_019.jpg"], "annotated": false, "compound": true}
{"title": "Swire: Sketch-based User Interface Retrieval", "pdf_hash": "f02ce7cbb5421c78f59535acba471cd5e32eae65", "year": 2019, "venue": "CHI", "alt_text": "This figure shows two rows of results for auto-complete queries. The first row of results consists of a FAB button on the bottom and a variety of content on top with the query only specifying the FAB button at the bottom part of the interfaces to be matched. The second row of results consisted of tabs on top and a variety of the content at the bottom, while the query only specifies the top tabs to be matched.", "levels": null, "corpus_id": 140226268, "sentences": ["This figure shows two rows of results for auto-complete queries.", "The first row of results consists of a FAB button on the bottom and a variety of content on top with the query only specifying the FAB button at the bottom part of the interfaces to be matched.", "The second row of results consisted of tabs on top and a variety of the content at the bottom, while the query only specifies the top tabs to be matched."], "caption": "", "local_uri": ["f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_022.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_024.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_025.jpg"], "annotated": false, "compound": true}
{"title": "Swire: Sketch-based User Interface Retrieval", "pdf_hash": "f02ce7cbb5421c78f59535acba471cd5e32eae65", "year": 2019, "venue": "CHI", "alt_text": "This figure shows four rows of results for querying alternative designs with high-fidelity screenshots. The first row shows a variety of menu bars given a menu bar sketch query. The second row shows a variety of login screens given a sign-up sketch query. The remaining rows are screenshots of lists and grid layouts respectively.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 140226268, "sentences": ["This figure shows four rows of results for querying alternative designs with high-fidelity screenshots.", "The first row shows a variety of menu bars given a menu bar sketch query.", "The second row shows a variety of login screens given a sign-up sketch query.", "The remaining rows are screenshots of lists and grid layouts respectively."], "caption": "Figure 7: Alternative Design Query Results. Swire is able to retrieve similar UIs in the dataset from queries of complete, high-fdelity UI screenshots.", "local_uri": ["f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_027.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_028.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_031.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_032.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_034.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_035.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_036.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_037.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_038.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_039.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_040.jpg", "f02ce7cbb5421c78f59535acba471cd5e32eae65_Image_041.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Brick: Toward A Model for Designing Synchronous Colocated Augmented Reality Games", "pdf_hash": "0394690f412ca3c37df3be6abc3c70536e7146fb", "year": 2019, "venue": "CHI", "alt_text": "A diagram showing the five main categories of interactions in a two-player shared-world AR environment: single-player, intrapersonal, multiplayer, interpersonal, environmental", "levels": null, "corpus_id": 140317169, "sentences": ["A diagram showing the five main categories of interactions in a two-player shared-world AR environment: single-player, intrapersonal, multiplayer, interpersonal, environmental"], "caption": "", "local_uri": ["0394690f412ca3c37df3be6abc3c70536e7146fb_Image_002.png"], "annotated": false, "compound": false}
{"title": "Brick: Toward A Model for Designing Synchronous Colocated Augmented Reality Games", "pdf_hash": "0394690f412ca3c37df3be6abc3c70536e7146fb", "year": 2019, "venue": "CHI", "alt_text": "This three-part diagram shows examples of augmented objects in Brick. (A) shows a pattern of empty slots as seen through a mobile device. (B) shows an example each of individual and collaborative bricks. (C) shows an example of a bomb.", "levels": null, "corpus_id": 140317169, "sentences": ["This three-part diagram shows examples of augmented objects in Brick. (A) shows a pattern of empty slots as seen through a mobile device.", "(B) shows an example each of individual and collaborative bricks. (C) shows an example of a bomb."], "caption": "Figure 2: A diagram showing the major augmented ob- jects that appear in Brick\u2013a pattern of empty slots as seen through a mobile device (A), individual and collaborative bricks (B), and the bomb (C). \u00a9Ketki Jadhav", "local_uri": ["0394690f412ca3c37df3be6abc3c70536e7146fb_Image_003.png"], "annotated": false, "compound": false}
{"title": "Brick: Toward A Model for Designing Synchronous Colocated Augmented Reality Games", "pdf_hash": "0394690f412ca3c37df3be6abc3c70536e7146fb", "year": 2019, "venue": "CHI", "alt_text": "This two-part diagram shows the two different single-player interactions that we prototyped in high-fidelity. (A) shows a slam interaction in which the player hits a brick with their phone to pick it up. (B) shows a tap-and-hold interaction in which a player taps the phone with their finger to pick up a nearby brick.", "levels": null, "corpus_id": 140317169, "sentences": ["This two-part diagram shows the two different single-player interactions that we prototyped in high-fidelity. (A) shows a slam interaction in which the player hits a brick with their phone to pick it up. (B) shows a tap-and-hold interaction in which a player taps the phone with their finger to pick up a nearby brick."], "caption": "Figure 3: A diagram showing the two major pick-up interac- tions we playtested during Brick\u2013(A) slam and (B) tap-and- hold. \u00a9Po Bhattacharyya", "local_uri": ["0394690f412ca3c37df3be6abc3c70536e7146fb_Image_007.png"], "annotated": false, "compound": false}
{"title": "Brick: Toward A Model for Designing Synchronous Colocated Augmented Reality Games", "pdf_hash": "0394690f412ca3c37df3be6abc3c70536e7146fb", "year": 2019, "venue": "CHI", "alt_text": "A three-part diagram showing the specifics of the tap-and-hold interaction. In (A) a player approaches a brick with their phone, and the brick begins to glow. In (B) the player taps and holds down their finger to pick up the brick. In (C) the players lifts their finger off the phone screen to release the brick.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 140317169, "sentences": ["A three-part diagram showing the specifics of the tap-and-hold interaction.", "In (A) a player approaches a brick with their phone, and the brick begins to glow.", "In (B) the player taps and holds down their finger to pick up the brick.", "In (C) the players lifts their finger off the phone screen to release the brick."], "caption": "Figure 4: A graphic of the tap-and-hold interaction showing pick up (A), carry (B), and drop of(C). \u00a9Po Bhattacharyya", "local_uri": ["0394690f412ca3c37df3be6abc3c70536e7146fb_Image_010.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments", "pdf_hash": "4715303bdb871e57cbb597b7e936a6ef4aa2f71a", "year": 2018, "venue": "ASSETS", "alt_text": "A screen shot of a web page from the shopping web site ASOS.com is shown.  It includes both an image of a suggested outfit around the product and lots of text describing the product.  Also shown in the figure is the output text read by a screen reader to illustrate its verbosity.  Included is hundreds of words in tiny print.  Also shown is the output from the Alt text.  In particular, the figure highlights that there is no Alt text associated with the image on the web page.", "levels": null, "corpus_id": 51932320, "sentences": ["A screen shot of a web page from the shopping web site ASOS.com is shown.", "It includes both an image of a suggested outfit around the product and lots of text describing the product.", "Also shown in the figure is the output text read by a screen reader to illustrate its verbosity.", "Included is hundreds of words in tiny print.", "Also shown is the output from the Alt text.", "In particular, the figure highlights that there is no Alt text associated with the image on the web page."], "caption": "ABSTRACT", "local_uri": ["4715303bdb871e57cbb597b7e936a6ef4aa2f71a_Image_001.png"], "annotated": false, "compound": false}
{"title": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments", "pdf_hash": "4715303bdb871e57cbb597b7e936a6ef4aa2f71a", "year": 2018, "venue": "ASSETS", "alt_text": "A schematic is shown to highlight how BrowseWithMe includes a front-end and back-end as well as how the back-end system includes both a computer vision and natural language processing module that analyzes a product web page.", "levels": null, "corpus_id": 51932320, "sentences": ["A schematic is shown to highlight how BrowseWithMe includes a front-end and back-end as well as how the back-end system includes both a computer vision and natural language processing module that analyzes a product web page."], "caption": "", "local_uri": ["4715303bdb871e57cbb597b7e936a6ef4aa2f71a_Image_003.png"], "annotated": false, "compound": false}
{"title": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments", "pdf_hash": "4715303bdb871e57cbb597b7e936a6ef4aa2f71a", "year": 2018, "venue": "ASSETS", "alt_text": "Shown are Likert scale results from eight participants for five questions.  Q6: BrowseWithMe will make online clothing shopping more accessible to people who are blind (Avg. 4.38).  Q7: BrowseWithMe will make clothing shopping more accessible to people who have low vision (Avg. 4.14).  Q8: BrowseWithMe would allow me to find clothing more quickly than my current online shopping experiences (Avg. 3.88).  Q9: BrowseWithMe makes me feel confident in using a computer to complete online shopping (Avg. 4.0).  Q10: I find online shopping to be difficult using BrowseWithMe (Avg. 2.25).", "levels": null, "corpus_id": 51932320, "sentences": ["Shown are Likert scale results from eight participants for five questions.", "Q6: BrowseWithMe will make online clothing shopping more accessible to people who are blind (Avg. 4.38).", "Q7: BrowseWithMe will make clothing shopping more accessible to people who have low vision (Avg. 4.14).", "Q8: BrowseWithMe would allow me to find clothing more quickly than my current online shopping experiences (Avg. 3.88).", "Q9: BrowseWithMe makes me feel confident in using a computer to complete online shopping (Avg. 4.0).", "Q10: I find online shopping to be difficult using BrowseWithMe (Avg. 2.25)."], "caption": "Figure 4. Study participants answers to several Likert scale questions asked about their experiences with BrowseWithMe; 1=Strongly Dis- agree; 2=Disagree; 3=Neutral; 4=Agree; 5=Strongly Agree.", "local_uri": ["4715303bdb871e57cbb597b7e936a6ef4aa2f71a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments", "pdf_hash": "4715303bdb871e57cbb597b7e936a6ef4aa2f71a", "year": 2018, "venue": "ASSETS", "alt_text": "Box plot results show the frequency with which humans could identify the correct image from a given description, both for BrowseWithMe generated descriptions and Alt text.", "levels": [[1]], "corpus_id": 51932320, "sentences": ["Box plot results show the frequency with which humans could identify the correct image from a given description, both for BrowseWithMe generated descriptions and Alt text."], "caption": "Figure 5. Human voting accuracy in choosing the correct image from \ufb01ve options, when given an image description from BrowseWithMe and Alt text. Shown are results for (a) 30 voting tasks which consist of (b-d) 10 voting tasks per clothing item for \u201ctops\", \u201cpants\", and \u201cskirts\" respectively. Each score represents the fraction of ten people who chose the correct image from \ufb01ve options. The central marks of the boxes denote the median values, box edges denote the 25th and 75th percentiles values, whiskers denote the adjacent value to the data point that is greater than one and a half times the size of the inter-quartile range, and black cross-hairs denote outliers. Also shown below the plots are the mean values. Alt text leads to accurate recognition only slightly more than half the time. BrowseWithMe yields a great improvement over Alt text; e.g., 20 percentage point increase in the median score.", "local_uri": ["4715303bdb871e57cbb597b7e936a6ef4aa2f71a_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Social Media Platforms for Low-Income Blind People in India", "pdf_hash": "898ed7c4734346b9708fc075ec048a845cd01c8b", "year": 2015, "venue": "ASSETS", "alt_text": "The picture shows three low-income blind participants using social media platforms in the computer lab.", "levels": null, "corpus_id": 2003331, "sentences": ["The picture shows three low-income blind participants using social media platforms in the computer lab."], "caption": "Figure 1. Blind participants using social media platforms in the computer lab.", "local_uri": ["898ed7c4734346b9708fc075ec048a845cd01c8b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Social Media Platforms for Low-Income Blind People in India", "pdf_hash": "898ed7c4734346b9708fc075ec048a845cd01c8b", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 3 shows a smiling blind participant trying out the demo of a voice forum. He has put his phone on speaker and is listening to the voice messages recorded by others.", "levels": null, "corpus_id": 2003331, "sentences": ["Figure 3 shows a smiling blind participant trying out the demo of a voice forum.", "He has put his phone on speaker and is listening to the voice messages recorded by others."], "caption": "Figure 3. A blind participant accessing the demo of a voice forum.", "local_uri": ["898ed7c4734346b9708fc075ec048a845cd01c8b_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Social Media Platforms for Low-Income Blind People in India", "pdf_hash": "898ed7c4734346b9708fc075ec048a845cd01c8b", "year": 2015, "venue": "ASSETS", "alt_text": "This figure shows a picture of a blind user engrossed in listening to the messages on the social media voice forum using his basic mobile phone.", "levels": null, "corpus_id": 2003331, "sentences": ["This figure shows a picture of a blind user engrossed in listening to the messages on the social media voice forum using his basic mobile phone."], "caption": "Figure 4. A blind participant accessing Sangeet Swara, a social media voice forum", "local_uri": ["898ed7c4734346b9708fc075ec048a845cd01c8b_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Jump and shoot!: prioritizing primary and alternative body gestures for intense gameplay", "pdf_hash": "2ca3672d083197a3cebdb15416487933b54d3650", "year": 2014, "venue": "CHI", "alt_text": "The proportion of body parts used and gesture types (left) and veteran gamers vs. non/normal gamers (right).", "levels": null, "corpus_id": 14316863, "sentences": ["The proportion of body parts used and gesture types (left) and veteran gamers vs. non/normal gamers (right)."], "caption": "", "local_uri": ["2ca3672d083197a3cebdb15416487933b54d3650_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Tactile graphics with a voice: using QR codes to access text in tactile graphics", "pdf_hash": "fd420c53c56d844ccd48efd6a261907b2bc53de9", "year": 2014, "venue": "ASSETS", "alt_text": "This is an example of someone using the system. The tactile graphic is visible with the labels as QR codes. There are two hands visible, one is pointing to a QR code and the other is holding the cell phone with the screen showing the QR code that is being pointed at.", "levels": [[-1], [-1], [-1]], "corpus_id": 297030, "sentences": ["This is an example of someone using the system.", "The tactile graphic is visible with the labels as QR codes.", "There are two hands visible, one is pointing to a QR code and the other is holding the cell phone with the screen showing the QR code that is being pointed at."], "caption": "Figure 1. The Tactile Graphics with a Voice system in use. The subject is using the finger pointing mode to select which QR code to scan.", "local_uri": ["fd420c53c56d844ccd48efd6a261907b2bc53de9_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Tactile graphics with a voice: using QR codes to access text in tactile graphics", "pdf_hash": "fd420c53c56d844ccd48efd6a261907b2bc53de9", "year": 2014, "venue": "ASSETS", "alt_text": "A line chart showing the average accuracies of the different modes for the Bar Chart task. The first session is the lowest and then the lines tend to increase. The fifth session has a large drop in accuracy for the silent and verbal modes. The Silent and Verbal lines tend to be similar and the Finger Pointing line is higher in the last three sessions. There is a single point for the Braille modes in session six which is between Silent and Verbal Modes and below Finger Pointing.", "levels": [[1], [3, 2], [3, 2], [3, 2], [3]], "corpus_id": 297030, "sentences": ["A line chart showing the average accuracies of the different modes for the Bar Chart task.", "The first session is the lowest and then the lines tend to increase.", "The fifth session has a large drop in accuracy for the silent and verbal modes.", "The Silent and Verbal lines tend to be similar and the Finger Pointing line is higher in the last three sessions.", "There is a single point for the Braille modes in session six which is between Silent and Verbal Modes and below Finger Pointing."], "caption": "Figure 3. A comparison of the average accuracy for the Bar Chart task across the six sessions for the three modes (n=10) and Braille (n=6) on the last session. Participants were asked to find the range of the tallest bar and their answer could be 0, 50 or 100% correct.", "local_uri": ["fd420c53c56d844ccd48efd6a261907b2bc53de9_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Tactile graphics with a voice: using QR codes to access text in tactile graphics", "pdf_hash": "fd420c53c56d844ccd48efd6a261907b2bc53de9", "year": 2014, "venue": "ASSETS", "alt_text": "A line chart showing the average time it took participants on all tasks. The y-axis of the chart is time in seconds (ranges from 0 to 60), the x-axis of the chart is session number (ranges from 1 to 6). There is a line for the three modes: Silent, Verbal and Finger Pointing. They all appear to be going down, but there is a big spike in the Verbal mode line at session 4. In general, the Finger Pointing mode is the highest (takes the most time), the Silent mode is next and the Verbal takes the least amount of time, although in the fourth and sixth sessions, the Verbal line is above the Silent one. There is a dot corresponding to the Braille mode at Session 6, it is between the Verbal and Silent modes.", "levels": [[1], [1], [1], [3], [3], [1]], "corpus_id": 297030, "sentences": ["A line chart showing the average time it took participants on all tasks.", "The y-axis of the chart is time in seconds (ranges from 0 to 60), the x-axis of the chart is session number (ranges from 1 to 6).", "There is a line for the three modes: Silent, Verbal and Finger Pointing.", "They all appear to be going down, but there is a big spike in the Verbal mode line at session 4.", "In general, the Finger Pointing mode is the highest (takes the most time), the Silent mode is next and the Verbal takes the least amount of time, although in the fourth and sixth sessions, the Verbal line is above the Silent one.", "There is a dot corresponding to the Braille mode at Session 6, it is between the Verbal and Silent modes."], "caption": "Figure 4. A comparison of the average time it took for each participant to give the answer for a task for the three modes (n=10) across the six sessions as well as for Braille (n=6) on the final session.", "local_uri": ["fd420c53c56d844ccd48efd6a261907b2bc53de9_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image is a visual comparison of GEtiT VR with GEtiT. It shows the differences in the user interfaces.", "levels": null, "corpus_id": 140210912, "sentences": ["This image is a visual comparison of GEtiT VR with GEtiT. It shows the differences in the user interfaces."], "caption": "", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image shows the differences between the 4 available difficulty levels: defined vector representation, undefined vector representation, semi-defined matrix representation and matrix representation.", "levels": null, "corpus_id": 140210912, "sentences": ["This image shows the differences between the 4 available difficulty levels: defined vector representation, undefined vector representation, semi-defined matrix representation and matrix representation."], "caption": "", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image shows the direct value configuration screen. It provides the option to enter self-obtained values in a 4x4 grid resembling the structure of a matrix.", "levels": null, "corpus_id": 140210912, "sentences": ["This image shows the direct value configuration screen.", "It provides the option to enter self-obtained values in a 4x4 grid resembling the structure of a matrix."], "caption": "Figure 2: On expert difculty, the direct value confguration screen has the structure of a 4 \u00d7 4 transformation matrix.", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image displays the direct value configuration screen implemented in GEtiT VR. Users can enter values into a matrix using the game controllers.", "levels": null, "corpus_id": 140210912, "sentences": ["This image displays the direct value configuration screen implemented in GEtiT VR.", "Users can enter values into a matrix using the game controllers."], "caption": "", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image displays one of GEtiT's levels. It shows a gap in the center of the level that has to be crossed by the users. This creates an additional gameplay challenge.", "levels": null, "corpus_id": 140210912, "sentences": ["This image displays one of GEtiT's levels.", "It shows a gap in the center of the level that has to be crossed by the users.", "This creates an additional gameplay challenge."], "caption": "Figure 4: GEtiT VR allows for a direct value input via a diegetic input interface.", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image shows the main menu of GEtiT VR. It displays the game console, the virtual HMD and the selection of available levels.", "levels": null, "corpus_id": 140210912, "sentences": ["This image shows the main menu of GEtiT VR.", "It displays the game console, the virtual HMD and the selection of available levels."], "caption": "Figure 6: GEtiT VR uses an HMD metaphor to allow for a transition between the playing room and a level. On the right hand side, level cubes representing each individual level are shown.", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image is a schematic representation of GEtiT's gameplay: Start level, select AT card, Transformation of object, Check if victory conditions are met, Activate portal.", "levels": null, "corpus_id": 140210912, "sentences": ["This image is a schematic representation of GEtiT's gameplay: Start level, select AT card, Transformation of object, Check if victory conditions are met, Activate portal."], "caption": "", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image shows the NASA-TLX interface in GEtiT VR. It consists of a canvas displaying the questionnaire's contents and rating options.", "levels": null, "corpus_id": 140210912, "sentences": ["This image shows the NASA-TLX interface in GEtiT VR.", "It consists of a canvas displaying the questionnaire's contents and rating options."], "caption": "Item TaskAssessment goals1Create and load a profleText input UI design2Solve \"Translation Easy 1\"AT card interaction Basic locomotion3Solve \"Translation Easy 4\"AT card interaction Advanced locomotion (overleaping a gap, see Figure 5)4Solve a specifc medium translation level (indi- cated by 2 stars)Level selection interface AT card interaction Direct value confgura- tion screen5Solve \"Rotation Expert 1\"AT card interaction Wiki interactionDirect value confgura- tion screen6Solve medium levels for 5 minutesGaming fow", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Usability of Gamified Knowledge Learning in VR and Desktop-3D", "pdf_hash": "36af0cdb50b7f1497de7894af171517c9748377f", "year": 2019, "venue": "CHI", "alt_text": "This image displays the lab in which the study was conducted. A user is wearing the HMD and game controllers. In the foreground, a computer screen shows the gameplay.", "levels": null, "corpus_id": 140210912, "sentences": ["This image displays the lab in which the study was conducted.", "A user is wearing the HMD and game controllers.", "In the foreground, a computer screen shows the gameplay."], "caption": "", "local_uri": ["36af0cdb50b7f1497de7894af171517c9748377f_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Stabilized Annotations for Mobile Remote Assistance", "pdf_hash": "9e34d1d10c087d5bd658b6ad0b19bec707743fa8", "year": 2016, "venue": "CHI", "alt_text": "Worker moving the tablet close to the whiteboard in the Graph task to draw the helper's attention to a particular symbol.", "levels": [[-1]], "corpus_id": 14716453, "sentences": ["Worker moving the tablet close to the whiteboard in the Graph task to draw the helper's attention to a particular symbol."], "caption": "Figure 5. Worker moving the tablet close to the whiteboard to draw helper\u2019s attention to a symbol", "local_uri": ["9e34d1d10c087d5bd658b6ad0b19bec707743fa8_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Developing accessible TV applications", "pdf_hash": "5fedbeff15f7f1885082343680f09d54f2190a2e", "year": 2011, "venue": "ASSETS", "alt_text": "Simulation of user visual impairments (top images), and estimation of cursor movement and times (down images)", "levels": null, "corpus_id": 15890289, "sentences": ["Simulation of user visual impairments (top images), and estimation of cursor movement and times (down images)"], "caption": "Figure 1: Simulation of user visual impairments (top images) and estimating cursor movement and times (down images)", "local_uri": ["5fedbeff15f7f1885082343680f09d54f2190a2e_Image_001.gif", "5fedbeff15f7f1885082343680f09d54f2190a2e_Image_002.gif", "5fedbeff15f7f1885082343680f09d54f2190a2e_Image_003.gif", "5fedbeff15f7f1885082343680f09d54f2190a2e_Image_004.gif"], "annotated": false, "compound": true}
{"title": "Developing accessible TV applications", "pdf_hash": "5fedbeff15f7f1885082343680f09d54f2190a2e", "year": 2011, "venue": "ASSETS", "alt_text": "User-trial photos: in the first photo is the screen showing multiple messages \"Welcome to GUIDE\" in a white screen with different colored fonts; in the second photo is an elderly man interacting with the wiiMote", "levels": [[-1]], "corpus_id": 15890289, "sentences": ["User-trial photos: in the first photo is the screen showing multiple messages \"Welcome to GUIDE\" in a white screen with different colored fonts; in the second photo is an elderly man interacting with the wiiMote"], "caption": "Figure 2: User-trials photos. Colour selection screen (on the left), and an elderly user using a Wii remote (on the right).", "local_uri": ["5fedbeff15f7f1885082343680f09d54f2190a2e_Image_005.gif", "5fedbeff15f7f1885082343680f09d54f2190a2e_Image_006.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "A tool to promote prolonged engagement in art therapy: design and development from arts therapist requirements", "pdf_hash": "1befad97bba058a5cb3bed9262560ef780160fde", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 3. An example sequence of a 50 second interaction in which the user looks away.", "levels": null, "corpus_id": 14192979, "sentences": ["Figure 3.", "An example sequence of a 50 second interaction in which the user looks away."], "caption": "User Action:", "local_uri": ["1befad97bba058a5cb3bed9262560ef780160fde_Image_032.jpg", "1befad97bba058a5cb3bed9262560ef780160fde_Image_033.jpg", "1befad97bba058a5cb3bed9262560ef780160fde_Image_035.jpg", "1befad97bba058a5cb3bed9262560ef780160fde_Image_036.jpg", "1befad97bba058a5cb3bed9262560ef780160fde_Image_037.jpg", "1befad97bba058a5cb3bed9262560ef780160fde_Image_038.jpg"], "annotated": false, "compound": true}
{"title": "StoryBlocks: A Tangible Programming Game To Create Accessible Audio Stories", "pdf_hash": "6b1f0c1b56c85dd3baea5c5868569446609ec23c", "year": 2019, "venue": "CHI", "alt_text": "Photograph of the StoryBlocks prototype being tested by a group of researchers, teachers, and students. The prototype features a rectangular frame with a camera mounted on top. A student manipulates tangible code blocks within the frame.", "levels": [[-1], [-1], [-1]], "corpus_id": 140319561, "sentences": ["Photograph of the StoryBlocks prototype being tested by a group of researchers, teachers, and students.", "The prototype features a rectangular frame with a camera mounted on top.", "A student manipulates tangible code blocks within the frame."], "caption": "Figure 1. StoryBlocks is a tangible programming game in which users create audio stories by combining code blocks. Here a visually impaired high school student creates a story with assistance from teachers.", "local_uri": ["6b1f0c1b56c85dd3baea5c5868569446609ec23c_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "StoryBlocks: A Tangible Programming Game To Create Accessible Audio Stories", "pdf_hash": "6b1f0c1b56c85dd3baea5c5868569446609ec23c", "year": 2019, "venue": "CHI", "alt_text": "Image shows a set of tangible blocks, similar in size and shape to jigsaw puzzle pieces. Each features a unique shape that can connect to other shape, and each has a tactile shape on the top.", "levels": null, "corpus_id": 140319561, "sentences": ["Image shows a set of tangible blocks, similar in size and shape to jigsaw puzzle pieces.", "Each features a unique shape that can connect to other shape, and each has a tactile shape on the top."], "caption": "Figure 2. Blocks used in the initial StoryBlocks prototype include characters (mouse, turtle, cat, snake, cheese), actions (explode, talk, run, dance, eat), and control structures (repeat, branch).", "local_uri": ["6b1f0c1b56c85dd3baea5c5868569446609ec23c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "StoryBlocks: A Tangible Programming Game To Create Accessible Audio Stories", "pdf_hash": "6b1f0c1b56c85dd3baea5c5868569446609ec23c", "year": 2019, "venue": "CHI", "alt_text": "Diagram of 3 blocks connected to each other. Each block uses tabs and grooves to connect to the other blocks.", "levels": null, "corpus_id": 140319561, "sentences": ["Diagram of 3 blocks connected to each other.", "Each block uses tabs and grooves to connect to the other blocks."], "caption": "Figure 3. Blocks represent the phrase \u201cThe mouse eats cheese.\u201d The tabs at the top and bottom indicate that more lines of code may be added to the program.", "local_uri": ["6b1f0c1b56c85dd3baea5c5868569446609ec23c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Modeling and synthesizing spatially inflected verbs for American sign language animations", "pdf_hash": "b69f1c0137468eabb6a1b945bbb0814c6df33078", "year": 2010, "venue": "ASSETS '10", "alt_text": "This image shows a front view and an overhead view of a virtual human signer.  This image also displays the specific arc location and numbering scheme used in our research. Floating in front of the signer at chest height is a semi-circle with seven dots on the position of each 30 degree on the semi-circle. The semi-circle shape is clear in the overhead view.  In the front view, the arc appears as a straight line.  From left to right, the seven dots are labeled as -0.9, -0.6, -0.3, 0, 0.3, 0.6, and 0.9. These labeled numbers are used in our scheme of work described as follows.  The value of 0 indicates the position immediately in front of the signer.  The value of 0.9 indicates the signer's left side.  The value of -0.9 indicates the signer's right side.", "levels": null, "corpus_id": 16596207, "sentences": ["This image shows a front view and an overhead view of a virtual human signer.", "This image also displays the specific arc location and numbering scheme used in our research.", "Floating in front of the signer at chest height is a semi-circle with seven dots on the position of each 30 degree on the semi-circle.", "The semi-circle shape is clear in the overhead view.", "In the front view, the arc appears as a straight line.", "From left to right, the seven dots are labeled as -0.9, -0.6, -0.3, 0, 0.3, 0.6, and 0.9.", "These labeled numbers are used in our scheme of work described as follows.", "The value of 0 indicates the position immediately in front of the signer.", "The value of 0.9 indicates the signer's left side.", "The value of -0.9 indicates the signer's right side."], "caption": "Fig. 2. Front & top view of arc-positions around the signer.", "local_uri": ["b69f1c0137468eabb6a1b945bbb0814c6df33078_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Blocks4All: Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments", "pdf_hash": "469054d43c7e1f45f1016ab366ee4d0e0599182f", "year": 2018, "venue": "CHI", "alt_text": "a) Shows the scratch environment with a large section for program output, a smaller one for toolbox next to it and then a space for the workspace with a progarm in it. b) Shows the Blcosk4All environment with a toolbox on the left side, a large workspace in the middle and a run program button on the right.", "levels": null, "corpus_id": 5068415, "sentences": ["a) Shows the scratch environment with a large section for program output, a smaller one for toolbox next to it and then a space for the workspace with a progarm in it. b) Shows the Blcosk4All environment with a toolbox on the left side, a large workspace in the middle and a run program button on the right."], "caption": "Figure 1: Image comparing the three main components (toolbox, workspace and program output) of blocks-based environments in (a) the Scratch environment [15] and (b) a version of the Blocks4All environment. In Blocks4All, we used a robot as the accessible program output, so only needed a button on the screen to run the program. The Blocks4All environment shows a \u201cRepeat Two Times\u201d loop with a nested \u201cMake Goat Noise\u201d block and a nested \u201cIf Dash Hears a Sound, Make Crocodile Noise\u201d statement.", "local_uri": ["469054d43c7e1f45f1016ab366ee4d0e0599182f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Blocks4All: Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments", "pdf_hash": "469054d43c7e1f45f1016ab366ee4d0e0599182f", "year": 2018, "venue": "CHI", "alt_text": "(a) A finger is dragging a block from left to right in the workspace and a text box says \"Place after move forward block\" and \"Place after turn left block\" indicating the locations that the block can be placed. (b) A finger is selecting a gray box after a \"move forward' block in the workspace, and a text box says \"Place after move forward block\". In the next window a menu pops up and the finger is selecting \"Drive category\"", "levels": null, "corpus_id": 5068415, "sentences": ["(a) A finger is dragging a block from left to right in the workspace and a text box says \"Place after move forward block\" and \"Place after turn left block\" indicating the locations that the block can be placed. (b) A finger is selecting a gray box after a \"move forward' block in the workspace, and a text box says \"Place after move forward block\".", "In the next window a menu pops up and the finger is selecting \"Drive category\""], "caption": "Figure 2. Two methods to move blocks: (a) audio-guided drag and drop, which speaks aloud the location of the block as it is dragged across the screen (gray box indicates audio output of program) and (b) location-first select, select, drop, where a location is selected via gray \u201cconnection blocks\u201d, then the toolbox of blocks that can be placed there appears.", "local_uri": ["469054d43c7e1f45f1016ab366ee4d0e0599182f_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Blocks4All: Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments", "pdf_hash": "469054d43c7e1f45f1016ab366ee4d0e0599182f", "year": 2018, "venue": "CHI", "alt_text": "(a) A finger is touching the inner block of an if statment that contains a \"make alligator noise block\" and a text box says \"Inside 'if' statement. block 4 of 6 in workspace\" indicating the nesting and location of the block. (b) A finger is touching a \"make crocodile noise\" block inside an if statement and a text box says \"\"if-spearcon'. Make crocodile noise. Block 4 of 6 in workspace. Inside 'if' statement\"", "levels": null, "corpus_id": 5068415, "sentences": ["(a) A finger is touching the inner block of an if statment that contains a \"make alligator noise block\" and a text box says \"Inside 'if' statement.", "block 4 of 6 in workspace\" indicating the nesting and location of the block. (b) A finger is touching a \"make crocodile noise\" block inside an if statement and a text box says \"\"if-spearcon'.", "Make crocodile noise.", "Block 4 of 6 in workspace.", "Inside 'if' statement\""], "caption": "Figure 3. Two methods to indicate the spatial structure of the code: (a) a spatial representation with nested statements placed vertically above inner blocks of enclosing statements, and (b) an audio representation with nesting communicated aurally with spearcons (shortened audio representations of words).", "local_uri": ["469054d43c7e1f45f1016ab366ee4d0e0599182f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Blocks4All: Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments", "pdf_hash": "469054d43c7e1f45f1016ab366ee4d0e0599182f", "year": 2018, "venue": "CHI", "alt_text": "A blocks program with two nested repeat loops is shown. The outer repeat loop has \"3x\" selected from a drop down menu and the inner repeat loop has \"2x\" selected from a drop down menu.", "levels": null, "corpus_id": 5068415, "sentences": ["A blocks program with two nested repeat loops is shown.", "The outer repeat loop has \"3x\" selected from a drop down menu and the inner repeat loop has \"2x\" selected from a drop down menu."], "caption": "Figure 4. The first method to access different block types: embedded typed blocks, accessed from a menu embedded within each block (e.g. \"Repeat 2/3 times\")", "local_uri": ["469054d43c7e1f45f1016ab366ee4d0e0599182f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Blocks4All: Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments", "pdf_hash": "469054d43c7e1f45f1016ab366ee4d0e0599182f", "year": 2018, "venue": "CHI", "alt_text": "A blocks program with two nested repeat loops is shown. The outer repeat loop has a \"2\" block on top of it and the inner repeat loop has  a \"3\" block on top of it. In the first window, the finger is selecting the \"3\" block and it says \"Beep beep three times\". The transition says \"beep beep three times selected\", and the second window says \"Beep beep, place three times as number of times to repeat.", "levels": null, "corpus_id": 5068415, "sentences": ["A blocks program with two nested repeat loops is shown.", "The outer repeat loop has a \"2\" block on top of it and the inner repeat loop has  a \"3\" block on top of it.", "In the first window, the finger is selecting the \"3\" block and it says \"Beep beep three times\".", "The transition says \"beep beep three times selected\", and the second window says \"Beep beep, place three times as number of times to repeat."], "caption": "Figure 5. The second method to access different block types: audio-cue typed blocks, when a typed block in the toolbox and the blocks in the workspace that accept it play the same distinct audio cues.", "local_uri": ["469054d43c7e1f45f1016ab366ee4d0e0599182f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Improving Real-Time Captioning Experiences for Deaf and Hard of Hearing Students", "pdf_hash": "29e65e682764c6dcc33cdefd8150521893fc2c94", "year": 2016, "venue": "ASSETS", "alt_text": "CART captioner using his stenographic keyboard to type captions that display on a nearby laptop.", "levels": [[-1]], "corpus_id": 9867775, "sentences": ["CART captioner using his stenographic keyboard to type captions that display on a nearby laptop."], "caption": "ABSTRACT                                  e", "local_uri": ["29e65e682764c6dcc33cdefd8150521893fc2c94_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Improving Real-Time Captioning Experiences for Deaf and Hard of Hearing Students", "pdf_hash": "29e65e682764c6dcc33cdefd8150521893fc2c94", "year": 2016, "venue": "ASSETS", "alt_text": "Eight of our stakeholders are around a table with sketching materials. Four are female students, one of whom is showing a sketch to the group. A sign language interpreter and a CART captioner are providing accommodation to the DHH participants.", "levels": null, "corpus_id": 9867775, "sentences": ["Eight of our stakeholders are around a table with sketching materials.", "Four are female students, one of whom is showing a sketch to the group.", "A sign language interpreter and a CART captioner are providing accommodation to the DHH participants."], "caption": "Figure 3. Co-design workshop with stakeholders.", "local_uri": ["29e65e682764c6dcc33cdefd8150521893fc2c94_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Broadening accessibility through special interests: a new approach for software customization", "pdf_hash": "b7856ed5fd22eb63f65ef6ab345eb5355d9e8539", "year": 2010, "venue": "ASSETS '10", "alt_text": "Figure 3: Images of the four game conditions. The customized games are featured in the bottom row, and the non-customized games are shown in the top row.", "levels": null, "corpus_id": 14864220, "sentences": ["Figure 3: Images of the four game conditions.", "The customized games are featured in the bottom row, and the non-customized games are shown in the top row."], "caption": "Figure 3. The four game conditions for one of our participants.", "local_uri": ["b7856ed5fd22eb63f65ef6ab345eb5355d9e8539_Image_007.png"], "annotated": false, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 1. An illustration of the Off-Limits concept: (a) a user looks at a view of North America on a display; (b) the user points at the location of Europe in off-screen space; (c) the user drags Europe onto the screen from its off-screen location.", "levels": [[-1], [-1]], "corpus_id": 15927048, "sentences": ["Figure 1.", "An illustration of the Off-Limits concept: (a) a user looks at a view of North America on a display; (b) the user points at the location of Europe in off-screen space; (c) the user drags Europe onto the screen from its off-screen location."], "caption": "Figure 1. An illustration of the Off-Limits concept: (a) a user looks at a view of North America on a display; (b) the user points at the location of Europe in off-screen space; (c) the user drags Europe onto the screen from its off-screen location.", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 3. Task durations across values. Error bars denote 95% confidence intervals.", "levels": [[0], [1], [2]], "corpus_id": 15927048, "sentences": ["Figure 3.", "Task durations across values.", "Error bars denote 95% confidence intervals."], "caption": "", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 4. Interaction count (drags) across values. Error bars denote 95% confidence intervals.", "levels": [[0], [1], [2, 1]], "corpus_id": 15927048, "sentences": ["Figure 4.", "Interaction count (drags) across values.", "Error bars denote 95% confidence intervals."], "caption": "Figure 3. Task durations across values. Error bars denote 95% confidence intervals.", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 6. Scatter plot and regressions of our model. Grey indi- cates the display; green areas show 95% prediction intervals.", "levels": [[0], [1], [2, 1]], "corpus_id": 15927048, "sentences": ["Figure 6.", "Scatter plot and regressions of our model.", "Grey indi- cates the display; green areas show 95% prediction intervals."], "caption": "", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 8. Task durations across values. Error bars denote 95% confidence intervals.", "levels": [[0], [1], [2]], "corpus_id": 15927048, "sentences": ["Figure 8.", "Task durations across values.", "Error bars denote 95% confidence intervals."], "caption": "Figure 7. Relationship between values in original input space (top) and corrected input space (bottom).", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 7. Relationship between values in original input space (top) and corrected input space (bottom).", "levels": [[0], [1]], "corpus_id": 15927048, "sentences": ["Figure 7.", "Relationship between values in original input space (top) and corrected input space (bottom)."], "caption": "Figure 8. Task durations across values. Error bars denote 95% confidence intervals.", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 10. Aggregation of drag movement across positive (top) and negative (bottom) values. The shaded areas show the pro- portion of interactions in different parts of the motor space.", "levels": [[0], [1], [1]], "corpus_id": 15927048, "sentences": ["Figure 10.", "Aggregation of drag movement across positive (top) and negative (bottom) values.", "The shaded areas show the pro- portion of interactions in different parts of the motor space."], "caption": "Figure 9. Interaction count (drags) across values. Error bars denote 95% confidence intervals.", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 9. Interaction count (drags) across values. Error bars denote 95% confidence intervals.", "levels": [[0], [1], [2]], "corpus_id": 15927048, "sentences": ["Figure 9.", "Interaction count (drags) across values.", "Error bars denote 95% confidence intervals."], "caption": "Figure 10. Aggregation of drag movement across positive (top) and negative (bottom) values. The shaded areas show the pro- portion of interactions in different parts of the motor space.", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "pdf_hash": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "year": 2016, "venue": "CHI", "alt_text": "Figure 11. A user interacting with the Kinect prototype: (a) a user looks at a view of North America on a display; (b) the user points the location of Europe in off-screen space; (c) the user drags Europe onto the screen from its off-screen location.", "levels": [[-1], [-1]], "corpus_id": 15927048, "sentences": ["Figure 11.", "A user interacting with the Kinect prototype: (a) a user looks at a view of North America on a display; (b) the user points the location of Europe in off-screen space; (c) the user drags Europe onto the screen from its off-screen location."], "caption": "Figure 11. A user interacting with the Kinect prototype: (a) a user looks at a view of North America on a display; (b) the user points the location of Europe in off-screen space; (c) the user drags Europe onto the screen from its off-screen location.", "local_uri": ["7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_012.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"With most of it being pictures now, I rarely use it\": Understanding Twitter's Evolving Accessibility to Blind Users", "pdf_hash": "e4acbc3656424766e39a6fbb0ae758d90554111e", "year": 2016, "venue": "CHI", "alt_text": "Screenshot of a Twitter profile page with a default profile image (a white egg) and a default header image (a blue rectangle).", "levels": null, "corpus_id": 1292068, "sentences": ["Screenshot of a Twitter profile page with a default profile image (a white egg) and a default header image (a blue rectangle)."], "caption": "Figure 1. This sample Twitter profile page uses the default profile image (an egg) and the default header image (a solid blue rectangle). The user\u2019s Twitter handle appears below the profile image, followed by the bio.", "local_uri": ["e4acbc3656424766e39a6fbb0ae758d90554111e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "\"With most of it being pictures now, I rarely use it\": Understanding Twitter's Evolving Accessibility to Blind Users", "pdf_hash": "e4acbc3656424766e39a6fbb0ae758d90554111e", "year": 2016, "venue": "CHI", "alt_text": "1.  photograph: 7 people standing posing for picture 2.  drawing:  a drawing of aunicorn with blue hair 3.  picture of text 4.  image with embedded text (screenshort):  shows a baler, and says, \"Enter to win a New Holland baler for one year's use.\" 5.  screenshot of mobile device (includes Instragram photo of a baby) 6.  a graph 7.  an inspirational quote written in cursive over a pastel background, says, \"nothing can dim the light that shines from within\" 8.  baby mafia meme:  says, \"no one hides from me, find this 'waldo' and bring me his head\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 1292068, "sentences": ["1", "photograph: 7 people standing posing for picture 2.", "drawing:  a drawing of aunicorn with blue hair 3.", "picture of text 4.", "image with embedded text (screenshort):  shows a baler, and says, \"Enter to win a New Holland baler for one year's use.\"", "5", "screenshot of mobile device (includes Instragram photo of a baby) 6.", "a graph 7.", "an inspirational quote written in cursive over a pastel background, says, \"nothing can dim the light that shines from within\" 8.", "baby mafia meme:  says, \"no one hides from me, find this 'waldo' and bring me his head\""], "caption": "Figure 3. Nine types of imagery commonly embedded in tweets. Top row: photograph; drawing; picture of text (\u201cscreenshort\u201d); image with embedded text; screenshot. Bottom row: graph; inspirational quote; meme, unofficial retweet.", "local_uri": ["e4acbc3656424766e39a6fbb0ae758d90554111e_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EDITalk: Towards Designing Eyes-free Interactions for Mobile Word Processing", "pdf_hash": "6d85739c6172489c098b071176b98faae914af01", "year": 2018, "venue": "CHI", "alt_text": "Figure 1. EDITalk allows the user to barge in (or interrupt) real time while listening to a text to facilitate eyes-free word processing. On user utterance, the system pauses real time and executes the desired user operation; system components are shown in the 2nd pane (numbers 1-7 show information flow); sample utterances and system output are shown in the 3rd pane.", "levels": [[-1], [-1], [-1]], "corpus_id": 5040620, "sentences": ["Figure 1.", "EDITalk allows the user to barge in (or interrupt) real time while listening to a text to facilitate eyes-free word processing.", "On user utterance, the system pauses real time and executes the desired user operation; system components are shown in the 2nd pane (numbers 1-7 show information flow); sample utterances and system output are shown in the 3rd pane."], "caption": "Meeting draft to revise but I cannot even look at my phone now. Wish I could do it eyes-free.", "local_uri": ["6d85739c6172489c098b071176b98faae914af01_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EDITalk: Towards Designing Eyes-free Interactions for Mobile Word Processing", "pdf_hash": "6d85739c6172489c098b071176b98faae914af01", "year": 2018, "venue": "CHI", "alt_text": "Figure 2. Schematic of EDITalk\u2019s command design. The red arrow within each box indicates the TTS location and is not part of the visual interface. The top-left corner block denotes the context and the TTS location when the user barges in. The blue blocks show sample utterances (above the individual boxes) for Meta, Core and Navigation commands. For each utterance, the context of use and resulting system action has been shown. The callouts in the Comment and Change commands are part of EDITalk\u2019s review mode functionalities and pop up on mouse over the edited text.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 5040620, "sentences": ["Figure 2.", "Schematic of EDITalk\u2019s command design.", "The red arrow within each box indicates the TTS location and is not part of the visual interface.", "The top-left corner block denotes the context and the TTS location when the user barges in.", "The blue blocks show sample utterances (above the individual boxes) for Meta, Core and Navigation commands.", "For each utterance, the context of use and resulting system action has been shown.", "The callouts in the Comment and Change commands are part of EDITalk\u2019s review mode functionalities and pop up on mouse over the edited text."], "caption": "Figure 2. Schematic of EDITalk\u2019s command design. The red arrow within each box indicates the TTS location and is not part of the visual interface. The top-left corner block denotes the context and the TTS location when the user barges in. The blue blocks show sample utterances (above the individual boxes) for Meta, Core and Navigation commands. For each utterance, the context of use and resulting system action has been shown. The callouts in the Comment and Change commands are part of EDITalk\u2019s review mode functionalities and pop up on mouse over the edited text.", "local_uri": ["6d85739c6172489c098b071176b98faae914af01_Image_005.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "What's on your mind?: investigating recommendations for inclusive social networking and older adults", "pdf_hash": "481c6a735232b4ff4160b97bcfb02e81b3bc7af8", "year": 2014, "venue": "CHI", "alt_text": "Two homepage screenshots of the prototype. The left screenshot utilises the Control UI whereas the right screenshot utilises the Modified UI.", "levels": null, "corpus_id": 7006663, "sentences": ["Two homepage screenshots of the prototype.", "The left screenshot utilises the Control UI whereas the right screenshot utilises the Modified UI."], "caption": "Figure 2. Homepages of the Control UI (left) and Modi\ufb01ed UI (right).", "local_uri": ["481c6a735232b4ff4160b97bcfb02e81b3bc7af8_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction", "pdf_hash": "2920de9be8d55f3c454d677d92cb6b3ebf8a4222", "year": 2019, "venue": "CHI", "alt_text": "Figure 1: (a) PersonalTouch frst collects touchscreen gestures, and then recommends personalized, optimal accessibility settings. (b) PersonalTouch displays the iOS accessibility settings for a 72-year old user with mild tremors, which improved the user's touchscreen input success rate from 48.4% to 66.3%.", "levels": null, "corpus_id": 140230341, "sentences": ["Figure 1: (a) PersonalTouch frst collects touchscreen gestures, and then recommends personalized, optimal accessibility settings.", "(b) PersonalTouch displays the iOS accessibility settings for a 72-year old user with mild tremors, which improved the user's touchscreen input success rate from 48.4% to 66.3%."], "caption": "", "local_uri": ["2920de9be8d55f3c454d677d92cb6b3ebf8a4222_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction", "pdf_hash": "2920de9be8d55f3c454d677d92cb6b3ebf8a4222", "year": 2019, "venue": "CHI", "alt_text": "Figure 2: Touch input tasks for collecting individual touch data. The tasks are designed to collect each of the 6 standard gestures supported by Android and iOS: (a) tap and long press, (b) swipe, (c) horizontal scroll, (d) vertical scroll, (e) pinch, and (f) rotate.", "levels": [[-1], [-1]], "corpus_id": 140230341, "sentences": ["Figure 2: Touch input tasks for collecting individual touch data.", "The tasks are designed to collect each of the 6 standard gestures supported by Android and iOS: (a) tap and long press, (b) swipe, (c) horizontal scroll, (d) vertical scroll, (e) pinch, and (f) rotate."], "caption": "", "local_uri": ["2920de9be8d55f3c454d677d92cb6b3ebf8a4222_Image_003.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction", "pdf_hash": "2920de9be8d55f3c454d677d92cb6b3ebf8a4222", "year": 2019, "venue": "CHI", "alt_text": "Figure 3: Touchscreen input success rate (%) for the 12 participants with motor impairments when using the default touchscreen settings versus PersonalTouch.", "levels": null, "corpus_id": 140230341, "sentences": ["Figure 3: Touchscreen input success rate (%) for the 12 participants with motor impairments when using the default touchscreen settings versus PersonalTouch."], "caption": "", "local_uri": ["2920de9be8d55f3c454d677d92cb6b3ebf8a4222_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "\"Pray before you step out\": describing personal and situational blind navigation behaviors", "pdf_hash": "0d5198b60c5c5e60036acbfee90d810e2cbdc027", "year": 2013, "venue": "ASSETS", "alt_text": "Man in 20s wearing hat and athletic wear along with sunglasses; photo of head and shoulders shows he's actively engaged in outdoor running", "levels": [[-1]], "corpus_id": 6533538, "sentences": ["Man in 20s wearing hat and athletic wear along with sunglasses; photo of head and shoulders shows he's actively engaged in outdoor running"], "caption": "The term \u201cpeople with vision impairments\u201d certainly describes a group  of people  with  medical conditions  that prevent full and clear vision. But it should not be the only term used to describe a technology\u2019s user base. Our interview findings, even with only a moderate number of participants, show that there are significant differences among individuals in this group. Given the history of assistive technology abandonment, basing assistive technology designs on a strong foundation of user data is imperative. Using the navigation attributes as a checklist, we have created example personas (illustrated below) that demonstrate the diversity of navigation styles displayed by our participants. While preliminary, these personas demonstrate how personality and scenario attributes can help us envision future technology, by illustrating the diversity of user needs and preferences.", "local_uri": ["0d5198b60c5c5e60036acbfee90d810e2cbdc027_Image_006.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "No Such Thing as Too Much Chocolate: Evidence Against Choice Overload in E-Commerce", "pdf_hash": "49cb31597722ec0acbf46eec54128ef8947dab02", "year": 2017, "venue": "CHI", "alt_text": "Graph of Model 6 priors showing two lines with a negative slope. The slope is the same, with the black line having an intercept of about +2 compared to the red line. Choice set size is the independent variable and satisfaction is the dependent variable.", "levels": [[1], [3, 2], [0]], "corpus_id": 207247195, "sentences": ["Graph of Model 6 priors showing two lines with a negative slope.", "The slope is the same, with the black line having an intercept of about +2 compared to the red line.", "Choice set size is the independent variable and satisfaction is the dependent variable."], "caption": "Model 6: No Interaction Term", "local_uri": ["49cb31597722ec0acbf46eec54128ef8947dab02_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "No Such Thing as Too Much Chocolate: Evidence Against Choice Overload in E-Commerce", "pdf_hash": "49cb31597722ec0acbf46eec54128ef8947dab02", "year": 2017, "venue": "CHI", "alt_text": "Graph of Model 7 priors showing two lines with a negative slope. The slope is different for the two lines: black, which lies above red, has a very slight negative slope (a change of about -.5 satisfaction). The red line slope is steeper (a change of about -4 satisfaction). Choice set size is the independent variable and satisfaction is the dependent variable.", "levels": [[3, 1], [3, 2], [3, 2], [0]], "corpus_id": 207247195, "sentences": ["Graph of Model 7 priors showing two lines with a negative slope.", "The slope is different for the two lines: black, which lies above red, has a very slight negative slope (a change of about -.5 satisfaction).", "The red line slope is steeper (a change of about -4 satisfaction).", "Choice set size is the independent variable and satisfaction is the dependent variable."], "caption": "Choice set size", "local_uri": ["49cb31597722ec0acbf46eec54128ef8947dab02_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "No Such Thing as Too Much Chocolate: Evidence Against Choice Overload in E-Commerce", "pdf_hash": "49cb31597722ec0acbf46eec54128ef8947dab02", "year": 2017, "venue": "CHI", "alt_text": "Graph of M1 posteriors showing a flat line. Choice set size is the independent variable and satisfaction is the dependent variable. It is identical to Table 3, Model 1", "levels": [[1], [0], [0]], "corpus_id": 207247195, "sentences": ["Graph of M1 posteriors showing a flat line.", "Choice set size is the independent variable and satisfaction is the dependent variable.", "It is identical to Table 3, Model 1"], "caption": "", "local_uri": ["49cb31597722ec0acbf46eec54128ef8947dab02_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "No Such Thing as Too Much Chocolate: Evidence Against Choice Overload in E-Commerce", "pdf_hash": "49cb31597722ec0acbf46eec54128ef8947dab02", "year": 2017, "venue": "CHI", "alt_text": "Graph of M6 posteriors showing two nearly flat lines. A slight negative slope is just visible. The slope is the same, with the black line having an intercept of about +1 compared to the red line. Choice set size is the independent variable and satisfaction is the dependent variable.", "levels": [[1], [2], [2], [0]], "corpus_id": 207247195, "sentences": ["Graph of M6 posteriors showing two nearly flat lines.", "A slight negative slope is just visible.", "The slope is the same, with the black line having an intercept of about +1 compared to the red line.", "Choice set size is the independent variable and satisfaction is the dependent variable."], "caption": "", "local_uri": ["49cb31597722ec0acbf46eec54128ef8947dab02_Image_015.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "No Such Thing as Too Much Chocolate: Evidence Against Choice Overload in E-Commerce", "pdf_hash": "49cb31597722ec0acbf46eec54128ef8947dab02", "year": 2017, "venue": "CHI", "alt_text": "Graph of Model 7 posteriors showing two nearly flat lines. The slope is different for the two lines. Black, which lies above red, has a very slight positive slope. The red line has a slight negative slope. The slopes are barely visible. Choice set size is the independent variable and satisfaction is the dependent variable.", "levels": [[1], [3], [3, 1], [3], [3], [0]], "corpus_id": 207247195, "sentences": ["Graph of Model 7 posteriors showing two nearly flat lines.", "The slope is different for the two lines.", "Black, which lies above red, has a very slight positive slope.", "The red line has a slight negative slope.", "The slopes are barely visible.", "Choice set size is the independent variable and satisfaction is the dependent variable."], "caption": "Satisfaction", "local_uri": ["49cb31597722ec0acbf46eec54128ef8947dab02_Image_016.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Player-Centric Approach to Designing Spatial Skill Training Games", "pdf_hash": "93e2bd05d01fc9e8f560309eb4959f95c81dc7b4", "year": 2019, "venue": "CHI", "alt_text": "A practice question form the online MRT test: Two of these four geometrical drawings show the same object. Can you find those two? Click on each one to select or deselect it, and then click the Check Answers button to see if you got it right! Make sure you select BOTH correct answers. There will always be exactly two correct answers.", "levels": null, "corpus_id": 140216473, "sentences": ["A practice question form the online MRT test: Two of these four geometrical drawings show the same object. Can you find those two? Click on each one to select or deselect it, and then click the Check Answers button to see if you got it right! Make sure you select BOTH correct answers.", "There will always be exactly two correct answers."], "caption": "", "local_uri": ["93e2bd05d01fc9e8f560309eb4959f95c81dc7b4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "A Player-Centric Approach to Designing Spatial Skill Training Games", "pdf_hash": "93e2bd05d01fc9e8f560309eb4959f95c81dc7b4", "year": 2019, "venue": "CHI", "alt_text": "Figure of video game genre preferences for low spatial skill vs high spatial skill group. High spatial group were most fond of Strategy and first person shooter games.", "levels": null, "corpus_id": 140216473, "sentences": ["Figure of video game genre preferences for low spatial skill vs high spatial skill group.", "High spatial group were most fond of Strategy and first person shooter games."], "caption": "", "local_uri": ["93e2bd05d01fc9e8f560309eb4959f95c81dc7b4_Image_008.gif"], "annotated": false, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Four images arranged horizontally each showing a colored gapped circle composed of dots on a grey background also composed of dots. Gapped circle color from left to right: red, magenta, blue, white.", "levels": null, "corpus_id": 1878902, "sentences": ["Four images arranged horizontally each showing a colored gapped circle composed of dots on a grey background also composed of dots.", "Gapped circle color from left to right: red, magenta, blue, white."], "caption": "Figure 1: CDT stimuli: four stills of gapped circles at max\u00ad imum difference from background in the computerized color differentiation test by Flatla and Gutwin.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Plot of how mean discrimination ellipsoid volume (dependent) varies across eight different lighting ratios (independent). As lighting ratio increases (i.e., the room gets brighter or the screen gets darker), ellipsoid volume increases (i.e., participant color differentiation abilities decrease).", "levels": [[1], [3]], "corpus_id": 1878902, "sentences": ["Plot of how mean discrimination ellipsoid volume (dependent) varies across eight different lighting ratios (independent).", "As lighting ratio increases (i.e., the room gets brighter or the screen gets darker), ellipsoid volume increases (i.e., participant color differentiation abilities decrease)."], "caption": "Figure 2: Mean discrimination ellipsoid volumes (in CIE L*u*v* units3) \u00b1 s.e. for each of eight lighting ratios.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Screenshot of WebCDT test in our LabintheWild deployment. Shows a white gapped circle on a square grey background. The square is surrounded by input buttons (labelled with icons showing different orientations of gapped circles) for participants to input the orientation of the gapped circle. Also shows the \"I can't tell\" button for when the orientation is not visible.", "levels": null, "corpus_id": 1878902, "sentences": ["Screenshot of WebCDT test in our LabintheWild deployment.", "Shows a white gapped circle on a square grey background.", "The square is surrounded by input buttons (labelled with icons showing different orientations of gapped circles) for participants to input the orientation of the gapped circle.", "Also shows the \"I can't tell\" button for when the orientation is not visible."], "caption": "", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Histogram plot of mean discrimination volumes (bin size = 250) for our participants. Rises sharply from 0-250 (over 500 participants) to a peak at 500-750 (over 3000 participants). Histogram then falls off with a very long tail. Minimum volume is 21.68, 25th quartile is at volume 804.62, 50th quartile (median) is at volume 1558.38, 75th quartile is at volume 3223.60, maximum volume is 1058397.75. Histogram is cut off above volumes of 12000.", "levels": [[1], [3, 2], [3], [2], [2]], "corpus_id": 1878902, "sentences": ["Histogram plot of mean discrimination volumes (bin size = 250) for our participants.", "Rises sharply from 0-250 (over 500 participants) to a peak at 500-750 (over 3000 participants).", "Histogram then falls off with a very long tail.", "Minimum volume is 21.68, 25th quartile is at volume 804.62, 50th quartile (median) is at volume 1558.38, 75th quartile is at volume 3223.60, maximum volume is 1058397.75.", "Histogram is cut off above volumes of 12000."], "caption": "Figure 4: Histogram of number of participants versus mean discrimination ellipsoid volumes. Participants with ellipsoid volumes above 12,000 are not shown for space reasons.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Plots of lower and upper estimates of the number of unique differentiable colors (independent) versus discrimination ellipsoid volume (dependent). Plots start very high (above 20000 unique colors), fall quickly as ellipsoid volume increases, and then level out into a long-tail. At volume=1000, upper estimate is about 10000 colors and the lower estimate is about 5000 colors; at volume=2000, upper=5000 and lower=2000; at volume 4000, upper=3000 and lower=1000.", "levels": [[1], [3, 2], [2]], "corpus_id": 1878902, "sentences": ["Plots of lower and upper estimates of the number of unique differentiable colors (independent) versus discrimination ellipsoid volume (dependent).", "Plots start very high (above 20000 unique colors), fall quickly as ellipsoid volume increases, and then level out into a long-tail.", "At volume=1000, upper estimate is about 10000 colors and the lower estimate is about 5000 colors; at volume=2000, upper=5000 and lower=2000; at volume 4000, upper=3000 and lower=1000."], "caption": "Figure 5: Estimate of the number of unique differentiable col\u00ad ors versus discrimination ellipsoid volume. Minimum and maximum values for color gamut based on ellipsoid shape are shown as green and blue lines respectively. Vertical lines show positions of the mean and the \ufb01rst standard deviation for our participant population.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Demonstration of ColorCheck's masking. Two images of a kitten sniffing a flower, original on left and masked on right. Masked image shows color pairs that are not differentiable by 15% of the population. A large portion of the background foliage and flowers are masked, however the kitten and some bright pink flowers are unmasked (still differentiable).", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 1878902, "sentences": ["Demonstration of ColorCheck's masking.", "Two images of a kitten sniffing a flower, original on left and masked on right.", "Masked image shows color pairs that are not differentiable by 15% of the population.", "A large portion of the background foliage and flowers are masked, however the kitten and some bright pink flowers are unmasked (still differentiable)."], "caption": "Original image                  (b) Color pairs that are not differen\u00adtiable by 15% of the population are masked.Figure 6: ColorCheck creates masked images, which black out color pairs that are not differentiable by a certain percent\u00ad age of the population.By using a color differentiation model collected from a broad population (such as the ellipsoid volumes computed from our cleaned dataset of 23,120 participants), ColorCheck is able to measure the proportion of the population that is unable to differentiate any pair of image colors. ColorCheck identi\ufb01es the ratio of pixels within the input image that are not differ\u00ad entiable. The output of this process is a data \ufb01le containing a two dimensional description of color differentiability for a given proportion of the population (e.g., one value in this de\u00ad scription might be that 60% of a given image\u2019s colors cannot be distinguished by 20% of the population).In addition to this broad understanding about how an image may be seen by a population, ColorCheck is able to layer a mask (shown as black pixels) on top of an image which shows the color differentiability for a speci\ufb01c part of the population. Using this, designers can easily identify the areas of an image that contain problematic color pairs. This is demonstrated in Figure 6: ColorCheck takes an original image (Figure 6a) and blacks out those pixels that are not differentiable by a given percentage of the population (Figure 6b).Note that these masked images are not intended to show how an image would be perceived by the population \u2013 this would be a dif\ufb01cult task given the variability of color perceptionwithin the population. Instead, ColorCheck allows design\u00ad ers to see if the critical parts of their images are perceivable (in terms of color differentiability) by their target population: Color-coded regions that are blacked out usually suggest that users lose important visual cues to understand data. If adja\u00ad cent colors are black, users will be unable to separate content regions. Finally, if large parts of an image are set to black, de\u00ad signers can assume that users will be unable to decipher the website or image\u2019s message, and aesthetic appreciation of the image could be signi\ufb01cantly altered.In practice, we expect that designers would view the same image with various differentiability thresholds, for example that of 50% or 80%. In the images generated by ColorCheck, all color pairs that are not differentiable by 50% and 20% of the population (respectively) will have been set to black. A designer can then use this information to decide whether the masked parts are essential for interpreting the content. If yes, they may decide to recolor critical areas, such as by choosing colors that are further apart in the color space.In its current form, ColorCheck leaves the decision which color pairs need to be differentiable to the designer. This decision was made because of the dif\ufb01culty to computation\u00ad ally determine the intentions of designers, and in particu\u00ad lar, which colors designers necessarily require to be differen\u00ad tiable. To support designers in this process, ColorCheck pro\u00ad vides a batch mode for processing 0-100% of the population, generating a separate masked image for each percentage. By visually scanning each image in increasing percentage order, pairs of problem colors can be identi\ufb01ed when both regions of the image become masked at the same percentage level.As ColorCheck relies on a data set of ellipsoid volumes, the output currently shows which parts of an image are not differ\u00ad entiable by our web-based participant population. This sam\u00ad ple might not be representative of the average computer user, and likely does not balance usage among situational lighting conditions and devices.  Our experiment therefore remains online; we plan to regularly update the tool with the result\u00ad ing data to achieve more representative forecasts of users\u2019 ability to differentiate colors. In addition, ColorCheck users can plug in their own data sets (e.g., the ellipsoid volumes of their target group obtained through the use of WebCDT). It is also possible to reduce the input data set to a speci\ufb01c popu\u00ad lation of interest based on demographics, device and monitorFigure 7: Mean proportion of image pixels differentiable for different percentages of the population.To estimate how many images in our datasets contain col\u00ad ors that are non-differentiable for different percentages of our participant population, we calculated the average proportion of differentiable colors within each image dataset (across all websites, and across all infographics, respectively). The re\u00ad sults of this analysis shows that 12% of our population can differentiate all colors (100% of pixels) in the websites and infographics in our dataset (see Figure 7). These are par\u00ad ticipants whose ellipsoid volumes were below 500 (see also Figure 4 for the distribution of ellipsoid volumes). Ellipsoid volumes larger than 500 result in a steady loss of differentia\u00ad bility in our image datasets. Roughly half of the population (52%) is unable to differentiate 10% of the colors in an aver\u00ad age website or infographic. As we increase the proportion of the population we are targeting, the mean differentiability fur\u00ad ther decreases to a near-plateau between 90% and 99% of the population differentiating just under 60% of website content and just over 40% of infographic content. One of the rea\u00ad sons for infographics to perform more poorly than websites is that infographics often use homogeneous color schemes, so neighboring colors vary only in intensity. We also observe a sharp decline when targeting more than 99% of the popu\u00ad lation, which are people with severe color differentiation dif\u00ad \ufb01culties in our dataset (i.e., those whose test data resulted in extremely high ellipsoid volume levels).Table 2: Comparison of situational lighting conditions and demographics of the 10% participants with the worst color differentiability, and the remaining 90%. Tests of the equality of two proportions report on Pearson\u2019s chi-squared test.Factor      Worst 10%   Top 90%            StatisticsOutdoors     30.05%     23.14%       \u03c72 = 5.38, p < .05settings, situational lighting conditions, or color vision de\ufb01\u00ad(1,20420)Monitor     M=62.81,    M=68.22,      t               \u22121 97 p     05ciency (e.g., users over a certain age or those on mobile de\u00adbrightness    SD=27.96   SD=29.10(106.03) =. , < .vices). Designers can sub-sample our open-access dataset, orAmbientM=53.81,M=47.10,t(142.93) = 3.50, p < .001use WebCDT to generate their own custom dataset.Color Differentiability in Websites and InfographicsWe employed ColorCheck to evaluate the level of differenti\u00ad ation between two example image datasets: (1) 450 website screenshots, selected to represent a range of domains and to vary in colorfulness, and (2) 3,000 infographics from the on- line community visual.ly.66These datasets have been previously published in [33] and [18] and were used with permission. \u00a0 brightness \u00a0 \u00a0SD=22.84  \u00a0SD=23.79 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Age         M=40.29,    M=30.21,     t(111.04) = 5.27, p < .0001(2,23120) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0SD=20.10   SD=14.83 \u00a0 \u00a0Male 37.39% 26.58% \u03c72 = 13.64, p < .01(1,23120) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 \u00a0 \u00a0CVD 23.04% 2.24% \u03c72 = 413.50, p < .0001But who are the 10% of participants that are most severely affected by a lack of color differentiability in websites and infographics? From our earlier analysis, we know that par\u00ad ticipants who reported being in brighter surroundings, and those who had set their monitors to lower brightness values performed worse. We also saw that being older, male, and  Original websiteColors pairs that are not differentiable by 20%      (c) Colors pairs that are not differentiable by 10% of the population have been set to black.                     of the population have been set to black.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_010.jpg", "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_011.jpg"], "annotated": false, "compound": true}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Plot of mean proportion of image pixels differentiable (independent) for 0% - 100% of the population (dependent) for websites and infographics. Increasing from 0% of the population, both plots start at 100% differentiable and gradually fall to 80% differentiable at 75% of the population. Plots begin to diverge here as they both fall off more quickly until a discontinuity plateau is reached at 88% of the population (websites = 60% differentiable, infographics = 50% differentiable). Plateau gradually declines to 99% of population (websites = 55% differentiable, infographics = 45% differentiable), and then both plots fall to 0% differentiable for 100% of the population.", "levels": [[1], [3, 2], [3, 2], [3, 2]], "corpus_id": 1878902, "sentences": ["Plot of mean proportion of image pixels differentiable (independent) for 0% - 100% of the population (dependent) for websites and infographics.", "Increasing from 0% of the population, both plots start at 100% differentiable and gradually fall to 80% differentiable at 75% of the population.", "Plots begin to diverge here as they both fall off more quickly until a discontinuity plateau is reached at 88% of the population (websites = 60% differentiable, infographics = 50% differentiable).", "Plateau gradually declines to 99% of population (websites = 55% differentiable, infographics = 45% differentiable), and then both plots fall to 0% differentiable for 100% of the population."], "caption": "Figure 7: Mean proportion of image pixels differentiable for different percentages of the population.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Enabling Designers to Foresee Which Colors Users Cannot See", "pdf_hash": "1110852f906fe08d42814a051e4e8c15f0b930b5", "year": 2016, "venue": "CHI", "alt_text": "Image masks generated by ColorCheck for one sample website and one sample infographic showing original image, masked at not differentiable for 20% of the population, and masked at not differentiable for 10% of the population. Website: ColorCheck finds minor differentiability problems at 20% and most text is still legible at 10% although some problems with gradient background colors are highlighted. Infographic: ColorCheck finds substantial differentiation problems at 20% (6 segments (half) of main pie chart are not differentiable) and some text difficulties. At 10%, most of the infographic colors are not differentiable.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 1878902, "sentences": ["Image masks generated by ColorCheck for one sample website and one sample infographic showing original image, masked at not differentiable for 20% of the population, and masked at not differentiable for 10% of the population.", "Website: ColorCheck finds minor differentiability problems at 20% and most text is still legible at 10% although some problems with gradient background colors are highlighted.", "Infographic: ColorCheck finds substantial differentiation problems at 20% (6 segments (half) of main pie chart are not differentiable) and some text difficulties.", "At 10%, most of the infographic colors are not differentiable."], "caption": "Original websiteColors pairs that are not differentiable by 20%      (c) Colors pairs that are not differentiable by 10% of the population have been set to black.                     of the population have been set to black.", "local_uri": ["1110852f906fe08d42814a051e4e8c15f0b930b5_Image_018.jpg", "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_019.jpg", "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_020.jpg", "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_021.jpg", "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_022.jpg", "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_023.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Where's my bus stop?: supporting independence of blind transit riders with StopInfo", "pdf_hash": "a714ac3cb1743e8d504e26439b953139648b2580", "year": 2014, "venue": "ASSETS", "alt_text": "Participant 1 took no trips with StopInfo. Participants 2, 4, and 6 used StopInfo for all trips. Participants 3 and 5 used StopInfo for some trips, mostly those ranked as less familiar.", "levels": null, "corpus_id": 14904133, "sentences": ["Participant 1 took no trips with StopInfo.", "Participants 2, 4, and 6 used StopInfo for all trips.", "Participants 3 and 5 used StopInfo for some trips, mostly those ranked as less familiar."], "caption": "Familiarity Level", "local_uri": ["a714ac3cb1743e8d504e26439b953139648b2580_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "A Spellchecker for Dyslexia", "pdf_hash": "d7364d4e74bd531561fde1f61a0ae8579895cc18", "year": 2015, "venue": "ASSETS", "alt_text": "This figure shows the workflow of the algorithm with and example. The sentence \"I like to eat soup with a spun\" goes throught all the filters: language model, Google ngrams module and parsing.", "levels": null, "corpus_id": 15048167, "sentences": ["This figure shows the workflow of the algorithm with and example.", "The sentence \"I like to eat soup with a spun\" goes throught all the filters: language model, Google ngrams module and parsing."], "caption": "", "local_uri": ["d7364d4e74bd531561fde1f61a0ae8579895cc18_Image_006.jpg", "d7364d4e74bd531561fde1f61a0ae8579895cc18_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "A Spellchecker for Dyslexia", "pdf_hash": "d7364d4e74bd531561fde1f61a0ae8579895cc18", "year": 2015, "venue": "ASSETS", "alt_text": "Screenshots of two sentences used in the experiment presented under the conditions of Detection Only (up) and Suggestions (down). The words that are understood as errors by our system are marked with a red underline. The suggestions appear down the word.", "levels": [[-1], [-1], [-1]], "corpus_id": 15048167, "sentences": ["Screenshots of two sentences used in the experiment presented under the conditions of Detection Only (up) and Suggestions (down).", "The words that are understood as errors by our system are marked with a red underline.", "The suggestions appear down the word."], "caption": "\u2018We can\u2019t give pig [credit] to your [his] words\u2019", "local_uri": ["d7364d4e74bd531561fde1f61a0ae8579895cc18_Image_018.jpg", "d7364d4e74bd531561fde1f61a0ae8579895cc18_Image_020.png", "d7364d4e74bd531561fde1f61a0ae8579895cc18_Image_022.jpg", "d7364d4e74bd531561fde1f61a0ae8579895cc18_Image_023.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "pdf_hash": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "year": 2016, "venue": "CHI", "alt_text": "Left: A person touching the screen with multiple fingers and their palm. Right: Five ovals depicting the touch contact regions registered by the system.", "levels": null, "corpus_id": 489935, "sentences": ["Left: A person touching the screen with multiple fingers and their palm.", "Right: Five ovals depicting the touch contact regions registered by the system."], "caption": "Figure 1. Touching with multiple fingers or various parts of the hand (left) creates various contact regions (right). Current touch screens are not designed to accommodate this kind of touch input when the user\u2019s goal is to activate just a single (x,y) point.", "local_uri": ["4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "pdf_hash": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "year": 2016, "venue": "CHI", "alt_text": "A participant with is seated in front of the PixelSense and is interacting with the study software.", "levels": [[-1]], "corpus_id": 489935, "sentences": ["A participant with is seated in front of the PixelSense and is interacting with the study software."], "caption": "we set the maximum number of trials to 110. Due to fatigue, however, many of our participants could not complete all 110 trials. Participants were instructed to complete as many trials as they could. On average, participants completed 94.4 trials.", "local_uri": ["4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "pdf_hash": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "year": 2016, "venue": "CHI", "alt_text": "A participant is using the entire left edge of his hand to interact with the touch screen.", "levels": null, "corpus_id": 489935, "sentences": ["A participant is using the entire left edge of his hand to interact with the touch screen."], "caption": "Figure 3. P6 interacting with the PixelSense. P6 is using the entire left edge of his hand to interact with the touch screen.", "local_uri": ["4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "pdf_hash": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "year": 2016, "venue": "CHI", "alt_text": "This graph depicts a series of boxplots for the distrubution of the average number of concurrent touches per trial for each participant. Key takeaway: each participant, except P4, averages more than 1 concurrent touch. The highest average is P10 with an average of about five and a half concurrent touches. Many of the participants average between 2 to 4 concurrent touches.", "levels": [[1], [2], [2], [2]], "corpus_id": 489935, "sentences": ["This graph depicts a series of boxplots for the distrubution of the average number of concurrent touches per trial for each participant.", "Key takeaway: each participant, except P4, averages more than 1 concurrent touch.", "The highest average is P10 with an average of about five and a half concurrent touches.", "Many of the participants average between 2 to 4 concurrent touches."], "caption": "Figure 4. Distribution of the average number of concurrent touches per trial. Due to their touch behavior, participants impacted the screen with various parts of the hand, resulting in multiple registered touches.", "local_uri": ["4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "pdf_hash": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "year": 2016, "venue": "CHI", "alt_text": "This figure displays touch input frames from participants P1, P5, P6, and P10. Touch input frames for each participant are shown as one or more ellipses. P1 has one large horizontal ellipse located below a crosshair. P5 has two ellipses, one large and the other medium, located to the right of a crosshair. P6 has two ellipses, one large and one small. The small ellipse is located directly over the crosshair, while the large ellipse is located below the crosshair. P10 has have ellipses, one large and four small. The pattern of the ellipses resemble a handprint, and is located to the left of the crosshair.", "levels": null, "corpus_id": 489935, "sentences": ["This figure displays touch input frames from participants P1, P5, P6, and P10.", "Touch input frames for each participant are shown as one or more ellipses.", "P1 has one large horizontal ellipse located below a crosshair.", "P5 has two ellipses, one large and the other medium, located to the right of a crosshair.", "P6 has two ellipses, one large and one small.", "The small ellipse is located directly over the crosshair, while the large ellipse is located below the crosshair.", "P10 has have ellipses, one large and four small.", "The pattern of the ellipses resemble a handprint, and is located to the left of the crosshair."], "caption": "Figure 6. Touch input frames from participants P1, P5, P6, and P10. Ellipses represent contact areas captured by the PixelSense.", "local_uri": ["4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "pdf_hash": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "year": 2016, "venue": "CHI", "alt_text": "Figure 10 shows two sets of ellipses. The ellipses overlap to show how the ellipses have been prepared to undergo the template-matching-process.", "levels": [[-1], [-1]], "corpus_id": 489935, "sentences": ["Figure 10 shows two sets of ellipses.", "The ellipses overlap to show how the ellipses have been prepared to undergo the template-matching-process."], "caption": "Figure 10. Touch data from two poses translated to the origin, prepared to undergo the template-matching process.", "local_uri": ["4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "pdf_hash": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "year": 2019, "venue": "ASSETS", "alt_text": "A 12 image grid of accessibility-related sidewalk structures labeled with a small circle on the structure. Three examples each of curb ramps, missing curb ramps, obstacles, and surface problems are shown. Curb ramps appear with or without friction strips. Missing curb ramps are shown at corners or at straight portions of curb. Examples of obstacles are a fire hydrant, trash bins, and overgrown foliage. Surface problems shown include sidewalk cracks and bumps.", "levels": null, "corpus_id": 199556928, "sentences": ["A 12 image grid of accessibility-related sidewalk structures labeled with a small circle on the structure.", "Three examples each of curb ramps, missing curb ramps, obstacles, and surface problems are shown.", "Curb ramps appear with or without friction strips.", "Missing curb ramps are shown at corners or at straight portions of curb.", "Examples of obstacles are a fire hydrant, trash bins, and overgrown foliage.", "Surface problems shown include sidewalk cracks and bumps."], "caption": "", "local_uri": ["48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "pdf_hash": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "year": 2019, "venue": "ASSETS", "alt_text": "A high level block diagram of our machine learning pipeline. Inputs are square cropped images as well as additional geopositional features for that image. The output is a prediction such as curb ramp. Intermediate stages are an input vector combining the two types of features, which passes through the ResNet including a fully connected output layer modified for the additional features.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 199556928, "sentences": ["A high level block diagram of our machine learning pipeline.", "Inputs are square cropped images as well as additional geopositional features for that image.", "The output is a prediction such as curb ramp.", "Intermediate stages are an input vector combining the two types of features, which passes through the ResNet including a fully connected output layer modified for the additional features."], "caption": "Figure 3: The structure of our modifed ResNet-18 architecture, which, in addition to pixel-based imagery, incorporates extra features such as position in scene, depth information, and geographic data.", "local_uri": ["48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "pdf_hash": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "year": 2019, "venue": "ASSETS", "alt_text": "An example based analysis of False Positives (left) and False Negatives (right), including 3 examples for each of the 4 label types: curb ramp, missing ramp, surface problem, and obstruction, with brief descriptions of why that example might be difficult to classify. Red arrows point to the labeled object in each image.", "levels": null, "corpus_id": 199556928, "sentences": ["An example based analysis of False Positives (left) and False Negatives (right), including 3 examples for each of the 4 label types: curb ramp, missing ramp, surface problem, and obstruction, with brief descriptions of why that example might be difficult to classify.", "Red arrows point to the labeled object in each image."], "caption": "", "local_uri": ["48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "pdf_hash": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "year": 2019, "venue": "ASSETS", "alt_text": "Three examples of the predicted labels created for the labeling task. The uppermost example is an intersection in a residential neighborhood, where the model performs excellently. All 6 curb ramps in the scene were correctly detected, and no erroneous predictions were made. The middle example is of an intersection in a residential neighborhood with mixed performance. The system correctly labels two curb ramps, but fails to identify a surface problem and a missing curb ramp. The system falsely labels two missing curb ramps, at both ends of a sidewalk. The sidewalk actually has curb ramps at both ends. The bottommost example shows poor performance. The model fails to label three surface problems on the left of the scene, and falsely labels a surface problem where one does not exist. The model correctly labels a surface problem at the right side of the scene.", "levels": null, "corpus_id": 199556928, "sentences": ["Three examples of the predicted labels created for the labeling task.", "The uppermost example is an intersection in a residential neighborhood, where the model performs excellently.", "All 6 curb ramps in the scene were correctly detected, and no erroneous predictions were made.", "The middle example is of an intersection in a residential neighborhood with mixed performance.", "The system correctly labels two curb ramps, but fails to identify a surface problem and a missing curb ramp.", "The system falsely labels two missing curb ramps, at both ends of a sidewalk.", "The sidewalk actually has curb ramps at both ends.", "The bottommost example shows poor performance.", "The model fails to label three surface problems on the left of the scene, and falsely labels a surface problem where one does not exist.", "The model correctly labels a surface problem at the right side of the scene."], "caption": "", "local_uri": ["48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "pdf_hash": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "year": 2019, "venue": "ASSETS", "alt_text": "A graph showing precision-recall curves for overall labeling performance, and performance for each of the four types of labels. Curb ramp performance is best, with performance for missing ramps, obstructions, and surface problems all being somewhat worse.", "levels": [[1], [3, 2]], "corpus_id": 199556928, "sentences": ["A graph showing precision-recall curves for overall labeling performance, and performance for each of the four types of labels.", "Curb ramp performance is best, with performance for missing ramps, obstructions, and surface problems all being somewhat worse."], "caption": "", "local_uri": ["48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "pdf_hash": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "year": 2019, "venue": "ASSETS", "alt_text": "A graph showing improvements in the models' precision and recall as more training data is added. The x-axis uses a log scale ranging from approximately 500 crops in the training set, to approximately two hundred thousand crops in the training set. Both overall precision and overall recall increase from approximately 60 percent to approximately 80 percent.", "levels": [[1], [1], [3, 2]], "corpus_id": 199556928, "sentences": ["A graph showing improvements in the models' precision and recall as more training data is added.", "The x-axis uses a log scale ranging from approximately 500 crops in the training set, to approximately two hundred thousand crops in the training set.", "Both overall precision and overall recall increase from approximately 60 percent to approximately 80 percent."], "caption": "Figure 8: Performance overall and by feature type as the size of the training set increases. Note the log scale on the x axis.", "local_uri": ["48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_015.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Rethinking the Mobile Food Journal: Exploring Opportunities for Lightweight Photo-Based Capture", "pdf_hash": "f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9", "year": 2015, "venue": "CHI", "alt_text": "And web interface, the left side is a photo of some Korean dishes, on the right are entries of the log such as time, enjoyment, location, people, etc.", "levels": null, "corpus_id": 207222553, "sentences": ["And web interface, the left side is a photo of some Korean dishes, on the right are entries of the log such as time, enjoyment, location, people, etc."], "caption": "Figure 1. An entry in our lightweight photo-based food journal. No calorie or nutrition information is shown, as the journal instead logs meal enjoyment, location context, and social context.", "local_uri": ["f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Rethinking the Mobile Food Journal: Exploring Opportunities for Lightweight Photo-Based Capture", "pdf_hash": "f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9", "year": 2015, "venue": "CHI", "alt_text": "The most frequent answer is vegetables (198), and the least is moderation (22).", "levels": null, "corpus_id": 207222553, "sentences": ["The most frequent answer is vegetables (198), and the least is moderation (22)."], "caption": "Figure 2. The 20 most frequent survey responses to \u201cWhat does healthy eating look like to you?\u201d Respondents identify a variety of themes, in contrast to a focus on calories in many current food journaling applications.", "local_uri": ["f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Rethinking the Mobile Food Journal: Exploring Opportunities for Lightweight Photo-Based Capture", "pdf_hash": "f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9", "year": 2015, "venue": "CHI", "alt_text": "In the mobile interface, the left side shows an entry page which allows photo, date, time, feeling before and after eating and notes. On the right is an album-like diary page.", "levels": null, "corpus_id": 207222553, "sentences": ["In the mobile interface, the left side shows an entry page which allows photo, date, time, feeling before and after eating and notes.", "On the right is an album-like diary page."], "caption": "in the photo, but unhealthy in labels). Consistent with our finding that calories are not primary in how many define healthy eating, these differences in skew show that low-level nutritional information often does not correspond with how people consider food healthiness.", "local_uri": ["f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9_Image_004.jpg", "f399fa3a221fa0ee8ae9fe4c514a8e67a860d4f9_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows the overview of the MANA system. It shows the connections between the components of the system. A participant wears sensors, these sensors communicate with a collator application running on a smartphone over bluetooth. This collator application communicates with the hub over the internet using standard and secure internet protocols.", "levels": null, "corpus_id": 51885373, "sentences": ["This figure shows the overview of the MANA system.", "It shows the connections between the components of the system.", "A participant wears sensors, these sensors communicate with a collator application running on a smartphone over bluetooth.", "This collator application communicates with the hub over the internet using standard and secure internet protocols."], "caption": "Figure 1. MANA system overview.", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure has three parts. Part A shows the case of the sensor opened with the sensor hardware and battery visible inside. Part B shows a shoe in profile, with the location of the sensor under the arch of the foot highlighted. It also shows the outline of the previous location of the sensor mounted on the ankle. Part C shows the top down view of the location of the sensor in the shoe.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 51885373, "sentences": ["This figure has three parts.", "Part A shows the case of the sensor opened with the sensor hardware and battery visible inside.", "Part B shows a shoe in profile, with the location of the sensor under the arch of the foot highlighted.", "It also shows the outline of the previous location of the sensor mounted on the ankle.", "Part C shows the top down view of the location of the sensor in the shoe."], "caption": "work\ufb02ow of the system is that a participant can wear the sen- sors to record motion data, which in turn uploads the data over Bluetooth Low Energy (BLE) to a smartphone via the installed Collator app. The Collator further uploads the data to the server for real-time processing, and the results are sent back to the Collator and also visualised in the web application. This is a closed-loop of real-time data collection, analysis, and feedback to the user. The \ufb01rst component of the system is the sensors, and their speci\ufb01cations and design.", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows the iterations of the sensor circuit board over time. There are eight iterations in two rows, the sensors themselves become smaller and more densely packed with components.", "levels": null, "corpus_id": 51885373, "sentences": ["This figure shows the iterations of the sensor circuit board over time.", "There are eight iterations in two rows, the sensors themselves become smaller and more densely packed with components."], "caption": "", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure is a graph of the battery power characteristics of the sensors. On the x-axis is time in days and on the y-axis is battery voltage in volts. There are three curves, all trending downward from approximately 4.2 volts. The ultra low standby curve slowly trends downwards over fifty to sixty days. The day-to-day curve trends down quicker over a two week period. Finally, the intensive use curve trends down even quicker still in three to four days.", "levels": [[1], [1], [3, 2], [3, 2], [3, 2], [3, 2]], "corpus_id": 51885373, "sentences": ["This figure is a graph of the battery power characteristics of the sensors.", "On the x-axis is time in days and on the y-axis is battery voltage in volts.", "There are three curves, all trending downward from approximately 4.2 volts.", "The ultra low standby curve slowly trends downwards over fifty to sixty days.", "The day-to-day curve trends down quicker over a two week period.", "Finally, the intensive use curve trends down even quicker still in three to four days."], "caption": "Figure 3. Iteration of sensor hardware over time.", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows the iterations of the sensor case over time, both from profile and top down perspectives. There are four iterations, the cases become less bulky, and thinner.", "levels": null, "corpus_id": 51885373, "sentences": ["This figure shows the iterations of the sensor case over time, both from profile and top down perspectives.", "There are four iterations, the cases become less bulky, and thinner."], "caption": "", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure has two parts. Part A shows the locations on the body where the sensors are located. Two are in the shoes, one is on the waist. Part B shows the two co-ordinate systems of the sensor reference frame and the global reference frame. They are mostly in line which each other, the slight deviation between the two forward directions is defined as the angle theta.", "levels": null, "corpus_id": 51885373, "sentences": ["This figure has two parts.", "Part A shows the locations on the body where the sensors are located.", "Two are in the shoes, one is on the waist.", "Part B shows the two co-ordinate systems of the sensor reference frame and the global reference frame.", "They are mostly in line which each other, the slight deviation between the two forward directions is defined as the angle theta."], "caption": "", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows a graph of the sensor data from the three sensors. On the x-axis is time in seconds and on the y-axis is either acceleration (in metres per second squared) or rotation rate (in radians per second). All six channels of accelerometer and gyroscope data are shown. The data from the foot sensors is more periodic than the waist sensor, which appears noisier.", "levels": [[1], [1], [1], [3, 2]], "corpus_id": 51885373, "sentences": ["This figure shows a graph of the sensor data from the three sensors.", "On the x-axis is time in seconds and on the y-axis is either acceleration (in metres per second squared) or rotation rate (in radians per second).", "All six channels of accelerometer and gyroscope data are shown.", "The data from the foot sensors is more periodic than the waist sensor, which appears noisier."], "caption": "Figure 6. Sensor locations on the body, and the transformation between sensor body and global reference frames.", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows a graph of a segment of the sensor data from both the left and right sensors on the shoes. On the x-axis is time in seconds and on the y-axis is acceleration in metres per second squared. For every stride there is a large peak followed by a trough, which represents the toe-off and heel-strike events of the step respectively. These peaks and trough pairs alternate between the left and right strides.", "levels": [[1], [1], [3], [3]], "corpus_id": 51885373, "sentences": ["This figure shows a graph of a segment of the sensor data from both the left and right sensors on the shoes.", "On the x-axis is time in seconds and on the y-axis is acceleration in metres per second squared.", "For every stride there is a large peak followed by a trough, which represents the toe-off and heel-strike events of the step respectively.", "These peaks and trough pairs alternate between the left and right strides."], "caption": "Figure 8. Annotated gait segment.", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "pdf_hash": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows the gait cycle. A figure in silhouette moves through the stages of a left step and then a right step. Each stage is annotated showing the heel strike and toe off moments. When both feet are on the ground it is known as \"double support time\", and when one foot is off the ground it is known as left or right \"single support time\". The stance phase of one step is from its heel strike to its toe off. The swing phase of one step is from its toe off to its heel strike.", "levels": null, "corpus_id": 51885373, "sentences": ["This figure shows the gait cycle.", "A figure in silhouette moves through the stages of a left step and then a right step.", "Each stage is annotated showing the heel strike and toe off moments.", "When both feet are on the ground it is known as \"double support time\", and when one foot is off the ground it is known as left or right \"single support time\".", "The stance phase of one step is from its heel strike to its toe off.", "The swing phase of one step is from its toe off to its heel strike."], "caption": "Figure 9. The gait cycle.", "local_uri": ["2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "BraillePlay: educational smartphone games for blind children", "pdf_hash": "88d0f2d52847de405a413a43638329de94b8911c", "year": 2014, "venue": "ASSETS", "alt_text": "Image of two android G1 smartphones. The one on the left is displaying the Braille letter Q with white dots on a black background.  The one on teh right is displaying a menu that reads: Word, Trials Left, Guessed Letters, Enter Letter, Instructions, with white lettering on a black background.", "levels": [[-1], [-1], [-1]], "corpus_id": 23921655, "sentences": ["Image of two android G1 smartphones.", "The one on the left is displaying the Braille letter Q with white dots on a black background.", "The one on teh right is displaying a menu that reads: Word, Trials Left, Guessed Letters, Enter Letter, Instructions, with white lettering on a black background."], "caption": "Figure 1. The VBraille interface for \u201creading\u201d and \u201cwriting\u201d Braille characters (left) and a menu from the VBHangman game that is based on the word game Hangman (right).", "local_uri": ["88d0f2d52847de405a413a43638329de94b8911c_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BraillePlay: educational smartphone games for blind children", "pdf_hash": "88d0f2d52847de405a413a43638329de94b8911c", "year": 2014, "venue": "ASSETS", "alt_text": "Image of a hand with two fingers extended, swiping to the right across a smartphone displaying a Braille character.", "levels": null, "corpus_id": 23921655, "sentences": ["Image of a hand with two fingers extended, swiping to the right across a smartphone displaying a Braille character."], "caption": "", "local_uri": ["88d0f2d52847de405a413a43638329de94b8911c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "BraillePlay: educational smartphone games for blind children", "pdf_hash": "88d0f2d52847de405a413a43638329de94b8911c", "year": 2014, "venue": "ASSETS", "alt_text": "A bar chart entitle time spent playing games. The vertical axis reads hours played and the horizontal axis reads p1- p8. P1, P2, P6, and P8 spent around an hour total playing VBReader (VBR) and VBWriter (VBW). P3, P4,  abd P7 spent close to 4 hours playing both games. P5 spent about an hour playing VBR and VBW and 4 hours playing VBH.", "levels": [[1], [1], [2], [2], [2]], "corpus_id": 23921655, "sentences": ["A bar chart entitle time spent playing games.", "The vertical axis reads hours played and the horizontal axis reads p1- p8.", "P1, P2, P6, and P8 spent around an hour total playing VBReader (VBR) and VBWriter (VBW).", "P3, P4,  abd P7 spent close to 4 hours playing both games.", "P5 spent about an hour playing VBR and VBW and 4 hours playing VBH."], "caption": "Figure 3. Time spent by participants playing VBHangman (VBH), VBReader (VBR), and VBWriter (VBW).", "local_uri": ["88d0f2d52847de405a413a43638329de94b8911c_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "BraillePlay: educational smartphone games for blind children", "pdf_hash": "88d0f2d52847de405a413a43638329de94b8911c", "year": 2014, "venue": "ASSETS", "alt_text": "Line graphs showing gameplay patterns (time to enter a letter and accuracy) for VBWriter and VBReader.", "levels": [[1]], "corpus_id": 23921655, "sentences": ["Line graphs showing gameplay patterns (time to enter a letter and accuracy) for VBWriter and VBReader."], "caption": "", "local_uri": ["88d0f2d52847de405a413a43638329de94b8911c_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "A panel of 4 images from left to right showing a range of textile-embedded 3d printed objects: first, a gold 3d printed box that has a rolling lid containing embedded fabric and strings for actuation; second, a watchband printed on a polyester mesh that supports a watchface; third, an android figure with a pressure-sensitive head that uses an embedded displacement sensor containing a mesh of nylon and spandex fibers to control a set of lights in jar; fourth, a 22-inch crown printed on a single piece of felt being worn by a young child.", "levels": null, "corpus_id": 9708065, "sentences": ["A panel of 4 images from left to right showing a range of textile-embedded 3d printed objects: first, a gold 3d printed box that has a rolling lid containing embedded fabric and strings for actuation; second, a watchband printed on a polyester mesh that supports a watchface; third, an android figure with a pressure-sensitive head that uses an embedded displacement sensor containing a mesh of nylon and spandex fibers to control a set of lights in jar; fourth, a 22-inch crown printed on a single piece of felt being worn by a young child."], "caption": "(a) (b) (c) (d)\u200c\u200c\u200c", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_001.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_002.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_003.jpg", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_004.png"], "annotated": false, "compound": true}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "A sequence of illustrations that shows how plastic can be printed with slight angles at its edges to constrain a piece of fabric as it folds.", "levels": null, "corpus_id": 9708065, "sentences": ["A sequence of illustrations that shows how plastic can be printed with slight angles at its edges to constrain a piece of fabric as it folds."], "caption": "Figure 3. Plastic segments that restrict the bend angles of a textile.", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_014.png"], "annotated": false, "compound": false}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "A textile with plastic slits that causes it to only be able to roll along the axis parallel to the plastic slits.", "levels": null, "corpus_id": 9708065, "sentences": ["A textile with plastic slits that causes it to only be able to roll along the axis parallel to the plastic slits."], "caption": "Figure 4. A textile that is only capable of rolling or \ufb02exing along the axis parallel to the plastic segments.", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_015.png"], "annotated": false, "compound": false}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "Side view of two String actuated mechanical arms on fabrics. The left one is has straight pieces of plastic printed on felt while the right has slightly slanted plastic pieces printed on a mesh.", "levels": null, "corpus_id": 9708065, "sentences": ["Side view of two String actuated mechanical arms on fabrics.", "The left one is has straight pieces of plastic printed on felt while the right has slightly slanted plastic pieces printed on a mesh."], "caption": "", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_016.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_017.png"], "annotated": false, "compound": true}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "The faces of a shell printed dodecahedron laid flat. On the right is the final form achieved by folding the faces and holding them in place with rubbers bands.", "levels": null, "corpus_id": 9708065, "sentences": ["The faces of a shell printed dodecahedron laid flat.", "On the right is the final form achieved by folding the faces and holding them in place with rubbers bands."], "caption": "", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_018.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_019.png"], "annotated": false, "compound": true}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "String actuated mechanical arms. On the left is orange 3d-printed pieces of plastic with the pieces of plastic printed parallel to one another. The right one is white pieces of plastic with pieces that are parallel but printed at a slight angle such that when the arm is actuated, the arm curls towards the left.", "levels": null, "corpus_id": 9708065, "sentences": ["String actuated mechanical arms.", "On the left is orange 3d-printed pieces of plastic with the pieces of plastic printed parallel to one another.", "The right one is white pieces of plastic with pieces that are parallel but printed at a slight angle such that when the arm is actuated, the arm curls towards the left."], "caption": "(b)\u200c\u200c\u200c", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_020.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_021.png"], "annotated": false, "compound": true}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "3D printed grommets in the shape of a heart, start, gear-like circle, a round shape, cross, and bell printed on a piece of polyester mesh.", "levels": null, "corpus_id": 9708065, "sentences": ["3D printed grommets in the shape of a heart, start, gear-like circle, a round shape, cross, and bell printed on a piece of polyester mesh."], "caption": "", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_022.png"], "annotated": false, "compound": false}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "A sequence showing different views of a shell-printed triangular prism. The first two views show a side and top view of a layer of fabric printed with 3 square frames printed next to each other in row. The last view shows the sides folded up to create the triangular prism.", "levels": null, "corpus_id": 9708065, "sentences": ["A sequence showing different views of a shell-printed triangular prism.", "The first two views show a side and top view of a layer of fabric printed with 3 square frames printed next to each other in row.", "The last view shows the sides folded up to create the triangular prism."], "caption": "Figure 7. A shell printed triangular prism \ufb01rst printed \ufb02at and then folded to its 3D form.", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_023.jpg"], "annotated": false, "compound": false}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "Purple felt with 3d printed snaps on it. some of the snaps are pressed closes while others remain open.", "levels": null, "corpus_id": 9708065, "sentences": ["Purple felt with 3d printed snaps on it.", "some of the snaps are pressed closes while others remain open."], "caption": "Figure 8. 3D Printed snaps printed onto a fabric.", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_024.png"], "annotated": false, "compound": false}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "The top left shows a red 3d printed displacement sensor with embedded polyester mesh. The top right shows a red 3d printed mechanical slider with a thumb piece that retracts.", "levels": null, "corpus_id": 9708065, "sentences": ["The top left shows a red 3d printed displacement sensor with embedded polyester mesh.", "The top right shows a red 3d printed mechanical slider with a thumb piece that retracts."], "caption": "", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_039.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_040.png"], "annotated": false, "compound": true}
{"title": "Stretching the Bounds of 3D Printing with Embedded Textiles", "pdf_hash": "ad6f760ce294bf8f016c986ada688db0f861fd40", "year": 2017, "venue": "CHI", "alt_text": "A side view of 3 joined panels of the shell printed lampshade is shown on the left. The 3D printed panels are visible. On the right is the fully constructed and lit lampshade hanging from a ceiling.", "levels": null, "corpus_id": 9708065, "sentences": ["A side view of 3 joined panels of the shell printed lampshade is shown on the left.", "The 3D printed panels are visible.", "On the right is the fully constructed and lit lampshade hanging from a ceiling."], "caption": "(b)\u200c\u200c", "local_uri": ["ad6f760ce294bf8f016c986ada688db0f861fd40_Image_043.png", "ad6f760ce294bf8f016c986ada688db0f861fd40_Image_044.png"], "annotated": false, "compound": true}
{"title": "Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface", "pdf_hash": "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59", "year": 2017, "venue": "ASSETS", "alt_text": "The 4-motor and 8-motor wristbands, mounted on a user\u2019s wrist, showing two bands for each version: the band with the motors and the band housing the wiring. It also shows close-up detail of circular vibromotors and the 3D-printed cases with embedded magnets for attaching each vibromotor.", "levels": null, "corpus_id": 10742502, "sentences": ["The 4-motor and 8-motor wristbands, mounted on a user\u2019s wrist, showing two bands for each version: the band with the motors and the band housing the wiring.", "It also shows close-up detail of circular vibromotors and the 3D-printed cases with embedded magnets for attaching each vibromotor."], "caption": "Figure 1. Wristbands with four and eight motors. Image adapted from our prior work [18], which used the same hardware and wristband configuration.", "local_uri": ["3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface", "pdf_hash": "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59", "year": 2017, "venue": "ASSETS", "alt_text": "The leftmost image shows a cross-section of the wrist, with the four-motor wristband, showing that the motors are not equidistant to each other when positioned at the left-most, right-most, top-most, and bottom-most edges of the wrist. Also, an illustration of how simple feedback vibrates the motor closest to the prompted angle, whereas interpolated feedback vibrates the two closest motors.", "levels": null, "corpus_id": 10742502, "sentences": ["The leftmost image shows a cross-section of the wrist, with the four-motor wristband, showing that the motors are not equidistant to each other when positioned at the left-most, right-most, top-most, and bottom-most edges of the wrist.", "Also, an illustration of how simple feedback vibrates the motor closest to the prompted angle, whereas interpolated feedback vibrates the two closest motors."], "caption": "", "local_uri": ["3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface", "pdf_hash": "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59", "year": 2017, "venue": "ASSETS", "alt_text": "The leftmost image shows a cross-section of the wrist, with the eight-motor wristband, showing that the motors are not equidistant to each other when positioned at the left-most, right-most, top-most, and bottom-most edges of the wrist. Also, an illustration of how simple feedback vibrates the motor closest to the prompted angle, whereas interpolated feedback vibrates the two closest motors.", "levels": null, "corpus_id": 10742502, "sentences": ["The leftmost image shows a cross-section of the wrist, with the eight-motor wristband, showing that the motors are not equidistant to each other when positioned at the left-most, right-most, top-most, and bottom-most edges of the wrist.", "Also, an illustration of how simple feedback vibrates the motor closest to the prompted angle, whereas interpolated feedback vibrates the two closest motors."], "caption": "Figure 2. The position of motors around the wrist and an example of haptic feedback of 4 or 8 motors with single motor vibration or interpolation. In this figure, \ud835\udf3d is the prompted direction (blue dotted arrow) and red and light red arrows are directions of vibration.", "local_uri": ["3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface", "pdf_hash": "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59", "year": 2017, "venue": "ASSETS", "alt_text": "This figure shows the experiment setup for study 1 and 2. It shows a participant doing a target finding task with wristband worn on the right hand. The participant is doing the task on a tablet computer.", "levels": null, "corpus_id": 10742502, "sentences": ["This figure shows the experiment setup for study 1 and 2.", "It shows a participant doing a target finding task with wristband worn on the right hand.", "The participant is doing the task on a tablet computer."], "caption": "Figure 4. The experimental setup for both studies. Visuals on the screen were for the experimenter and were not visible to sighted participants nor perceptible to blind participants.", "local_uri": ["3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface", "pdf_hash": "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59", "year": 2017, "venue": "ASSETS", "alt_text": "This is a sample path used in the study. The path starts from the center and has three straight lines of segments.", "levels": null, "corpus_id": 10742502, "sentences": ["This is a sample path used in the study.", "The path starts from the center and has three straight lines of segments."], "caption": "", "local_uri": ["3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface", "pdf_hash": "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59", "year": 2017, "venue": "ASSETS", "alt_text": "This is a sample path used in the study. The path starts from the center and has two straight segments and one curved segment.", "levels": null, "corpus_id": 10742502, "sentences": ["This is a sample path used in the study.", "The path starts from the center and has two straight segments and one curved segment."], "caption": "Figure 6. Sample paths used in the path-tracing task. All path started at the center of the screen.", "local_uri": ["3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_021.jpg", "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_022.jpg", "3dbbb9d301fb4d60c0231b108115b4c6ef4a5a59_Image_023.jpg"], "annotated": false, "compound": true}
{"title": "Closing the Gap: Designing for the Last-Few-Meters Wayfinding Problem for People with Visual Impairments", "pdf_hash": "49b74f82af6b6caff2184be4298f8dd613d616b2", "year": 2019, "venue": "ASSETS", "alt_text": "Figure 1 shows a screenshot of the landmark AI iPhone app, showing the buttons for the available channels. A photo of a storefront illustrates how the system can help a user navigate by identifying the door of the store and reading out the store's sign.", "levels": null, "corpus_id": 198935272, "sentences": ["Figure 1 shows a screenshot of the landmark AI iPhone app, showing the buttons for the available channels.", "A photo of a storefront illustrates how the system can help a user navigate by identifying the door of the store and reading out the store's sign."], "caption": "", "local_uri": ["49b74f82af6b6caff2184be4298f8dd613d616b2_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Closing the Gap: Designing for the Last-Few-Meters Wayfinding Problem for People with Visual Impairments", "pdf_hash": "49b74f82af6b6caff2184be4298f8dd613d616b2", "year": 2019, "venue": "ASSETS", "alt_text": "Figure 2 shows a grid of images of custom locations (e.g., a statue, a park bench, a ticket booth) that the user can recognize using system's Place Channel.", "levels": null, "corpus_id": 198935272, "sentences": ["Figure 2 shows a grid of images of custom locations (e.g., a statue, a park bench, a ticket booth) that the user can recognize using system's Place Channel."], "caption": "Figure 2. Examples of places for the Place Channel. (a) \u201cthe park bench near the fountain\u201d (b) \u201cticket counter at IMAX\u201d (c) \u201cnear the statue next to the Chanel store\u201d", "local_uri": ["49b74f82af6b6caff2184be4298f8dd613d616b2_Image_003.png"], "annotated": false, "compound": false}
{"title": "Care and Connect: Exploring Dementia-Friendliness Through an Online Community Commissioning Platform", "pdf_hash": "58b7af04affc915ffb73810998961bbef6bbd727", "year": 2017, "venue": "CHI", "alt_text": "Photograph shows a table with printed material set out on it, including postcards explaining the Care and Connect app", "levels": [[-1]], "corpus_id": 6348792, "sentences": ["Photograph shows a table with printed material set out on it, including postcards explaining the Care and Connect app"], "caption": "Figure 2: we used printed and digital material in our workshops to stimulate discussion around Care & Connect", "local_uri": ["58b7af04affc915ffb73810998961bbef6bbd727_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Care and Connect: Exploring Dementia-Friendliness Through an Online Community Commissioning Platform", "pdf_hash": "58b7af04affc915ffb73810998961bbef6bbd727", "year": 2017, "venue": "CHI", "alt_text": "Photograph shows carers and people with dementia (2 couples) interacting with care staff and workshop materials", "levels": [[-1]], "corpus_id": 6348792, "sentences": ["Photograph shows carers and people with dementia (2 couples) interacting with care staff and workshop materials"], "caption": "Figure 3: participants at a Care & Connect workshop", "local_uri": ["58b7af04affc915ffb73810998961bbef6bbd727_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Bypassing lists: accelerating screen-reader fact-finding with guided tours", "pdf_hash": "fb8712aa203c167f7d5952160fefd89b993eb16d", "year": 2013, "venue": "ASSETS", "alt_text": "Figure 2: The presence or absence of the index-level scent matching the expected cue the user has in mind for a given task determines the relative efficiency of index, guided-tour or mixed-navigation patterns during fact-finding.", "levels": null, "corpus_id": 17916337, "sentences": ["Figure 2: The presence or absence of the index-level scent matching the expected cue the user has in mind for a given task determines the relative efficiency of index, guided-tour or mixed-navigation patterns during fact-finding."], "caption": "Figure 2. The presence or absence of the index-level scent matching the expected cue the user has in mind for a given task determines the relative efficiency of index, guided-tour or mixed-navigation patterns during fact-finding.", "local_uri": ["fb8712aa203c167f7d5952160fefd89b993eb16d_Image_002.png"], "annotated": false, "compound": false}
{"title": "Bypassing lists: accelerating screen-reader fact-finding with guided tours", "pdf_hash": "fb8712aa203c167f7d5952160fefd89b993eb16d", "year": 2013, "venue": "ASSETS", "alt_text": "Figure 4. When indexes did not contain relevant scent and aims were placed in the first half of a 30-item collection, guided-tour and mixed-pattern navigation significantly reduced time-on-task and number of pages visited, compared to index navigation. Number of keystrokes was significantly reduced only in the mixed pattern condition. Asterisks show significant differences.", "levels": null, "corpus_id": 17916337, "sentences": ["Figure 4. When indexes did not contain relevant scent and aims were placed in the first half of a 30-item collection, guided-tour and mixed-pattern navigation significantly reduced time-on-task and number of pages visited, compared to index navigation.", "Number of keystrokes was significantly reduced only in the mixed pattern condition.", "Asterisks show significant differences."], "caption": "Figure 4. When indexes did not contain relevant scent and aims were placed in the first half of a 30-item collection, guided-tour and mixed-pattern navigation significantly reduced time-on-task and number of pages visited, compared to index navigation. Number of keystrokes was significantly reduced only in the mixed-pattern condition. Asterisks show significant differences.", "local_uri": ["fb8712aa203c167f7d5952160fefd89b993eb16d_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Bypassing lists: accelerating screen-reader fact-finding with guided tours", "pdf_hash": "fb8712aa203c167f7d5952160fefd89b993eb16d", "year": 2013, "venue": "ASSETS", "alt_text": "Figure 5. When indexes did not contain useful scent and task aims were placed in the second half of a 30-item collection, mixed pattern significantly reduced the time on task, number of pages visited, and number of keystrokes as compared with index navigation. Guided tour navigation significantly reduced the number of pages visited and yielded the lowest cognitive effort. Asterisks show significant differences.", "levels": null, "corpus_id": 17916337, "sentences": ["Figure 5.", "When indexes did not contain useful scent and task aims were placed in the second half of a 30-item collection, mixed pattern significantly reduced the time on task, number of pages visited, and number of keystrokes as compared with index navigation.", "Guided tour navigation significantly reduced the number of pages visited and yielded the lowest cognitive effort.", "Asterisks show significant differences."], "caption": "Figure 5. When indexes lack scent and aims were placed in the second half of a 30-item collection, mixed-pattern significantly reduced time-on-task, pages visited, and keystrokes as compared to the index. Guided tour navigation significantly reduced the number of pages visited and yielded the lowest cognitive effort. Asterisks show significant differences.", "local_uri": ["fb8712aa203c167f7d5952160fefd89b993eb16d_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility by demonstration: enabling end users to guide developers to web accessibility solutions", "pdf_hash": "55e53f58022271c2b7542b7e14c4d4c99076e4c6", "year": 2010, "venue": "ASSETS '10", "alt_text": "A screenshot of WAVE, showing how it displays a large number of warnings and errors.", "levels": [[-1]], "corpus_id": 1528574, "sentences": ["A screenshot of WAVE, showing how it displays a large number of warnings and errors."], "caption": "", "local_uri": ["55e53f58022271c2b7542b7e14c4d4c99076e4c6_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Accessibility by demonstration: enabling end users to guide developers to web accessibility solutions", "pdf_hash": "55e53f58022271c2b7542b7e14c4d4c99076e4c6", "year": 2010, "venue": "ASSETS '10", "alt_text": "A keyboard with the left tab key outlined  in red.  The caption reads \"next focusable element\" and to the display shows a sequence of events going down in both size and opacity that reads TAB, TAB,TAB, CTRL H, CTRL H...", "levels": null, "corpus_id": 1528574, "sentences": ["A keyboard with the left tab key outlined  in red.", "The caption reads \"next focusable element\" and to the display shows a sequence of events going down in both size and opacity that reads TAB, TAB,TAB, CTRL H, CTRL H..."], "caption": "", "local_uri": ["55e53f58022271c2b7542b7e14c4d4c99076e4c6_Image_003.gif"], "annotated": false, "compound": false}
{"title": "Accessibility by demonstration: enabling end users to guide developers to web accessibility solutions", "pdf_hash": "55e53f58022271c2b7542b7e14c4d4c99076e4c6", "year": 2010, "venue": "ASSETS '10", "alt_text": "A screenshot of a web page reading \"Evaluateas Follows\" followed by some instructions indicating to use WAVE to evaluate the page.  Then there is a heading that says \"Edit the Web Page\" followed by HTML sourcecode in a big textarea.", "levels": null, "corpus_id": 1528574, "sentences": ["A screenshot of a web page reading \"Evaluateas Follows\" followed by some instructions indicating to use WAVE to evaluate the page.", "Then there is a heading that says \"Edit the Web Page\" followed by HTML sourcecode in a big textarea."], "caption": "", "local_uri": ["55e53f58022271c2b7542b7e14c4d4c99076e4c6_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Objects in the Physical World: Relatedness and Psychological Ownership in Augmented Reality", "pdf_hash": "a928a5b2ed09dd8c0d8266eb16d35a03e25d8b42", "year": 2019, "venue": "CHI", "alt_text": "https://lh6.googleusercontent.com/4JXXqErw8DYrjJq8HmowVMt3x9U-O_Shx4J4nUOfF32GdAt4-8EpUNNWDEwAaGN0N0PUe4Ju971t4E8qzMZUZb8ZLcBQQW_x1Pu6j7Hww38-Dupi7Jig_WSulMKmwgcp3XOP0wcn", "levels": null, "corpus_id": 140225719, "sentences": ["https://lh6.googleusercontent.com/4JXXqErw8DYrjJq8HmowVMt3x9U-O_Shx4J4nUOfF32GdAt4-8EpUNNWDEwAaGN0N0PUe4Ju971t4E8qzMZUZb8ZLcBQQW_x1Pu6j7Hww38-Dupi7Jig_WSulMKmwgcp3XOP0wcn"], "caption": "Figure 1a and 1b: The view of the application with the virtual dog represented in VE mode (left) and AR mode (right).", "local_uri": ["a928a5b2ed09dd8c0d8266eb16d35a03e25d8b42_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "The Effects of \"Not Knowing What You Don't Know\" on Web Accessibility for Blind Web Users", "pdf_hash": "cd7ed653bd4cfa7c5a80a43247dbd0f130a6a061", "year": 2017, "venue": "ASSETS", "alt_text": "The page of instruction used to guide participants through the study asekd them to first find an answer to a question, then answer questions about what they found diffiucult, and they they thought those things happened.", "levels": null, "corpus_id": 26147581, "sentences": ["The page of instruction used to guide participants through the study asekd them to first find an answer to a question, then answer questions about what they found diffiucult, and they they thought those things happened."], "caption": "Figure 2: An example task page from our study, which asks partic- ipants to answer a question on the given page, times how long they take to answer, and asks them questions about the task.", "local_uri": ["cd7ed653bd4cfa7c5a80a43247dbd0f130a6a061_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Making Sense of Art: Access for Gallery Visitors with Vision Impairments", "pdf_hash": "c75f98c03e9b67a84ee04ec775d1e40ee7396a3f", "year": 2019, "venue": "CHI", "alt_text": "Happy Ending?: Photograph of a large outdoor sculpture of a teddy bear, bird and tree; tactile graphic of the three items in a row with braille labels; small 3D printed replica. I ate the rainbow: Highly detailed painting of two women; simplified tactile graphic; laser cut version with 4 high-contrast layers.", "levels": [[-1], [-1], [-1]], "corpus_id": 140306327, "sentences": ["Happy Ending?:", "Photograph of a large outdoor sculpture of a teddy bear, bird and tree; tactile graphic of the three items in a row with braille labels; small 3D printed replica.", "I ate the rainbow: Highly detailed painting of two women; simplified tactile graphic; laser cut version with 4 high-contrast layers."], "caption": "", "local_uri": ["c75f98c03e9b67a84ee04ec775d1e40ee7396a3f_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of SingleTapBraille Keyboard: A Text Entry Method that Utilizes Braille Patterns on Touchscreen Devices", "pdf_hash": "763fb4ca5e01a0ba9a4f1bb38af182e6415658a1", "year": 2016, "venue": "ASSETS", "alt_text": "Corrected Error Rate equals (the number of erroneous keystrokes and they are corrected in the transcribed text) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) * 100%", "levels": [[-1]], "corpus_id": 15878402, "sentences": ["Corrected Error Rate equals (the number of erroneous keystrokes and they are corrected in the transcribed text) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) * 100%"], "caption": "(2)", "local_uri": ["763fb4ca5e01a0ba9a4f1bb38af182e6415658a1_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of SingleTapBraille Keyboard: A Text Entry Method that Utilizes Braille Patterns on Touchscreen Devices", "pdf_hash": "763fb4ca5e01a0ba9a4f1bb38af182e6415658a1", "year": 2016, "venue": "ASSETS", "alt_text": "Not Corrected Error Rate equals (the number of erroneous keystrokes and they are corrected in the transcribed text) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) * 100%", "levels": [[-1]], "corpus_id": 15878402, "sentences": ["Not Corrected Error Rate equals (the number of erroneous keystrokes and they are corrected in the transcribed text) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) * 100%"], "caption": "(3)", "local_uri": ["763fb4ca5e01a0ba9a4f1bb38af182e6415658a1_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of SingleTapBraille Keyboard: A Text Entry Method that Utilizes Braille Patterns on Touchscreen Devices", "pdf_hash": "763fb4ca5e01a0ba9a4f1bb38af182e6415658a1", "year": 2016, "venue": "ASSETS", "alt_text": "keystrokes per character rate equals ( the number of correct keystrokes  + the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text + the number of keystrokes used to correct errors, such as delete or backspace) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text)", "levels": [[-1]], "corpus_id": 15878402, "sentences": ["keystrokes per character rate equals ( the number of correct keystrokes  + the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text + the number of keystrokes used to correct errors, such as delete or backspace) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text)"], "caption": "(4)", "local_uri": ["763fb4ca5e01a0ba9a4f1bb38af182e6415658a1_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of SingleTapBraille Keyboard: A Text Entry Method that Utilizes Braille Patterns on Touchscreen Devices", "pdf_hash": "763fb4ca5e01a0ba9a4f1bb38af182e6415658a1", "year": 2016, "venue": "ASSETS", "alt_text": "Total Error Rate equals (the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) * 100%", "levels": [[-1]], "corpus_id": 15878402, "sentences": ["Total Error Rate equals (the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) divide by (the number of correct keystrokes +  the number of erroneous keystrokes that are not corrected in the transcribed text + the number of erroneous keystrokes and they are corrected in the transcribed text) * 100%"], "caption": "Not corrected error rate", "local_uri": ["763fb4ca5e01a0ba9a4f1bb38af182e6415658a1_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Evaluation of SingleTapBraille Keyboard: A Text Entry Method that Utilizes Braille Patterns on Touchscreen Devices", "pdf_hash": "763fb4ca5e01a0ba9a4f1bb38af182e6415658a1", "year": 2016, "venue": "ASSETS", "alt_text": "This image represents other letters that are located under letters on the normal keyboard, such as letter i with accent.", "levels": null, "corpus_id": 15878402, "sentences": ["This image represents other letters that are located under letters on the normal keyboard, such as letter i with accent."], "caption": "", "local_uri": ["763fb4ca5e01a0ba9a4f1bb38af182e6415658a1_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "ForeSee: A Customizable Head-Mounted Vision Enhancement System for People with Low Vision", "pdf_hash": "81c2001c8fadbcc2e1c6ae8d80be865cb325e5a6", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 2. The visual effects of five enhancement methods: Magnification, Contrast, Edge Enhancement, and Black/White Reversal; in two display modes: Full Display Mode and Window Display Mode", "levels": null, "corpus_id": 33934824, "sentences": ["Figure 2.", "The visual effects of five enhancement methods: Magnification, Contrast, Edge Enhancement, and Black/White Reversal; in two display modes: Full Display Mode and Window Display Mode"], "caption": "Figure 2. The visual effects of five enhancement methods: Magnification, Contrast, Edge Enhancement, and Black/White Reversal; in two display modes: Full Display Mode and Window Display Mode", "local_uri": ["81c2001c8fadbcc2e1c6ae8d80be865cb325e5a6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "ForeSee: A Customizable Head-Mounted Vision Enhancement System for People with Low Vision", "pdf_hash": "81c2001c8fadbcc2e1c6ae8d80be865cb325e5a6", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 3. Experiment Materials: (a) four printed signs of numbers or writings hung on the wall (b) a handheld printed page", "levels": null, "corpus_id": 33934824, "sentences": ["Figure 3. Experiment Materials: (a) four printed signs of numbers or writings hung on the wall (b) a handheld printed page"], "caption": "(a)", "local_uri": ["81c2001c8fadbcc2e1c6ae8d80be865cb325e5a6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Designing Game-Based Myoelectric Prosthesis Training", "pdf_hash": "c7050f7393a6f582b53da9502174cd0615c977cd", "year": 2017, "venue": "CHI", "alt_text": "Three screenshots displaying visual feedback and training games. The first screen displays raw EMG signals, the second screen displays a simulated hand, and the third screen displays the car game, as described below in the text.", "levels": null, "corpus_id": 28782328, "sentences": ["Three screenshots displaying visual feedback and training games.", "The first screen displays raw EMG signals, the second screen displays a simulated hand, and the third screen displays the car game, as described below in the text."], "caption": "", "local_uri": ["c7050f7393a6f582b53da9502174cd0615c977cd_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Designing Game-Based Myoelectric Prosthesis Training", "pdf_hash": "c7050f7393a6f582b53da9502174cd0615c977cd", "year": 2017, "venue": "CHI", "alt_text": "Two screen shots display different customizeable themes for the falling of momo. The first screen displays the default theme, with a mountain and green field in the background and a simple blue circle shaped Momo character. The second screen shows an unlocked theme with a white cat as the character with pink mountains and a candy-based motif.", "levels": null, "corpus_id": 28782328, "sentences": ["Two screen shots display different customizeable themes for the falling of momo.", "The first screen displays the default theme, with a mountain and green field in the background and a simple blue circle shaped Momo character.", "The second screen shows an unlocked theme with a white cat as the character with pink mountains and a candy-based motif."], "caption": "", "local_uri": ["c7050f7393a6f582b53da9502174cd0615c977cd_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "ChewIt. An Intraoral Interface for Discreet Interactions", "pdf_hash": "3243e171a2fae21f188a2baa380be2e803bcafe1", "year": 2019, "venue": "CHI", "alt_text": "Two images of the initial prototype. On the left the device is open in halh and shows a flexible PCB attached to a coin cell. On the right the prototype is closed.", "levels": null, "corpus_id": 140226947, "sentences": ["Two images of the initial prototype.", "On the left the device is open in halh and shows a flexible PCB attached to a coin cell.", "On the right the prototype is closed."], "caption": "Figure 2: ChewIt prototype \u2013 basic hardware is integrated with fexible custom-made PCB, placed inside the 3D- printed casing, developed from a polylactic acid flament.", "local_uri": ["3243e171a2fae21f188a2baa380be2e803bcafe1_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Embracing Errors: Examining How Context of Use Impacts Blind Individuals' Acceptance of Navigation Aid Errors", "pdf_hash": "8a338666ca6223b67aad70aa11cdb6ade2070019", "year": 2017, "venue": "CHI", "alt_text": "Infographic showing: 1) that the context of device errors (error type, building feature, and setting) affects user acceptance, 2) there are many device errors that users will accept, and 3) that device errors may also lead to unwanted attention from bystanders.\u201d", "levels": [[-1]], "corpus_id": 23188679, "sentences": ["Infographic showing: 1) that the context of device errors (error type, building feature, and setting) affects user acceptance, 2) there are many device errors that users will accept, and 3) that device errors may also lead to unwanted attention from bystanders.\u201d"], "caption": "Figure 1. Findings from our survey suggest that many device errors are acceptable depending on elements of the context, like the type of device error made. Participants accepted 42% of all device errors in our survey (41% of errors were not accepted and 17% of errors were disregarded). Device errors may also lead to unwanted attention from bystanders.", "local_uri": ["8a338666ca6223b67aad70aa11cdb6ade2070019_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Time Series Visualisations to Improve the User Experience", "pdf_hash": "ea737bd1dc18fee26da112d1a933835994bb2ec1", "year": 2016, "venue": "CHI", "alt_text": "Selected time series visualisations with highlighting and tooltip interaction techniques.", "levels": null, "corpus_id": 16534003, "sentences": ["Selected time series visualisations with highlighting and tooltip interaction techniques."], "caption": "", "local_uri": ["ea737bd1dc18fee26da112d1a933835994bb2ec1_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Sharing Economy Design Cards", "pdf_hash": "d5929ebd33d9ebb81643b7aa2ecca1ece6d07729", "year": 2019, "venue": "CHI", "alt_text": "Figure 2. The Sharing Economy Design Sprint: (a) input materials including scenario, design brief, personas; (b) worksheets including map template, prototype template and idea template; (c) eight activities, namely 1 Introduction 25min Group Sharing Economy Design Cards (SEDC); 2 Understanding 10min Group Design Brief, Scenario, Personas; 3 Mapping 10min Group Map Canvas,SEDC(Themes&Questions); 4 Sketching 15min Individual SEDC(Goal & Recommendation); 5 Deciding 15min Group Idea Template; 6 Prototyping 20min Ind/Group Prototype Template, SEDC(Example); 7 Advocating 10min Group All above; 8 Reflecting 15min Individual All above", "levels": null, "corpus_id": 140268070, "sentences": ["Figure 2. The Sharing Economy Design Sprint: (a) input materials including scenario, design brief, personas; (b) worksheets including map template, prototype template and idea template; (c) eight activities, namely 1 Introduction 25min Group Sharing Economy Design Cards (SEDC); 2 Understanding 10min Group Design Brief, Scenario, Personas; 3 Mapping 10min Group Map Canvas,SEDC(Themes&Questions); 4 Sketching 15min Individual SEDC(Goal & Recommendation); 5 Deciding 15min Group Idea Template; 6 Prototyping 20min Ind/Group Prototype Template, SEDC(Example); 7 Advocating 10min Group All above; 8 Reflecting 15min Individual All above"], "caption": "Figure 2. The Sharing Economy Design Sprint: (a) input materials; (b) worksheets; (c) activities", "local_uri": ["d5929ebd33d9ebb81643b7aa2ecca1ece6d07729_Image_003.png"], "annotated": false, "compound": false}
{"title": "Sharing Economy Design Cards", "pdf_hash": "d5929ebd33d9ebb81643b7aa2ecca1ece6d07729", "year": 2019, "venue": "CHI", "alt_text": "Figure 3. (a) The sharing economy design sprint setting: five people sitting around the table in a well-lit room. The table is full of design and card set materials. One facilitator is present; (b) The example of produced materials: filled map template with cards, filled idea template and filled prototype templates", "levels": null, "corpus_id": 140268070, "sentences": ["Figure 3. (a) The sharing economy design sprint setting: five people sitting around the table in a well-lit room.", "The table is full of design and card set materials.", "One facilitator is present; (b) The example of produced materials: filled map template with cards, filled idea template and filled prototype templates"], "caption": "Figure 3. (a) The sharing economy design sprint setting. (b) The example of the produced materials during the sprint.", "local_uri": ["d5929ebd33d9ebb81643b7aa2ecca1ece6d07729_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Sharing Economy Design Cards", "pdf_hash": "d5929ebd33d9ebb81643b7aa2ecca1ece6d07729", "year": 2019, "venue": "CHI", "alt_text": "Figure 4. Design materials produced at the sprint: (a) value-added features based on the cards; (b) user flows, examples of content, target audiences, and UX goals; (c) wireframes and their relation to the cards as represented by with post-it notes", "levels": null, "corpus_id": 140268070, "sentences": ["Figure 4. Design materials produced at the sprint: (a) value-added features based on the cards; (b) user flows, examples of content, target audiences, and UX goals; (c) wireframes and their relation to the cards as represented by with post-it notes"], "caption": "Figure 4. Design materials produced at the sprint: (a) value-added features based on the cards highlighted in yellow with red arrows; (b) user flows, examples of content, target audiences, and UX goals; (c) wireframes and their relation to the cards.", "local_uri": ["d5929ebd33d9ebb81643b7aa2ecca1ece6d07729_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Why Users Disintermediate Peer-to-Peer Marketplaces", "pdf_hash": "5069cbf15d4f2e9600d5a60bede1c6d8e32613a0", "year": 2017, "venue": "CHI", "alt_text": "Frequency % of use of types of service reported used by 97 respondents who engaged in P2P transactions. Few responses to \u201cOther\u201d were given, suggesting categories have good coverage of main types of service used.", "levels": null, "corpus_id": 26691809, "sentences": ["Frequency % of use of types of service reported used by 97 respondents who engaged in P2P transactions.", "Few responses to \u201cOther\u201d were given, suggesting categories have good coverage of main types of service used."], "caption": "Figure 2: Frequency % (Y-axis) of use of types of service reported by 97 respondents who engaged in P2P transactions. Few responses to \u201cOther\u201d were given, suggesting categories have good coverage.", "local_uri": ["5069cbf15d4f2e9600d5a60bede1c6d8e32613a0_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "DynamicDuo: Co-presenting with Virtual Agents", "pdf_hash": "99fe5a56beab1296bed86f37647878230adb7430", "year": 2015, "venue": "CHI", "alt_text": "Self-reported state anxiety for human-only and human-agent conditions from non-native and native participants.", "levels": null, "corpus_id": 6050687, "sentences": ["Self-reported state anxiety for human-only and human-agent conditions from non-native and native participants."], "caption": "", "local_uri": ["99fe5a56beab1296bed86f37647878230adb7430_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "DynamicDuo: Co-presenting with Virtual Agents", "pdf_hash": "99fe5a56beab1296bed86f37647878230adb7430", "year": 2015, "venue": "CHI", "alt_text": "Self-reported speaker confidence for human-only and human-agent conditions from non-native and native participants.", "levels": null, "corpus_id": 6050687, "sentences": ["Self-reported speaker confidence for human-only and human-agent conditions from non-native and native participants."], "caption": "", "local_uri": ["99fe5a56beab1296bed86f37647878230adb7430_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Crumbs: Lightweight Daily Food Challenges to Promote Engagement and Mindfulness", "pdf_hash": "b5cf1579fe2cc2e3264b094c96231c7154709d8d", "year": 2016, "venue": "CHI", "alt_text": "A banner which says, \"Today's Challenge: Eat something that is good for your eyes.\"", "levels": null, "corpus_id": 10738283, "sentences": ["A banner which says, \"Today's Challenge: Eat something that is good for your eyes.\""], "caption": "", "local_uri": ["b5cf1579fe2cc2e3264b094c96231c7154709d8d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Crumbs: Lightweight Daily Food Challenges to Promote Engagement and Mindfulness", "pdf_hash": "b5cf1579fe2cc2e3264b094c96231c7154709d8d", "year": 2016, "venue": "CHI", "alt_text": "Completion rate generally declined over the study across conditions, approaching 20% of participants completing a challenge near the end of the study. However, certain challenges, especially in the social conditions, had substantially higher completion rates.", "levels": null, "corpus_id": 10738283, "sentences": ["Completion rate generally declined over the study across conditions, approaching 20% of participants completing a challenge near the end of the study.", "However, certain challenges, especially in the social conditions, had substantially higher completion rates."], "caption": "Figure 2. Certain challenges, particularly in the social conditions, resulted in greater completion.", "local_uri": ["b5cf1579fe2cc2e3264b094c96231c7154709d8d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Crumbs: Lightweight Daily Food Challenges to Promote Engagement and Mindfulness", "pdf_hash": "b5cf1579fe2cc2e3264b094c96231c7154709d8d", "year": 2016, "venue": "CHI", "alt_text": "This participant completed a challenge that required them to eat something that was cooked in an over. Another participant responded asking for the recipe, which the original participant provided.", "levels": null, "corpus_id": 10738283, "sentences": ["This participant completed a challenge that required them to eat something that was cooked in an over.", "Another participant responded asking for the recipe, which the original participant provided."], "caption": "", "local_uri": ["b5cf1579fe2cc2e3264b094c96231c7154709d8d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Crumbs: Lightweight Daily Food Challenges to Promote Engagement and Mindfulness", "pdf_hash": "b5cf1579fe2cc2e3264b094c96231c7154709d8d", "year": 2016, "venue": "CHI", "alt_text": "One participant posted Arugula to complete a challenge to find a food starting with the letter \"A\". Two other participants commented on how they were getting tired of seeing submissions with pictures of apples.", "levels": null, "corpus_id": 10738283, "sentences": ["One participant posted Arugula to complete a challenge to find a food starting with the letter \"A\".", "Two other participants commented on how they were getting tired of seeing submissions with pictures of apples."], "caption": "Figure 4. People in social conditions used Food4Thought to", "local_uri": ["b5cf1579fe2cc2e3264b094c96231c7154709d8d_Image_006.png"], "annotated": false, "compound": false}
{"title": "Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images", "pdf_hash": "e4d7780cff87334906ede9036ed6aafc837997e2", "year": 2017, "venue": "CHI", "alt_text": "Black and white photo of Hillary Clinton walking onto a stage with a crowd of people in the background.", "levels": null, "corpus_id": 20580817, "sentences": ["Black and white photo of Hillary Clinton walking onto a stage with a crowd of people in the background."], "caption": "", "local_uri": ["e4d7780cff87334906ede9036ed6aafc837997e2_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images", "pdf_hash": "e4d7780cff87334906ede9036ed6aafc837997e2", "year": 2017, "venue": "CHI", "alt_text": "Two copies of the statue of David digitally altered so one has his arm around the other and they are looking at each other.", "levels": null, "corpus_id": 20580817, "sentences": ["Two copies of the statue of David digitally altered so one has his arm around the other and they are looking at each other."], "caption": "Figure 1: Image tweeted by Hillary Clinton used as a probe. The tweet text reads, \"Some on the other side may say our best days are behind us. Let's prove them wrong.\" The computer generated caption says, \"I am not really confident, but I think it's a man is doing a trick on a skateboard at night.\u201d The image is actually a black and white photo of Hillary Clinton walking onto a stage with a crowd of people in the background.", "local_uri": ["e4d7780cff87334906ede9036ed6aafc837997e2_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images", "pdf_hash": "e4d7780cff87334906ede9036ed6aafc837997e2", "year": 2017, "venue": "CHI", "alt_text": "The image shows a plate of pasta with a green pesto sauce and fresh basil leaves mixed in.", "levels": null, "corpus_id": 20580817, "sentences": ["The image shows a plate of pasta with a green pesto sauce and fresh basil leaves mixed in."], "caption": "The image shows a plate of pasta with a green pesto sauce and fresh basil leaves mixed in.", "local_uri": ["e4d7780cff87334906ede9036ed6aafc837997e2_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images", "pdf_hash": "e4d7780cff87334906ede9036ed6aafc837997e2", "year": 2017, "venue": "CHI", "alt_text": "The image shows a dinner table with plates and cutlery all set out. Near the middle of the table there's an image of Donald Trump\u2019s head. It sort of looks like it was printed on paper and cut out and placed on the table.", "levels": null, "corpus_id": 20580817, "sentences": ["The image shows a dinner table with plates and cutlery all set out.", "Near the middle of the table there's an image of Donald Trump\u2019s head.", "It sort of looks like it was printed on paper and cut out and placed on the table."], "caption": "The image shows a dinner table with plates and cutlery all set out. Near the middle of the table there's an image of Donald Trump\u2019s head. It sort of looks like it was printed on paper and cut out and placed on the table.", "local_uri": ["e4d7780cff87334906ede9036ed6aafc837997e2_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images", "pdf_hash": "e4d7780cff87334906ede9036ed6aafc837997e2", "year": 2017, "venue": "CHI", "alt_text": "The image shows Hilary Clinton walking onto a stage in front of a large crowd of people.", "levels": null, "corpus_id": 20580817, "sentences": ["The image shows Hilary Clinton walking onto a stage in front of a large crowd of people."], "caption": "The image shows Hilary Clinton walking onto a stage in front of a large crowd of people.", "local_uri": ["e4d7780cff87334906ede9036ed6aafc837997e2_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Fatigue and Stamina Management Opportunities and Challenges in Wheelchair Basketball", "pdf_hash": "89d2e72b5be7a5f189a0dd0dc45153b6825cd2b3", "year": 2017, "venue": "ASSETS", "alt_text": "Wheelchair basketball players competing. One player is preparing to take a shot. Two other players are reaching toward the ball to contest the shot.", "levels": null, "corpus_id": 26058597, "sentences": ["Wheelchair basketball players competing.", "One player is preparing to take a shot.", "Two other players are reaching toward the ball to contest the shot."], "caption": "Figure 1. Wheelchair Basketball is one of the world\u2019s most popular adaptive sports. It is a team sport that incorporates multiple roles and is designed to be inclusive of people with differing abilities.", "local_uri": ["89d2e72b5be7a5f189a0dd0dc45153b6825cd2b3_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "Diagram depicting a side view of the haptic keypad evaluated in the paper. The image depicts three key caps with two motors attached to the shaft of each key. Each motor has an offset weight attached that causes the vibration.", "levels": null, "corpus_id": 18835204, "sentences": ["Diagram depicting a side view of the haptic keypad evaluated in the paper.", "The image depicts three key caps with two motors attached to the shaft of each key.", "Each motor has an offset weight attached that causes the vibration."], "caption": "Figure 1: A haptic keypad was developed and evaluated to convey key information to visually impaired users, such as the location of active devices on an ATM", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "two examples of how the clockface metaphor is mapped to the haptic keypad. The central 5 button and the 2 button above it virbrate in sequence to indicate an upwards direction, 12 oclock. the 5 and the 2 vibrate in sequence to indicate a direction of 2 oclock.", "levels": null, "corpus_id": 18835204, "sentences": ["two examples of how the clockface metaphor is mapped to the haptic keypad.", "The central 5 button and the 2 button above it virbrate in sequence to indicate an upwards direction, 12 oclock.", "the 5 and the 2 vibrate in sequence to indicate a direction of 2 oclock."], "caption": "Figure 2: The clock face metaphor and its mapping to the ATM keypad", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "diagram showing how the different devices can be mapped onto the haptic keypad. vibrating 5 and 2 can direct the user towards the cash drawer. Vibrating the 5 and 3 can direct users to the card slot or the receipt slot depending on viration intensity.", "levels": null, "corpus_id": 18835204, "sentences": ["diagram showing how the different devices can be mapped onto the haptic keypad. vibrating 5 and 2 can direct the user towards the cash drawer.", "Vibrating the 5 and 3 can direct users to the card slot or the receipt slot depending on viration intensity."], "caption": "Figure 3: Coding of haptic sequences to test apparatus devices", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "An image of the wooden ATM test apparatus used for the study. the test apparatus is spray painted grey, contains the main devices of a regular ATM, and has haptic discs fitted at the bottom left corner of each devices location.", "levels": null, "corpus_id": 18835204, "sentences": ["An image of the wooden ATM test apparatus used for the study.", "the test apparatus is spray painted grey, contains the main devices of a regular ATM, and has haptic discs fitted at the bottom left corner of each devices location."], "caption": "Figure 4: The ATM Test Apparatus", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "A series of images showing showing the participants hand movement from the haptic keypad to the haptic disc at the side of the receipt slot. The hand moves almost directly to the haptic disc, the user places their hand just below the haptic disc before moving it up onto the disc.", "levels": null, "corpus_id": 18835204, "sentences": ["A series of images showing showing the participants hand movement from the haptic keypad to the haptic disc at the side of the receipt slot.", "The hand moves almost directly to the haptic disc, the user places their hand just below the haptic disc before moving it up onto the disc."], "caption": "Figure 6: Participant 6\u2019s hand path", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "a series of 8 frames depicting the hand movement of participant p3. The participant initially moved their hand to the card slot but as it was not virbating at the time her hand was on it continued to explore the machine before finding the appropriate haptic disc.", "levels": null, "corpus_id": 18835204, "sentences": ["a series of 8 frames depicting the hand movement of participant p3.", "The participant initially moved their hand to the card slot but as it was not virbating at the time her hand was on it continued to explore the machine before finding the appropriate haptic disc."], "caption": "Figure 7: Participant 3\u2019s Hand Path", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "bar chart showing average completion times for finding active devices:  card = 13.5 seconds  cash = 8 seconds  receipt 8.9 seconds", "levels": [[2, 1]], "corpus_id": 18835204, "sentences": ["bar chart showing average completion times for finding active devices:  card = 13.5 seconds  cash = 8 seconds  receipt 8.9 seconds"], "caption": "Figure 8: Average Device Location Times", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_008.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "four small images depicting users hand placements to detect haptic pulses:  flat palm, hand is placed flat with fingers directly over the keys.  two handed overlay, user places right hand on the 4 to 6 keys with the left hand being placed over the right hand with fingers resting on the 1 to 3 keys.  two handed adjacent, users place both hands next to each other covering the 1 2 4 and 5 keys with the left hand and the 2 3 5 and 6 keys with the right hand.  dynamic palm, user places the right hand on the 4 5 and 6 keys and moves the hand up to detect vibrations on the 1 2 and 3 keys with the tips of their fingers.", "levels": null, "corpus_id": 18835204, "sentences": ["four small images depicting users hand placements to detect haptic pulses:  flat palm, hand is placed flat with fingers directly over the keys.", "two handed overlay, user places right hand on the 4 to 6 keys with the left hand being placed over the right hand with fingers resting on the 1 to 3 keys.", "two handed adjacent, users place both hands next to each other covering the 1 2 4 and 5 keys with the left hand and the 2 3 5 and 6 keys with the right hand.", "dynamic palm, user places the right hand on the 4 5 and 6 keys and moves the hand up to detect vibrations on the 1 2 and 3 keys with the tips of their fingers."], "caption": "Figure 9: Hand positions while using Directional Haptics", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "A haptic ATM interface to assist visually impaired users", "pdf_hash": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "year": 2013, "venue": "ASSETS", "alt_text": "an example of how numbers can be traced on users palms using an ATM keypad.  vibrating the 2 5 and 8 keys in sequence could represent the number one. The following vibration sequences are also pictured:  1 2 3 6 5 4 7 8 9 = the number 2  1 2 3 6 9 8 7 = the number 3  1 4 7 8 9 = the number 4  3 2 1 4 5 6 9 8 7 = the number 5  3 2 1 4 7 = the number 6  1 2 3 6 9 = the number 7  9 8 7 4 5 6 3 2 1 = the number 8  3 6 9 8 7 = the number 9", "levels": null, "corpus_id": 18835204, "sentences": ["an example of how numbers can be traced on users palms using an ATM keypad.", "vibrating the 2 5 and 8 keys in sequence could represent the number one.", "The following vibration sequences are also pictured:  1 2 3 6 5 4 7 8 9 = the number 2  1 2 3 6 9 8 7 = the number 3  1 4 7 8 9 = the number 4  3 2 1 4 5 6 9 8 7 = the number 5  3 2 1 4 7 = the number 6  1 2 3 6 9 = the number 7  9 8 7 4 5 6 3 2 1 = the number 8  3 6 9 8 7 = the number 9"], "caption": "Figure 8: Numerical haptic output based on Unistroke", "local_uri": ["710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the testing environment. It consists of a smartphone on a tripod pointed at a printed mat placed on the ground.", "levels": null, "corpus_id": 5501677, "sentences": ["This figure shows the testing environment.", "It consists of a smartphone on a tripod pointed at a printed mat placed on the ground."], "caption": "", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_005.png"], "annotated": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the mat design in diagram form. On the edge of the mat are alternating black and white squares each 10 cm in length. The mat itself is 500 cm in length and 90 cm in width.", "levels": null, "corpus_id": 5501677, "sentences": ["This figure shows the mat design in diagram form.", "On the edge of the mat are alternating black and white squares each 10 cm in length.", "The mat itself is 500 cm in length and 90 cm in width."], "caption": "Figure 2: Walking mat. All sizes in the figure are in cm. The red dashed line denotes the walking path in our experiment.", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the system pipeline. It consists of three main components, mat extraction, shoe detection, and then stride length estimation. Each component has three sub components. Mat Extraction consists of video segmentation, image preprocessing, and marker extraction. Shoe detection consists of shoe region segmentation, shoe edge extraction, and edge interpolation. Stride length estimation consists of stationary frame detection, front point location, and location mapping.", "levels": null, "corpus_id": 5501677, "sentences": ["This figure shows the system pipeline.", "It consists of three main components, mat extraction, shoe detection, and then stride length estimation.", "Each component has three sub components.", "Mat Extraction consists of video segmentation, image preprocessing, and marker extraction.", "Shoe detection consists of shoe region segmentation, shoe edge extraction, and edge interpolation.", "Stride length estimation consists of stationary frame detection, front point location, and location mapping."], "caption": "", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the five stages of mat construction, from the original cropped mat image to a binary image, then to an image of the marker contours. Following that is an image of the marker contours which have been paired, and finally an image with the perspective lines showing the orientation of the mat.", "levels": [[-1], [-1]], "corpus_id": 5501677, "sentences": ["This figure shows the five stages of mat construction, from the original cropped mat image to a binary image, then to an image of the marker contours.", "Following that is an image of the marker contours which have been paired, and finally an image with the perspective lines showing the orientation of the mat."], "caption": "Perspective InformationIn our system a smartphone camera is placed on a tri\u00ad pod and pointed towards the mat, but this position may not always be exactly the same. Therefore the goal of this component is to find the relative size, location, and orien\u00ad tation of the mat automatically so that the camera position does not have to be re-calibrated every time it is set up or bumped.Before we do any processing  on the recorded video it must be segmented into multiple sections, those with walk\u00ad ing (walking videos) and those without (mat videos). This is performed using a simple background subtraction approach to detect motion [31, 4]. In this section we will process the mat videos, and in the next section we will use the walking videos. We define the perspective information as the ex\u00ad tracted set of paired markers which make up the mat. From this set of paired markers we can ascertain the relative ori\u00ad entation of the mat to the camera and estimate distances in our real world co-ordinate system.A procedure overview is detailed in Algorithm 1. The pro\u00ad cedure for extracting perspective information from a video is automatic with the exception of the following step. First, we manually crop the frames of the mat video to include only the mat itself using a simple UI. Figure 4a shows the cropped image. We have not automated this step because it needs to be able to handle different conditions and environ\u00ad ments and it is possible that the mat could blend into the background. Therefore we ask the user to specify the loca\u00ad tion of the mat. This step reduces both overall processing time and the impact of other objects in the frame. From this step onward, the system is completely automatic.Next, we use a few basic image processing algorithms [21] to prepare the image for analysis. First we use the histogram equalization method to compensate for differing lighting.Then we use use a basic Gaussian filter to remove the noise in the middle of the empty mat. Lastly, we transform the color image to a binary one using a threshold method. Fig\u00ad ure 4b shows the processed image from the video.With this binary image we now locate, count, and pair off the mat markers. For simplicity sake, we will define the bottom of the mat as the edge of the mat that is closest to the camera, and conversely the top of the mat is the edge furthest away from the camera. The idea is straightforward: we count the number of markers in the bottom of the original mat and then align these markers to the markers in the top of the mat. We use the Canny operator [2] to find the contours of all of the markers (see Figure 4c).Due to the perspective of the camera, we find that there are a few markers at the top of the mat that have no cor\u00ad responding markers on the bottom of the mat and therefore these need to be excluded. Here, we choose the line that passes through the left edge of the bottom first marker and shift the line 5 to 10 pixels to left. All top markers to the left of this line are removed from the binary image of the mat. Likewise, we do this on the right using the bottom rightmost marker, and therefore only the markers which can be paired remain. Figure 4d shows the mat after all of the markers have been paired, with the redundant markers removed.Finally we choose critical points from each marker\u2019s left and right edges to draw the line between the bottom and top markers. We take the top and bottom marker edges, and then we calculate the line of best fit that connects them. This is repeated for all pairs of markers. To improve the success of this method we only use the edge points between 5% and 95% of the total height of the marker, thus removing any influence of the corner of the marker.  Figure 4e shows the final perspective information as displayed on the mat.Algorithm 1 Mat Extraction Algorithm 1: procedure Mat\u2013Extraction2:        Crop frame of mat video3:        Perform histogram equalization operation 4:       Remove noise using Gaussian Filter5:        Convert frame to binary image6:        Use Canny operator to extract edges of markers 7:       Count the number of markers8:        Exclude unpaired markers9:        Connect paired markers with a line of best fit 10: end procedureTo get high accuracy perspective information we recom\u00ad mend the following steps. Ensure the camera is stable and record in as close to a uniform lighting environment as pos\u00ad sible. It is also important to fine tune the parameters for the techniques used to get a clear contour of the edge of the markers. Even with this calibration we may not always get a continuous contour of the edge, in this event we can use the technique described in Section 5.2 to find a continuous contour. Next, we describe the process to detect the shoe and find its contour.", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the process of shoe segmentation. The first sub-figure is a frame of a subjects feet while working on the mat. The two other sub figures show the left and right feet segmented from the original frame.", "levels": null, "corpus_id": 5501677, "sentences": ["This figure shows the process of shoe segmentation.", "The first sub-figure is a frame of a subjects feet while working on the mat.", "The two other sub figures show the left and right feet segmented from the original frame."], "caption": "", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure shows the improved Canny operator result. The first sub-figure shows the preliminary noisy contour of the shoe. The second sub-figure shows the improved result of the edge traversing method. The last two sub-figures show the contour after reducing the gap size and the contour after interpolating any remaining gaps, respectively.", "levels": null, "corpus_id": 5501677, "sentences": ["This figure shows the improved Canny operator result.", "The first sub-figure shows the preliminary noisy contour of the shoe.", "The second sub-figure shows the improved result of the edge traversing method.", "The last two sub-figures show the contour after reducing the gap size and the contour after interpolating any remaining gaps, respectively."], "caption": "Figure 6: Modification on Canny operator result.", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure is a line graph with frame index number on the horizontal axis and number of edge lines on the vertical axis. It shows both the original and filtered result. In the filtered result, the peaks are prominent and easier to identify.", "levels": [[1], [1], [3]], "corpus_id": 5501677, "sentences": ["This figure is a line graph with frame index number on the horizontal axis and number of edge lines on the vertical axis.", "It shows both the original and filtered result.", "In the filtered result, the peaks are prominent and easier to identify."], "caption": "", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_020.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure is a line graph with frame index number on the horizontal axis and foot position in pixels on the vertical axis. It is split into two panels, the top half shows the absolute distance of left and right feet, with an expected step function like curve. The lower panel has the first order difference of the absolute difference, and shows the expected left and right foot oscillations.", "levels": [[1], [1], [1]], "corpus_id": 5501677, "sentences": ["This figure is a line graph with frame index number on the horizontal axis and foot position in pixels on the vertical axis.", "It is split into two panels, the top half shows the absolute distance of left and right feet, with an expected step function like curve.", "The lower panel has the first order difference of the absolute difference, and shows the expected left and right foot oscillations."], "caption": "Figure 8: Front point position on foot contour", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_021.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "pdf_hash": "84e867b0e07ef33492478682a68899d99ea211d8", "year": 2016, "venue": "ASSETS", "alt_text": "This figure has three sub-figures. The first two show both of the mapping function strategies, one using a simple perpendicular distance, and the other using the arc length. The second sub-figure shows how to estimate the vertex curvature for the b-splines interpolation.", "levels": null, "corpus_id": 5501677, "sentences": ["This figure has three sub-figures.", "The first two show both of the mapping function strategies, one using a simple perpendicular distance, and the other using the arc length.", "The second sub-figure shows how to estimate the vertex curvature for the b-splines interpolation."], "caption": "", "local_uri": ["84e867b0e07ef33492478682a68899d99ea211d8_Image_024.jpg"], "annotated": false, "compound": false}
{"title": "Interactiles: 3D Printed Tactile Interfaces to Enhance Mobile Touchscreen Accessibility", "pdf_hash": "4f6800463ceb4c8f24cd8f4b7af90469528d3d89", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 1. Interactiles allows people with visual impairments to interact with mobile touchscreen phones using physical attachments, including a number pad (left) and a multi-purpose physical scrollbar (right).", "levels": [[-1], [-1]], "corpus_id": 52944617, "sentences": ["Figure 1.", "Interactiles allows people with visual impairments to interact with mobile touchscreen phones using physical attachments, including a number pad (left) and a multi-purpose physical scrollbar (right)."], "caption": "Figure 1. Interactiles allows people with visual impairments to interact with mobile touchscreen phones using physical attachments, including a number pad (left) and a multi-purpose physical scrollbar (right).", "local_uri": ["4f6800463ceb4c8f24cd8f4b7af90469528d3d89_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Interactiles: 3D Printed Tactile Interfaces to Enhance Mobile Touchscreen Accessibility", "pdf_hash": "4f6800463ceb4c8f24cd8f4b7af90469528d3d89", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 2. Floating windows created for number pad (left), scrollbar (right), and control button (right bottom). The windows can be transparent; we use colors for demonstration.", "levels": [[-1], [-1], [-1]], "corpus_id": 52944617, "sentences": ["Figure 2.", "Floating windows created for number pad (left), scrollbar (right), and control button (right bottom).", "The windows can be transparent; we use colors for demonstration."], "caption": "Figure 2. Floating windows created for number pad (left), scrollbar (right), and control button (right bottom). The windows can be transparent; we use colors for demonstration.", "local_uri": ["4f6800463ceb4c8f24cd8f4b7af90469528d3d89_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Interactiles: 3D Printed Tactile Interfaces to Enhance Mobile Touchscreen Accessibility", "pdf_hash": "4f6800463ceb4c8f24cd8f4b7af90469528d3d89", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 3. The average Likert scale rating (strongly disagree = -2, strongly agree = 2) with standard deviation from participants for tasks. Participants were asked how easy, quick, intuitive, and how confident they felt completing each task with the control condition (only TalkBack) and Interactiles. Locate/relocate was rated as one task. P5 did not complete or rate the holistic task.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 52944617, "sentences": ["Figure 3.", "The average Likert scale rating (strongly disagree = -2, strongly agree = 2) with standard deviation from participants for tasks.", "Participants were asked how easy, quick, intuitive, and how confident they felt completing each task with the control condition (only TalkBack) and Interactiles.", "Locate/relocate was rated as one task.", "P5 did not complete or rate the holistic task."], "caption": "Figure 3. The average Likert scale rating (strongly disagree = -2, strongly agree = 2) with standard deviation from participants for tasks. Participants were asked how easy, quick, intuitive, and how confident they felt completing each task with the control condition (only TalkBack) and Interactiles. Locate/relocate was rated as one task. P5 did not complete or rate the holistic task.", "local_uri": ["4f6800463ceb4c8f24cd8f4b7af90469528d3d89_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Interactiles: 3D Printed Tactile Interfaces to Enhance Mobile Touchscreen Accessibility", "pdf_hash": "4f6800463ceb4c8f24cd8f4b7af90469528d3d89", "year": 2018, "venue": "ASSETS", "alt_text": "Figure 4. Average task completion time of each task in the study. P4 did not complete app switching on the control condition, and P5 did not complete the holistic task.", "levels": [[0], [1], [3]], "corpus_id": 52944617, "sentences": ["Figure 4.", "Average task completion time of each task in the study.", "P4 did not complete app switching on the control condition, and P5 did not complete the holistic task."], "caption": "Figure 4. Average task completion time of each task in the study. P4 did not complete app switching on the control condition, and P5 did not complete the holistic task.", "local_uri": ["4f6800463ceb4c8f24cd8f4b7af90469528d3d89_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TurningPoint: narrative-driven presentation planning", "pdf_hash": "dabd064e255f67ff47fe3e903a9f4b182cb521dc", "year": 2014, "venue": "CHI", "alt_text": "TurningPoint probe for presentation planning, showing narrative strip (top) and content canvas (bottom)", "levels": null, "corpus_id": 6398747, "sentences": ["TurningPoint probe for presentation planning, showing narrative strip (top) and content canvas (bottom)"], "caption": "Figure 1. TurningPoint probe for presentation planning, showing narrative strip (top) and content canvas (bottom)", "local_uri": ["dabd064e255f67ff47fe3e903a9f4b182cb521dc_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Towards Large Scale Evaluation of Novel Sonification Techniques for Non Visual Shape Exploration", "pdf_hash": "4bbcd7e6a82781a99da657da9cdfa1b42d1a8ba8", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 2 is a screenshot of the answer view from the Invisible Puzzle application. It shows the text \"Which drawing did you explore?\" and four images representing the explored shape and three alternative answers.", "levels": null, "corpus_id": 2969379, "sentences": ["Figure 2 is a screenshot of the answer view from the Invisible Puzzle application.", "It shows the text \"Which drawing did you explore?\" and four images representing the explored shape and three alternative answers."], "caption": "", "local_uri": ["4bbcd7e6a82781a99da657da9cdfa1b42d1a8ba8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Accessibility of Smartphone Photography for People with Motor Impairments", "pdf_hash": "c81d9f68ddf12a8b29df2515117705c0f04da881", "year": 2018, "venue": "CHI", "alt_text": "Two images demonstrating Pair Photography. Left image shows a person holding a phone with two hands to frame a picture of a vase. The bottom image shows a person holding a phone with one hand while viewing the image of the vase.", "levels": [[-1], [-1], [-1]], "corpus_id": 5049395, "sentences": ["Two images demonstrating Pair Photography.", "Left image shows a person holding a phone with two hands to frame a picture of a vase.", "The bottom image shows a person holding a phone with one hand while viewing the image of the vase."], "caption": "Figure 1. We used two smartphones connected through a Skype call to demonstrate Pair Photography. The image framed by the operator (left) is streamed to the director\u2019s phone (right) in real-time.", "local_uri": ["c81d9f68ddf12a8b29df2515117705c0f04da881_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding the Accessibility of Smartphone Photography for People with Motor Impairments", "pdf_hash": "c81d9f68ddf12a8b29df2515117705c0f04da881", "year": 2018, "venue": "CHI", "alt_text": "Two images. The top image shows a conference room with multiple webcams attached to lamp posts. The bottom image shows a corner of the conference room visible from one of the webcams.", "levels": null, "corpus_id": 5049395, "sentences": ["Two images.", "The top image shows a conference room with multiple webcams attached to lamp posts.", "The bottom image shows a corner of the conference room visible from one of the webcams."], "caption": "Figure 2. We demonstrated ICC using a meeting room outfitted with 11 webcams (top). Different perspectives of the room (bottom) could be viewed by selecting hotspots (the red circles in the top image) in the web app.", "local_uri": ["c81d9f68ddf12a8b29df2515117705c0f04da881_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"I Bought This for Me to Look More Ordinary\": A Study of Blind People Doing Online Shopping", "pdf_hash": "e3e19a6eaa622b0bf0d98598a72b9dd41cce2bc0", "year": 2019, "venue": "CHI", "alt_text": "A participant (P8) told us about his shopping experience by operating on his smartphone", "levels": null, "corpus_id": 140224460, "sentences": ["A participant (P8) told us about his shopping experience by operating on his smartphone"], "caption": "Figure 1: A participant (P8) told us about his shopping expe- rience by operating on his smartphone", "local_uri": ["e3e19a6eaa622b0bf0d98598a72b9dd41cce2bc0_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "The left side of the image shows the \"Tracking Site\", where a person holds a tracked object visible to the tracking camears. The right side of the image shows the \"Manipulator Site\", where a person appears to be looking at a similar object, except this is beind held by a robot arm. Both objects are oriented in the same way.", "levels": null, "corpus_id": 3355133, "sentences": ["The left side of the image shows the \"Tracking Site\", where a person holds a tracked object visible to the tracking camears.", "The right side of the image shows the \"Manipulator Site\", where a person appears to be looking at a similar object, except this is beind held by a robot arm.", "Both objects are oriented in the same way."], "caption": "Figure 1. Remote Manipulator (ReMa) has two parts: it detects manipulations on an object (Left-yellow) using a set of sensors (Left-red), and then reproduces these with a proxy object (Right-yellow) using a Baxter robot arm (Right-red). ReMa allows shows the Manipulator Site collaborator (Right) the object with the same orientation as at the Tracking Site (Left). Collaborators can also use video chat (blue).", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "This image is similar to Figure 1, except shows three \"stopped in the moment\" pictures of ReMa at work. First, the object (a bird house) is rotated toward the user; second, the bird house is rotated slightly to the left, and finally, the bird house is rotated upside-down. These orientations are rendered at the manipulator site using a robot.", "levels": null, "corpus_id": 3355133, "sentences": ["This image is similar to Figure 1, except shows three \"stopped in the moment\" pictures of ReMa at work.", "First, the object (a bird house) is rotated toward the user; second, the bird house is rotated slightly to the left, and finally, the bird house is rotated upside-down.", "These orientations are rendered at the manipulator site using a robot."], "caption": "Figure 2: The ReMa system includes a Tracking Site (TS, top- left) and a Manipulator Site (MS, top-right) with bird house object. As the birdhouse is rotated at the TS, the proxy birdhouse at MS is also rotated.", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "A two by two matrix showing the four conditions of the first study. The first dimension is whether we are using ReMa or video conferencing; the second dimension is whether both participants are virtually on the \"same side\" of the table or \"opposite sides\".", "levels": null, "corpus_id": 3355133, "sentences": ["A two by two matrix showing the four conditions of the first study.", "The first dimension is whether we are using ReMa or video conferencing; the second dimension is whether both participants are virtually on the \"same side\" of the table or \"opposite sides\"."], "caption": "Figure 3. Study 1 compared different perspectives (shared vs. opposing) using both a video chat condition and the ReMa.", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "This shows a particular exerpt for Group 3 in the Opposing VC condition. The first set of images shows how Joe and Frank are trying to get their objects oriented similarly, but having difficulty. The second image set of images show that both have it oriented similarly.", "levels": null, "corpus_id": 3355133, "sentences": ["This shows a particular exerpt for Group 3 in the Opposing VC condition.", "The first set of images shows how Joe and Frank are trying to get their objects oriented similarly, but having difficulty.", "The second image set of images show that both have it oriented similarly."], "caption": "Figure 4: Group 3 Opposing-VC. Frank (A) tries to explain what he sees on one side of the trophy, but Joe rotates his trophy in the wrong direction (B). Frank explains the orientation of his trophy to Joe (C), but Joe is still confused whether he is holding his trophy in the correct orientation (D).", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "Screen captures from the data in Study 1, Group 1, Opposing Video Chat condition. Here, the first image shows Brenda holding the trophy up to the camera in an effort to get Alan to do the same action. Alan cannot see the particular thing Brenda is pointing at, and they repair the interaction by rotating the trophy differently.", "levels": [[-1], [-1], [-1]], "corpus_id": 3355133, "sentences": ["Screen captures from the data in Study 1, Group 1, Opposing Video Chat condition.", "Here, the first image shows Brenda holding the trophy up to the camera in an effort to get Alan to do the same action.", "Alan cannot see the particular thing Brenda is pointing at, and they repair the interaction by rotating the trophy differently."], "caption": "Figure 4: Group 1 Opposing-VC. Brenda wants to show Alan a sticker inside the trophy (B), but Alan cannot see the sticker (A). Alan tells Brenda to orient the trophy that both shared the same perspective (C, D).", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "A three by one matrix showing the different study conditions: in all cases, participants are seated \"on the same side\", but they experience the ReMa and VC conditions independently before the ReMa+VC condition.", "levels": null, "corpus_id": 3355133, "sentences": ["A three by one matrix showing the different study conditions: in all cases, participants are seated \"on the same side\", but they experience the ReMa and VC conditions independently before the ReMa+VC condition."], "caption": "Figure 5. Study 2 compared shared video chat and ReMa", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration", "pdf_hash": "f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4", "year": 2018, "venue": "CHI", "alt_text": "Clara uses a hand gesture to indicate how the object should be moved. This appears as an open hand with a twisting motion.", "levels": null, "corpus_id": 3355133, "sentences": ["Clara uses a hand gesture to indicate how the object should be moved.", "This appears as an open hand with a twisting motion."], "caption": "Figure 8: VC+ReMa \u2013 Group 1. Clara (TS) uses spatial hand gesture to describe the movement Lina (MS) should execute (annotated for clarity).", "local_uri": ["f9c5f7e7847f24aa89a2250d053a8bc1b0aa84e4_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Customizable 3D Printed Tactile Maps as Interactive Overlays", "pdf_hash": "b7fd256e6e8464665e72fa319b0034799b884e29", "year": 2016, "venue": "ASSETS", "alt_text": "The interactive map has 6 conductive touchpoints at the intersections of major roadways and is held in place against a smartphone by a custom case. The map can be removed and replaced by other maps. The case has 6 additional conductive buttons to active app features.", "levels": [[-1], [-1], [-1]], "corpus_id": 15486072, "sentences": ["The interactive map has 6 conductive touchpoints at the intersections of major roadways and is held in place against a smartphone by a custom case.", "The map can be removed and replaced by other maps.", "The case has 6 additional conductive buttons to active app features."], "caption": "Figure 6. An interactive map with 6 black conductive touchpoints. The map is held in a case with 6 conductive buttons that houses a Samsung Note 2 with a 5.5-inch screen.", "local_uri": ["b7fd256e6e8464665e72fa319b0034799b884e29_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Customizable 3D Printed Tactile Maps as Interactive Overlays", "pdf_hash": "b7fd256e6e8464665e72fa319b0034799b884e29", "year": 2016, "venue": "ASSETS", "alt_text": "A custom case holds the map to the tablet. The map depicts a college campus with buildings printed as interactive touchpoints.", "levels": [[-1], [-1]], "corpus_id": 15486072, "sentences": ["A custom case holds the map to the tablet.", "The map depicts a college campus with buildings printed as interactive touchpoints."], "caption": "Figure 9. An interactive map of a college campus on a Samsung Galaxy Tab 10.1 tablet. The black buildings act as touchpoints and announce the building name when touched.", "local_uri": ["b7fd256e6e8464665e72fa319b0034799b884e29_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "Life shows a single prediction when a woman is expected to have her period, and when she is expected to ovulate.", "levels": null, "corpus_id": 13372934, "sentences": ["Life shows a single prediction when a woman is expected to have her period, and when she is expected to ovulate."], "caption": "", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "Clue predicts period and ovulation as a range of dates the period and fertile window are expected to occur. Clue provides error estimates for each prediction. The screenshot shows the next fertile window will happen on September 1st plus or minus one day.", "levels": [[-1], [-1], [-1]], "corpus_id": 13372934, "sentences": ["Clue predicts period and ovulation as a range of dates the period and fertile window are expected to occur.", "Clue provides error estimates for each prediction.", "The screenshot shows the next fertile window will happen on September 1st plus or minus one day."], "caption": "Figure 1. Phone apps predict a woman\u2019s next period or ovulation. Life (a) surfaces this prediction as a point estimate, while Clue (b) provides a range of potential dates.", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "When setting up My Cycles, the app asks women to specify how long their period and their cycle duration are.", "levels": null, "corpus_id": 13372934, "sentences": ["When setting up My Cycles, the app asks women to specify how long their period and their cycle duration are."], "caption": "", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "The Period Tracker app asks for a detault cycle length. It says, \"until period tracker has three months of user inputted data, it will use a default period length to calculate future period start dates\".", "levels": [[-1], [-1]], "corpus_id": 13372934, "sentences": ["The Period Tracker app asks for a detault cycle length.", "It says, \"until period tracker has three months of user inputted data, it will use a default period length to calculate future period start dates\"."], "caption": "(a)                                                 (b)", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "Period diary presents period predictions in a pink flower over most of the phone screen.", "levels": null, "corpus_id": 13372934, "sentences": ["Period diary presents period predictions in a pink flower over most of the phone screen."], "caption": "", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "When setting up, Glow asks women what their goal for monitoring their menstrual cycle is. It has four options: avoiding pregnancy, trying to conceive, fertility treaments, and one option for male users.", "levels": null, "corpus_id": 13372934, "sentences": ["When setting up, Glow asks women what their goal for monitoring their menstrual cycle is. It has four options: avoiding pregnancy, trying to conceive, fertility treaments, and one option for male users."], "caption": "", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "pdf_hash": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "year": 2017, "venue": "CHI", "alt_text": "My Period Tracker allows a woman to share her data with her partner. The iconography implies a male partner.", "levels": [[-1], [-1]], "corpus_id": 13372934, "sentences": ["My Period Tracker allows a woman to share her data with her partner.", "The iconography implies a male partner."], "caption": "Figure 4. In Glow (a), people who identify as male are directed to an alternate view of the app. Clue\u2019s iconography (b) suggests a male sexual partner, while the iconography in My Period Tracker (c) implies a female sharing data with a male partner.", "local_uri": ["ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How Relevant are Incidental Power Poses for HCI?", "pdf_hash": "a81adf83dbac7a957b1da4451562d85a705a780f", "year": 2018, "venue": "CHI", "alt_text": "Two images. On the left the workstation of an air traffic controller comprising screens and constructions to hold strips of paper in place. On the right a workstation of a financial worker with two large displays showing tables and charts, a mouse, and a keyboard.", "levels": [[-1], [-1], [-1]], "corpus_id": 5040144, "sentences": ["Two images.", "On the left the workstation of an air traffic controller comprising screens and constructions to hold strips of paper in place.", "On the right a workstation of a financial worker with two large displays showing tables and charts, a mouse, and a keyboard."], "caption": "Figure 2. Left: an air traf\ufb01c controller workstation. Photo courtesy: US Navy 100714-N-5574R-003 CC-BY 2.0. Right: a Bloomberg terminal featuring a double screen controlled by keyboard and mouse. Photo courtesy: Flickr user Travis Wise CC-BY 2.0.", "local_uri": ["a81adf83dbac7a957b1da4451562d85a705a780f_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How Relevant are Incidental Power Poses for HCI?", "pdf_hash": "a81adf83dbac7a957b1da4451562d85a705a780f", "year": 2018, "venue": "CHI", "alt_text": "Two density plots for the two measures, adjusted number of pumps and percent change.", "levels": [[1]], "corpus_id": 5040144, "sentences": ["Two density plots for the two measures, adjusted number of pumps and percent change."], "caption": "Figure 8. Density plots of the raw data for both measures.", "local_uri": ["a81adf83dbac7a957b1da4451562d85a705a780f_Image_104.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 1. Indoor wayfinding system using color codes: (a) wayfinding using 1-D barcode and color target[9]", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 1.", "Indoor wayfinding system using color codes: (a) wayfinding using 1-D barcode and color target[9]"], "caption": "(a)(b)", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 1. Indoor wayfinding system using color codes: (b) wayfinding using QR codes[7]", "levels": null, "corpus_id": 305672, "sentences": ["Figure 1.", "Indoor wayfinding system using color codes: (b) wayfinding using QR codes[7]"], "caption": "Figure 1. Indoor wayfinding system using color codes:", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (a) SURFs extracted from 'corridor'", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (a) SURFs extracted from 'corridor'"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_014.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (a) SURFs extracted from 'door'", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (a) SURFs extracted from 'door'"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_015.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (a) SURFs extracted from 'hall'", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (a) SURFs extracted from 'hall'"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_016.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (b) SURFs extracted from 'corridor'", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (b) SURFs extracted from 'corridor'"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_017.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (b) SURFs extracted from 'door'", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (b) SURFs extracted from 'door'"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_018.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (b) SURFs extracted from 'hall'", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (b) SURFs extracted from 'hall'"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_019.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Situation-based indoor wayfinding system for the visually impaired", "pdf_hash": "df19e217899b6bc8f50a745d5e575e65bb8203ce", "year": 2011, "venue": "ASSETS", "alt_text": "Figure 6. Characteristics of SURF distributions: (c) SURF distribution accumulated with 5", "levels": [[-1], [-1]], "corpus_id": 305672, "sentences": ["Figure 6.", "Characteristics of SURF distributions: (c) SURF distribution accumulated with 5"], "caption": "", "local_uri": ["df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_020.jpg", "df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_021.jpg", "df19e217899b6bc8f50a745d5e575e65bb8203ce_Image_022.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a Reconfigurable Virtual Reality Controller", "pdf_hash": "b4911b1f16d4a6c0712b2d4780d5ae771b429b20", "year": 2018, "venue": "CHI", "alt_text": "(left) A user holding our device with the index finger resting on a wheel. (center) a set of 9 wheels, each with various textures and shapes imprinted on them (right) a demo scene of a user interacting with a card table", "levels": null, "corpus_id": 4794490, "sentences": ["(left) A user holding our device with the index finger resting on a wheel. (center) a set of 9 wheels, each with various textures and shapes imprinted on them (right) a demo scene of a user interacting with a card table"], "caption": "Figure 1. (left) Our Haptic Revolver device uses a wheel that raises and lowers and spins underneath the fngertip to render various haptic sensations. (center) The haptic wheels are interchangeable and can be customized to render arbitrary textures, shapes, or interactive elements. (right) Wheel features are spatially registered with the virtual environment, so the user can reach out and feel virtual surfaces.", "local_uri": ["b4911b1f16d4a6c0712b2d4780d5ae771b429b20_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a Reconfigurable Virtual Reality Controller", "pdf_hash": "b4911b1f16d4a6c0712b2d4780d5ae771b429b20", "year": 2018, "venue": "CHI", "alt_text": "An illustration of how the rendering engine positions features on the wheel based on the position of the finger in space", "levels": null, "corpus_id": 4794490, "sentences": ["An illustration of how the rendering engine positions features on the wheel based on the position of the finger in space"], "caption": "Figure 6. (left) When a user hovers over the blue surface, the render- ing engine places the appropriate wheel surface under the fnger and begins to track the nearby edge of the black surface. (center) As the user approaches the edge, the rendering engine positions the wheel so that the edge approaches the fnger. (right) While hovering over the smaller black surface, the rendering engine adjusts the gain of the wheel so that the two edges are rendered correctly.", "local_uri": ["b4911b1f16d4a6c0712b2d4780d5ae771b429b20_Image_009.png"], "annotated": false, "compound": false}
{"title": "Usability issues with 3D user interfaces for adolescents with high functioning autism", "pdf_hash": "d5466ed3ad919365cc5752b7ca9693b2a8dfc167", "year": 2014, "venue": "ASSETS", "alt_text": "Figure 4. The distributions of the time performance across groups on rotation questions 10, 13 and 15.", "levels": [[0], [1]], "corpus_id": 18448457, "sentences": ["Figure 4.", "The distributions of the time performance across groups on rotation questions 10, 13 and 15."], "caption": "Figure 4. The distributions of the time performance across groups on rotation questions 10, 13 and 15.", "local_uri": ["d5466ed3ad919365cc5752b7ca9693b2a8dfc167_Image_004.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Usability issues with 3D user interfaces for adolescents with high functioning autism", "pdf_hash": "d5466ed3ad919365cc5752b7ca9693b2a8dfc167", "year": 2014, "venue": "ASSETS", "alt_text": "Figure 5. The distributions of the time performance across groups on translation questions 10, 13 and 15.", "levels": [[0], [1]], "corpus_id": 18448457, "sentences": ["Figure 5.", "The distributions of the time performance across groups on translation questions 10, 13 and 15."], "caption": "Figure 5. The distributions of the time performance across groups on translation questions 10, 13 and 15.", "local_uri": ["d5466ed3ad919365cc5752b7ca9693b2a8dfc167_Image_005.gif"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Making Well-being: Exploring the Role of Makerspaces in Long Term Care Facilities", "pdf_hash": "7b4ae5767e885c27f7529f3bdc51b34e8a314478", "year": 2019, "venue": "CHI", "alt_text": "Photo Grid: 92 year old woman uses a 3D printer pen; 87 year old woman uses a button maker; a series of four 3D printed vases.", "levels": null, "corpus_id": 140241206, "sentences": ["Photo Grid: 92 year old woman uses a 3D printer pen; 87 year old woman uses a button maker; a series of four 3D printed vases."], "caption": "", "local_uri": ["7b4ae5767e885c27f7529f3bdc51b34e8a314478_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Making Well-being: Exploring the Role of Makerspaces in Long Term Care Facilities", "pdf_hash": "7b4ae5767e885c27f7529f3bdc51b34e8a314478", "year": 2019, "venue": "CHI", "alt_text": "Photo Grid: Artifacts made by participants of the study. 3D sculpture, handmade book, earrings and bracelet made with semi-precious beads, 3D printed necklace, embroidered apron.", "levels": null, "corpus_id": 140241206, "sentences": ["Photo Grid: Artifacts made by participants of the study.", "3D sculpture, handmade book, earrings and bracelet made with semi-precious beads, 3D printed necklace, embroidered apron."], "caption": "Figure 2: Various projects the residents completed during the eight week study, including 3D printing, book making, jewelry making, and digital embroidery. \u00a9Kayla Carucci", "local_uri": ["7b4ae5767e885c27f7529f3bdc51b34e8a314478_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies", "pdf_hash": "abfd76ebf841802ffc6c26cb28e9106211a658f3", "year": 2019, "venue": "CHI", "alt_text": "A horizontal red bar represents the passage of time. A shaded area in the middle represents the recording window. A blue dotted line is drawn through the middle of the shaded area and labeled \"Anchor Event.\" The shaded portion to the left of the blue line is labeled \"Antecedent Recording\" and the shared portion to the righ tof the blue line is labeled \"Ensuing Recording.\"", "levels": [[1], [1], [1], [1]], "corpus_id": 140213309, "sentences": ["A horizontal red bar represents the passage of time.", "A shaded area in the middle represents the recording window.", "A blue dotted line is drawn through the middle of the shaded area and labeled \"Anchor Event.\"", "The shaded portion to the left of the blue line is labeled \"Antecedent Recording\" and the shared portion to the righ tof the blue line is labeled \"Ensuing Recording.\""], "caption": "Figure 1: Timeline of an AAS data collection event. The re- searcher defnes events of interest, which become the an- chor points. Recording occurs during a sliding window sur- rounding the anchor event.", "local_uri": ["abfd76ebf841802ffc6c26cb28e9106211a658f3_Image_002.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies", "pdf_hash": "abfd76ebf841802ffc6c26cb28e9106211a658f3", "year": 2019, "venue": "CHI", "alt_text": "On the left is a list view with items that say: \"limit recording duration,\" \"set recording duration,\" \"set recording preceding time,\" \"auto restart recording after stopped,\" \"record in background,\" and \"always recording,\" with check boxes next to some options. On the top right is a screenshot with a red recording icon and a toast notification overlaid on top of it that says, \"Recording...\" On the bottom right is a screenshot of a notification shade pulled down to show a notification that says \"Recording...\"", "levels": null, "corpus_id": 140213309, "sentences": ["On the left is a list view with items that say: \"limit recording duration,\" \"set recording duration,\" \"set recording preceding time,\" \"auto restart recording after stopped,\" \"record in background,\" and \"always recording,\" with check boxes next to some options.", "On the top right is a screenshot with a red recording icon and a toast notification overlaid on top of it that says, \"Recording...\" On the bottom right is a screenshot of a notification shade pulled down to show a notification that says \"Recording...\""], "caption": "", "local_uri": ["abfd76ebf841802ffc6c26cb28e9106211a658f3_Image_003.png"], "annotated": false, "compound": false}
{"title": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies", "pdf_hash": "abfd76ebf841802ffc6c26cb28e9106211a658f3", "year": 2019, "venue": "CHI", "alt_text": "On the left is a list view that shows multiple file names. On the right, a dialog box is overlaid on top of the same list view, with a single file name that has been selected, a seek bar, and a play button.", "levels": [[-1], [-1]], "corpus_id": 140213309, "sentences": ["On the left is a list view that shows multiple file names.", "On the right, a dialog box is overlaid on top of the same list view, with a single file name that has been selected, a seek bar, and a play button."], "caption": "Figure 3: Default UI for reviewing recordings. Left: the par- ticipant can view their complete data set. Right: Each record- ing can be reviewed.", "local_uri": ["abfd76ebf841802ffc6c26cb28e9106211a658f3_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies", "pdf_hash": "abfd76ebf841802ffc6c26cb28e9106211a658f3", "year": 2019, "venue": "CHI", "alt_text": "On the left is a screenshot of an app with an icon of a book in the center and the text \"Time to read!\" underneath. A red recording dot is shown in the corner. On the right is a screenshot of an app displaying a large face of a monster with bits of food in its teeth.", "levels": null, "corpus_id": 140213309, "sentences": ["On the left is a screenshot of an app with an icon of a book in the center and the text \"Time to read!\" underneath.", "A red recording dot is shown in the corner.", "On the right is a screenshot of an app displaying a large face of a monster with bits of food in its teeth."], "caption": "", "local_uri": ["abfd76ebf841802ffc6c26cb28e9106211a658f3_Image_006.png"], "annotated": false, "compound": false}
{"title": "Airport Accessibility and Navigation Assistance for People with Visual Impairments", "pdf_hash": "df6db5315ae076761a2ce7e8337cb881b6e2db76", "year": 2019, "venue": "CHI", "alt_text": "It shows 4 different images. The first shows a blind participant walking with a white cane and finding a large crowd in front of a restaurant, blocking his path. The second shows the perspective of an user showing a very large open space in the center core of the terminal. The third shows a white-cane user walking in the direction of a stand that is in the middle of the corridor. The fourth shows a white-cane user detecting an obstacle with the cane (it is a big television screen in the middle of the corridor).", "levels": null, "corpus_id": 140210926, "sentences": ["It shows 4 different images.", "The first shows a blind participant walking with a white cane and finding a large crowd in front of a restaurant, blocking his path.", "The second shows the perspective of an user showing a very large open space in the center core of the terminal.", "The third shows a white-cane user walking in the direction of a stand that is in the middle of the corridor.", "The fourth shows a white-cane user detecting an obstacle with the cane (it is a big television screen in the middle of the corridor)."], "caption": "A stand or an obstacle in the middle of the corridor", "local_uri": ["df6db5315ae076761a2ce7e8337cb881b6e2db76_Image_001.jpg", "df6db5315ae076761a2ce7e8337cb881b6e2db76_Image_002.jpg", "df6db5315ae076761a2ce7e8337cb881b6e2db76_Image_004.jpg", "df6db5315ae076761a2ce7e8337cb881b6e2db76_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Airport Accessibility and Navigation Assistance for People with Visual Impairments", "pdf_hash": "df6db5315ae076761a2ce7e8337cb881b6e2db76", "year": 2019, "venue": "CHI", "alt_text": "It shows the 4 routes of the user study, indicating the respective scale. The description of the routes is detailed in the main text.", "levels": null, "corpus_id": 140210926, "sentences": ["It shows the 4 routes of the user study, indicating the respective scale.", "The description of the routes is detailed in the main text."], "caption": "", "local_uri": ["df6db5315ae076761a2ce7e8337cb881b6e2db76_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Airport Accessibility and Navigation Assistance for People with Visual Impairments", "pdf_hash": "df6db5315ae076761a2ce7e8337cb881b6e2db76", "year": 2019, "venue": "CHI", "alt_text": "Two images showing both a guide dog user and a white-cane user passing right next to the escalator, when they were supposed to go on the escalator.", "levels": null, "corpus_id": 140210926, "sentences": ["Two images showing both a guide dog user and a white-cane user passing right next to the escalator, when they were supposed to go on the escalator."], "caption": "", "local_uri": ["df6db5315ae076761a2ce7e8337cb881b6e2db76_Image_015.png"], "annotated": false, "compound": false}
{"title": "\"Tricky to get your head around\": Information Work of People Managing Chronic Kidney Disease in the UK", "pdf_hash": "8b761322ddca998686ebe5e92c1f337b93bb3ea7", "year": 2019, "venue": "CHI", "alt_text": "This diagram displays an x-axis over time with notations for Diagnosis, then a Treatment decision, followed by a potential transplant or other treatment shift. Below this timeline, we see boxes showing that the Learning phase starts at diagnosis and ends shortly after the treatment decision. Then the Living With phase begins. However, the important part of this diagram is that the Learning phase reoccurs around the transplant or other treatment shift demarcation, and then after some time adjusting to the new transplant or treatment, the patient would transition back to the Living With stage. Thus the phases may reoccur over time.", "levels": null, "corpus_id": 140214028, "sentences": ["This diagram displays an x-axis over time with notations for Diagnosis, then a Treatment decision, followed by a potential transplant or other treatment shift.", "Below this timeline, we see boxes showing that the Learning phase starts at diagnosis and ends shortly after the treatment decision.", "Then the Living With phase begins.", "However, the important part of this diagram is that the Learning phase reoccurs around the transplant or other treatment shift demarcation, and then after some time adjusting to the new transplant or treatment, the patient would transition back to the Living With stage.", "Thus the phases may reoccur over time."], "caption": "Figure 1. Illustration of Learning and Living With Phase reoccurrence", "local_uri": ["8b761322ddca998686ebe5e92c1f337b93bb3ea7_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Bendtroller:: An Exploration of In-Game Action Mappings with a Deformable Game Controller", "pdf_hash": "008f51c5a995cf3d19bdcf2c7bb01648774daafb", "year": 2017, "venue": "CHI", "alt_text": "The user is holding the bendable controller in both hands, with the right hand moving the controller's top towards the user.", "levels": null, "corpus_id": 26384834, "sentences": ["The user is holding the bendable controller in both hands, with the right hand moving the controller's top towards the user."], "caption": "Figure 1. Twist input using our bendable game controller.", "local_uri": ["008f51c5a995cf3d19bdcf2c7bb01648774daafb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Bendtroller:: An Exploration of In-Game Action Mappings with a Deformable Game Controller", "pdf_hash": "008f51c5a995cf3d19bdcf2c7bb01648774daafb", "year": 2017, "venue": "CHI", "alt_text": "a 3x3 images illustrating the 3 schemes (columns) and the 3 games (rows) mappings", "levels": null, "corpus_id": 26384834, "sentences": ["a 3x3 images illustrating the 3 schemes (columns) and the 3 games (rows) mappings"], "caption": "Figure 4. Control Schemes for study one for Donkey Kong (top row), Punch Out (middle), and Tetris (bottom). L=Left, R=Right, U=Up and D=Down.", "local_uri": ["008f51c5a995cf3d19bdcf2c7bb01648774daafb_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Bendtroller:: An Exploration of In-Game Action Mappings with a Deformable Game Controller", "pdf_hash": "008f51c5a995cf3d19bdcf2c7bb01648774daafb", "year": 2017, "venue": "CHI", "alt_text": "For each measure, we present 3 stacked graphs with the number of people who rated each scheme.", "levels": [[1]], "corpus_id": 26384834, "sentences": ["For each measure, we present 3 stacked graphs with the number of people who rated each scheme."], "caption": "Figure 8. Naturalness and Fun ratings. 1 is negative (very unnatural/boring), 5 is positive (very natural/fun)\u200c", "local_uri": ["008f51c5a995cf3d19bdcf2c7bb01648774daafb_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Using extracted features to inform alignment-driven design ideas in an educational game", "pdf_hash": "e890ac8eef6ef41ae5db8d2b204089643252fd5d", "year": 2014, "venue": "CHI", "alt_text": "There is a partially built tower in the middle of the field with an alien standing on a cliff to the right, and a deactivated spaceship to the left.", "levels": null, "corpus_id": 9188070, "sentences": ["There is a partially built tower in the middle of the field with an alien standing on a cliff to the right, and a deactivated spaceship to the left."], "caption": "", "local_uri": ["e890ac8eef6ef41ae5db8d2b204089643252fd5d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Using extracted features to inform alignment-driven design ideas in an educational game", "pdf_hash": "e890ac8eef6ef41ae5db8d2b204089643252fd5d", "year": 2014, "venue": "CHI", "alt_text": "A tower with a red line showing the extents of its base, a blue line showing the height of its center of mass, and a green line showing the angle used for symmetry.", "levels": null, "corpus_id": 9188070, "sentences": ["A tower with a red line showing the extents of its base, a blue line showing the height of its center of mass, and a green line showing the angle used for symmetry."], "caption": "", "local_uri": ["e890ac8eef6ef41ae5db8d2b204089643252fd5d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Using extracted features to inform alignment-driven design ideas in an educational game", "pdf_hash": "e890ac8eef6ef41ae5db8d2b204089643252fd5d", "year": 2014, "venue": "CHI", "alt_text": "Two towers from a RumbleBlocks level. The left tower is an inverted T shape while the right tower is an arch shape.", "levels": null, "corpus_id": 9188070, "sentences": ["Two towers from a RumbleBlocks level.", "The left tower is an inverted T shape while the right tower is an arch shape."], "caption": "", "local_uri": ["e890ac8eef6ef41ae5db8d2b204089643252fd5d_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Using extracted features to inform alignment-driven design ideas in an educational game", "pdf_hash": "e890ac8eef6ef41ae5db8d2b204089643252fd5d", "year": 2014, "venue": "CHI", "alt_text": "Three solutions to a level of RumbleBlocks. The top tower is pyramid shaped, while the bottom two towers each have the spaceship on top of a single square block.", "levels": null, "corpus_id": 9188070, "sentences": ["Three solutions to a level of RumbleBlocks.", "The top tower is pyramid shaped, while the bottom two towers each have the spaceship on top of a single square block."], "caption": "Frequency (%)", "local_uri": ["e890ac8eef6ef41ae5db8d2b204089643252fd5d_Image_011.png"], "annotated": false, "compound": false}
{"title": "Using extracted features to inform alignment-driven design ideas in an educational game", "pdf_hash": "e890ac8eef6ef41ae5db8d2b204089643252fd5d", "year": 2014, "venue": "CHI", "alt_text": "Five substructures of RumbleBlocks towers. Four on the left are shaded red and all contain square blocks iwth empty space next to them. The single solution on the right is shaded green and has the spaceship on top of a wide block.", "levels": null, "corpus_id": 9188070, "sentences": ["Five substructures of RumbleBlocks towers.", "Four on the left are shaded red and all contain square blocks iwth empty space next to them.", "The single solution on the right is shaded green and has the spaceship on top of a wide block."], "caption": "Figure 9. Rendered results of a Chi2 analysis of structural features in RumbleBlocks which predict the success of a tower in the earthquake. Student solutions which contained the features in the red shaded region to the left were more likely to be unsuccessful in the earthquake while solutions which contained the feature in the green region to the right were more likely to be successful.", "local_uri": ["e890ac8eef6ef41ae5db8d2b204089643252fd5d_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Exploration and avoidance of surrounding obstacles for the visually impaired", "pdf_hash": "c4205307c399541fee920fffc5ec5e7ad8598ba8", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 2. The overview of the system  It has 4 small photoes.   (a) A subject wears the 3DOD system, on the right side;   (b) 3D TOF camera (weight 1kg), on the top left side;   (c) a portable multiple line Braille display (weight 600g), at the mid left;   (d) a portable computer, at the bottom left;", "levels": null, "corpus_id": 18623028, "sentences": ["Figure 2.", "The overview of the system  It has 4 small photoes.", "(a) A subject wears the 3DOD system, on the right side;   (b) 3D TOF camera (weight 1kg), on the top left side;   (c) a portable multiple line Braille display (weight 600g), at the mid left;   (d) a portable computer, at the bottom left;"], "caption": "Figure 2. Components of the 3DOD system (a) A subject wears the 3DOD system; (b) 3D TOF camera (weight 1kg); (c) a portable multiple line Braille display (weight 600g); (d) a portable computer;", "local_uri": ["c4205307c399541fee920fffc5ec5e7ad8598ba8_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Exploration and avoidance of surrounding obstacles for the visually impaired", "pdf_hash": "c4205307c399541fee920fffc5ec5e7ad8598ba8", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 3 The flow chart of walking mode  It describes how the system works in the walking mode. When pressing Button \"B\" to start the walking mode (the users start to wal), the system will automatically detect the surrounding until some obstacles in 2 meters. Then the wiimote cane will vibrate a short amount of time, and then the system will render the current situation on the Braille display via pre-designed tactile symbols. The user can also continue to the walking mode after finding a open space, or change to the inspection mode.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 18623028, "sentences": ["Figure 3 The flow chart of walking mode  It describes how the system works in the walking mode.", "When pressing Button \"B\" to start the walking mode (the users start to wal), the system will automatically detect the surrounding until some obstacles in 2 meters.", "Then the wiimote cane will vibrate a short amount of time, and then the system will render the current situation on the Braille display via pre-designed tactile symbols.", "The user can also continue to the walking mode after finding a open space, or change to the inspection mode."], "caption": "Figure 3. The flow chart of walking mode", "local_uri": ["c4205307c399541fee920fffc5ec5e7ad8598ba8_Image_006.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploration and avoidance of surrounding obstacles for the visually impaired", "pdf_hash": "c4205307c399541fee920fffc5ec5e7ad8598ba8", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 4. The layout of representation 3D obstacles on a 2D Braille display.  It indicates how to render the obstacle symbols in 4 meters on the tactile display. There is a reference grid in the center and 3 obstacles on the display.", "levels": null, "corpus_id": 18623028, "sentences": ["Figure 4.", "The layout of representation 3D obstacles on a 2D Braille display.", "It indicates how to render the obstacle symbols in 4 meters on the tactile display.", "There is a reference grid in the center and 3 obstacles on the display."], "caption": "", "local_uri": ["c4205307c399541fee920fffc5ec5e7ad8598ba8_Image_031.gif"], "annotated": false, "compound": false}
{"title": "Exploration and avoidance of surrounding obstacles for the visually impaired", "pdf_hash": "c4205307c399541fee920fffc5ec5e7ad8598ba8", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 7. The real test environments in a classroom.   The photo captured in our real test environments in a big classroom. We set up 12 obstalces in the   center, in which 5 of them are hanging at the head-level or the upper waist level. The obstacles are 3 chairs, 1 long table, 4 hanging boards, 1 hanging balloon, 1 thin pole, and 2 boxes.", "levels": null, "corpus_id": 18623028, "sentences": ["Figure 7.", "The real test environments in a classroom.", "The photo captured in our real test environments in a big classroom.", "We set up 12 obstalces in the   center, in which 5 of them are hanging at the head-level or the upper waist level.", "The obstacles are 3 chairs, 1 long table, 4 hanging boards, 1 hanging balloon, 1 thin pole, and 2 boxes."], "caption": "Figure 7. The real test environment in a classroom", "local_uri": ["c4205307c399541fee920fffc5ec5e7ad8598ba8_Image_125.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "An image showing the components of a set of stairs, including tread, riser, nosing, stringer, and railing.", "levels": null, "corpus_id": 52939346, "sentences": ["An image showing the components of a set of stairs, including tread, riser, nosing, stringer, and railing."], "caption": "Figure 1. Stair structure and the basic components.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "This figure includes five images, showing an indoor decorative curved staircase, the two steps on the landing of an emergency exit staircase, a full emergency exit staircase, an indoor decorative wooden staircase, and two steps of outdoor decorative stairs.", "levels": null, "corpus_id": 52939346, "sentences": ["This figure includes five images, showing an indoor decorative curved staircase, the two steps on the landing of an emergency exit staircase, a full emergency exit staircase, an indoor decorative wooden staircase, and two steps of outdoor decorative stairs."], "caption": "Figure 2. Stairs in the study: A. indoor decorative curved stairs; B. Emergency exit stairs part I: two steps at the landing; C. Emergency exit stairs part II: two sets of stairs with a landing; D. indoor decorative wooden stairs; E. outdoor stairs.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "This figure includes two images, showing the metal contrast stripes on the wooden stairs, and the extra contrast stripe at the curved stairs.", "levels": null, "corpus_id": 52939346, "sentences": ["This figure includes two images, showing the metal contrast stripes on the wooden stairs, and the extra contrast stripe at the curved stairs."], "caption": "Figure 3. Contrast stripes: A. the metal stripes on the wooden stairs; B. extra contrast stripe at the curved stairs.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "This figure includes two images, showing the shadow of the riser on the outdoor stairs, and the shadow of the railing on the landing that generates illusion of steps.", "levels": null, "corpus_id": 52939346, "sentences": ["This figure includes two images, showing the shadow of the riser on the outdoor stairs, and the shadow of the railing on the landing that generates illusion of steps."], "caption": "Figure 4. Shadows: A. shadow of the riser cast on stair tread; B. shadow of the railing generating illusion of steps.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "This figure includes two images, showing the yellow highlight on the curb, and a curb cut with tactile domes.", "levels": null, "corpus_id": 52939346, "sentences": ["This figure includes two images, showing the yellow highlight on the curb, and a curb cut with tactile domes."], "caption": "Figure 5. A. Curb highlight; B. a curb cut with tactile domes.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "This figure includes two images, showing the grouting line and the texture change on the ground, which generate illusion of surface level changes.", "levels": null, "corpus_id": 52939346, "sentences": ["This figure includes two images, showing the grouting line and the texture change on the ground, which generate illusion of surface level changes."], "caption": "Figure 6. A. Grouting line between two ground pieces; B. the texture changes on the ground.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "\"It Looks Beautiful but Scary\": How Low Vision People Navigate Stairs and Other Surface Level Changes", "pdf_hash": "d5285197a72a56688bdc4674ae5be8db689f9fd8", "year": 2018, "venue": "ASSETS", "alt_text": "This figure includes two images, showing a small piece of exercise equipment on the ground as a low obstacle, and the glass walls in the built environment.", "levels": null, "corpus_id": 52939346, "sentences": ["This figure includes two images, showing a small piece of exercise equipment on the ground as a low obstacle, and the glass walls in the built environment."], "caption": "Figure 7. A. Low obstacle: a small piece of exercise equipment on the carpet. B. glass walls.", "local_uri": ["d5285197a72a56688bdc4674ae5be8db689f9fd8_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Examining Wikipedia With a Broader Lens: Quantifying the Value of Wikipedia's Relationships with Other Large-Scale Online Communities", "pdf_hash": "41f10ac51667c80e76a566ed0c9f18805365b929", "year": 2018, "venue": "CHI", "alt_text": "This image shows a TIL Reddit post which summarizes a Wikipedia article about a 1999 Hotmail hack - the post is entirely based on the linked Wikipedia article.", "levels": null, "corpus_id": 5064945, "sentences": ["This image shows a TIL Reddit post which summarizes a Wikipedia article about a 1999 Hotmail hack - the post is entirely based on the linked Wikipedia article."], "caption": "Figure 1. This image shows a popular \u201cToday I Learned\u201d", "local_uri": ["41f10ac51667c80e76a566ed0c9f18805365b929_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Power of Control in Autonomous Vehicles for People with Vision Impairment", "pdf_hash": "8fd8eef9bff9c700278d2d5ffab768e198a8753f", "year": 2018, "venue": "ASSETS", "alt_text": "a design on top of black felt and a corkboard. the design is modeled off a white cane and is a pipe cleaner with a piece of clay attached to one end with a marble on the end of the piece of clay", "levels": null, "corpus_id": 52937872, "sentences": ["a design on top of black felt and a corkboard.", "the design is modeled off a white cane and is a pipe cleaner with a piece of clay attached to one end with a marble on the end of the piece of clay"], "caption": "Figure 1- P2's pothole navigation device modeled off a white cane", "local_uri": ["8fd8eef9bff9c700278d2d5ffab768e198a8753f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Power of Control in Autonomous Vehicles for People with Vision Impairment", "pdf_hash": "8fd8eef9bff9c700278d2d5ffab768e198a8753f", "year": 2018, "venue": "ASSETS", "alt_text": "1 participant is passing around 3 popsicle sticks arranged on a corkboard such that one stick is in the center of the board, one stick is attached to the end of the stick in the center and pointed up, and the other stick is also attached to the end of the center but pointed down.", "levels": null, "corpus_id": 52937872, "sentences": ["1 participant is passing around 3 popsicle sticks arranged on a corkboard such that one stick is in the center of the board, one stick is attached to the end of the stick in the center and pointed up, and the other stick is also attached to the end of the center but pointed down."], "caption": "Figure 2 \u2013 P13\u2019s solution for determining the car's location", "local_uri": ["8fd8eef9bff9c700278d2d5ffab768e198a8753f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Tactile Accessibility: Does Anyone Need a Haptic Glove?", "pdf_hash": "d341eb6169aa2101a284b7385070377422900e48", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 2 \u2026 The FeelX glove prototype  This figure displays the photo of FeelX prototype", "levels": null, "corpus_id": 2128850, "sentences": ["Figure 2 \u2026 The FeelX glove prototype  This figure displays the photo of FeelX prototype"], "caption": "Figure 2 \u2013 The FeelX glove prototype", "local_uri": ["d341eb6169aa2101a284b7385070377422900e48_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Tactile Accessibility: Does Anyone Need a Haptic Glove?", "pdf_hash": "d341eb6169aa2101a284b7385070377422900e48", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 3 - Metec Braille Cell and specifications. This figure shows the physical dimensions of a single Braille cell.", "levels": null, "corpus_id": 2128850, "sentences": ["Figure 3 - Metec Braille Cell and specifications.", "This figure shows the physical dimensions of a single Braille cell."], "caption": "Figure 3 \u2013 Metec Braille cell and specifications", "local_uri": ["d341eb6169aa2101a284b7385070377422900e48_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Tactile Accessibility: Does Anyone Need a Haptic Glove?", "pdf_hash": "d341eb6169aa2101a284b7385070377422900e48", "year": 2016, "venue": "ASSETS", "alt_text": "Displays the position of Breille Cell alignment under finger aligned condition.", "levels": null, "corpus_id": 2128850, "sentences": ["Displays the position of Breille Cell alignment under finger aligned condition."], "caption": "", "local_uri": ["d341eb6169aa2101a284b7385070377422900e48_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Real-Time Mobile Personalized Simulations of Impaired Colour Vision", "pdf_hash": "db59784588a97b0a1147fab50c76a495253232ff", "year": 2016, "venue": "ASSETS", "alt_text": "A blue 'C' on a grey background. The 'C' is rotated such that the gap points down and to the right.", "levels": [[-1], [-1]], "corpus_id": 14923987, "sentences": ["A blue 'C' on a grey background.", "The 'C' is rotated such that the gap points down and to the right."], "caption": "", "local_uri": ["db59784588a97b0a1147fab50c76a495253232ff_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Real-Time Mobile Personalized Simulations of Impaired Colour Vision", "pdf_hash": "db59784588a97b0a1147fab50c76a495253232ff", "year": 2016, "venue": "ASSETS", "alt_text": "Line graph showing histogram of differences between personalized and adjustable simulation  results for protan and deutan simulations. Differences measured in CIE Luv units) Both lines rise sharply from 0 and drop sharply at 20 units, with a long tail to around 65 units.", "levels": [[1], [3, 2]], "corpus_id": 14923987, "sentences": ["Line graph showing histogram of differences between personalized and adjustable simulation  results for protan and deutan simulations.", "Differences measured in CIE Luv units) Both lines rise sharply from 0 and drop sharply at 20 units, with a long tail to around 65 units."], "caption": "", "local_uri": ["db59784588a97b0a1147fab50c76a495253232ff_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Real-Time Mobile Personalized Simulations of Impaired Colour Vision", "pdf_hash": "db59784588a97b0a1147fab50c76a495253232ff", "year": 2016, "venue": "ASSETS", "alt_text": "Shift amounts for personalized and adjustable simulations for colours between green and blue (blue-greens) and blue and red (purples). In general, shift directions agree, but shift magnitudes differ between the two techniques. Blue-greens are shifted more with personalized, whereas purples are shifted less with personalized.", "levels": null, "corpus_id": 14923987, "sentences": ["Shift amounts for personalized and adjustable simulations for colours between green and blue (blue-greens) and blue and red (purples).", "In general, shift directions agree, but shift magnitudes differ between the two techniques.", "Blue-greens are shifted more with personalized, whereas purples are shifted less with personalized."], "caption": "", "local_uri": ["db59784588a97b0a1147fab50c76a495253232ff_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Modeling the Speed and Timing of American Sign Language to Generate Realistic Animations", "pdf_hash": "2f54a34d2bf141d33c7c10a8d4c044eebcbe3d0e", "year": 2018, "venue": "ASSETS", "alt_text": "An example of a script for a sign language sentence (labelled \"a\") with three phases of processing to insert pauses between some words (labelled \"b\"), to adjust the speed of individual words (labelled \"c\"), and to adjust the duration of the pauses (labelled \"d\").  The example transcript included in the image is: they make computer program it name #chess program play game chess use #SUPER computer.  For each step of the process, the image displays the same sentence, illustrated with the words as individual rectangles of different width and with some amount of space between them. The width of the rectangle indicates word length, and the space between them indicates if there is a pause during the timeline between those two words.", "levels": null, "corpus_id": 52942147, "sentences": ["An example of a script for a sign language sentence (labelled \"a\") with three phases of processing to insert pauses between some words (labelled \"b\"), to adjust the speed of individual words (labelled \"c\"), and to adjust the duration of the pauses (labelled \"d\").", "The example transcript included in the image is: they make computer program it name #chess program play game chess use #SUPER computer.", "For each step of the process, the image displays the same sentence, illustrated with the words as individual rectangles of different width and with some amount of space between them.", "The width of the rectangle indicates word length, and the space between them indicates if there is a pause during the timeline between those two words."], "caption": "Figure 1: This series of timeline images show how an ASL animation system may sequence decisions regarding speed and timing:", "local_uri": ["2f54a34d2bf141d33c7c10a8d4c044eebcbe3d0e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Modeling the Speed and Timing of American Sign Language to Generate Realistic Animations", "pdf_hash": "2f54a34d2bf141d33c7c10a8d4c044eebcbe3d0e", "year": 2018, "venue": "ASSETS", "alt_text": "Graph displaying the Root Mean Squared Error (RMSE) values for the two regression models: Differential Rate and Pause Insertion.   For Differential Rate, \"ASL-SPEED\" is 0.64, and \"2008 Model\" is 0.84.  For Pause Duration, \"ASL-SPEED\" is 5.31, and \"2008 Model\" is 6.23.  In both cases, the ASL-SPEED has a lower value, which indicates a better result.", "levels": [[1], [2], [2], [3]], "corpus_id": 52942147, "sentences": ["Graph displaying the Root Mean Squared Error (RMSE) values for the two regression models: Differential Rate and Pause Insertion.", "For Differential Rate, \"ASL-SPEED\" is 0.64, and \"2008 Model\" is 0.84.", "For Pause Duration, \"ASL-SPEED\" is 5.31, and \"2008 Model\" is 6.23.", "In both cases, the ASL-SPEED has a lower value, which indicates a better result."], "caption": "Figure 3: Comparison among new ASL-Speed model and the 2008 Model \u2013 for Differential Rate and Pause Duration.", "local_uri": ["2f54a34d2bf141d33c7c10a8d4c044eebcbe3d0e_Image_004.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Modeling the Speed and Timing of American Sign Language to Generate Realistic Animations", "pdf_hash": "2f54a34d2bf141d33c7c10a8d4c044eebcbe3d0e", "year": 2018, "venue": "ASSETS", "alt_text": "Screenshot of a virtual human character performing ASL, with the following transcript shown: MANY PEOPLE THEY GO CAMPING FOREST VARIOUS STATES FIRST COLORADO SECOND WYOMING THIRD CALIFORNIA FOURTH WASHINGTON THEY SCARED WHY BLACK BEAR BROWN BEAR IF ATTACK DO THEY THINK SHOOT BUT SCIENTISTS UNIVERSITY ALASKA MAKE NEW CHEMICAL DEFENSE SPECIAL RED #PEPPER SPRAY AGAINST BEAR SHOO LAST YEAR RESEARCH EXPERIMENT SPRAY THERE RIFLE THERE COMPARE THERE STOP BEAR ATTACK #60 PERCENT SPRAY BETTER STOP #90 PERCENT ATTACK OTHER SCIENTISTS AFRICA MAKE SPRAY AGAINST INSECT READY WHEN NEXT YEAR", "levels": null, "corpus_id": 52942147, "sentences": ["Screenshot of a virtual human character performing ASL, with the following transcript shown: MANY PEOPLE THEY GO CAMPING FOREST VARIOUS STATES FIRST COLORADO SECOND WYOMING THIRD CALIFORNIA FOURTH WASHINGTON THEY SCARED WHY BLACK BEAR BROWN BEAR IF ATTACK DO THEY THINK SHOOT BUT SCIENTISTS UNIVERSITY ALASKA MAKE NEW CHEMICAL DEFENSE SPECIAL RED #PEPPER SPRAY AGAINST BEAR SHOO LAST YEAR RESEARCH EXPERIMENT SPRAY THERE RIFLE THERE COMPARE THERE STOP BEAR ATTACK #60 PERCENT SPRAY BETTER STOP #90 PERCENT ATTACK OTHER SCIENTISTS AFRICA MAKE SPRAY AGAINST INSECT READY WHEN NEXT YEAR"], "caption": "Figure 4: Image of animation (left) seen participants in Phase 4, and transcript (right). Participants did not see transcript.", "local_uri": ["2f54a34d2bf141d33c7c10a8d4c044eebcbe3d0e_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Perception of ultrasonic haptic feedback on the hand: localisation and apparent motion", "pdf_hash": "39122f08b4af30a4def2d530d1c5aeb03613e9f6", "year": 2014, "venue": "CHI", "alt_text": "a 5 x 5 grid of heat maps, relative to the grid of focal points. Maps show the density of locations where feedback were perceived", "levels": [[-1], [-1]], "corpus_id": 17098762, "sentences": ["a 5 x 5 grid of heat maps, relative to the grid of focal points.", "Maps show the density of locations where feedback were perceived"], "caption": "Figure 3: Heat map of perceived stimulus locations relative to intended locations (crosshairs) in the 5 x 5 grid, averaged across both durations. Density increases from blue to red.", "local_uri": ["39122f08b4af30a4def2d530d1c5aeb03613e9f6_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Social Play in an Exergame: How the Need to Belong Predicts Adherence", "pdf_hash": "2210f7ae230f721856338e2faffe5fc151d322d6", "year": 2019, "venue": "CHI", "alt_text": "A player stands on the launch pad to Dozo Quest. Players' stickers are enlarged and displayed on each side.", "levels": null, "corpus_id": 140223441, "sentences": ["A player stands on the launch pad to Dozo Quest. Players' stickers are enlarged and displayed on each side."], "caption": "Figure 2: A player stands on the launch pad to Dozo Quest. Players\u2019 stickers are enlarged and displayed on each side.", "local_uri": ["2210f7ae230f721856338e2faffe5fc151d322d6_Image_003.png"], "annotated": false, "compound": false}
{"title": "Social Play in an Exergame: How the Need to Belong Predicts Adherence", "pdf_hash": "2210f7ae230f721856338e2faffe5fc151d322d6", "year": 2019, "venue": "CHI", "alt_text": "The figure shows four of Liberi's games. From left to right, these are A. Bobo Ranch. B. Dozo Quest. C. Gekku Race. D. Wiskin Defense.", "levels": null, "corpus_id": 140223441, "sentences": ["The figure shows four of Liberi's games.", "From left to right, these are A. Bobo Ranch.", "B. Dozo Quest. C. Gekku Race. D. Wiskin Defense."], "caption": "", "local_uri": ["2210f7ae230f721856338e2faffe5fc151d322d6_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Choice-based preference elicitation for collaborative filtering recommender systems", "pdf_hash": "97bd85c0febf21750e38575b199e64dfe43cdd7c", "year": 2014, "venue": "CHI", "alt_text": "The lower image presents a detail page for a particular movie (with director, actors, genres and tags marked as hyperlinks used for navigating to a list of corresponding movies).", "levels": null, "corpus_id": 14241242, "sentences": ["The lower image presents a detail page for a particular movie (with director, actors, genres and tags marked as hyperlinks used for navigating to a list of corresponding movies)."], "caption": "Figure 4. Two partial screenshots of the manual explora- tion interface. The upper image shows a list of movies with various filter options. The lower image presents a de- tail page for a particular movie (with director, actors, genres and tags marked as hyperlinks used for navigating to a list of corresponding movies).", "local_uri": ["97bd85c0febf21750e38575b199e64dfe43cdd7c_Image_028.jpg"], "annotated": false, "compound": false}
{"title": "Choice-based preference elicitation for collaborative filtering recommender systems", "pdf_hash": "97bd85c0febf21750e38575b199e64dfe43cdd7c", "year": 2014, "venue": "CHI", "alt_text": "While the left set contains lowbrow action movies, the right-hand side displays more serious movies with a rather dark mood.", "levels": null, "corpus_id": 14241242, "sentences": ["While the left set contains lowbrow action movies, the right-hand side displays more serious movies with a rather dark mood."], "caption": "Figure 5. Screenshot showing two movie sets that differ strongly in a single factor. While the left set contains low- brow action movies, the right-hand side displays more se- rious movies with a rather dark mood.", "local_uri": ["97bd85c0febf21750e38575b199e64dfe43cdd7c_Image_030.jpg"], "annotated": false, "compound": false}
{"title": "Identifying Speech Input Errors Through Audio-Only Interaction", "pdf_hash": "343623ed81f67a3fd7b8183ad10a520817a0356b", "year": 2018, "venue": "CHI", "alt_text": "This figure shows the experiment setup for study 1. It shows a participant recording a speech input with a tablet device on a table.", "levels": null, "corpus_id": 5041411, "sentences": ["This figure shows the experiment setup for study 1.", "It shows a participant recording a speech input with a tablet device on a table."], "caption": "Figure 1. The experimental setup of Study 1.", "local_uri": ["343623ed81f67a3fd7b8183ad10a520817a0356b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Identifying Speech Input Errors Through Audio-Only Interaction", "pdf_hash": "343623ed81f67a3fd7b8183ad10a520817a0356b", "year": 2018, "venue": "CHI", "alt_text": "There is a progress bar at the top of the screen which shows the current step in a study. Below the progress bar, there are set number, the reference phrase participants should read, a speaker icon to play the audio clip, and boxes of words to mark the recognition errors with instructions for each.", "levels": [[-1], [-1]], "corpus_id": 5041411, "sentences": ["There is a progress bar at the top of the screen which shows the current step in a study.", "Below the progress bar, there are set number, the reference phrase participants should read, a speaker icon to play the audio clip, and boxes of words to mark the recognition errors with instructions for each."], "caption": "Figure 3. Screenshot of the online testbed used for Studies 2, 3, and 4, showing a single trial. A trial consisted of reading a presented phrase, listening to an audio clip of what a speech recognition engine had heard, and marking errors in therecognized version (i.e., discrepancies between text and audio).", "local_uri": ["343623ed81f67a3fd7b8183ad10a520817a0356b_Image_019.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"Beautiful Seams\": Strategic Revelations and Concealments", "pdf_hash": "b0301e344b3de665fd745071c0fff0ffa995f13a", "year": 2019, "venue": "CHI", "alt_text": "From Chalmers (2003), an image of the distribution of network connectivity overlaid on a map. Green areas signify areas of high connectivity, while red and yellow signify low to no connectivity. The photo shows that inside main buildings, there is high connectivity; however, as one gets farther from the center, the areas are red and yellow.", "levels": [[-1], [-1], [-1]], "corpus_id": 140244627, "sentences": ["From Chalmers (2003), an image of the distribution of network connectivity overlaid on a map.", "Green areas signify areas of high connectivity, while red and yellow signify low to no connectivity.", "The photo shows that inside main buildings, there is high connectivity; however, as one gets farther from the center, the areas are red and yellow."], "caption": "", "local_uri": ["b0301e344b3de665fd745071c0fff0ffa995f13a_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FootNotes", "pdf_hash": "4e5d4073d3b8d5c7ec943212e16baa2e6bced30a", "year": 2018, "venue": "", "alt_text": "A bronze statue of kids playing in the rain, situated in a local park, used in the user study. The caption contains the annotation text used.", "levels": [[-1], [-1]], "corpus_id": 214702978, "sentences": ["A bronze statue of kids playing in the rain, situated in a local park, used in the user study.", "The caption contains the annotation text used."], "caption": "FootNotes\u2019 functional annotations indicate how one should use a point of interest, such as hours for a busi- ness or accessibility information (e.g., describing the location of stairs or obstacles). The functional infor- mation can be gleaned from websites associated with certain points of interest, as well as added in situ by volunteer contributors.", "local_uri": ["4e5d4073d3b8d5c7ec943212e16baa2e6bced30a_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Quick, Print This Page! The Value of Analogue Media in a Digital World", "pdf_hash": "4256b5578ade0285b961ba4e290222a43b041def", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Person holding a tablet saying \"This book is no longer available!\". The person looking at the tablet is not pleased being unable to read the requested book.", "levels": null, "corpus_id": 233987396, "sentences": ["Person holding a tablet saying \"This book is no longer available!\".", "The person looking at the tablet is not pleased being unable to read the requested book."], "caption": "", "local_uri": ["4256b5578ade0285b961ba4e290222a43b041def_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Videostrates: Collaborative, Distributed and Programmable Video Manipulation", "pdf_hash": "0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b", "year": 2019, "venue": "UIST", "alt_text": "Shows three examples of Videostrates in use. In the first picture, two users collaboratively edit the same videostrate on each their laptop, one with a timeline-based editor and the other with a subtitle editor. The results appear in a live, interactive preview on a large screen in front of them. The second picture shows how Videostrates aggregates, broadcasts and records multiple live streams, here from a statically mounted camera and a smartphone. The third picture shows a Videostrate-based computational notebook using Codestrates to programmatically create a WebGL animation and synchronize its playback with recorded video composited with a green screen.", "levels": null, "corpus_id": 202713762, "sentences": ["Shows three examples of Videostrates in use.", "In the first picture, two users collaboratively edit the same videostrate on each their laptop, one with a timeline-based editor and the other with a subtitle editor.", "The results appear in a live, interactive preview on a large screen in front of them.", "The second picture shows how Videostrates aggregates, broadcasts and records multiple live streams, here from a statically mounted camera and a smartphone.", "The third picture shows a Videostrate-based computational notebook using Codestrates to programmatically create a WebGL animation and synchronize its playback with recorded video composited with a green screen."], "caption": "Figure 1. Videostrates examples: A) Two users collaboratively edit the same videostrate, one with a timeline-based editor and the other with a subtitle editor. The results appear in a live, interactive preview on a large screen. B) Videostrates aggregates, broadcasts and records multiple live streams, here from a statically mounted camera and a smartphone. C) A Videostrate-based computational notebook uses Codestrates to programatically create a WebGL animation and synchronize its playback with recorded video composited with a green screen.", "local_uri": ["0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Videostrates: Collaborative, Distributed and Programmable Video Manipulation", "pdf_hash": "0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b", "year": 2019, "venue": "UIST", "alt_text": "Shows a subtitle editor. To the left, a preview of the video is shown with a progress bar for navigation. To the right, a table of all subtitles with their start and end time is shown.", "levels": [[-1], [-1], [-1]], "corpus_id": 202713762, "sentences": ["Shows a subtitle editor.", "To the left, a preview of the video is shown with a progress bar for navigation.", "To the right, a table of all subtitles with their start and end time is shown."], "caption": "", "local_uri": ["0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Videostrates: Collaborative, Distributed and Programmable Video Manipulation", "pdf_hash": "0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b", "year": 2019, "venue": "UIST", "alt_text": "Shows a simple timeline-based video editor inspired by Apple's iMovie. The top left shows thumbnails of the source material, the top right a preview of the final movie, and the bottom part an interactive timeline of the movie.", "levels": null, "corpus_id": 202713762, "sentences": ["Shows a simple timeline-based video editor inspired by Apple's iMovie.", "The top left shows thumbnails of the source material, the top right a preview of the final movie, and the bottom part an interactive timeline of the movie."], "caption": "Figure 3. A simple timeline-based video editor, inspired by Apple\u2019s iMovie. A videostrate is opened in the editor using transclusion, and video material is uploaded to the videostrate and arranged in the time- line. A preview in the top right is streamed live from vStreamer.", "local_uri": ["0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Videostrates: Collaborative, Distributed and Programmable Video Manipulation", "pdf_hash": "0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b", "year": 2019, "venue": "UIST", "alt_text": "Shows the stream studio videostrate simultaneously open on a laptop and a tablet. Two streams from mobile phone cameras are shown in the interface.", "levels": null, "corpus_id": 202713762, "sentences": ["Shows the stream studio videostrate simultaneously open on a laptop and a tablet.", "Two streams from mobile phone cameras are shown in the interface."], "caption": "Figure 5. The stream studio videostrate opens a controller on a laptop and a viewer on a tablet. The controller shows \ufb01ve streams, three of which are minimized as thumbnails and do not appear on the tablet.", "local_uri": ["0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Videostrates: Collaborative, Distributed and Programmable Video Manipulation", "pdf_hash": "0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b", "year": 2019, "venue": "UIST", "alt_text": "Shows three frames from Grace's animation where a video of Grace gradually dissolves into cubes disappearing into empty space.", "levels": null, "corpus_id": 202713762, "sentences": ["Shows three frames from Grace's animation where a video of Grace gradually dissolves into cubes disappearing into empty space."], "caption": "Recorded video can be added to another videostrate for editing into the highlights reel. Time shifting could be implemented by programatically playing back the videostrate containing the recordings inside the stream studio, although this is not currently implemented in our prototype.", "local_uri": ["0ec1c8fb9d8293980fae9e44fa8cf6c0ea44bb5b_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "There are three parts in the figure, the left is a person icon speaking the phrase \"yeah it's insert raining emoji a lot recently\". The middle is the interface of Voicemoji, displaying the recognized speech and subsituting the \"raining emoji\" with an emoji. The right is the spoken output returned by Voicemoji. The content is \"yeah it's emoji umbrella with rain drops a lot recently! Emoji suggestions available\"", "levels": null, "corpus_id": 233987201, "sentences": ["There are three parts in the figure, the left is a person icon speaking the phrase \"yeah it's insert raining emoji a lot recently\".", "The middle is the interface of Voicemoji, displaying the recognized speech and subsituting the \"raining emoji\" with an emoji.", "The right is the spoken output returned by Voicemoji.", "The content is \"yeah it's emoji umbrella with rain drops a lot recently!", "Emoji suggestions available\""], "caption": "Figure 1: The fow of using Voicemoji. Voicemoji is a web application that allows the user to speak text and emojis. It also provides context-sensitive emoji suggestions based on the spoken content.", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_001.png"], "annotated": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "a person speaking to voicemoji interface with a search command. The interface shows the emoji suggestions. On the right is the output of the screen reader", "levels": null, "corpus_id": 233987201, "sentences": ["a person speaking to voicemoji interface with a search command.", "The interface shows the emoji suggestions.", "On the right is the output of the screen reader"], "caption": "", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_035.png"], "annotated": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "a person speaking to voicemoji interface with a command. The interface shows the transcribed text and substitue the command with an emoji . On the right is the output of the screen reader", "levels": null, "corpus_id": 233987201, "sentences": ["a person speaking to voicemoji interface with a command.", "The interface shows the transcribed text and substitue the command with an emoji .", "On the right is the output of the screen reader"], "caption": "Figure 5: Emoji insertion command fow. When the user speaks the command, \u201cInsert + description + emoji,\u201d or \u201csin- gle word description + emoji,\u201d Voicemoji will return the tran- scribed text with the emoji replacement.", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_038.png"], "annotated": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "a person speaking to voicemoji interface with a regular phrase. The interface shows the transcribed text and emoji suggestions. On the right is the output of the screen reader", "levels": null, "corpus_id": 233987201, "sentences": ["a person speaking to voicemoji interface with a regular phrase.", "The interface shows the transcribed text and emoji suggestions.", "On the right is the output of the screen reader"], "caption": "Figure 6: When no emoji command is received, fve emoji suggestions are produced by Voicemoji based on the spoken word content. For example, for the phrase, \u201chow about din- ner tonight?\u201d, Voicemoji produces a fork and knife emoji, smiley face licking its lips emoji, plate of spaghetti emoji, smirking face emoji, and dinner plate with utensils emoji.", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_042.png"], "annotated": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "a person speaking to the interface about the skin modification command. On the right is the output of the screen reader", "levels": null, "corpus_id": 233987201, "sentences": ["a person speaking to the interface about the skin modification command.", "On the right is the output of the screen reader"], "caption": "Figure 7: Color/skin modifcation usage fow. When the user speaks the command, \u201cChange the emoji to + skin/color mod- ifer phrase,\u201d Voicemoji will change the last inserted emoji to its corresponding color/skin variation.", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_048.png"], "annotated": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "Line charts showing there is an interaction effect of method and nation. The y axis is the total entry time, and x axis is the method. the two lines crossed with each other", "levels": [[1], [1], [3]], "corpus_id": 233987201, "sentences": ["Line charts showing there is an interaction effect of method and nation.", "The y axis is the total entry time, and x axis is the method.", "the two lines crossed with each other"], "caption": "Entry Time(             )1, N =648Figure 8 shows the total entry time and emoji entry time of the two methods. The average total entry time for Voicemoji was 6.9 sec- onds, which was 87.1% shorter than the 56.6 seconds for the Apple iOS keyboard. We log-transformed total entry time and emoji entry time to make both ft gamma distributions, as is common practice with time measures [37]. We performed analyses of variance us- ing a generalized linear mixed model (GLMM) with gamma link function [39] on total entry time and emoji entry time separately, treating entry method and nation as fxed efects, and participant and trial as random efects. For total entry time, we found a signif- icant main efect of entry method (\u03c7 2 = 486.29, p < .001),indicating that Voicemoji was signifcantly faster than the Apple iOS keyboard. The efect of nation was not statistically signifcant (\u03c7 2 = 0.09, n.s.). There was a signifcant interaction be-(1, N =648)                                              2(             )tween nation and entry method (\u03c7 1, N =648 = 9.22, p < .01), asshown in Figure 9(b).\u00d7Figure 9: Interaction efect of Method Nation on log entry time. Both interaction efects indicate that the time saved by Voicemoji in the U.S. group is more than in the Chinese group.\u200c(\u03c7 2 = 513.55, p < .001). There was no statistically signif-(1, N =648)                      2(             )cant efect of nation (\u03c7 1, N =648   = 0.06, n.s.). There was also a sig- nifcant interaction between nation and entry method (\u03c7 2 =16.11, p < .001), as shown in Figure 9(b).(1, N =648)Figure 8: The average (a) total entry time and (b) emoji entry time of the two methods by nation (they were log- transformed in the analysis). Error bars represent 95% conf- dence intervals (CIs).The average emoji entry time for Vcoiemoji was 4.7 seconds, which was 91.2% shorter than the 53.7 seconds for the Apple iOS keyboard. We found a signifcant main efect of entry methodFrom the time distribution (Figure 10), we noticed that the users also entered certain emojis quickly with the iOS keyboard. The could be explained by several observations in the study: 1) partic- ipants used certain emojis more frequently (e.g., ) than others (e.g., ), so that they were familiar with the keywords of those emojis; 2) when using the emoji keyboard, some participants with residual vision entered the emojis faster as they could utilize the visual information to guide the search procedure.Taken together, then, our results for time make it clear that Voicemoji was signifcantly faster than the Apple iOS keyboard for entering text with emojis.", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_061.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Voicemoji: Emoji Entry Using Voice for Visually Impaired People", "pdf_hash": "7ad140271d649d31a0cd72fe4b56fb7febf56b69", "year": 2021, "venue": "CHI", "alt_text": "Two Barcharts. Y axis are total entry time and emoji entry time, x axis is the method, including ios keyboard and voicemoji. The results are described in section 6.2", "levels": [[1], [1], [0]], "corpus_id": 233987201, "sentences": ["Two Barcharts.", "Y axis are total entry time and emoji entry time, x axis is the method, including ios keyboard and voicemoji.", "The results are described in section 6.2"], "caption": "(\u03c7 2 = 513.55, p < .001). There was no statistically signif-", "local_uri": ["7ad140271d649d31a0cd72fe4b56fb7febf56b69_Image_062.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Making Mobile Augmented Reality Applications Accessible", "pdf_hash": "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8", "year": 2020, "venue": "ASSETS", "alt_text": "'Ikea Place' shows a virtual stool on the floor. 'Statue of Liberty AR' shows a virtual model of the Statue of Liberty", "levels": null, "corpus_id": 221368552, "sentences": ["'Ikea Place' shows a virtual stool on the floor. 'Statue of Liberty AR' shows a virtual model of the Statue of Liberty"], "caption": "Figure 1: Left: IKEA Place [28] allows users to view furniture in AR. Right: Statue of Liberty AR [58] displays historical facts along with to-scale models of the Statue of Liberty.", "local_uri": ["8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8_Image_003.jpg", "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Making Mobile Augmented Reality Applications Accessible", "pdf_hash": "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8", "year": 2020, "venue": "ASSETS", "alt_text": "Grid of 105 iPhone app icons arranged into five categories. Entertainment: 41 apps (39%). Education: 33 apps (31%). Retail: 17 apps (16%). Utility: 9 apps (9%). Other: 5 apps (5%).", "levels": null, "corpus_id": 221368552, "sentences": ["Grid of 105 iPhone app icons arranged into five categories.", "Entertainment: 41 apps (39%).", "Education: 33 apps (31%).", "Retail: 17 apps (16%).", "Utility: 9 apps (9%).", "Other: 5 apps (5%)."], "caption": "Figure 2: Icons for the 105 AR apps that we analyzed, organized by category.", "local_uri": ["8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Making Mobile Augmented Reality Applications Accessible", "pdf_hash": "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8", "year": 2020, "venue": "ASSETS", "alt_text": "Table that shows the 5 task categories that we identified and lists the tasks for each.", "levels": null, "corpus_id": 221368552, "sentences": ["Table that shows the 5 task categories that we identified and lists the tasks for each."], "caption": "", "local_uri": ["8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8_Image_006.png"], "annotated": false, "compound": false}
{"title": "Making Mobile Augmented Reality Applications Accessible", "pdf_hash": "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8", "year": 2020, "venue": "ASSETS", "alt_text": "Bar chart showing the amount of times each task was observed, broken down by app category.", "levels": [[1]], "corpus_id": 221368552, "sentences": ["Bar chart showing the amount of times each task was observed, broken down by app category."], "caption": "Figure 4: A breakdown of each constituent task we identifed and how frequently it appeared in each app category.", "local_uri": ["8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Vibrosight++: City-Scale Sensing Using Existing Retroreflective Signs and Markers", "pdf_hash": "c5134f7f6bc86cf2654aded683ff0902efc7cf57", "year": 2021, "venue": "CHI", "alt_text": "Two illustrations of the two typical micro-structures of retroreflective materials: beads and corner reflectors.", "levels": null, "corpus_id": 233987902, "sentences": ["Two illustrations of the two typical micro-structures of retroreflective materials: beads and corner reflectors."], "caption": "Figure 2: Two typical micro-structures of retrorefective ma- terials: beads and corner refectors.", "local_uri": ["c5134f7f6bc86cf2654aded683ff0902efc7cf57_Image_004.png"], "annotated": false, "compound": false}
{"title": "Vibrosight++: City-Scale Sensing Using Existing Retroreflective Signs and Markers", "pdf_hash": "c5134f7f6bc86cf2654aded683ff0902efc7cf57", "year": 2021, "venue": "CHI", "alt_text": "Four pictures that illustrate the reflector search pipeline. From top to bottom are: a depth map, a reflectance map, a reference image, and closeup photos of six example reflectors in the scene.", "levels": null, "corpus_id": 233987902, "sentences": ["Four pictures that illustrate the reflector search pipeline.", "From top to bottom are: a depth map, a reflectance map, a reference image, and closeup photos of six example reflectors in the scene."], "caption": "Figure 5: Depth map, refectance map, and reference image of a parking lot where we conducted the retrorefector search study. Green rectangles denote found retrofectors.", "local_uri": ["c5134f7f6bc86cf2654aded683ff0902efc7cf57_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Vibrosight++: City-Scale Sensing Using Existing Retroreflective Signs and Markers", "pdf_hash": "c5134f7f6bc86cf2654aded683ff0902efc7cf57", "year": 2021, "venue": "CHI", "alt_text": "Three spectrum plots of a 2Hz-2kHz swept-frequency signal, recorded signal from an accelerometer affixed to the speaker diaphragm, and from Vibrosight++.", "levels": [[1]], "corpus_id": 233987902, "sentences": ["Three spectrum plots of a 2Hz-2kHz swept-frequency signal, recorded signal from an accelerometer affixed to the speaker diaphragm, and from Vibrosight++."], "caption": "", "local_uri": ["c5134f7f6bc86cf2654aded683ff0902efc7cf57_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Vibrosight++: City-Scale Sensing Using Existing Retroreflective Signs and Markers", "pdf_hash": "c5134f7f6bc86cf2654aded683ff0902efc7cf57", "year": 2021, "venue": "CHI", "alt_text": "A series of pictures that illustrate the vibration sensing on a mailbox. Pictures on the left show the sensor setup, and two signal plots on the right show the signal collected from the reference accelerometer, and from Vibrosight++.", "levels": [[-1], [-1]], "corpus_id": 233987902, "sentences": ["A series of pictures that illustrate the vibration sensing on a mailbox.", "Pictures on the left show the sensor setup, and two signal plots on the right show the signal collected from the reference accelerometer, and from Vibrosight++."], "caption": "", "local_uri": ["c5134f7f6bc86cf2654aded683ff0902efc7cf57_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SIG: Making Maps Accessible and Putting Accessibility in Maps", "pdf_hash": "50366931e51fb5670cfcf7c934acc8b37cee312b", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "Screenshot of the website wheelmap.org showing a map with accessibility information for wheelchair users", "levels": null, "corpus_id": 5060600, "sentences": ["Screenshot of the website wheelmap.org showing a map with accessibility information for wheelchair users"], "caption": "Figure 1. Wheelmap.org collects and visualizes the wheelchair accessibility of POIs such as bus stops, restaurants, and public bathrooms. Image courtesy SOZIALHELDEN e.V.", "local_uri": ["50366931e51fb5670cfcf7c934acc8b37cee312b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SIG: Making Maps Accessible and Putting Accessibility in Maps", "pdf_hash": "50366931e51fb5670cfcf7c934acc8b37cee312b", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "Photograph of a visually impaired user interacting with the Augmented Reality Map", "levels": [[-1]], "corpus_id": 5060600, "sentences": ["Photograph of a visually impaired user interacting with the Augmented Reality Map"], "caption": "Figure 6. A visually impaired user exploring an interactive map for visually impaired people based on Augmented Reality [2] (Project VISTE http://www.visteproject.eu/).", "local_uri": ["50366931e51fb5670cfcf7c934acc8b37cee312b_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mediating Intimacy with DearBoard: a Co-Customizable Keyboard for Everyday Messaging", "pdf_hash": "df413788a95acf40e8aa40efde2fea6bf620fd20", "year": 2021, "venue": "CHI", "alt_text": "Four sequential snapshots of DearBoard, showing how Alice and Bob co-customize it. The first snapshot shows DearBoard with a yellow color theme and a shortcut to a thumbs-up emoji. The second snapshot shows that Alice adds two more emojis to the shortcuts, which Bob can also see. The third snapshot shows that the keyboard is now pink and there is a GIF menu open, which Bob is using to add three cat GIFs to the shortcuts. It also shows a menu for changing the color theme of the keyboard which includes a color palette and two buttons to select background color or text color modes. The fourth snapshot shows the keyboard after being co-customized, with a pink background and a toolbar of emojis and GIFs.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987786, "sentences": ["Four sequential snapshots of DearBoard, showing how Alice and Bob co-customize it.", "The first snapshot shows DearBoard with a yellow color theme and a shortcut to a thumbs-up emoji.", "The second snapshot shows that Alice adds two more emojis to the shortcuts, which Bob can also see.", "The third snapshot shows that the keyboard is now pink and there is a GIF menu open, which Bob is using to add three cat GIFs to the shortcuts.", "It also shows a menu for changing the color theme of the keyboard which includes a color palette and two buttons to select background color or text color modes.", "The fourth snapshot shows the keyboard after being co-customized, with a pink background and a toolbar of emojis and GIFs."], "caption": "", "local_uri": ["df413788a95acf40e8aa40efde2fea6bf620fd20_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mediating Intimacy with DearBoard: a Co-Customizable Keyboard for Everyday Messaging", "pdf_hash": "df413788a95acf40e8aa40efde2fea6bf620fd20", "year": 2021, "venue": "CHI", "alt_text": "A yellow keyboard with blue text and a toolbar of emojis and GIFs: a sad kid GIF, a crying emoji, a Beyonce GIF, a face-mask emoji, a crazy tongue-out face emoji, and a drag-queen GIF.", "levels": [[-1]], "corpus_id": 233987786, "sentences": ["A yellow keyboard with blue text and a toolbar of emojis and GIFs: a sad kid GIF, a crying emoji, a Beyonce GIF, a face-mask emoji, a crazy tongue-out face emoji, and a drag-queen GIF."], "caption": "Figure 2: The Grape friends co-custommized the expres- sion shortcuts with a frequent GIF meme, emojis related to their conversations about COVID-19, and GIFs represent- ing their shared love for Beyonc\u00e9 and Drag Race.", "local_uri": ["df413788a95acf40e8aa40efde2fea6bf620fd20_Image_009.jpg", "df413788a95acf40e8aa40efde2fea6bf620fd20_Image_012.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Mediating Intimacy with DearBoard: a Co-Customizable Keyboard for Everyday Messaging", "pdf_hash": "df413788a95acf40e8aa40efde2fea6bf620fd20", "year": 2021, "venue": "CHI", "alt_text": "A light-green keyboard with dark-green text and a toolbar of emojis: a musical figure emoji, a cake emoji, a confeti emoji, a sparks emoji, a fried chicken emoji, and a husky dog emoji.", "levels": [[-1]], "corpus_id": 233987786, "sentences": ["A light-green keyboard with dark-green text and a toolbar of emojis: a musical figure emoji, a cake emoji, a confeti emoji, a sparks emoji, a fried chicken emoji, and a husky dog emoji."], "caption": "Figure 4: The Berry friends used the toolbar to surprise each other; in this case, Berry-A set a birthday theme for Berry-B.", "local_uri": ["df413788a95acf40e8aa40efde2fea6bf620fd20_Image_019.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mediating Intimacy with DearBoard: a Co-Customizable Keyboard for Everyday Messaging", "pdf_hash": "df413788a95acf40e8aa40efde2fea6bf620fd20", "year": 2021, "venue": "CHI", "alt_text": "A keyboard with lime-green background, red text, and a toolbar of emojis: an alien emoji, a hospital emoji, a syringe emoji, and a crown emoji.", "levels": [[-1]], "corpus_id": 233987786, "sentences": ["A keyboard with lime-green background, red text, and a toolbar of emojis: an alien emoji, a hospital emoji, a syringe emoji, and a crown emoji."], "caption": "Figure 6: The Mango couple co-customized DearBoard to feature a COVID-19 conversation theme.", "local_uri": ["df413788a95acf40e8aa40efde2fea6bf620fd20_Image_021.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mediating Intimacy with DearBoard: a Co-Customizable Keyboard for Everyday Messaging", "pdf_hash": "df413788a95acf40e8aa40efde2fea6bf620fd20", "year": 2021, "venue": "CHI", "alt_text": "Left: A yellow keyboard with dark-blue text and a toolbar of emojis: an angel-face emoji, a laughing-with-tears emoji, a beach emoji, a face-with-sunglasses emoji, a heart-eyes emoji, and a unicorn emoji. Right: A purple keyboard with white text and a toolbar of emojis: a Shaka sign, a heart-eyes emoji, a broken-heart emoji, a red heart emoji, a raising-hands emoji, and a heart-kiss emoji.", "levels": [[-1], [-1]], "corpus_id": 233987786, "sentences": ["Left: A yellow keyboard with dark-blue text and a toolbar of emojis: an angel-face emoji, a laughing-with-tears emoji, a beach emoji, a face-with-sunglasses emoji, a heart-eyes emoji, and a unicorn emoji.", "Right: A purple keyboard with white text and a toolbar of emojis: a Shaka sign, a heart-eyes emoji, a broken-heart emoji, a red heart emoji, a raising-hands emoji, and a heart-kiss emoji."], "caption": "Figure 7: Left: The Apple friends split the ownership of DearBoard so that Apple-A was in charge of the expression shortcuts and Apple-B in charge of the color theme. Right: The Pear couple split the ownership of the expression shortcuts to have some for Pear-A and some for Pear-B, and some for both; their color theme was purple, their favorite color.", "local_uri": ["df413788a95acf40e8aa40efde2fea6bf620fd20_Image_023.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "pdf_hash": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "year": 2017, "venue": "UIST", "alt_text": "Shows a shape memory spring connected with a normal spring and joined with a hook at the end.", "levels": null, "corpus_id": 10597389, "sentences": ["Shows a shape memory spring connected with a normal spring and joined with a hook at the end."], "caption": "", "local_uri": ["4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "pdf_hash": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "year": 2017, "venue": "UIST", "alt_text": "Shows the shape memory alloy spring connected by a hook at the end without the normal spring.", "levels": null, "corpus_id": 10597389, "sentences": ["Shows the shape memory alloy spring connected by a hook at the end without the normal spring."], "caption": "(a)                        (b)                   (c)            (d)                   (e)            (f)", "local_uri": ["4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "pdf_hash": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "year": 2017, "venue": "UIST", "alt_text": "Three parallel shape memory alloy springs on wthe wrist with the polyester band underneath", "levels": null, "corpus_id": 10597389, "sentences": ["Three parallel shape memory alloy springs on wthe wrist with the polyester band underneath"], "caption": "", "local_uri": ["4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "pdf_hash": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "year": 2017, "venue": "UIST", "alt_text": "Shows that users rated MultiClench sensations between 4-7 on a scale from very uncomfortable to very comfortable. And between 1-4 on a scale of not annoying at all to very annoying.", "levels": [[3], [3]], "corpus_id": 10597389, "sentences": ["Shows that users rated MultiClench sensations between 4-7 on a scale from very uncomfortable to very comfortable.", "And between 1-4 on a scale of not annoying at all to very annoying."], "caption": "Figure 9: Comfort & annoyance boxplots for MultiClench", "local_uri": ["4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "pdf_hash": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "year": 2017, "venue": "UIST", "alt_text": "Bar graph depicting 75%, 45%, 80% accuracy for hold, continuous, staggered respectively for 30s. And 85%, 65%, 90% accuracy for hold, continuous, staggered, respectively for 60s.", "levels": [[1], [1]], "corpus_id": 10597389, "sentences": ["Bar graph depicting 75%, 45%, 80% accuracy for hold, continuous, staggered respectively for 30s.", "And 85%, 65%, 90% accuracy for hold, continuous, staggered, respectively for 60s."], "caption": "Figure 10: Mean Accuracy % for all three pulses for both durations. Continuous pulse is least accurate. [95% CI]", "local_uri": ["4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_015.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "pdf_hash": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "year": 2017, "venue": "UIST", "alt_text": "Consists of two panels. The first shows a loose, wrinkled band on the wrist. The second shows the band tightened onto the wrist after the SMA spring inside contracts.", "levels": null, "corpus_id": 10597389, "sentences": ["Consists of two panels.", "The first shows a loose, wrinkled band on the wrist.", "The second shows the band tightened onto the wrist after the SMA spring inside contracts."], "caption": "Figure 11: A loose bracelet squeezing into the skin", "local_uri": ["4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "PrivacyMic: Utilizing Inaudible Frequencies for Privacy Preserving Daily Activity Recognition", "pdf_hash": "47728a7d0b7cc0836abbe6ccde4720c52823f3e2", "year": 2021, "venue": "CHI", "alt_text": "5-pane figure showing A) PrivacyMic's hardware B) an FFT of unfiltered audio C) the Bode plot of the audible filter D) an FFT of filtered audio and E) PrivacyMic successfully classifying use of the sink", "levels": [[-1]], "corpus_id": 233987259, "sentences": ["5-pane figure showing A) PrivacyMic's hardware B) an FFT of unfiltered audio C) the Bode plot of the audible filter D) an FFT of filtered audio and E) PrivacyMic successfully classifying use of the sink"], "caption": "", "local_uri": ["47728a7d0b7cc0836abbe6ccde4720c52823f3e2_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PrivacyMic: Utilizing Inaudible Frequencies for Privacy Preserving Daily Activity Recognition", "pdf_hash": "47728a7d0b7cc0836abbe6ccde4720c52823f3e2", "year": 2021, "venue": "CHI", "alt_text": "An image of the wide-band capture apparatus showing 3 microphones and a webcam mounted on a stand.", "levels": [[-1]], "corpus_id": 233987259, "sentences": ["An image of the wide-band capture apparatus showing 3 microphones and a webcam mounted on a stand."], "caption": "Figure 2: An image of the wide-band audio capture rig, con- sisting of infrasound, audible, and ultrasonic microphones, along with a camera. The set-up is used to capture sounds from 127 diferent objects and devices commonly found in residential homes and commercial buildings. The data is used to determine which frequency components are most predictive of object and device usage.", "local_uri": ["47728a7d0b7cc0836abbe6ccde4720c52823f3e2_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PrivacyMic: Utilizing Inaudible Frequencies for Privacy Preserving Daily Activity Recognition", "pdf_hash": "47728a7d0b7cc0836abbe6ccde4720c52823f3e2", "year": 2021, "venue": "CHI", "alt_text": "The distance response curves for PrivacyMic's ultrasonic microphone, showing effectiveness past 15m.", "levels": null, "corpus_id": 233987259, "sentences": ["The distance response curves for PrivacyMic's ultrasonic microphone, showing effectiveness past 15m."], "caption": "", "local_uri": ["47728a7d0b7cc0836abbe6ccde4720c52823f3e2_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Modeling Two Dimensional Touch Pointing", "pdf_hash": "ade438b208f4146ce0c28af0ab33e701cfc2dc44", "year": 2020, "venue": "UIST", "alt_text": "Figure 1 is an illustration of the first option of defining amplitude and direction constraints by using nominal width (x-length) and height (y-length) to define amplitude (W) and directional (H) constraints. First, in vertical movement direction, y-length is W and x-length is H. Second, in horizontal movement direction, x-length is W and y-length is H. Third, in angled movement direction, if the direction falls within the grey area, x-length is W and y-length is H; if the direction is within the white area, y-length is W and x-length is H.", "levels": null, "corpus_id": 222799793, "sentences": ["Figure 1 is an illustration of the first option of defining amplitude and direction constraints by using nominal width (x-length) and height (y-length) to define amplitude (W) and directional (H) constraints.", "First, in vertical movement direction, y-length is W and x-length is H. Second, in horizontal movement direction, x-length is W and y-length is H. Third, in angled movement direction, if the direction falls within the grey area, x-length is W and y-length is H; if the direction is within the white area, y-length is W and x-length is H."], "caption": "", "local_uri": ["ade438b208f4146ce0c28af0ab33e701cfc2dc44_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Modeling Two Dimensional Touch Pointing", "pdf_hash": "ade438b208f4146ce0c28af0ab33e701cfc2dc44", "year": 2020, "venue": "UIST", "alt_text": "Figure 2 is an illustration of the second option of defining amplitude and direction constraints. It defines apparent width along the pointing direction and apparent height which is perpendicular to the pointing direction. Apparent width is the amplitude constraint and apparent height is the directional constraints.", "levels": null, "corpus_id": 222799793, "sentences": ["Figure 2 is an illustration of the second option of defining amplitude and direction constraints.", "It defines apparent width along the pointing direction and apparent height which is perpendicular to the pointing direction.", "Apparent width is the amplitude constraint and apparent height is the directional constraints."], "caption": "Figure 2: A illustration of option 2: using apparent width (in blue) and height (in green) to de\ufb01ne amplitude (W ) and directional (H) constraints.", "local_uri": ["ade438b208f4146ce0c28af0ab33e701cfc2dc44_Image_019.png"], "annotated": false, "compound": false}
{"title": "Modeling Two Dimensional Touch Pointing", "pdf_hash": "ade438b208f4146ce0c28af0ab33e701cfc2dc44", "year": 2020, "venue": "UIST", "alt_text": "Figure 3 is an illustration of the experimental setting. It shows 3 possible movement distances. Theta is the angle between movement direction and the x-axis of the screen coordinate system. Two targets are displayed on the screen: the starting rectangle which is shown in blue color and the target rectangle which is shown in red color.", "levels": null, "corpus_id": 222799793, "sentences": ["Figure 3 is an illustration of the experimental setting.", "It shows 3 possible movement distances.", "Theta is the angle between movement direction and the x-axis of the screen coordinate system.", "Two targets are displayed on the screen: the starting rectangle which is shown in blue color and the target rectangle which is shown in red color."], "caption": "", "local_uri": ["ade438b208f4146ce0c28af0ab33e701cfc2dc44_Image_024.jpg"], "annotated": false, "compound": false}
{"title": "Modeling Two Dimensional Touch Pointing", "pdf_hash": "ade438b208f4146ce0c28af0ab33e701cfc2dc44", "year": 2020, "venue": "UIST", "alt_text": "Figure 4 shows a participant in the study on the left, and a screenshot of the task on the right.", "levels": null, "corpus_id": 222799793, "sentences": ["Figure 4 shows a participant in the study on the left, and a screenshot of the task on the right."], "caption": "Figure 4: Left: a participant in the study. Right: a screenshot of the task.", "local_uri": ["ade438b208f4146ce0c28af0ab33e701cfc2dc44_Image_025.jpg", "ade438b208f4146ce0c28af0ab33e701cfc2dc44_Image_026.jpg"], "annotated": false, "compound": true}
{"title": "Real-Time Depth-Camera Based Hand Tracking for ASL Recognition", "pdf_hash": "d52e19ab3da615b4bb58dd79508465894ad14264", "year": 2017, "venue": "ASSETS", "alt_text": "A Confusion Matrix indicating the classification accuracy for each letter. None are perfectly classified. D, K, W, C are correctly classified 90% of the time. B, E, I, L, X, Y, O, P are correctly classified 80% of the time. A, F, M, N, R, G are classified 70% of the time. S, V are correctly classified 60% of the time. T is correctly classified 40% of the time. U is correctly classified 30% of the time. H and Q are correctly classified 20% of the time.", "levels": [[1], [1], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 8121994, "sentences": ["A Confusion Matrix indicating the classification accuracy for each letter.", "None are perfectly classified.", "D, K, W, C are correctly classified 90% of the time.", "B, E, I, L, X, Y, O, P are correctly classified 80% of the time.", "A, F, M, N, R, G are classified 70% of the time.", "S, V are correctly classified 60% of the time.", "T is correctly classified 40% of the time.", "U is correctly classified 30% of the time.", "H and Q are correctly classified 20% of the time."], "caption": "Figure 1. The Confusion Matrix for ASL Alphabet classification. The true class is shown on the y-axis, with the predicted class shown along the x-axis. Each participant recorded each letter once, so the rows will sum to 10. The \u2018G\u2019 and \u2018H\u2019 conditions are an exception due to a recording error.", "local_uri": ["d52e19ab3da615b4bb58dd79508465894ad14264_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Prepare for Trouble and Make It Double: The Power Motive Predicts Pok\u00e9mon Choices Based on Apparent Strength", "pdf_hash": "8e5369231248e20f2d1ac5c58c278480467c2130", "year": 2021, "venue": "CHI", "alt_text": "This figure shows three Pok\u00e9mon next to each other. The first Pok\u00e9mon was rated as cute, the second as strong-looking and the third as neither strong nor cute.", "levels": null, "corpus_id": 233987808, "sentences": ["This figure shows three Pok\u00e9mon next to each other.", "The first Pok\u00e9mon was rated as cute, the second as strong-looking and the third as neither strong nor cute."], "caption": "", "local_uri": ["8e5369231248e20f2d1ac5c58c278480467c2130_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Towards Recommending Accessibility Features on Mobile Devices", "pdf_hash": "4b84139348822a6a87ee9e1b43627db6d18b5818", "year": 2020, "venue": "ASSETS", "alt_text": "Four screenshots each showing recommendations being surfaced. On the far left, we show a push notification based recommendation. The center left shows a home screen widget that contains a recommendation. The center right shows a setting recommended as a search suggestion. The far right shows a web browser with a pop up bubble for a feature recommendation.", "levels": null, "corpus_id": 225954331, "sentences": ["Four screenshots each showing recommendations being surfaced.", "On the far left, we show a push notification based recommendation.", "The center left shows a home screen widget that contains a recommendation.", "The center right shows a setting recommended as a search suggestion.", "The far right shows a web browser with a pop up bubble for a feature recommendation."], "caption": "Figure 1: Many smartphone users are not aware of accessibil- ity features that they could beneft from. We explored meth- ods for detecting accessibility needs and surfacing them to the user.", "local_uri": ["4b84139348822a6a87ee9e1b43627db6d18b5818_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Demonstrating fastForce: Real-Time Reinforcement of Laser-Cut Structures", "pdf_hash": "eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Series of images showing how fast force reinforces a wheelbarrow handle. The first image show the handle breaking under heavy load. The next pair depicts fast force automatically reinforcing the handle while the user models it in kyub. The last image shows the complete wheelbarrow with seven different reinforced elements.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987021, "sentences": ["Series of images showing how fast force reinforces a wheelbarrow handle.", "The first image show the handle breaking under heavy load.", "The next pair depicts fast force automatically reinforcing the handle while the user models it in kyub.", "The last image shows the complete wheelbarrow with seven different reinforced elements."], "caption": "Figure 1: (a) The handle of this laser-cut wheelbarrow breaks, when the user is trying to lift a heavy load. (b) Our software tool fastForce addresses this. The moment the user clicks in the editor to create the handle, (c) not only the handle appears, but fastForce also detects the resulting point of failure and resolves it in real-time by extending the lower plate of the handle into the internal space of the wheelbarrow. (d) The resulting model has the same outer shape as the original model, yet allows lifting loads that are 45x times higher. Note that fastForce has detected additional points of failure (black arrows 2-7) and has generated corresponding reinforcement (white 2-7).", "local_uri": ["eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Demonstrating fastForce: Real-Time Reinforcement of Laser-Cut Structures", "pdf_hash": "eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "a pair of images showing the snowboarder sculpture. The one on the right uses arrows to show four possible ways that the model can break.", "levels": null, "corpus_id": 233987021, "sentences": ["a pair of images showing the snowboarder sculpture.", "The one on the right uses arrows to show four possible ways that the model can break."], "caption": "", "local_uri": ["eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Demonstrating fastForce: Real-Time Reinforcement of Laser-Cut Structures", "pdf_hash": "eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "a series of pictures depicting guitar stands. The first two show different designs made by the user. The third one shows that the preferred design is automatically reinforced by fast force. The last two show iterations made by the user on the reinforced design.", "levels": null, "corpus_id": 233987021, "sentences": ["a series of pictures depicting guitar stands.", "The first two show different designs made by the user.", "The third one shows that the preferred design is automatically reinforced by fast force.", "The last two show iterations made by the user on the reinforced design."], "caption": "Figure 3: (a) The user\u2019s preferred guitar stand design competes with (b) their backup design. (c) The preferred model shows no warnings (and fipping it around shows why: it has already been reinforced automatically using our system fastForce). (d)This allows the user to drop the fallback design and focus the design exploration on the preferred design.", "local_uri": ["eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Demonstrating fastForce: Real-Time Reinforcement of Laser-Cut Structures", "pdf_hash": "eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "a series of images showing the steps a user takes to make a wall mount. The third image shows fast force automatically reinforcing the object during modelling. The fifth image shows that fast force switches the reinforce to allow solve assemblibility. The last image shows lasercut wall mount supporting a bench.", "levels": null, "corpus_id": 233987021, "sentences": ["a series of images showing the steps a user takes to make a wall mount.", "The third image shows fast force automatically reinforcing the object during modelling.", "The fifth image shows that fast force switches the reinforce to allow solve assemblibility.", "The last image shows lasercut wall mount supporting a bench."], "caption": "Figure 4: User session showing the creation of a wall mount using Kyub with fastForce integration.", "local_uri": ["eb9223eb5d5b5d17e6c2b6bcfefd26ee1eb33ce2_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Vicariously Experiencing it all Without Going Outside", "pdf_hash": "b00d390a1788c63cd1dc6e3aa18666b6fb5e5de0", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Fig. 1. Screenshots of outdoor livestreams on (a) Douyu.tv and (b) Douyin (TikTok). Note that both livestreams have (i) a number of Danmaku comments and display (ii) virtual gifts that have been sent to the streamer alongside Danmaku conversations that are ongoing.", "levels": null, "corpus_id": 207959098, "sentences": ["Fig. 1.", "Screenshots of outdoor livestreams on (a) Douyu.tv and (b) Douyin (TikTok).", "Note that both livestreams have (i) a number of Danmaku comments and display (ii) virtual gifts that have been sent to the streamer alongside Danmaku conversations that are ongoing."], "caption": "Fig. 1. Screenshots of outdoor livestreams on (a) Douyu.tv and (b) Douyin (TikTok). Note that both livestreams have (i) a number of Danmaku comments and display (ii) virtual gifts that have been sent to the streamer alongside Danmaku conversations that are ongoing.", "local_uri": ["b00d390a1788c63cd1dc6e3aa18666b6fb5e5de0_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Opportunities and Challenges of Text Input in Portable Virtual Reality", "pdf_hash": "4dfa509d0b89dbcb7ccd2e1ea23288345b3c04ee", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Teaser image, A user copy editing text in a relaxing virtual world provided by a portable HMD setup.", "levels": null, "corpus_id": 218482738, "sentences": ["Teaser image, A user copy editing text in a relaxing virtual world provided by a portable HMD setup."], "caption": "Figure 1: User copy editing text in a relaxing virtual world provided by a portable HMD setup.", "local_uri": ["4dfa509d0b89dbcb7ccd2e1ea23288345b3c04ee_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "Interaction with a dual-observable input system: (1) An always-available input system is initially untrained, requiring per person per gesture training demonstrations. (2) A person wears the always-available device during normal activities. (3) Normal interactions with other devices collect labeled training demonstrations for the always-available input system. (4) The always-available system is available for interaction, but also continues to learn.", "levels": null, "corpus_id": 1331359, "sentences": ["Interaction with a dual-observable input system: (1) An always-available input system is initially untrained, requiring per person per gesture training demonstrations.", "(2) A person wears the always-available device during normal activities.", "(3) Normal interactions with other devices collect labeled training demonstrations for the always-available input system.", "(4) The always-available system is available for interaction, but also continues to learn."], "caption": "Figure 1: Interaction with a dual-observable input system: (1) An always-available input system is initially untrained, requiring per person per gesture training demonstrations. (2) A person wears the always-available device during normal activities. (3) Normal interactions with other devices collect labeled training demonstrations for the always-available input system. (4) The always-available system is available for interaction, but also continues to learn.", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "Our prototype hardware, a custom EMG board with dry EMG electrodes and Bluetooth communication. The setup is fully mobile and can be worn relatively comfortably for a significant period of time.", "levels": null, "corpus_id": 1331359, "sentences": ["Our prototype hardware, a custom EMG board with dry EMG electrodes and Bluetooth communication.", "The setup is fully mobile and can be worn relatively comfortably for a significant period of time."], "caption": "RELATED WORK", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "We collect left swipe, right swipe, tap, and long press demonstrations from the Android launcher (left). We collect complex user-defined gestures from the Android lock screen (right).", "levels": null, "corpus_id": 1331359, "sentences": ["We collect left swipe, right swipe, tap, and long press demonstrations from the Android launcher (left).", "We collect complex user-defined gestures from the Android lock screen (right)."], "caption": "Figure 3: We collect left swipe, right swipe, tap, and long press demonstrations from the Android launcher (left). We collect complex user-de\ufb01ned gestures from the Android lock screen (right).", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "Gestures are classified by: (1) Considering the past 2 seconds of signal from each EMG channel. (2) Filtering and smoothing the signals. (3) Comparing against all alignments to training demonstrations using normalized cross correlation. (4) Using KNN to choose a class.", "levels": null, "corpus_id": 1331359, "sentences": ["Gestures are classified by: (1) Considering the past 2 seconds of signal from each EMG channel. (2) Filtering and smoothing the signals.", "(3) Comparing against all alignments to training demonstrations using normalized cross correlation.", "(4) Using KNN to choose a class."], "caption": "Figure 4: Gestures are classi\ufb01ed by: (1) Considering the past 2 seconds of signal from each EMG channel. (2) Filtering and smoothing the signals. (3) Comparing against all alignments to training demonstrations using normalized cross correlation. (4) Using KNN to choose a class.", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "Recognition accuracy continues to improve with data from additional sessions. Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions.", "levels": [[3], [4]], "corpus_id": 1331359, "sentences": ["Recognition accuracy continues to improve with data from additional sessions.", "Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions."], "caption": "", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3, 4], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "ThumbsUp has an overall 82.9% accuracy at classifying left swipe, right swipe, tap, and long press thumb gestures.", "levels": null, "corpus_id": 1331359, "sentences": ["ThumbsUp has an overall 82.9% accuracy at classifying left swipe, right swipe, tap, and long press thumb gestures."], "caption": "Figure 6: ThumbsUp has an overall 82.9% accuracy at classifying left swipe, right swipe, tap, and long press thumb gestures.", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "pdf_hash": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "year": 2015, "venue": "UIST", "alt_text": "Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations.", "levels": null, "corpus_id": 1331359, "sentences": ["Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations."], "caption": "Figure 7: Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations.", "local_uri": ["f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "TactiHelm: Tactile Feedback in a Cycling Helmet for Collision Avoidance", "pdf_hash": "e32cc19533e0278e7ea12b83df544992d460ee53", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figures depicts how tactihelm was built with three pictures. The first picture shows the back of the helmet with an Arduino board, the bluetooth module, a battery and all the circuitry used to trigger the tactile actuators. In the second picture, the actuators are shown and hilighted inside the helmet. The final picture shows a participant cycling wearing the helmet prototype.", "levels": null, "corpus_id": 233987261, "sentences": ["The figures depicts how tactihelm was built with three pictures.", "The first picture shows the back of the helmet with an Arduino board, the bluetooth module, a battery and all the circuitry used to trigger the tactile actuators.", "In the second picture, the actuators are shown and hilighted inside the helmet.", "The final picture shows a participant cycling wearing the helmet prototype."], "caption": "Figure 1: The TactiHelm prototype. It has 4 tactile actuators located at each of cardinal directions inside the helmet. The actuator is driven by an Arduino microcontroller that can be controlled remotely via Bluetooth. The rightmost picture shows a participant wearing TactiHelm during the evaluation.", "local_uri": ["e32cc19533e0278e7ea12b83df544992d460ee53_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces", "pdf_hash": "102e57022824bf46970b7978999eddd3f4e27949", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3: \"Exploded view of the hot-end mount design showing all of the components. A second photo shows the fabricated and assembled hot-end on a 3D printer.\"", "levels": null, "corpus_id": 218482638, "sentences": ["Figure 3: \"Exploded view of the hot-end mount design showing all of the components.", "A second photo shows the fabricated and assembled hot-end on a 3D printer.\""], "caption": "Figure 3: Component view of our hot-end mount design showing the PC4-M6 connector threaded into the hot-end. (A). Our hydrogel", "local_uri": ["102e57022824bf46970b7978999eddd3f4e27949_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces", "pdf_hash": "102e57022824bf46970b7978999eddd3f4e27949", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 5: \"Graphic showing the different parameters of the actuator design space. Hydrogel can be controlled through concentration and pattern. Textile can be controlled through type, shape, and grain orientation (on woven textiles). Hydration state can be either dry or wet.\"", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218482638, "sentences": ["Figure 5: \"Graphic showing the different parameters of the actuator design space.", "Hydrogel can be controlled through concentration and pattern.", "Textile can be controlled through type, shape, and grain orientation (on woven textiles).", "Hydration state can be either dry or wet.\""], "caption": "", "local_uri": ["102e57022824bf46970b7978999eddd3f4e27949_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces", "pdf_hash": "102e57022824bf46970b7978999eddd3f4e27949", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 6: \"Example of a woven textile showing the straight grain parallel to the selvage; the cross grain perpendicular to the selvage; and the bias along the diagonal.\"", "levels": null, "corpus_id": 218482638, "sentences": ["Figure 6: \"Example of a woven textile showing the straight grain parallel to the selvage; the cross grain perpendicular to the selvage; and the bias along the diagonal.\""], "caption": "", "local_uri": ["102e57022824bf46970b7978999eddd3f4e27949_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces", "pdf_hash": "102e57022824bf46970b7978999eddd3f4e27949", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 7: \"Results of different circular pieces of textile can actuate in different ways when the hydrogel dries based on the pattern. One is curled up like a taco (printed with concentric circle patterns), another is in the shape of a rose petal (printed with horizontal lines) and a third looks like a cone shape (concentric semi-circle pattern on one side of the circular shape).\"", "levels": null, "corpus_id": 218482638, "sentences": ["Figure 7: \"Results of different circular pieces of textile can actuate in different ways when the hydrogel dries based on the pattern.", "One is curled up like a taco (printed with concentric circle patterns), another is in the shape of a rose petal (printed with horizontal lines) and a third looks like a cone shape (concentric semi-circle pattern on one side of the circular shape).\""], "caption": "", "local_uri": ["102e57022824bf46970b7978999eddd3f4e27949_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces", "pdf_hash": "102e57022824bf46970b7978999eddd3f4e27949", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 11: \"Three images showing a weather-responsive arrow. The first shows the arrow with dried hydrogel pointing up into the air. The second shows the arrow hydrated and now it has transformed to point 90-degrees to the left. The final image shows the printed hydrogel pattern consisting of horizontal lines along the shape of the arrow.\"", "levels": null, "corpus_id": 218482638, "sentences": ["Figure 11: \"Three images showing a weather-responsive arrow.", "The first shows the arrow with dried hydrogel pointing up into the air.", "The second shows the arrow hydrated and now it has transformed to point 90-degrees to the left.", "The final image shows the printed hydrogel pattern consisting of horizontal lines along the shape of the arrow.\""], "caption": "", "local_uri": ["102e57022824bf46970b7978999eddd3f4e27949_Image_017.png"], "annotated": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "Large teaser figure showcasing the three high-level stages of ArticuLev in three separate columns: Analyze, Assemble and Animate.  Analyze: Threads and cloth on the levitator ground, a \"target shape\" overlay to show how those should be assembled in the air.  Assemble: Physically lifting the levitation primitives (threads, cloth), joining them in mid-air, bringing them to target pose.  Animate: Projection mapping on the cloth with the user's face.", "levels": null, "corpus_id": 233987469, "sentences": ["Large teaser figure showcasing the three high-level stages of ArticuLev in three separate columns: Analyze, Assemble and Animate.  Analyze: Threads and cloth on the levitator ground, a \"target shape\" overlay to show how those should be assembled in the air.", "Assemble: Physically lifting the levitation primitives (threads, cloth), joining them in mid-air, bringing them to target pose.", "Animate: Projection mapping on the cloth with the user's face."], "caption": "Figure 1: ArticuLev provides an integrated pipeline for the identifcation, assembly and mid-air placement of shape primitives. The developer specifes the target structure of primitives required (\"Target shape\", green). The pipeline automatically matches existing primitives to the intended shape (Analyze). After lifting the primitives, ArticuLev joins them and manipulates the shape in mid-air to match the target pose (Assemble). The levitated shapes can be programmed and manipulated in real-time (Animate) and easily combined with input/output devices (e.g., Microsof Kinect and projectors).", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "Examples from previous literature that use different materials for levitation. We categorize them into Beads, Threads, Cloth and PoV.", "levels": null, "corpus_id": 233987469, "sentences": ["Examples from previous literature that use different materials for levitation.", "We categorize them into Beads, Threads, Cloth and PoV."], "caption": "", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_004.jpg", "ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_005.jpg", "ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_006.jpg", "ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "Setup of our pipeline implementation including camera placement and measurements.", "levels": null, "corpus_id": 233987469, "sentences": ["Setup of our pipeline implementation including camera placement and measurements."], "caption": "\u00d7", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "The visual interface for the developer as a Unity plugin to create target shapes. The figure contains the dataflow, a screenshot of the plugin in the Unity scene view and a photo of the resulting application.", "levels": [[-1], [-1]], "corpus_id": 233987469, "sentences": ["The visual interface for the developer as a Unity plugin to create target shapes.", "The figure contains the dataflow, a screenshot of the plugin in the Unity scene view and a photo of the resulting application."], "caption": "Beads & connections            Hierarchy\u200c", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "Example for a set of beads and hierarchical connections specified by the developer. There are five beads in the depiction, which are connected to form a \"face\" in the middle and two \"arms\".", "levels": null, "corpus_id": 233987469, "sentences": ["Example for a set of beads and hierarchical connections specified by the developer.", "There are five beads in the depiction, which are connected to form a \"face\" in the middle and two \"arms\"."], "caption": "", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "The hierarchy of the connections with the root, the \"face\" as child of the root, and the two \"arms\" as children of the \"face\"", "levels": null, "corpus_id": 233987469, "sentences": ["The hierarchy of the connections with the root, the \"face\" as child of the root, and the two \"arms\" as children of the \"face\""], "caption": "", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "A plot of the stiffness of traps that approach each other horizontally versus vertically. The plots (top) show that the stiffness varies considerably in the vertical case, whereas horizontally merged traps have relatively stable stiffness along the way. The bottom part shows the pressure fields during the motion until they are merged (rightmost).", "levels": [[1], [3], [1]], "corpus_id": 233987469, "sentences": ["A plot of the stiffness of traps that approach each other horizontally versus vertically.", "The plots (top) show that the stiffness varies considerably in the vertical case, whereas horizontally merged traps have relatively stable stiffness along the way.", "The bottom part shows the pressure fields during the motion until they are merged (rightmost)."], "caption": "", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_024.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "Comparing how threads behave when moving them in an articulated shape as one connected prop (left, all beads are connected, in particular, the thread is connected to the same bead as the cloth) versus two separate primitives (right, the thread primitive consists of two beads, whereas one is joined with a bead of the cloth).", "levels": null, "corpus_id": 233987469, "sentences": ["Comparing how threads behave when moving them in an articulated shape as one connected prop (left, all beads are connected, in particular, the thread is connected to the same bead as the cloth) versus two separate primitives (right, the thread primitive consists of two beads, whereas one is joined with a bead of the cloth)."], "caption": "", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_026.jpg"], "annotated": false, "compound": false}
{"title": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "pdf_hash": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "year": 2021, "venue": "CHI", "alt_text": "Results of our evaluation. The top row shows simplified sketches of the test applications. From left to right: six beads, a thread with three beads plus two separate beads, a square piece of cloth (four beads) and two separate beads, two threads (one with 3 beads, one with 2 beads, merged at one of their beads), A cloth triangle and a thread with two beads (joined at one bead), a cloth square and a cloth triangle (joined at two beads)", "levels": [[-1], [-1], [-1]], "corpus_id": 233987469, "sentences": ["Results of our evaluation.", "The top row shows simplified sketches of the test applications.", "From left to right: six beads, a thread with three beads plus two separate beads, a square piece of cloth (four beads) and two separate beads, two threads (one with 3 beads, one with 2 beads, merged at one of their beads), A cloth triangle and a thread with two beads (joined at one bead), a cloth square and a cloth triangle (joined at two beads)"], "caption": "Table 1: Summary of results obtained from our evaluation. Columns represent each of the target shapes tested, while the rows represent shape input parameters (beads and con- nections) and measured parameters for each stage. All per- centages in the stage success rates are in relation to the total amount of trials independent of the success of the previous stage, i.e., they can only remain equal or get lower in subse- quent stages.", "local_uri": ["ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_027.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Assets 2015", "pdf_hash": "ee75abc3266a555aa000a25d272939b42390ce0e", "year": 2016, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "Jon Schull giving his keynote speech. A 3D-printed prosthetic hand is shown in the slides. An ASL interpreter is right beside him.", "levels": null, "corpus_id": 20486134, "sentences": ["Jon Schull giving his keynote speech.", "A 3D-printed prosthetic hand is shown in the slides.", "An ASL interpreter is right beside him."], "caption": "", "local_uri": ["ee75abc3266a555aa000a25d272939b42390ce0e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Making the web easier to see with opportunistic accessibility improvement", "pdf_hash": "2edca4294ee9d6c7c9d7e13fb24185c93fecadeb", "year": 2014, "venue": "UIST", "alt_text": "A large computer screen with highly-magnified text is shown. A user, partially out of the photo, is sitting close to the screen pointing at something on the screen.", "levels": null, "corpus_id": 17268873, "sentences": ["A large computer screen with highly-magnified text is shown.", "A user, partially out of the photo, is sitting close to the screen pointing at something on the screen."], "caption": "Figure 1. People with low vision use a combination of magni\ufb01cation soft\u00ad ware and physical accommodations like positioning themselves closer to the screen. Opportunistic accessibility is about improving accessibility when possible without adverse side effects, even if to a small degree.", "local_uri": ["2edca4294ee9d6c7c9d7e13fb24185c93fecadeb_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Making the web easier to see with opportunistic accessibility improvement", "pdf_hash": "2edca4294ee9d6c7c9d7e13fb24185c93fecadeb", "year": 2014, "venue": "UIST", "alt_text": "The UIST 2014 web page is shown before and after magnification. While the magnification achieved was only 1.3x, the difference is small but noticeable.", "levels": null, "corpus_id": 17268873, "sentences": ["The UIST 2014 web page is shown before and after magnification.", "While the magnification achieved was only 1.3x, the difference is small but noticeable."], "caption": "1.0x", "local_uri": ["2edca4294ee9d6c7c9d7e13fb24185c93fecadeb_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 1 shows the main concept of our work, using a tilt sensor on a digital drawing board to support continuous sensor-driven transitions. These include interface dualities such as reading/writing, public/private, and person-space/task-space based on the angle of the display. A \"Tilt Transfer Function\" transforms the raw input data for these applications, and a \"Tilt Side-Channel menu\" surfaces generic cross-application commands such as clutching to temporarily disengage tilt sensing from application responses, if desired.", "levels": [[-1], [-1], [-1]], "corpus_id": 222805010, "sentences": ["Figure 1 shows the main concept of our work, using a tilt sensor on a digital drawing board to support continuous sensor-driven transitions.", "These include interface dualities such as reading/writing, public/private, and person-space/task-space based on the angle of the display.", "A \"Tilt Transfer Function\" transforms the raw input data for these applications, and a \"Tilt Side-Channel menu\" surfaces generic cross-application commands such as clutching to temporarily disengage tilt sensing from application responses, if desired."], "caption": "Figure 1. Tilting a digital drawing board from vertical to a low- angle posture transforms the current app\u2019s user experience via continuous, interactive, sensor-driven transitions.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 2 has three columns. The leftmost column shows the main display states that we consider (vertical, diagonal states in-between, and a low-angled state of 20 degrees which matches the lowest tilt angle supported by the Surface Studio). The other two columns illustrate two application scenarios. In the middle column, we see how a Document application transitions from reading/editing text (when vertical) to annotating a document, with pen tools coming to the fore, when tilted down. The rightmost column shows a Lock Screen that fades from a public view prominently showing only the time and date, to a more personal view that includes notifications and calendar details of upcoming meetings. Behind this, as a sort of interactive screen-saver, the lock screen shows a slow motion time-lapse of red and yellow clouds that drift upwards (or retract downwards) in correspondence with the tilting motion.", "levels": null, "corpus_id": 222805010, "sentences": ["Figure 2 has three columns.", "The leftmost column shows the main display states that we consider (vertical, diagonal states in-between, and a low-angled state of 20 degrees which matches the lowest tilt angle supported by the Surface Studio).", "The other two columns illustrate two application scenarios.", "In the middle column, we see how a Document application transitions from reading/editing text (when vertical) to annotating a document, with pen tools coming to the fore, when tilted down.", "The rightmost column shows a Lock Screen that fades from a public view prominently showing only the time and date, to a more personal view that includes notifications and calendar details of upcoming meetings.", "Behind this, as a sort of interactive screen-saver, the lock screen shows a slow motion time-lapse of red and yellow clouds that drift upwards (or retract downwards) in correspondence with the tilting motion."], "caption": "Figure 2. Left: The Document app shifts from Reading to Writing annotations, based on posture. Right: At sign-in, the Lock Screen transitions from Public to more Personal views.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 4 shows the \"App Bar\" which is used to switch between the different applications in our prototype window manager, simply by tapping an icon via direct touch. This app bar appears at the bottom center of the screen.", "levels": [[-1], [-1]], "corpus_id": 222805010, "sentences": ["Figure 4 shows the \"App Bar\" which is used to switch between the different applications in our prototype window manager, simply by tapping an icon via direct touch.", "This app bar appears at the bottom center of the screen."], "caption": "Figure 4. The App Bar (at bottom center of screen) lets users tap to switch between various task scenarios in our prototype.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 5 illustrates the variable gain function used by our Tilt Transfer Function. It has central deadband (zero gain) with an ease-in/ease-out function providing variable gain for small movements. For larger movements, the movement reverts to absolute control, i.e. a 1:1 gain.", "levels": null, "corpus_id": 222805010, "sentences": ["Figure 5 illustrates the variable gain function used by our Tilt Transfer Function.", "It has central deadband (zero gain) with an ease-in/ease-out function providing variable gain for small movements.", "For larger movements, the movement reverts to absolute control, i.e. a 1:1 gain."], "caption": "Figure 5. Our Tilt Transfer Function uses deadband, relative, and absolute control regions depending on change in angle \u0394\u03b8.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 9 shows the Teleconference (middle column) and Maps (right column) applications. The left column is a header column, identifying the vertical, diagonal, and low-angled states of each experience. The Teleconf app has person-space when vertical, and cross-fades to a shared document that can be marked-up with a pen in \"task space\" when the screen tilts down to a low angle. The Maps application shows a city skyline view when vertical, transitions through intermediate perspectives as the display tilts, and finally shows a straight-overhead view when the screen is angled all the way down.", "levels": null, "corpus_id": 222805010, "sentences": ["Figure 9 shows the Teleconference (middle column) and Maps (right column) applications.", "The left column is a header column, identifying the vertical, diagonal, and low-angled states of each experience.", "The Teleconf app has person-space when vertical, and cross-fades to a shared document that can be marked-up with a pen in \"task space\" when the screen tilts down to a low angle.", "The Maps application shows a city skyline view when vertical, transitions through intermediate perspectives as the display tilts, and finally shows a straight-overhead view when the screen is angled all the way down."], "caption": "Figure 9. Left: The Teleconf App transitions between Person Space (for face-to-face video) and Task Space (for mark-up of shared content, at a low screen angle). Right: Maps transitions between Skyline (vertical) and Overhead (low-angle) views.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 10 shows the Sticky-Notes and Presentation apps. For Sticky-Notes, in the low angle, the user has a workspace where they can leave yellow stickies in somewhat messy fashion all over the screen. Tilting to vertical hides some stickies and tidies others in a compact representation at the lower corner of the screen, allowing quick transition to a tidy workspace if an unexpected guest (like your boss) walks into your office. The Presentation app transitions from an authoring view (at a low angle) to a presentation view when vertical. In the presentation view the pen can be used to spotlight pieces of content on the current slide.", "levels": null, "corpus_id": 222805010, "sentences": ["Figure 10 shows the Sticky-Notes and Presentation apps.", "For Sticky-Notes, in the low angle, the user has a workspace where they can leave yellow stickies in somewhat messy fashion all over the screen.", "Tilting to vertical hides some stickies and tidies others in a compact representation at the lower corner of the screen, allowing quick transition to a tidy workspace if an unexpected guest (like your boss) walks into your office.", "The Presentation app transitions from an authoring view (at a low angle) to a presentation view when vertical.", "In the presentation view the pen can be used to spotlight pieces of content on the current slide."], "caption": "Figure 10. Left: Sticky-Notes hide Messy, Personal information by automatically switching to a Tidy, Public view when the user tilts the screen up\u2014such as when one\u2019s boss walks in. Right: The Presentation app shifts roles when the screen goes vertical, from Authoring to Presenting, where the pen acts as a spotlight.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Tilt-Responsive Techniques for Digital Drawing Boards", "pdf_hash": "a6ee8d3cb7469494d485a718807d501b22e48be3", "year": 2020, "venue": "UIST", "alt_text": "Figure 11 is a time-lapse showing how the Tilt Side-Channel menu fades in at the edge of the screen. It fades in quickly at motion onset, and then fades out slowly if the display remains motionless. The user can engage it with a thumb as long as it is still partially visible, giving access to system-wide functions such as clutching (to temporarily disengage tilt responses, if desired).", "levels": null, "corpus_id": 222805010, "sentences": ["Figure 11 is a time-lapse showing how the Tilt Side-Channel menu fades in at the edge of the screen.", "It fades in quickly at motion onset, and then fades out slowly if the display remains motionless.", "The user can engage it with a thumb as long as it is still partially visible, giving access to system-wide functions such as clutching (to temporarily disengage tilt responses, if desired)."], "caption": "Figure 11. Time sequence for the Tilt Side-Channel technique, which appears at the edge of the screen to allow thumb input. The icons appear quickly at motion onset. They then fade slowly when motion stops. Touching an icon activates it, even if fading.", "local_uri": ["a6ee8d3cb7469494d485a718807d501b22e48be3_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "pdf_hash": "166d91ef569340148a9d77fe64330e4599239c2f", "year": 2021, "venue": "CHI", "alt_text": "The left image is a heatmap of smartwatch bezel with 24 segments. The segments on the left and left top corner of this heatmap are dark red and the segments on the right, right button, and button of the display are dark green. The other image shows a spider graph with 24 data points presenting 24 segments. This graph is another representation of the heatmap but with exact screen visibility numbers.", "levels": [[1], [1], [1], [1]], "corpus_id": 233987806, "sentences": ["The left image is a heatmap of smartwatch bezel with 24 segments.", "The segments on the left and left top corner of this heatmap are dark red and the segments on the right, right button, and button of the display are dark green.", "The other image shows a spider graph with 24 data points presenting 24 segments.", "This graph is another representation of the heatmap but with exact screen visibility numbers."], "caption": "Figure 4: Left: heatmap from the screen occlusion experi- ment. The indicated area has screen visibility of at least 90% from 15\u25e6 (2:30 on a clock face) to 225\u25e6 (7:30 on a clock face). Right: spider graph representing the mean value of screen visibility for each segment in the screen occlusion study. The indicated bezel segments in green represent screen visi- bility of at least 90%.", "local_uri": ["166d91ef569340148a9d77fe64330e4599239c2f_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "pdf_hash": "166d91ef569340148a9d77fe64330e4599239c2f", "year": 2021, "venue": "CHI", "alt_text": "Figure 5 shows how FBG is working. The image shows a smartwatch display with a line graph and the user interacting with the smartwatch bezel. There is a line starting from the touchpoint on the smartwatch bezel to the center of the screen. The intersection of this line and the data point on the graph shows the data points' values.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987806, "sentences": ["Figure 5 shows how FBG is working.", "The image shows a smartwatch display with a line graph and the user interacting with the smartwatch bezel.", "There is a line starting from the touchpoint on the smartwatch bezel to the center of the screen.", "The intersection of this line and the data point on the graph shows the data points' values."], "caption": "(interaction area on the bezel) to the new range (projection area on the bezel).", "local_uri": ["166d91ef569340148a9d77fe64330e4599239c2f_Image_008.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "pdf_hash": "166d91ef569340148a9d77fe64330e4599239c2f", "year": 2021, "venue": "CHI", "alt_text": "The image shows a smartwatch display with a line graph and the user interacting with the smartwatch bezel. There is a line starting from the touch point on the smartwatch bezel to the other side of the bezel. The intersection of this line and the data point on the graph shows the value of the data point.", "levels": [[1], [1], [1]], "corpus_id": 233987806, "sentences": ["The image shows a smartwatch display with a line graph and the user interacting with the smartwatch bezel.", "There is a line starting from the touch point on the smartwatch bezel to the other side of the bezel.", "The intersection of this line and the data point on the graph shows the value of the data point."], "caption": "", "local_uri": ["166d91ef569340148a9d77fe64330e4599239c2f_Image_010.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "pdf_hash": "166d91ef569340148a9d77fe64330e4599239c2f", "year": 2021, "venue": "CHI", "alt_text": "Figure 7 shows the second experiment setup. The left image shows the participant in the standing condition performing the data value detection task with three interaction techniques. In the second image the participant performs the task on the treadmill while walking.", "levels": [[-1], [-1], [-1]], "corpus_id": 233987806, "sentences": ["Figure 7 shows the second experiment setup.", "The left image shows the participant in the standing condition performing the data value detection task with three interaction techniques.", "In the second image the participant performs the task on the treadmill while walking."], "caption": "", "local_uri": ["166d91ef569340148a9d77fe64330e4599239c2f_Image_012.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "pdf_hash": "166d91ef569340148a9d77fe64330e4599239c2f", "year": 2021, "venue": "CHI", "alt_text": "Figure 8 shows the result of the second experiment. The median response time for Shift is approximately 4000, for FBG 2600 and for PBG 2200 milliseconds.  The other bar chart shows the response time divided by interaction techniques and mobility conditions.  Shift-walking and shift standing are very close and they are the worst. PBG-walking and PBG standing are very close and they have the best performance", "levels": [[1], [2], [1], [3, 2], [3, 2]], "corpus_id": 233987806, "sentences": ["Figure 8 shows the result of the second experiment.", "The median response time for Shift is approximately 4000, for FBG 2600 and for PBG 2200 milliseconds.", "The other bar chart shows the response time divided by interaction techniques and mobility conditions.", "Shift-walking and shift standing are very close and they are the worst.", "PBG-walking and PBG standing are very close and they have the best performance"], "caption": "Figure 8: Left: Overall average response times (ms) for Shift, FBG, and PBG. Right: average response times for each inter- action technique, separated out by mobility condition.", "local_uri": ["166d91ef569340148a9d77fe64330e4599239c2f_Image_013.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "pdf_hash": "166d91ef569340148a9d77fe64330e4599239c2f", "year": 2021, "venue": "CHI", "alt_text": "Figure 9 left shows a spider graph with three interaction techniques. This spider graph shows the average response time for each one of 24 segments on the screen. For almost all of the segments PBG is faster than FBG and Shift.", "levels": [[1], [-1], [2]], "corpus_id": 233987806, "sentences": ["Figure 9 left shows a spider graph with three interaction techniques.", "This spider graph shows the average response time for each one of 24 segments on the screen.", "For almost all of the segments PBG is faster than FBG and Shift."], "caption": "", "local_uri": ["166d91ef569340148a9d77fe64330e4599239c2f_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching for Target Selection in Hangry Piggos", "pdf_hash": "81169d7c531a54953765b7d5ea8e0b58a481941e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: \"Four charts that show an example of the four velocities captured in a template.\"", "levels": [[1]], "corpus_id": 218483469, "sentences": ["Figure 1: \"Four charts that show an example of the four velocities captured in a template.\""], "caption": "", "local_uri": ["81169d7c531a54953765b7d5ea8e0b58a481941e_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching for Target Selection in Hangry Piggos", "pdf_hash": "81169d7c531a54953765b7d5ea8e0b58a481941e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2: \"Top view of a ray pointer acquisition movement. Both the head and controller change in position and angle.\"", "levels": null, "corpus_id": 218483469, "sentences": ["Figure 2: \"Top view of a ray pointer acquisition movement.", "Both the head and controller change in position and angle.\""], "caption": "", "local_uri": ["81169d7c531a54953765b7d5ea8e0b58a481941e_Image_003.png"], "annotated": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching for Target Selection in Hangry Piggos", "pdf_hash": "81169d7c531a54953765b7d5ea8e0b58a481941e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3: \"View from player one using the Cursor Snapping Power Up, where the application snaps the cursor to the target predicted using the HC-KTM model.\"", "levels": null, "corpus_id": 218483469, "sentences": ["Figure 3: \"View from player one using the Cursor Snapping Power Up, where the application snaps the cursor to the target predicted using the HC-KTM model.\""], "caption": "", "local_uri": ["81169d7c531a54953765b7d5ea8e0b58a481941e_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Designing an Eyes-Reduced Document Skimming App for Situational Impairments", "pdf_hash": "4cb879a5af25d72a4265bb55a8500f5c505d2689", "year": 2020, "venue": "CHI", "alt_text": "A participant in the study facing a big monitor which emulates bus commuting scenario in a controlled laboratory.", "levels": null, "corpus_id": 218483221, "sentences": ["A participant in the study facing a big monitor which emulates bus commuting scenario in a controlled laboratory."], "caption": "", "local_uri": ["4cb879a5af25d72a4265bb55a8500f5c505d2689_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "Constructing a sofa in the living room with protopiper, which is a hand-held physical sketching device. Unlike smaller devices that extrude plastic, protopiper creates tubes with connectors from adhesive tape. By allowing users to sketch and prototype room-sized objects at actual-scale, users can verify their designs during prototyping. For example, they can make sure that objects fit with the room or their bodies.", "levels": null, "corpus_id": 804203, "sentences": ["Constructing a sofa in the living room with protopiper, which is a hand-held physical sketching device.", "Unlike smaller devices that extrude plastic, protopiper creates tubes with connectors from adhesive tape.", "By allowing users to sketch and prototype room-sized objects at actual-scale, users can verify their designs during prototyping.", "For example, they can make sure that objects fit with the room or their bodies."], "caption": "Figure 1: Protopiper is a hand-held physical sketching device. Unlike smaller devices that extrude plastic, protopiper creates tubes with connectors from adhesive tape. By allowing users to sketch and prototype room-sized objects at actual-scale, users can verify their designs during prototyping. For example, they can make sure that objects fit with the room or their bodies.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "Entire kitchen populated with protopiped objects. Working at actual scale allows discovering design mishaps (e.g. ergonomic issues) already during prototyping.", "levels": null, "corpus_id": 804203, "sentences": ["Entire kitchen populated with protopiped objects.", "Working at actual scale allows discovering design mishaps (e.g. ergonomic issues) already during prototyping."], "caption": "Figure 2: Working at actual scale allows discovering design mishaps (e.g. ergonomic issues) already during prototyping.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "Protopiper provides tubes with wing connectors, which can connect to: walls, other objects, tubes.", "levels": null, "corpus_id": 804203, "sentences": ["Protopiper provides tubes with wing connectors, which can connect to: walls, other objects, tubes."], "caption": "Figure 4: Protopiper provides tubes with wing connectors, which can connect to: (a) walls, (b) other objects, (c) tubes.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "Protopiper allows creating simple mechanisms, such as hinges that allow opening-closing an umbrella.", "levels": null, "corpus_id": 804203, "sentences": ["Protopiper allows creating simple mechanisms, such as hinges that allow opening-closing an umbrella."], "caption": "", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "The protopiper device produces tubes in 5 steps: pulling the tape off, curling, sealing, cutting, and making the connector.", "levels": null, "corpus_id": 804203, "sentences": ["The protopiper device produces tubes in 5 steps: pulling the tape off, curling, sealing, cutting, and making the connector."], "caption": "Figure 7: The protopiper device produces tubes in 5 steps.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "A series of stencils shapes the tape into a tube by successively increasing curvature.", "levels": null, "corpus_id": 804203, "sentences": ["A series of stencils shapes the tape into a tube by successively increasing curvature."], "caption": "Figure 10: A series of stencils shapes the tape into a tube by successively increasing curvature.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "Protopiper can replicate tubes of the same length, and automatically generates diagonal struts.", "levels": null, "corpus_id": 804203, "sentences": ["Protopiper can replicate tubes of the same length, and automatically generates diagonal struts."], "caption": "Figure 17: Protopiper allows (a) replicating tubes of the same length, and (b) automatically generates diagonal struts.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Protopiper: Physically Sketching Room-Sized Objects at Actual Scale", "pdf_hash": "2376106eb10f26c0545c078d256d3921a32a080d", "year": 2015, "venue": "UIST", "alt_text": "Freeform shapes created by combining triangle strips, connected with one or two hinges.", "levels": null, "corpus_id": 804203, "sentences": ["Freeform shapes created by combining triangle strips, connected with one or two hinges."], "caption": "Figure 25: (a) The basic element of the triangle strip is a tube with one or two hinges. (b) A freeform shape from triangles.", "local_uri": ["2376106eb10f26c0545c078d256d3921a32a080d_Image_025.jpg"], "annotated": false, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Diagram of the Glance system. Analysts use the analyst user interface (AUI) to make a request to crowd workers, who identify events in video using the worker user interface. The results are passed to a merging server which forwards the analyst a single aggregate crowd response.", "levels": null, "corpus_id": 207216527, "sentences": ["Diagram of the Glance system.", "Analysts use the analyst user interface (AUI) to make a request to crowd workers, who identify events in video using the worker user interface.", "The results are passed to a merging server which forwards the analyst a single aggregate crowd response."], "caption": "", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Glance analyst user interface (AUI). Analysts can see their video, playback information, tools to make a new request to workers, and a set of bars below the video, aligned with the playback controls, that show the results of past queries from the crowd.", "levels": [[-1], [-1]], "corpus_id": 207216527, "sentences": ["Glance analyst user interface (AUI).", "Analysts can see their video, playback information, tools to make a new request to workers, and a set of bars below the video, aligned with the playback controls, that show the results of past queries from the crowd."], "caption": "Figure 2. The Glance analyst user interface (AUI). Analysts can load a video from YouTube, ask if or when an event occurs in the video in natural language, and set parameters to control cost and speed. Crowd workers then process this query and return results in a fraction of the playtime of the video. These results are aggregated to simplify the answer for the analyst, and make it more reliable than a single worker\u2019s answer.", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Worker interface showing a video with two people talking to each other. Controls on the bottom allow the worker to make multiple time ranges in which the even they are coding for occurs.", "levels": null, "corpus_id": 207216527, "sentences": ["Worker interface showing a video with two people talking to each other.", "Controls on the bottom allow the worker to make multiple time ranges in which the even they are coding for occurs."], "caption": "", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Plot of the playback speed results. As the playback speed (Y axis) increased, the response speed (X axis) decreases less than linearly.", "levels": [[1], [3, 1]], "corpus_id": 207216527, "sentences": ["Plot of the playback speed results.", "As the playback speed (Y axis) increased, the response speed (X axis) decreases less than linearly."], "caption": "Figure 5. As playback speed is increased, there is a non-linear decrease in the response rate of the \ufb01rst N workers (gist mode).", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_013.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Plot of the number of clips completed (Y axis) over time (X axis). The curve formed is logarithmic.", "levels": [[1], [1]], "corpus_id": 207216527, "sentences": ["Plot of the number of clips completed (Y axis) over time (X axis).", "The curve formed is logarithmic."], "caption": "Figure 6. A plot of the number of 1-minute clips from an hour-long video being completed by crowd workers in real-time as part of our live trial. In two minutes, 20% of the content was labeled. In \ufb01ve minutes, 80% of the 60 clips were labeled. This demonstrates that large groups of workers (> 50 people in all in this case) can be recruited simultaneously to complete our coding task quickly.", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_014.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Graph comparing all of the different aggregation approaches tested, measured after each aggregation scheme was applied. Scanning with adjustment for ``early birds'' strictly outperforms other methods.", "levels": [[1], [2]], "corpus_id": 207216527, "sentences": ["Graph comparing all of the different aggregation approaches tested, measured after each aggregation scheme was applied.", "Scanning with adjustment for ``early birds'' strictly outperforms other methods."], "caption": "Figure 7. Precision, recall, and F1 score results for all 6 of the worker aggregation schemes we explore. Using our \u201cscanning\u201d approach with additional \ufb01ltering and adjustments proves the most effective overall, and is signi\ufb01cantly better than simple \ufb01ltering or k-means clustering (p < .05).", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_015.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Glance: rapidly coding behavioral video with the crowd", "pdf_hash": "b32fa6180fb20a8e4752df8bd5aff50ac24ad12f", "year": 2014, "venue": "UIST", "alt_text": "Plot of the F1 score (X axis) and count variance (Y axis). As count variance increases, so does the F1 score, with a clear correlation.", "levels": [[1], [3]], "corpus_id": 207216527, "sentences": ["Plot of the F1 score (X axis) and count variance (Y axis).", "As count variance increases, so does the F1 score, with a clear correlation."], "caption": "Figure 8. Correlation between our agreement score (variance in the number of segments marked by workers in a single clip) and the aligned F1 score (R2 = .74). This suggests that the level of agreement between workers can be used as a predictor of the overall answer quality even when no baseline is available to compare to.", "local_uri": ["b32fa6180fb20a8e4752df8bd5aff50ac24ad12f_Image_017.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Effects of System Interpretation Errors on Learning New Input Mechanisms", "pdf_hash": "4bffbc08f623b853d2293ca54793d9ad73fcf00d", "year": 2021, "venue": "CHI", "alt_text": "An example of the visual guide participants could display with Left Shift. The cardinal layout of the categories and items indicated the arrow key direction to press to select that category/item.", "levels": null, "corpus_id": 233987372, "sentences": ["An example of the visual guide participants could display with Left Shift.", "The cardinal layout of the categories and items indicated the arrow key direction to press to select that category/item."], "caption": "", "local_uri": ["4bffbc08f623b853d2293ca54793d9ad73fcf00d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The Effects of System Interpretation Errors on Learning New Input Mechanisms", "pdf_hash": "4bffbc08f623b853d2293ca54793d9ad73fcf00d", "year": 2021, "venue": "CHI", "alt_text": "An example of the system interface that participants would see when completing a trial in Study 1. A target item would appear at the top of the screen. Participants could see their current selections in a box in the middle of the screen. The visual guide would appear in the bottom half of the screen.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987372, "sentences": ["An example of the system interface that participants would see when completing a trial in Study 1.", "A target item would appear at the top of the screen.", "Participants could see their current selections in a box in the middle of the screen.", "The visual guide would appear in the bottom half of the screen."], "caption": "", "local_uri": ["4bffbc08f623b853d2293ca54793d9ad73fcf00d_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Effects of System Interpretation Errors on Learning New Input Mechanisms", "pdf_hash": "4bffbc08f623b853d2293ca54793d9ad73fcf00d", "year": 2021, "venue": "CHI", "alt_text": "An example of the system interface that participants would see when completing a trial in Study 2. Participants could see their current selections in a box in the middle of the screen. The visual guide would appear in the bottom half of the screen. A box of ordered target items was shown on the left edge of the screen. A similar box of user-selected items was shown on the right side of the screen.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987372, "sentences": ["An example of the system interface that participants would see when completing a trial in Study 2.", "Participants could see their current selections in a box in the middle of the screen.", "The visual guide would appear in the bottom half of the screen.", "A box of ordered target items was shown on the left edge of the screen.", "A similar box of user-selected items was shown on the right side of the screen."], "caption": "", "local_uri": ["4bffbc08f623b853d2293ca54793d9ad73fcf00d_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FamilyStories: Asynchronous Audio Storytelling for Family Members Across Time Zones", "pdf_hash": "9770c66bf8d07674ffb7fdf17ebbfb0661ebe022", "year": 2020, "venue": "CHI", "alt_text": "Left: The three devices of FamilyStories. Middle: All inputs and switches shown including On/Off and buttons for record, play, and share. Right: The interior Printed Circuit Board (PCB) designed to easily fit in one hand.", "levels": null, "corpus_id": 218483058, "sentences": ["Left: The three devices of FamilyStories.", "Middle: All inputs and switches shown including On/Off and buttons for record, play, and share.", "Right: The interior Printed Circuit Board (PCB) designed to easily fit in one hand."], "caption": "Figure 1. Left: The three devices of FamilyStories. Middle: All inputs and switches shown including On/Off and buttons for record, play, and share. Right: The interior Printed Circuit Board (PCB) designed to easily fit in one hand.", "local_uri": ["9770c66bf8d07674ffb7fdf17ebbfb0661ebe022_Image_001.jpg", "9770c66bf8d07674ffb7fdf17ebbfb0661ebe022_Image_002.jpg", "9770c66bf8d07674ffb7fdf17ebbfb0661ebe022_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "FamilyStories: Asynchronous Audio Storytelling for Family Members Across Time Zones", "pdf_hash": "9770c66bf8d07674ffb7fdf17ebbfb0661ebe022", "year": 2020, "venue": "CHI", "alt_text": "Left: Ayla placed the devices on her desk. Right: Shea kept FamilyStories on her nightstand in her room.", "levels": null, "corpus_id": 218483058, "sentences": ["Left: Ayla placed the devices on her desk.", "Right: Shea kept FamilyStories on her nightstand in her room."], "caption": "Figure 3. Left: Ayla placed the devices on her desk. Right: Shea kept FamilyStories on her night stand in her room.", "local_uri": ["9770c66bf8d07674ffb7fdf17ebbfb0661ebe022_Image_007.jpg", "9770c66bf8d07674ffb7fdf17ebbfb0661ebe022_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "What does the Oscilloscope Say?: Comparing the Efficiency of In-Situ Visualisations during Circuit Analysis", "pdf_hash": "c21909d31d496412a868903b917cbb8b964b57d0", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2: \"Wiring task used in the experiment. Ten connections were evaluated with each Visualisation Modality resulting in a total number of forty connections. Each circuit block had a randomised distinct connection with the opposite circuit block.\"", "levels": null, "corpus_id": 218482578, "sentences": ["Figure 2: \"Wiring task used in the experiment.", "Ten connections were evaluated with each Visualisation Modality resulting in a total number of forty connections.", "Each circuit block had a randomised distinct connection with the opposite circuit block.\""], "caption": "", "local_uri": ["c21909d31d496412a868903b917cbb8b964b57d0_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "What does the Oscilloscope Say?: Comparing the Efficiency of In-Situ Visualisations during Circuit Analysis", "pdf_hash": "c21909d31d496412a868903b917cbb8b964b57d0", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3: \"Setup of the circuit with four different Visualisation Modalities: (a) oscilloscope, (b) in-situ projection, (c) user positioned tablet, and (d) HMD. The red contour denotes the visualisation.\"", "levels": null, "corpus_id": 218482578, "sentences": ["Figure 3: \"Setup of the circuit with four different Visualisation Modalities: (a) oscilloscope, (b) in-situ projection, (c) user positioned tablet, and (d) HMD.", "The red contour denotes the visualisation.\""], "caption": "tionnaire after each condition to measure their subjectively perceived workload [6, 7]. After the experiment, participants were asked for their personal ranking regarding the four", "local_uri": ["c21909d31d496412a868903b917cbb8b964b57d0_Image_006.jpg", "c21909d31d496412a868903b917cbb8b964b57d0_Image_007.jpg", "c21909d31d496412a868903b917cbb8b964b57d0_Image_008.jpg", "c21909d31d496412a868903b917cbb8b964b57d0_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "What does the Oscilloscope Say?: Comparing the Efficiency of In-Situ Visualisations during Circuit Analysis", "pdf_hash": "c21909d31d496412a868903b917cbb8b964b57d0", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 4: \"Task completion times for each Visualisation Modality. On average, the oscilloscope required the least task completion time compared to the other modalities. The error bars depict the standard error. Brackets indicate significant differences.\"", "levels": [[1], [2], [1], [1]], "corpus_id": 218482578, "sentences": ["Figure 4: \"Task completion times for each Visualisation Modality.", "On average, the oscilloscope required the least task completion time compared to the other modalities.", "The error bars depict the standard error.", "Brackets indicate significant differences.\""], "caption": "", "local_uri": ["c21909d31d496412a868903b917cbb8b964b57d0_Image_012.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "What does the Oscilloscope Say?: Comparing the Efficiency of In-Situ Visualisations during Circuit Analysis", "pdf_hash": "c21909d31d496412a868903b917cbb8b964b57d0", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 5: \"Raw NASA-TLX scores for each modality. The user positioned tablet elicited the least workload compared to the other modalities. The error bars depict the standard error. Brackets indicate significant differences\"", "levels": [[1], [2], [1], [1]], "corpus_id": 218482578, "sentences": ["Figure 5: \"Raw NASA-TLX scores for each modality.", "The user positioned tablet elicited the least workload compared to the other modalities.", "The error bars depict the standard error.", "Brackets indicate significant differences\""], "caption": "Figure 5: Raw NASA-TLX scores for each modality. The user", "local_uri": ["c21909d31d496412a868903b917cbb8b964b57d0_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments", "pdf_hash": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c", "year": 2017, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Study 1 data collection. Left: a participant follows on-screen instructions to complete our data collection protocol while wearing the prototype. Right: Example skin images collected during study 1 across the 15 fine-grained classes.", "levels": [[-1], [-1], [-1]], "corpus_id": 4015773, "sentences": ["Study 1 data collection.", "Left: a participant follows on-screen instructions to complete our data collection protocol while wearing the prototype.", "Right: Example skin images collected during study 1 across the 15 fine-grained classes."], "caption": "Participant following on-screen data collection protocol                                       (b) Example skin images from Study IFig. 3. (a) Data collection setup showing our prototype, location and gesture instructions, and camera video feed. (b) Example skin-surface images recorded by  our finger-mounted camera (fingerprint images  omitted to  protect  our participants\u2019 privacy).XX:10 \u2022 L. Stearns et al.Procedure. The procedure lasted up to 90 minutes. After a brief demographic questionnaire and setup period (i.e., selecting rings, putting on the prototype), participants completed the following tasks, in order:Location-specific touches. Participants touched and held their finger in place at 15 locations (Figure 2b) with each location prompted visually on a monitor (Figure 3a). After confirming the location and image quality, the experimenter logged the current location (e.g., timestamp, location label) and triggered the start of the next trial. Participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 15 locations (150 trials in total). In total, this dataset includes 3600 location-specific touches across all participants. Example images are shown in Figure 3b.Location-specific gestures. Participants performed the eight basic gestures: tap, swipe up, swipe down, swipe left, swipe right, circle, triangle, and square (Figure 2c) at three body locations: the palm, wrist, and thigh. These locations were selected from the 15 locations in the first task because they are easy to access, unobtrusive, and have a relatively large input area thus allowing for more complex gestures. As with the first task, participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 24 gesture and location combinations (240 trials in total). This dataset includes 5,760 location-specific gestures across all participants.", "local_uri": ["a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c_Image_012.jpg", "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c_Image_013.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments", "pdf_hash": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c", "year": 2017, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Left: TouchCam RealTime prototype, shown from the side and top worn on the right index finger and wrist. Right: Comparison between the TouchCam Offline and Realtime hardware, showing differences in materials, ring design, and sensor positions.", "levels": null, "corpus_id": 4015773, "sentences": ["Left: TouchCam RealTime prototype, shown from the side and top worn on the right index finger and wrist.", "Right: Comparison between the TouchCam Offline and Realtime hardware, showing differences in materials, ring design, and sensor positions."], "caption": "a                                                                   b", "local_uri": ["a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments", "pdf_hash": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c", "year": 2017, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Visualizations of the three interaction techniques. Shows, in order left to right, a user swiping to the right at different body locations to navigate sequentially through a list of applications, a user touching different locations on the palm to select applications, and a user touching different body locations to select applications.", "levels": null, "corpus_id": 4015773, "sentences": ["Visualizations of the three interaction techniques.", "Shows, in order left to right, a user swiping to the right at different body locations to navigate sequentially through a list of applications, a user touching different locations on the palm to select applications, and a user touching different body locations to select applications."], "caption": "Location-independent gestures (LI)                      (b) Location-specific palm gestures (LSpalm)      (c)Location-specific body gestures (LSbody)", "local_uri": ["a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "2 part Image of person in a starting position to a final position. Demonstrating that both movement and angle is necessary to track for both head and controller.", "levels": null, "corpus_id": 218483029, "sentences": ["2 part Image of person in a starting position to a final position.", "Demonstrating that both movement and angle is necessary to track for both head and controller."], "caption": "Figure 3. Top view of a ray pointer acquisition movement. Both the head and controller change in position and angle.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "Gray 3D environment with a VR controller and two spheres. One sphere is yellow, showing the active target, the other is semi-transparent gray.", "levels": null, "corpus_id": 218483029, "sentences": ["Gray 3D environment with a VR controller and two spheres.", "One sphere is yellow, showing the active target, the other is semi-transparent gray."], "caption": "Figure 6. First person view of the study environment.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "Image showing a controller being moved from left to right. Image demonstrates that both position and angle are being predicted.", "levels": null, "corpus_id": 218483029, "sentences": ["Image showing a controller being moved from left to right.", "Image demonstrates that both position and angle are being predicted."], "caption": "Figure 5. To predict the final landing position of the ray, the prediction for the final angle and position of the controller are combined.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "A two part image. First image shows an overhead view of a user with two target in front. There are lines going from the HMD to the targets showing the depth (for start  and end target), an angle between these lines showing theta and two wedges around the previous lines. These wedges demonstrate angular width. Second part shows a person facing forward with 8 points around at the cardinal locations, showing positions (in 45 degree increments)", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483029, "sentences": ["A two part image.", "First image shows an overhead view of a user with two target in front.", "There are lines going from the HMD to the targets showing the depth (for start  and end target), an angle between these lines showing theta and two wedges around the previous lines.", "These wedges demonstrate angular width.", "Second part shows a person facing forward with 8 points around at the cardinal locations, showing positions (in 45 degree increments)"], "caption": "Figure 7. The target layout. a) Reciprocal targets were located on opposite sides of the z-axis at varying depths, with equal angular widths. b) Targets appeared at one of 8 angles.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "A bar graph comparing the motion of the head and controller at 25, 50 and 75 degrees. The controller in all cases moves significantly more than the HMD", "levels": [[1], [2]], "corpus_id": 218483029, "sentences": ["A bar graph comparing the motion of the head and controller at 25, 50 and 75 degrees.", "The controller in all cases moves significantly more than the HMD"], "caption": "", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_010.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "A scatter plot making comet like images for each cardinal direction tracked. There's one dot for each trial. The nucleus of the comet is where the controller was pointing when the user clicked, the tail is where the HMD was looking when the user clicked", "levels": [[1], [1], [1]], "corpus_id": 218483029, "sentences": ["A scatter plot making comet like images for each cardinal direction tracked.", "There's one dot for each trial.", "The nucleus of the comet is where the controller was pointing when the user clicked, the tail is where the HMD was looking when the user clicked"], "caption": "Figure 9. The HMD and Controller angular movements, with respect to the Angular Distance to the target. Error bars represent standard error.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_011.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "A line graph showing 3 velocity curves for 25, 50 and 75 degrees. This graph shows the velocities for the controller angle. In the first 200 milliseconds, there is almost no spread between the 3 curves", "levels": [[1], [1], [3]], "corpus_id": 218483029, "sentences": ["A line graph showing 3 velocity curves for 25, 50 and 75 degrees.", "This graph shows the velocities for the controller angle.", "In the first 200 milliseconds, there is almost no spread between the 3 curves"], "caption": "Figure 11. Representative angular velocity profiles for the HMD and Controller by movement angle. The highlighted regions illustrate that in the first 150 ms of movement, profiles only diverge for the HMD, not the controller.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "A line graph showing 4 curves, one each for HMD position, HMD angle, Controller position, and Controller Angle. HMD curves are a tiny bit apart showing a lower value up until about 60%. After this the Controller angle drops below the other curves. The Controller position does as well, but not as significantly.", "levels": [[1], [3, 2], [3], [3]], "corpus_id": 218483029, "sentences": ["A line graph showing 4 curves, one each for HMD position, HMD angle, Controller position, and Controller Angle.", "HMD curves are a tiny bit apart showing a lower value up until about 60%.", "After this the Controller angle drops below the other curves.", "The Controller position does as well, but not as significantly."], "caption": "Figure 12. The prediction accuracy derived from each input channel at different stages of the movement.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "A line graph showing the accuracy of the four stated conditions. the prediction curves are better for each subsequent curve, going from worst to best: KTM, KTM-7, HC-KTM-1, HC-KTM-7", "levels": [[1], [2]], "corpus_id": 218483029, "sentences": ["A line graph showing the accuracy of the four stated conditions.", "the prediction curves are better for each subsequent curve, going from worst to best: KTM, KTM-7, HC-KTM-1, HC-KTM-7"], "caption": "Figure 13. Accuracy curves for the Baseline and 4 variants of the model, i.e., HC-KTM-7, HC-KTM-1, KTM-7, KTM.", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "pdf_hash": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "year": 2020, "venue": "CHI", "alt_text": "Image of a table (colored with green being good, red being bad), showing the results of evaluating different participants against other user's templates.", "levels": null, "corpus_id": 218483029, "sentences": ["Image of a table (colored with green being good, red being bad), showing the results of evaluating different participants against other user's templates."], "caption": "Table 2. Accuracies (at 40%) for each participant (rows) when using another user\u2019s template library (columns).", "local_uri": ["be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "pdf_hash": "dfa09494b50030571735aafbe5c114cd9ba80eba", "year": 2015, "venue": "W4A", "alt_text": "Four screenshots showing enhanced area touch working on existing Android application interface, from left to right: (a) Touch area over single target. (b) Touch area intersecting with multiple targets. (c) Disambiguation in a magnified area of interest. (d) Disambiguation in a full screen list of captured target descriptions.", "levels": null, "corpus_id": 5815451, "sentences": ["Four screenshots showing enhanced area touch working on existing Android application interface, from left to right: (a) Touch area over single target.", "(b) Touch area intersecting with multiple targets. (c) Disambiguation in a magnified area of interest. (d) Disambiguation in a full screen list of captured target descriptions."], "caption": "Figure 1. Enhanced area touch working on existing Android application interface: (a) Touch area over single target. (b) Touch area intersecting with multiple targets. (c) Disambiguation in a magni\ufb01ed area of interest. (d) Disambiguation in a full screen list of captured target descriptions.", "local_uri": ["dfa09494b50030571735aafbe5c114cd9ba80eba_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "pdf_hash": "dfa09494b50030571735aafbe5c114cd9ba80eba", "year": 2015, "venue": "W4A", "alt_text": "Three screenshots of interfaces on which enhanced area touch were evaluated. From left to right (a) a standard icon interface; (b) an interface on which buttons are nested and (c) an interface with small and dense targets.", "levels": null, "corpus_id": 5815451, "sentences": ["Three screenshots of interfaces on which enhanced area touch were evaluated.", "From left to right (a) a standard icon interface; (b) an interface on which buttons are nested and (c) an interface with small and dense targets."], "caption": "OrderAppTask1Touch GuardActivation2Android SettingsOpen exploration3PhoneCalling 1st speed dial4HangoutsSend a SMS5HangoutsRead a SMS6YoutubeSearch for a band7MapsSearch for directionSwitching disambiguation mode8Android SystemRead a noti\ufb01cation9Android SystemTurn Wi-Fi On/off10CameraTake a photo11Google PlusShare a photo12Play StoreInstall a game13Game installedOpen exploration14LauncherCount apps installed", "local_uri": ["dfa09494b50030571735aafbe5c114cd9ba80eba_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "pdf_hash": "dfa09494b50030571735aafbe5c114cd9ba80eba", "year": 2015, "venue": "W4A", "alt_text": "A line chart showing mean acquisition time across touch types and trial interfaces, showing magnification is the slowest touch type, and then targets list and then conventional tapping.", "levels": [[2, 1]], "corpus_id": 5815451, "sentences": ["A line chart showing mean acquisition time across touch types and trial interfaces, showing magnification is the slowest touch type, and then targets list and then conventional tapping."], "caption": "Figure 3. Mean acquisition time across touch types and trial interfaces.", "local_uri": ["dfa09494b50030571735aafbe5c114cd9ba80eba_Image_003.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "pdf_hash": "dfa09494b50030571735aafbe5c114cd9ba80eba", "year": 2015, "venue": "W4A", "alt_text": "A bar chart showing mean error rates across touch types and interfaces, it shows lower rates for list based area touch, especially on small and dense targets. Error bars show moderate stand deviation.", "levels": [[2, 1], [3]], "corpus_id": 5815451, "sentences": ["A bar chart showing mean error rates across touch types and interfaces, it shows lower rates for list based area touch, especially on small and dense targets.", "Error bars show moderate stand deviation."], "caption": "Figure 4. Mean error rates across touch types and interfaces show lower rates for list based area touch, especially on small and dense targets.", "local_uri": ["dfa09494b50030571735aafbe5c114cd9ba80eba_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "pdf_hash": "dfa09494b50030571735aafbe5c114cd9ba80eba", "year": 2015, "venue": "W4A", "alt_text": "A bar chat showing perceived usefulness, perceived ease of use and user acceptance, it shows that magnification is less preferred. Overall rates are positive. Error bars show moderate stand deviation.", "levels": [[3, 1], [3], [3]], "corpus_id": 5815451, "sentences": ["A bar chat showing perceived usefulness, perceived ease of use and user acceptance, it shows that magnification is less preferred.", "Overall rates are positive.", "Error bars show moderate stand deviation."], "caption": "Figure 5. Perceived usefulness, perceived ease of use and user acceptance show that magni\ufb01cation is less preferred.", "local_uri": ["dfa09494b50030571735aafbe5c114cd9ba80eba_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "A blue artboard is titled \"Harry Potter.\" Below the title is a small image of a download button. Below the artboard (outside the slide) is red text that says \"My favorite book.\"", "levels": [[-1], [-1], [-1]], "corpus_id": 233987344, "sentences": ["A blue artboard is titled \"Harry Potter.\"", "Below the title is a small image of a download button.", "Below the artboard (outside the slide) is red text that says \"My favorite book.\""], "caption": "P1\u2019s slide on Harry Poter.P5\u2019s slide on Cryptonomicon.P7\u2019s slide on Erik Weihenmayer.Figure 2: Three artboards created by participants during the Phase 1 observation. In (a), P1 made a PowerPoint presen- tation on J. K. Rowling\u2019s Harry Poter, and wanted to make the slide more engaging by adding color. In (b), P3 made a Keynote presentation on Neal Stephenson\u2019s Cryptonomicon. P3 thought the subtitle placeholder was directly below the title, but the subtitle was much smaller and located at the bottom edge of the artboard, which made it difcult for a sighted person to read. In (c), P7 searched for a photo of the author-adventurer Erik Weihenmayer but ended up with a photo of a young child that was attached to an article about him.that involved a lack of information, unhelpful information, or a complex interface. The researchers coalesced these patterns into four themes: (1) high cognitive load, (2) object relationship deter- mination, (3) object manipulation success confrmation, and (4) education and professional barriers caused by artboard inaccessi- bility.", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "A white artboard is titled in big bold letters: \"Why the criptonomicon is the best book ever written\". At the bottom of the artboard, partially obscured by another software window, tiny text reads \"...interest on both cryptography but more than a book about technology it is also a book about people.\"", "levels": [[-1], [-1]], "corpus_id": 233987344, "sentences": ["A white artboard is titled in big bold letters: \"Why the criptonomicon is the best book ever written\".", "At the bottom of the artboard, partially obscured by another software window, tiny text reads \"...interest on both cryptography but more than a book about technology it is also a book about people.\""], "caption": "P5\u2019s slide on Cryptonomicon.", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "A small green oval and a large red rectangle are on the artboard near the top-left corner. The red rectangle contains the words ``Hello, world.'' A black arrow pointing up is at the bottom center of the slide.", "levels": null, "corpus_id": 233987344, "sentences": ["A small green oval and a large red rectangle are on the artboard near the top-left corner.", "The red rectangle contains the words ``Hello, world.'' A black arrow pointing up is at the bottom center of the slide."], "caption": "", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "A horizontal bar graph which lists tasks on the Y-axis and percent success from 0.0%-100.0% on the X-axis. The list of tasks, with percentages, are as follows: 1) Identify objects on the pre-made slide; 91.7%. 2) Identify rectangle color; 25.0%. 3) Identify rectangle width; 58.3%. 4) Determine rectangle's and oval's positions on the slide; 50.0%. 5) Determine rectangle's and oval's positions in relation to each other; 36.4%. 6) Determine the size of the rectangle relative to the oval; 54.5%. 7) Determine the arrow's position on the slide; 45.5%. 8) Determine the direction the arrow is pointing; 0.0%. 9) Determine the text box's position on the slide; 45.5%. 10) Identify the text in the text box; 90.9%. 11) Determine the text box's position relative to the other objects; 36.4%", "levels": [[1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1]], "corpus_id": 233987344, "sentences": ["A horizontal bar graph which lists tasks on the Y-axis and percent success from 0.0%-100.0% on the X-axis.", "The list of tasks, with percentages, are as follows: 1) Identify objects on the pre-made slide; 91.7%.", "2) Identify rectangle color; 25.0%.", "3) Identify rectangle width; 58.3%.", "4) Determine rectangle's and oval's positions on the slide; 50.0%.", "5) Determine rectangle's and oval's positions in relation to each other; 36.4%.", "6) Determine the size of the rectangle relative to the oval; 54.5%.", "7) Determine the arrow's position on the slide; 45.5%.", "8) Determine the direction the arrow is pointing; 0.0%.", "9) Determine the text box's position on the slide; 45.5%.", "10) Identify the text in the text box; 90.9%.", "11) Determine the text box's position relative to the other objects; 36.4%"], "caption": "The interpretive tasks (#1-11) for the task-based usability test, including task number, description, and the percentage of participants who were able to par- tially or fully succeed in completing the task.", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "Box chart of durations for tasks 1-11. Time in minutes on the Y-axis, and task numbers on the X-axis. There is a higher variance in duration for task 4 than for the rest of the tasks, and it ranges from 0.25 to just over 4 minutes, with the majority between 1 and 2.5 minutes. Tasks 8 and 10 have noticeably short durations, close to zero, with little variance. 1, 2, and 11 range between 0 and 0.5 minutes. 3, 5, and 7 range between 0.25 and 1.25 minutes, and 6 and 9 reach nearly 1.5 minutes.    Box chart of durations for tasks 2-23. Half of the tasks (12, 13, 14, 15, 16, and 17) have an extremely high variance. The boxes for 12, 16, and 17 range from around 1.25 minutes to 5 minutes. 13 has whiskers from 1 to 5, and the box is between 2 and 4. 14 has whiskers from 0 to 5 (the entire Y axis) and a box between 1.25 and 4.5. 15 is lower, the box ranging between 0.5 and 3 minutes. Tasks 19, and 21 have noticeably short durations, between nearly zero and 0.25 minutes, with little variance. 18, 20, 22, and 23 have boxes which span 1.5 minutes; 18 ranges from almost 0 up to 1.25, 22 ranges from 1.5 to 3 with a whisker extending below 0.5. 22 and 23 are nearly identical, ranging from about 0.75 to 2.5 minutes.", "levels": [[1], [1], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 233987344, "sentences": ["Box chart of durations for tasks 1-11.", "Time in minutes on the Y-axis, and task numbers on the X-axis.", "There is a higher variance in duration for task 4 than for the rest of the tasks, and it ranges from 0.25 to just over 4 minutes, with the majority between 1 and 2.5 minutes.", "Tasks 8 and 10 have noticeably short durations, close to zero, with little variance.", "1, 2, and 11 range between 0 and 0.5 minutes.", "3, 5, and 7 range between 0.25 and 1.25 minutes, and 6 and 9 reach nearly 1.5 minutes.", "Box chart of durations for tasks 2-23.", "Half of the tasks (12, 13, 14, 15, 16, and 17) have an extremely high variance.", "The boxes for 12, 16, and 17 range from around 1.25 minutes to 5 minutes.", "13 has whiskers from 1 to 5, and the box is between 2 and 4.", "14 has whiskers from 0 to 5 (the entire Y axis) and a box between 1.25 and 4.5.", "15 is lower, the box ranging between 0.5 and 3 minutes.", "Tasks 19, and 21 have noticeably short durations, between nearly zero and 0.25 minutes, with little variance.", "18, 20, 22, and 23 have boxes which span 1.5 minutes; 18 ranges from almost 0 up to 1.25, 22 ranges from 1.5 to 3 with a whisker extending below 0.5.", "22 and 23 are nearly identical, ranging from about 0.75 to 2.5 minutes."], "caption": "Task durations for the interpretive tasks, #1-11.Task durations for the generative tasks, #12-23.", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_010.jpg", "88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "Keyboard focus is on Shape Fill. Below the menu ribbon is a blue triangle. Part of a PowerPoint menu ribbon is visible, and the top left corner of a white canvas with a blue triangle on it, with bounding box visible and rotated 90 degrees. The Shape Fill button is styled as one button and a focus border is around it, but the left side of it, a paint can icon with an orange bar underneath, is darker than the rest of it, indicating focus on that part of the button but not the other side, which has a triangle pointing down that indicates it is a dropdown menu.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987344, "sentences": ["Keyboard focus is on Shape Fill.", "Below the menu ribbon is a blue triangle.", "Part of a PowerPoint menu ribbon is visible, and the top left corner of a white canvas with a blue triangle on it, with bounding box visible and rotated 90 degrees.", "The Shape Fill button is styled as one button and a focus border is around it, but the left side of it, a paint can icon with an orange bar underneath, is darker than the rest of it, indicating focus on that part of the button but not the other side, which has a triangle pointing down that indicates it is a dropdown menu."], "caption": "P2 locates the Shape Fill button, which is actually two but- tons: the left one flls the selected shape with a default color and the right one opens a dropdown menu with more colors.The artboard\u2019s state after P2 clicked the button, which flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle.The artboard\u2019s state after P2 changed the artboard red. A key command she tried silently switched focus to the art- board and opened a \u201cFormat Background\u201d task pane, which she used to change the color of the artboard\u2019s background, leaving her with an orange triangle and a red artboard.Figure 12: P2\u2019s artboard state as she attempts to change the color of the triangle in the upper left corner from blue to red, without any feedback indicating the color changes were successful or what objects the changes were applied to.he would \u201cnever dare present my PowerPoint presentation\u201d without someone sighted looking at it frst.Accidental manipulations occurred several times during the task- based usability study due to the lack of feedback both for manipula- tions and object focus, as screen readers did not always announce when the keyboard focus changed from one object to another, orthe announcement was nested in a long string of announcements and was missed by the user. For example, when P2 attempted to change the color of the triangle in task #16 (referenced in Figure 1 and Figure 12), she selected the \"Shape Fill\" button, which flls the selected shape with a default color or can be opened as a dropdown menu to choose more colors (see Figure 12a). When she clicked the button, it flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle (see Figure 12b). P2 was confused and navigated back to the Shape Fill button and clicked it again, and it switched focus back to her triangle once more, prompting her to voice confusion and frustration: \u201cWhat? Why won\u2019t it do ... It won\u2019t let me get into ...\u201d She tried a key command, which silently switched focus to the artboard and opened a \u201cFormat Background\u201d task pane with a dropdown fll button, which she used to select a shade of red that was applied to the artboard, leaving her with an orange triangle and a red artboard (see Figure 12c). After reading through the other formatting options in the task pane and noticing they referred to the \u201cbackground,\u201d she switched keyboard focus and found that it switched to the triangle, which she had thought she already had focus on. At that point, she decided to end the task, saying, \u201cUh, I tried. Did not know if it worked, or if I made the whole slide red. [laughs] I did what I could, I don\u2019t know what else I can do.\u201d", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_020.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "Keyboard focus is on the orange triangle. The menu ribbon is still visible but does not have keyboard focus.", "levels": null, "corpus_id": 233987344, "sentences": ["Keyboard focus is on the orange triangle.", "The menu ribbon is still visible but does not have keyboard focus."], "caption": "The artboard\u2019s state after P2 clicked the button, which flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle.", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_021.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Blind Screen-Reader Users\u2019 Experiences of Digital Artboards", "pdf_hash": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "year": 2021, "venue": "CHI", "alt_text": "Red canvas with an orange triangle in the upper left corner and a format task pane to the right of the canvas. The task pane is titled ``Format Background'' and has formatting options under the header ``Fill.'' Next to the label ``Color'' is a button featuring a paint can icon with a red bar underneath and a triangle pointed down, indicating a dropdown menu. Keyboard focus is on this button. At the bottom of the task pane are two buttons, ``Apply to All'' and ``Reset Background.''", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987344, "sentences": ["Red canvas with an orange triangle in the upper left corner and a format task pane to the right of the canvas.", "The task pane is titled ``Format Background'' and has formatting options under the header ``Fill.''", "Next to the label ``Color'' is a button featuring a paint can icon with a red bar underneath and a triangle pointed down, indicating a dropdown menu.", "Keyboard focus is on this button.", "At the bottom of the task pane are two buttons, ``Apply to All'' and ``Reset Background.''"], "caption": "The artboard\u2019s state after P2 changed the artboard red. A key command she tried silently switched focus to the art- board and opened a \u201cFormat Background\u201d task pane, which she used to change the color of the artboard\u2019s background, leaving her with an orange triangle and a red artboard.Figure 12: P2\u2019s artboard state as she attempts to change the color of the triangle in the upper left corner from blue to red, without any feedback indicating the color changes were successful or what objects the changes were applied to.he would \u201cnever dare present my PowerPoint presentation\u201d without someone sighted looking at it frst.Accidental manipulations occurred several times during the task- based usability study due to the lack of feedback both for manipula- tions and object focus, as screen readers did not always announce when the keyboard focus changed from one object to another, orthe announcement was nested in a long string of announcements and was missed by the user. For example, when P2 attempted to change the color of the triangle in task #16 (referenced in Figure 1 and Figure 12), she selected the \"Shape Fill\" button, which flls the selected shape with a default color or can be opened as a dropdown menu to choose more colors (see Figure 12a). When she clicked the button, it flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle (see Figure 12b). P2 was confused and navigated back to the Shape Fill button and clicked it again, and it switched focus back to her triangle once more, prompting her to voice confusion and frustration: \u201cWhat? Why won\u2019t it do ... It won\u2019t let me get into ...\u201d She tried a key command, which silently switched focus to the artboard and opened a \u201cFormat Background\u201d task pane with a dropdown fll button, which she used to select a shade of red that was applied to the artboard, leaving her with an orange triangle and a red artboard (see Figure 12c). After reading through the other formatting options in the task pane and noticing they referred to the \u201cbackground,\u201d she switched keyboard focus and found that it switched to the triangle, which she had thought she already had focus on. At that point, she decided to end the task, saying, \u201cUh, I tried. Did not know if it worked, or if I made the whole slide red. [laughs] I did what I could, I don\u2019t know what else I can do.\u201d", "local_uri": ["88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_022.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World", "pdf_hash": "b8f58a830e2c1150d8050941f0a67cd49cf3159b", "year": 2016, "venue": "UIST", "alt_text": "User study setup. A printer's interface is printed out on paper and used for training. The microwave interface was used for controlled testing, followed by more exploratory use of other interfaces nearby (e.g., remote control, thermostat, vending machine). The study was conducted in a hotel room and was video and audio recorded. Laptop, audio recorder, microwave, and study iPhone 5c are placed on the table; experimenter and participant seats are next to each other; video recorder set up from behind with tripod.", "levels": null, "corpus_id": 207242814, "sentences": ["User study setup.", "A printer's interface is printed out on paper and used for training.", "The microwave interface was used for controlled testing, followed by more exploratory use of other interfaces nearby (e.g., remote control, thermostat, vending machine).", "The study was conducted in a hotel room and was video and audio recorded.", "Laptop, audio recorder, microwave, and study iPhone 5c are placed on the table; experimenter and participant seats are next to each other; video recorder set up from behind with tripod."], "caption": "Figure 6. User study setup. A printer\u2019s interface is printed out on paper and used for training. The microwave interface was used for controlled testing, followed by more exploratory use of other interfaces nearby (e.g., remote control, thermostat, vending machine). The study was con\u00ad ducted in a hotel room and was video and audio recorded.", "local_uri": ["b8f58a830e2c1150d8050941f0a67cd49cf3159b_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World", "pdf_hash": "b8f58a830e2c1150d8050941f0a67cd49cf3159b", "year": 2016, "venue": "UIST", "alt_text": "VizLens works robustly across various skin colors and lighting conditions. The images show users withe different skin colors pointing on the microwave in different lighting conditions and they were processed by computer vision and successfully identified the finger locations.", "levels": null, "corpus_id": 207242814, "sentences": ["VizLens works robustly across various skin colors and lighting conditions.", "The images show users withe different skin colors pointing on the microwave in different lighting conditions and they were processed by computer vision and successfully identified the finger locations."], "caption": "Figure 8. VizLens works robustly across various skin colors and lighting conditions. These are images from participants that were processed by computer vision and successfully identi\ufb01ed the \ufb01nger locations.", "local_uri": ["b8f58a830e2c1150d8050941f0a67cd49cf3159b_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World", "pdf_hash": "b8f58a830e2c1150d8050941f0a67cd49cf3159b", "year": 2016, "venue": "UIST", "alt_text": "VizLens::State Detection detects screen state and adapts to it. In this example, VizLens figures out which of six states this fancy coffee machine is in, and provides feedback or guidance specific to that screen. State 1: drink type, 2: coffee type, 3: add flavor, 4: size, 5: strength, 6: start.", "levels": null, "corpus_id": 207242814, "sentences": ["VizLens::State Detection detects screen state and adapts to it.", "In this example, VizLens figures out which of six states this fancy coffee machine is in, and provides feedback or guidance specific to that screen.", "State 1: drink type, 2: coffee type, 3: add flavor, 4: size, 5: strength, 6: start."], "caption": "", "local_uri": ["b8f58a830e2c1150d8050941f0a67cd49cf3159b_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "User interacting with HybridSpace using pinching in freespace while still having the ability to use a mouse and keyboard.", "levels": null, "corpus_id": 12104262, "sentences": ["User interacting with HybridSpace using pinching in freespace while still having the ability to use a mouse and keyboard."], "caption": "Figure 1: The HybridSpace system allows users to interact with standard 2D input and freehand spatial input. The system can also transition between 2D and 3D display modes.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "Transitions times seperated based on position of the object and on transition type. The graphs shows that transitions times are higher for the 2D to 3D transition regardless of object position.", "levels": [[1], [3]], "corpus_id": 12104262, "sentences": ["Transitions times seperated based on position of the object and on transition type.", "The graphs shows that transitions times are higher for the 2D to 3D transition regardless of object position."], "caption": "Figure 2: Average comfortable transition times for each condition. Error bars show the standard error.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "Illustration of the docking trial task. A white wire sphere appears in the center, once it is clicked a red sphere appears in one of the target locations, then once the red sphere is selected the docking location appears.", "levels": [[-1], [-1]], "corpus_id": 12104262, "sentences": ["Illustration of the docking trial task.", "A white wire sphere appears in the center, once it is clicked a red sphere appears in one of the target locations, then once the red sphere is selected the docking location appears."], "caption": "Figure 4: a) The start target began a trial. b) The user then selected a goal target. c) Translating the goal to a docking position.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "Example of the translation widget with 3 perpendicular handles for modifying location based on the x, y, and z axis and an example of the blue cursor that is shown during free hand pinching interaction.", "levels": null, "corpus_id": 12104262, "sentences": ["Example of the translation widget with 3 perpendicular handles for modifying location based on the x, y, and z axis and an example of the blue cursor that is shown during free hand pinching interaction."], "caption": "Figure 5: a) The translation widget used in the 2D Only technique.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "Sample image of the HybridSpace setup consisting of a mouse, a keyboard, stereo flip glasses, a large screen monitor, and motion tracking bands for the fingers.", "levels": null, "corpus_id": 12104262, "sentences": ["Sample image of the HybridSpace setup consisting of a mouse, a keyboard, stereo flip glasses, a large screen monitor, and motion tracking bands for the fingers."], "caption": "Figure 8: The HybridSpace hardware configuration.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "Depiction of the color selection tool. In 2D the tool consists of a circle of varying hues and a bar undearneath it to select saturation. For the 3D version the circle is a cylinder whose height is mapped to the saturation value and the user can interact with it directly by pushing their finger into the cylinder.", "levels": [[-1], [-1], [-1]], "corpus_id": 12104262, "sentences": ["Depiction of the color selection tool.", "In 2D the tool consists of a circle of varying hues and a bar undearneath it to select saturation.", "For the 3D version the circle is a cylinder whose height is mapped to the saturation value and the user can interact with it directly by pushing their finger into the cylinder."], "caption": "Figure 10: a) The 2D color palette. b) The full 3D color palette. c) An illustration of the cutaway behavior for the 3D color palette dur- ing direct interaction.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_013.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "pdf_hash": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "year": 2014, "venue": "2014 IEEE Symposium on 3D User Interfaces (3DUI)", "alt_text": "Image to illustrate the transition to 3D that is made when objects are translated along a path perpendicular to the screen plane.", "levels": null, "corpus_id": 12104262, "sentences": ["Image to illustrate the transition to 3D that is made when objects are translated along a path perpendicular to the screen plane."], "caption": "Figure 13: A translation along the z axis (\u00b130\u00b0) triggers a transition to 3D Stereo. The system returns to 2D when after the operation.", "local_uri": ["cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"Person Wearing Wireality for VR. A woman wearing Wireality on her left shoulder and a VR headset on her head. As she extends her left hand to a virtual lion sculpture, multiple parts of her hand are pulled back by strings that are extended out of Wireality, and her hand conforms to the contour of the lion.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Figure 1: \"Person Wearing Wireality for VR.", "A woman wearing Wireality on her left shoulder and a VR headset on her head.", "As she extends her left hand to a virtual lion sculpture, multiple parts of her hand are pulled back by strings that are extended out of Wireality, and her hand conforms to the contour of the lion.\""], "caption": "Figure 1. Wireality enables strong, whole-hand haptic feed- back for complex objects in VR experiences.", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"Motor-based Early Prototype. Components of this prototype are (from left to right): DC motor, spool, string, ratchet gear, solenoid, potentiometer. The solenoid is vertically below the ratchet gear, and other components are attached concentrically.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Figure 2: \"Motor-based Early Prototype.", "Components of this prototype are (from left to right): DC motor, spool, string, ratchet gear, solenoid, potentiometer.", "The solenoid is vertically below the ratchet gear, and other components are attached concentrically.\""], "caption": "", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"Exploded illustration of the CAD model of one haptic module. The components of the exploded CAD model are (from top to bottom): ratchet gear, string + spool, solenoid, ratchet pawl, torsional spring, retractor case, and housing.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Figure 3: \"Exploded illustration of the CAD model of one haptic module.", "The components of the exploded CAD model are (from top to bottom): ratchet gear, string + spool, solenoid, ratchet pawl, torsional spring, retractor case, and housing.\""], "caption": "Figure 3. Exploded illustration of one haptic module.", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: \"An Example of Seven Wirealtiy Modules Combined as a Unit. Seven Wireality modules are connected via long bolts. Wires come out of the modules and are connected to the driver board. Translucent finger caps are shown as examples. The Wireality unit is also connected to a shoulder strap.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Figure 5: \"An Example of Seven Wirealtiy Modules Combined as a Unit.", "Seven Wireality modules are connected via long bolts.", "Wires come out of the modules and are connected to the driver board.", "Translucent finger caps are shown as examples.", "The Wireality unit is also connected to a shoulder strap.\""], "caption": "Figure 5. Example Wireality setup with seven haptic modules.", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Table 1: \"Comprehensive comparison between Wireality and other hand-centric haptic systems. Details of the table are in the supplemental material.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Table 1: \"Comprehensive comparison between Wireality and other hand-centric haptic systems.", "Details of the table are in the supplemental material.\""], "caption": "Table 1. A comparison of prior hand-centric haptic systems and Wireality.", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: \"Five exemplary objects. Five exemplary objects as described in the caption.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Figure 6: \"Five exemplary objects.", "Five exemplary objects as described in the caption.\""], "caption": "Figure 6. Exemplary objects used in the study. Left to right: a wall, tilted flat surface, sphere, pole, and irregular object.", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics", "pdf_hash": "ee7f663d0a3752904584b2a27084e9dd726c83a4", "year": 2020, "venue": "CHI", "alt_text": "Figure 10: \"Four exemplary VR scenes. Four exemplary VR scenes labeled A, B, C, and D. All four scene shows a virtual hand interacting with objects described in the caption.\"", "levels": null, "corpus_id": 218483027, "sentences": ["Figure 10: \"Four exemplary VR scenes.", "Four exemplary VR scenes labeled A, B, C, and D. All four scene shows a virtual hand interacting with objects described in the caption.\""], "caption": "", "local_uri": ["ee7f663d0a3752904584b2a27084e9dd726c83a4_Image_011.png"], "annotated": false, "compound": false}
{"title": "Eyes-Free Art", "pdf_hash": "b88fc4bd8047cb3b27f809e438079618ae9fa518", "year": 2017, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Eyes-Free Art is a proxemic audio interface that changes the audio interpretation based on a user\u2019s distance from a painting. From furthest to closest the user hears: 1) background music, 2) a novel sonification technique, 3) sound effects, and 4) detailed verbal description. Image: The Blue Rider (Wassily Kandinsky, 1903).", "levels": null, "corpus_id": 12884584, "sentences": ["Eyes-Free Art is a proxemic audio interface that changes the audio interpretation based on a user\u2019s distance from a painting.", "From furthest to closest the user hears: 1) background music, 2) a novel sonification technique, 3) sound effects, and 4) detailed verbal description.", "Image: The Blue Rider (Wassily Kandinsky, 1903)."], "caption": "", "local_uri": ["b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Eyes-Free Art", "pdf_hash": "b88fc4bd8047cb3b27f809e438079618ae9fa518", "year": 2017, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Eyes-Free Art presented in a room. The painting is projected on the wall to simulate a gallery, with the Kinect sensor below. On the floor is a white tape and cardboard \u201cladder\u201d used as a tactile cue to navigate between the zones.", "levels": null, "corpus_id": 12884584, "sentences": ["Eyes-Free Art presented in a room.", "The painting is projected on the wall to simulate a gallery, with the Kinect sensor below. On the floor is a white tape and cardboard \u201cladder\u201d used as a tactile cue to navigate between the zones."], "caption": "", "local_uri": ["b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Eyes-Free Art", "pdf_hash": "b88fc4bd8047cb3b27f809e438079618ae9fa518", "year": 2017, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "4. From left to right: a) manually drawn edges for Self-Portrait with Thorn Necklace and Hummingbird; b) sound effect regions for Self-Portrait with Thorn Necklace and Hummingbird. The regions were manually colored (green = leaves, orange = monkey, pink = cat, red = butterflies, yellow = bird).", "levels": null, "corpus_id": 12884584, "sentences": ["4. From left to right: a) manually drawn edges for Self-Portrait with Thorn Necklace and Hummingbird; b) sound effect regions for Self-Portrait with Thorn Necklace and Hummingbird.", "The regions were manually colored (green = leaves, orange = monkey, pink = cat, red = butterflies, yellow = bird)."], "caption": "\u201cI was able to get a sense of the quantity and variety of colors.\u201d4", "local_uri": ["b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_013.jpg", "b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_014.jpg", "b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_015.jpg", "b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_016.jpg", "b88fc4bd8047cb3b27f809e438079618ae9fa518_Image_017.jpg"], "annotated": false, "compound": true}
{"title": "A Human Touch: Social Touch Increases the Perceived Human-likeness of Agents in Virtual Reality", "pdf_hash": "71fd9dedab4f8b1ed53ea3ca59c49ed51242f8fb", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"Virtual Character as shown in Study; heat-able silicone hand prototype; Performance of social touch event\"", "levels": null, "corpus_id": 218483258, "sentences": ["Figure 1: \"Virtual Character as shown in Study; heat-able silicone hand prototype; Performance of social touch event\""], "caption": "Figure 1. Is the presented character a human-controlled avatar or computer-controlled agent? If the character reaches out and physically performs a social touch on a user, it will blur the boundaries between avatar and agent. For this we used a heat-able hand prototype with \ufb02exible joints, to recreate a touch sensation that is indistinguishable from a real hand.", "local_uri": ["71fd9dedab4f8b1ed53ea3ca59c49ed51242f8fb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "From Tactile to NavTile: Opportunities and Challenges with Multi-Modal Feedback for Guiding Surfaces during Non-Visual Navigation", "pdf_hash": "276ca9ad8056f5104e65167e5ec855ecee3c4aca", "year": 2021, "venue": "CHI", "alt_text": "Figure 1. has three images. On the left there is an image of two intersecting tactile guidance surfaces without warnings, in the middle, there is a tactile guidance surface leading to stairs without warning, the right a tactile guidance surface is incorrectly positioned as it installed beside the entrance to the crosswalk.", "levels": null, "corpus_id": 233987732, "sentences": ["Figure 1. has three images.", "On the left there is an image of two intersecting tactile guidance surfaces without warnings, in the middle, there is a tactile guidance surface leading to stairs without warning, the right a tactile guidance surface is incorrectly positioned as it installed beside the entrance to the crosswalk."], "caption": "", "local_uri": ["276ca9ad8056f5104e65167e5ec855ecee3c4aca_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "From Tactile to NavTile: Opportunities and Challenges with Multi-Modal Feedback for Guiding Surfaces during Non-Visual Navigation", "pdf_hash": "276ca9ad8056f5104e65167e5ec855ecee3c4aca", "year": 2021, "venue": "CHI", "alt_text": "Figure 2. has four images. The first image shows a pattern of truncated domes in tactile surfaces are placed equally distant from each other. The second image shows the truncated domes in each row offset from each other. The third image shows a tactile surface with rectangular long bars parallel to walking direction, these are used for guiding. The fourth image shows a rectangular bar that are perpendicular to walking direction.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987732, "sentences": ["Figure 2.", "has four images.", "The first image shows a pattern of truncated domes in tactile surfaces are placed equally distant from each other.", "The second image shows the truncated domes in each row offset from each other.", "The third image shows a tactile surface with rectangular long bars parallel to walking direction, these are used for guiding.", "The fourth image shows a rectangular bar that are perpendicular to walking direction."], "caption": "", "local_uri": ["276ca9ad8056f5104e65167e5ec855ecee3c4aca_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "From Tactile to NavTile: Opportunities and Challenges with Multi-Modal Feedback for Guiding Surfaces during Non-Visual Navigation", "pdf_hash": "276ca9ad8056f5104e65167e5ec855ecee3c4aca", "year": 2021, "venue": "CHI", "alt_text": "Figure 3. Shows a user interface for planning where to place the tile and what message to play on each tile. The user interface has various controls like sliders, drop down menus for choosing the type of tile, and entering the audio message to be played.", "levels": null, "corpus_id": 233987732, "sentences": ["Figure 3. Shows a user interface for planning where to place the tile and what message to play on each tile.", "The user interface has various controls like sliders, drop down menus for choosing the type of tile, and entering the audio message to be played."], "caption": "Figure 3: Shows a web application UI for custom planning of the tiles and recording audio prompts. This application is running on a web server and is sending audio prompts to users phone", "local_uri": ["276ca9ad8056f5104e65167e5ec855ecee3c4aca_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "From Tactile to NavTile: Opportunities and Challenges with Multi-Modal Feedback for Guiding Surfaces during Non-Visual Navigation", "pdf_hash": "276ca9ad8056f5104e65167e5ec855ecee3c4aca", "year": 2021, "venue": "CHI", "alt_text": "Figure 4. Three panel image showing part of the NavTiles manufacturing process. On the left are the two vertical shapes placed on the vacuum former. In the middle image, there are the four black vacuum formed NavTiles on a table. Two are molded in the shape of directional arrows, and two are molded in the shape of the vertical lines. In the right image, there are underlying electronics to navtile and are displayed with a microcontroller board, battery, and a pressure sensor.", "levels": null, "corpus_id": 233987732, "sentences": ["Figure 4.", "Three panel image showing part of the NavTiles manufacturing process.", "On the left are the two vertical shapes placed on the vacuum former.", "In the middle image, there are the four black vacuum formed NavTiles on a table.", "Two are molded in the shape of directional arrows, and two are molded in the shape of the vertical lines.", "In the right image, there are underlying electronics to navtile and are displayed with a microcontroller board, battery, and a pressure sensor."], "caption": "Figure 4: (A) Shows 3D printable textures and a thermo- former for making a texture, (B) Shows guidance surfaces inter-locked with each other (C) Underlying electronics, a sensor to make the tiles multi-modal.", "local_uri": ["276ca9ad8056f5104e65167e5ec855ecee3c4aca_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Haptic keyclick feedback improves typing speed and reduces typing errors on a flat keyboard", "pdf_hash": "7c8447714d54ee9bda4a950e769a65100bad6b41", "year": 2015, "venue": "2015 IEEE World Haptics Conference (WHC)", "alt_text": "Title: Microsoft Wired 600 keyboard (left) and Microsoft Type Cover (right) that used to collect baseline performance metrics.", "levels": null, "corpus_id": 15523816, "sentences": ["Title: Microsoft Wired 600 keyboard (left) and Microsoft Type Cover (right) that used to collect baseline performance metrics."], "caption": "(d)", "local_uri": ["7c8447714d54ee9bda4a950e769a65100bad6b41_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Typing on Glasses: Adapting Text Entry to Smart Eyewear", "pdf_hash": "aa3ac6592bb3b7ffebe2610809d2fa5ebf9eb43d", "year": 2015, "venue": "MobileHCI", "alt_text": "Description: A finger swiping left on level 1 of Swipeboard and swiping right on level 2, to select the character 'D'.", "levels": null, "corpus_id": 13568308, "sentences": ["Description: A finger swiping left on level 1 of Swipeboard and swiping right on level 2, to select the character 'D'."], "caption": "Figure 2. The original Swipeboard technique. a) The first swipe specifies one of the nine regions subdivided from a QWERTY keyboard. b) The second swipe specifies the character, in this example, \u2018D\u2019. Figure from Chen et al. [7].", "local_uri": ["aa3ac6592bb3b7ffebe2610809d2fa5ebf9eb43d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "ApplianceReader: A Wearable, Crowdsourced, Vision-based System to Make Appliances Accessible", "pdf_hash": "b8b478aceed305d1f1b948243785e2ccc727bac7", "year": 2015, "venue": "CHI Extended Abstracts", "alt_text": "ApplianceReader is a wearable point-of-view camera-based system that combine crowdsourcing and computer vision to collaboratively label and recognize button controls on appliances, thus allowing visually impaired users to interactively explore and use appliances.", "levels": null, "corpus_id": 15284493, "sentences": ["ApplianceReader is a wearable point-of-view camera-based system that combine crowdsourcing and computer vision to collaboratively label and recognize button controls on appliances, thus allowing visually impaired users to interactively explore and use appliances."], "caption": "", "local_uri": ["b8b478aceed305d1f1b948243785e2ccc727bac7_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "ApplianceReader: A Wearable, Crowdsourced, Vision-based System to Make Appliances Accessible", "pdf_hash": "b8b478aceed305d1f1b948243785e2ccc727bac7", "year": 2015, "venue": "CHI Extended Abstracts", "alt_text": "System overview, including initial and real-time stages using CV, crowdsourcing and moderator algorithms.", "levels": null, "corpus_id": 15284493, "sentences": ["System overview, including initial and real-time stages using CV, crowdsourcing and moderator algorithms."], "caption": "", "local_uri": ["b8b478aceed305d1f1b948243785e2ccc727bac7_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "ApplianceReader: A Wearable, Crowdsourced, Vision-based System to Make Appliances Accessible", "pdf_hash": "b8b478aceed305d1f1b948243785e2ccc727bac7", "year": 2015, "venue": "CHI Extended Abstracts", "alt_text": "Initial (one-time) photo taking and crowd labeling of ApplianceReader: (a) an initial photo taken by the visually impaired user, (b) crowd workers verify the quality of the image and label interface elements using the interface.", "levels": null, "corpus_id": 15284493, "sentences": ["Initial (one-time) photo taking and crowd labeling of ApplianceReader: (a) an initial photo taken by the visually impaired user, (b) crowd workers verify the quality of the image and label interface elements using the interface."], "caption": "Figure 3: Initial (one-time) photo taking and crowd labeling of ApplianceReader: (a) an initial photo taken by the visually impaired user, (b) crowd workers verify the quality of the image and label interface elements.", "local_uri": ["b8b478aceed305d1f1b948243785e2ccc727bac7_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "ApplianceReader: A Wearable, Crowdsourced, Vision-based System to Make Appliances Accessible", "pdf_hash": "b8b478aceed305d1f1b948243785e2ccc727bac7", "year": 2015, "venue": "CHI Extended Abstracts", "alt_text": "Real-time recognition and control using ApplianceReader. Recognition result, showing (a) reference image, and (b) input image. (c) Cropped input image. (d) Transformed reference image. (e) Subtraction result of c and d. (f) Calculated fingertip location.", "levels": null, "corpus_id": 15284493, "sentences": ["Real-time recognition and control using ApplianceReader.", "Recognition result, showing (a) reference image, and (b) input image. (c) Cropped input image. (d) Transformed reference image. (e) Subtraction result of c and d. (f) Calculated fingertip location."], "caption": "Figure 4: Real-time recognition and control using ApplianceReader. Recognition result, showing (a) reference image, and (b) input image. (c) Cropped input image. (d) Transformed reference image. (e) Subtraction result. (f) Calculated \ufb01ngertip location.", "local_uri": ["b8b478aceed305d1f1b948243785e2ccc727bac7_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Serious Snacking: A Survival Analysis of how Snacking Mechanics Affect Attrition in a Mobile Serious Game", "pdf_hash": "31ed4d565a6600af5bef14167f17a27641e7e0e1", "year": 2021, "venue": "CHI", "alt_text": "Infinitus algebraica screen shot 4/5 and 5/5. Selection of the second number and the selected tikes disapear", "levels": null, "corpus_id": 233986985, "sentences": ["Infinitus algebraica screen shot 4/5 and 5/5.", "Selection of the second number and the selected tikes disapear"], "caption": "", "local_uri": ["31ed4d565a6600af5bef14167f17a27641e7e0e1_Image_011.jpg", "31ed4d565a6600af5bef14167f17a27641e7e0e1_Image_012.jpg"], "annotated": false, "compound": true}
{"title": "Ninja Hands: Using Many Hands to Improve Target Selection in VR", "pdf_hash": "3163e8b8420479f65f7eaf4911e14b79a1db6b9d", "year": 2021, "venue": "CHI", "alt_text": "Showing a single physical hand with a SteamVR Knuckles DV prototype controller strapped to it, overlaid on top of a large number of evenly distributed red virtual hands in the interior of a virtual house.", "levels": null, "corpus_id": 233987735, "sentences": ["Showing a single physical hand with a SteamVR Knuckles DV prototype controller strapped to it, overlaid on top of a large number of evenly distributed red virtual hands in the interior of a virtual house."], "caption": "Figure 1: A sample VR scene with Ninja Hands. It maps one physical hand to many distributed virtual hands, allowing the user to comfortably reach distant objects.", "local_uri": ["3163e8b8420479f65f7eaf4911e14b79a1db6b9d_Image_001.png"], "annotated": false, "compound": false}
{"title": "Ninja Hands: Using Many Hands to Improve Target Selection in VR", "pdf_hash": "3163e8b8420479f65f7eaf4911e14b79a1db6b9d", "year": 2021, "venue": "CHI", "alt_text": "A large grey virtual room seen from the top corner, filled with a large number of red green spherical distracters and a single green target. The user's location is indicated by a red circle on the floor.", "levels": null, "corpus_id": 233987735, "sentences": ["A large grey virtual room seen from the top corner, filled with a large number of red green spherical distracters and a single green target.", "The user's location is indicated by a red circle on the floor."], "caption": "", "local_uri": ["3163e8b8420479f65f7eaf4911e14b79a1db6b9d_Image_026.png"], "annotated": false, "compound": false}
{"title": "Ninja Hands: Using Many Hands to Improve Target Selection in VR", "pdf_hash": "3163e8b8420479f65f7eaf4911e14b79a1db6b9d", "year": 2021, "venue": "CHI", "alt_text": "1, 8, 27, and 64 hands that are placed within cuboids generated by subdividing the total space of the room by these amounts. The hands are black and the standard size used in the SteamVR asset.", "levels": null, "corpus_id": 233987735, "sentences": ["1, 8, 27, and 64 hands that are placed within cuboids generated by subdividing the total space of the room by these amounts.", "The hands are black and the standard size used in the SteamVR asset."], "caption": "Figure 6: The frst of eight target and distractor layouts in the second study, seen from the top corner of the room. Par- ticipants stand on the red mark on the foor at the back of the room.", "local_uri": ["3163e8b8420479f65f7eaf4911e14b79a1db6b9d_Image_027.jpg"], "annotated": false, "compound": false}
{"title": "Object-focused mixed reality storytelling: technology-driven content creation and dissemination for engaging user experiences", "pdf_hash": "e7291a551189d84b045e9283af3e478dc47f9e9c", "year": 2018, "venue": "PCI", "alt_text": "A picture containing person, indoor, man, wall\n\nDescription automatically generated", "levels": null, "corpus_id": 57757415, "sentences": ["A picture containing person, indoor, man, wall\n\nDescription automatically generated"], "caption": "", "local_uri": ["e7291a551189d84b045e9283af3e478dc47f9e9c_Image_043.jpg"], "annotated": false, "compound": false}
{"title": "CAN: composable accessibility infrastructure via data-driven crowdsourcing", "pdf_hash": "d3f7f2c9d25a3ed7330f70590e3d10bebd2653d7", "year": 2015, "venue": "W4A", "alt_text": "The CAN Framework. End Users contribute different websites' accessiblity issues implicitly, and the AC contributors contribute ACs to the system explicitly.", "levels": null, "corpus_id": 3245164, "sentences": ["The CAN Framework.", "End Users contribute different websites' accessiblity issues implicitly, and the AC contributors contribute ACs to the system explicitly."], "caption": "", "local_uri": ["d3f7f2c9d25a3ed7330f70590e3d10bebd2653d7_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "CAN: composable accessibility infrastructure via data-driven crowdsourcing", "pdf_hash": "d3f7f2c9d25a3ed7330f70590e3d10bebd2653d7", "year": 2015, "venue": "W4A", "alt_text": "CAN prompted messages, developer's inputs and one sample AC (with source code) are displayed in the developer console window.", "levels": null, "corpus_id": 3245164, "sentences": ["CAN prompted messages, developer's inputs and one sample AC (with source code) are displayed in the developer console window."], "caption": "", "local_uri": ["d3f7f2c9d25a3ed7330f70590e3d10bebd2653d7_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Super-Resolution Capacitive Touchscreens", "pdf_hash": "732fd206609535e93fd8783847b533c63441fb72", "year": 2021, "venue": "CHI", "alt_text": "The image is split into five parts left to right show casing the 5 steps of the super-resolution pipeline in a D&D game example. Step 1 to 4 shows the progress of super-resolving the AprilTag and step 5 the D&D game.", "levels": null, "corpus_id": 233987287, "sentences": ["The image is split into five parts left to right show casing the 5 steps of the super-resolution pipeline in a D&D game example.", "Step 1 to 4 shows the progress of super-resolving the AprilTag and step 5 the D&D game."], "caption": "", "local_uri": ["732fd206609535e93fd8783847b533c63441fb72_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Super-Resolution Capacitive Touchscreens", "pdf_hash": "732fd206609535e93fd8783847b533c63441fb72", "year": 2021, "venue": "CHI", "alt_text": "The left side (A) shows a hand moving a nickel over the screen and the tablet's screen shows nickel detected. The right side (B) shows a hand moving a key over the tablet's screen the screen is showing \"key 1 recognized.\"", "levels": null, "corpus_id": 233987287, "sentences": ["The left side (A) shows a hand moving a nickel over the screen and the tablet's screen shows nickel detected.", "The right side (B) shows a hand moving a key over the tablet's screen the screen is showing \"key 1 recognized.\""], "caption": "translation and rotation, while other geometric approaches such as [11, 17] only consider translation and are not suitable for objects", "local_uri": ["732fd206609535e93fd8783847b533c63441fb72_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Super-Resolution Capacitive Touchscreens", "pdf_hash": "732fd206609535e93fd8783847b533c63441fb72", "year": 2021, "venue": "CHI", "alt_text": "The left side shows the back of a Pok\u00e9mon card with a 36h11 AprilTag attached, and the right side shows the sample app in which the card gets scanned by placing the card on the screen in a Pok\u00e9mon arena shown on the screen.", "levels": null, "corpus_id": 233987287, "sentences": ["The left side shows the back of a Pok\u00e9mon card with a 36h11 AprilTag attached, and the right side shows the sample app in which the card gets scanned by placing the card on the screen in a Pok\u00e9mon arena shown on the screen."], "caption": "", "local_uri": ["732fd206609535e93fd8783847b533c63441fb72_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Super-Resolution Capacitive Touchscreens", "pdf_hash": "732fd206609535e93fd8783847b533c63441fb72", "year": 2021, "venue": "CHI", "alt_text": "A 16h5 and 36h11 AprilTag in four different representations, the photos, the seed images, the super-resolved images, and the final thresholded image. On the seed image of the 16h5 tag, the pattern is not clearly visible. However, on the super-resolved images and the final image, the pattern is visible. For the 36h11 AprilTag, the pattern is always visible but better resolved on the later two representations.", "levels": null, "corpus_id": 233987287, "sentences": ["A 16h5 and 36h11 AprilTag in four different representations, the photos, the seed images, the super-resolved images, and the final thresholded image.", "On the seed image of the 16h5 tag, the pattern is not clearly visible.", "However, on the super-resolved images and the final image, the pattern is visible.", "For the 36h11 AprilTag, the pattern is always visible but better resolved on the later two representations."], "caption": "", "local_uri": ["732fd206609535e93fd8783847b533c63441fb72_Image_033.gif"], "annotated": false, "compound": false}
{"title": "Super-Resolution Capacitive Touchscreens", "pdf_hash": "732fd206609535e93fd8783847b533c63441fb72", "year": 2021, "venue": "CHI", "alt_text": "Four keys in four different representations, the photos, the seed images, the super-resolved images, and the final thresholded image: one smaller key, one key with a squared head, and two keys which only differ in the thoughts. The super-resolution reveals clearly more detail. And in the thresholded image, they all can be differentiated.", "levels": null, "corpus_id": 233987287, "sentences": ["Four keys in four different representations, the photos, the seed images, the super-resolved images, and the final thresholded image: one smaller key, one key with a squared head, and two keys which only differ in the thoughts.", "The super-resolution reveals clearly more detail.", "And in the thresholded image, they all can be differentiated."], "caption": "0    25  50  75     0    25  50  75      0    25  50  75     0    25 50 75", "local_uri": ["732fd206609535e93fd8783847b533c63441fb72_Image_042.jpg"], "annotated": false, "compound": false}
{"title": "Vid2Doppler: Synthesizing Doppler Radar Data from Videos for Training Privacy-Preserving Activity Recognition", "pdf_hash": "8e1c35e1be345d37c2a8093d51de9eaa79ef69de", "year": 2021, "venue": "CHI", "alt_text": "Figure showing the mesh fit to the user in the input video from different viewpoints.", "levels": [[-1]], "corpus_id": 233987106, "sentences": ["Figure showing the mesh fit to the user in the input video from different viewpoints."], "caption": "Figure 2: An input video is frst processed to extract a 3D mesh. We can then simulate diferent virtual viewpoints (here you can see 3 of our 9 synthesized views). This both in- creases our training data volume and improves robustness in real world conditions.", "local_uri": ["8e1c35e1be345d37c2a8093d51de9eaa79ef69de_Image_004.jpg", "8e1c35e1be345d37c2a8093d51de9eaa79ef69de_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Vid2Doppler: Synthesizing Doppler Radar Data from Videos for Training Privacy-Preserving Activity Recognition", "pdf_hash": "8e1c35e1be345d37c2a8093d51de9eaa79ef69de", "year": 2021, "venue": "CHI", "alt_text": "Figure showcasing the live classification of three example activities by our activity recognition classifier.", "levels": null, "corpus_id": 233987106, "sentences": ["Figure showcasing the live classification of three example activities by our activity recognition classifier."], "caption": "", "local_uri": ["8e1c35e1be345d37c2a8093d51de9eaa79ef69de_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Vid2Doppler: Synthesizing Doppler Radar Data from Videos for Training Privacy-Preserving Activity Recognition", "pdf_hash": "8e1c35e1be345d37c2a8093d51de9eaa79ef69de", "year": 2021, "venue": "CHI", "alt_text": "Figure showing our data capture apparatus, namely a mmWave Radar to capture Doppler data and webcam to capture video.", "levels": [[-1]], "corpus_id": 233987106, "sentences": ["Figure showing our data capture apparatus, namely a mmWave Radar to capture Doppler data and webcam to capture video."], "caption": "", "local_uri": ["8e1c35e1be345d37c2a8093d51de9eaa79ef69de_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "On How Deaf People Might Use Speech to Control Devices", "pdf_hash": "dd94d3d8dced1cc57e783525f0a4886253c8593e", "year": 2017, "venue": "ASSETS", "alt_text": "Examples of devices that accept speech input, including a smartwatch, iPhone, robot vaccuum cleaner, Amazon Echo, in-car entertainment and navigation system.", "levels": null, "corpus_id": 35757739, "sentences": ["Examples of devices that accept speech input, including a smartwatch, iPhone, robot vaccuum cleaner, Amazon Echo, in-car entertainment and navigation system."], "caption": "Figure 1: Voice-activated devices have become common-place. Users can use voice to control their smart watches, mobile phones, personal assistant devices such as Amazon\u2019s Echo or Apple\u2019s Ho- mePod, voice-activated vacuum robots, even smart cars. However, most voice-activated devices are not accessible to deaf people.", "local_uri": ["dd94d3d8dced1cc57e783525f0a4886253c8593e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Your Paper is Dead!: Bringing Life to Research Articles with Animated Figures", "pdf_hash": "bd4f357adef33a0a66c8502d750b335fc02a8c71", "year": 2015, "venue": "CHI Extended Abstracts", "alt_text": "This static verison of the animation includes callouts to all the animation effects.", "levels": null, "corpus_id": 14312410, "sentences": ["This static verison of the animation includes callouts to all the animation effects."], "caption": "Figure 5: The static representation of our animation includes callouts, in place of the actual animation effects.", "local_uri": ["bd4f357adef33a0a66c8502d750b335fc02a8c71_Image_009.gif"], "annotated": false, "compound": false}
{"title": "Your Paper is Dead!: Bringing Life to Research Articles with Animated Figures", "pdf_hash": "bd4f357adef33a0a66c8502d750b335fc02a8c71", "year": 2015, "venue": "CHI Extended Abstracts", "alt_text": "Two side by side images from PortraitSketch, comparing the results with and without assistance.", "levels": null, "corpus_id": 14312410, "sentences": ["Two side by side images from PortraitSketch, comparing the results with and without assistance."], "caption": "", "local_uri": ["bd4f357adef33a0a66c8502d750b335fc02a8c71_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "A Peripheral Tactile Feedback System for Lateral Epicondilytus Rehabilitation Exercise", "pdf_hash": "82dabdb09e45f15a78b1874c4c79c71903a26791", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A hand with a closed fist is angled upward at the wrist, then is moved to a downward angle", "levels": null, "corpus_id": 233987465, "sentences": ["A hand with a closed fist is angled upward at the wrist, then is moved to a downward angle"], "caption": "", "local_uri": ["82dabdb09e45f15a78b1874c4c79c71903a26791_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "A Peripheral Tactile Feedback System for Lateral Epicondilytus Rehabilitation Exercise", "pdf_hash": "82dabdb09e45f15a78b1874c4c79c71903a26791", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "There are two image. The first shows a persons fist clenched, with a strap around the palm which an intertial measurement unit is attached at the strap on the back of the hand in a similar fashion to a watch if it were wrapped around the hand instead of the wrist. The second image shows shows the underside of the strap on the persons palm. Their hand is open, showing a vibrotactile actuator under the strap being pressed against the palm", "levels": null, "corpus_id": 233987465, "sentences": ["There are two image.", "The first shows a persons fist clenched, with a strap around the palm which an intertial measurement unit is attached at the strap on the back of the hand in a similar fashion to a watch if it were wrapped around the hand instead of the wrist.", "The second image shows shows the underside of the strap on the persons palm.", "Their hand is open, showing a vibrotactile actuator under the strap being pressed against the palm"], "caption": "Figure 2: The hardware confguration for HALA: (upper) shows the IMU ftted around the top of the hand, with USB connection to the Unity application, (lower) shows the vibra- tion actuator ftted on the palm side of the hand.", "local_uri": ["82dabdb09e45f15a78b1874c4c79c71903a26791_Image_003.jpg", "82dabdb09e45f15a78b1874c4c79c71903a26791_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "A Peripheral Tactile Feedback System for Lateral Epicondilytus Rehabilitation Exercise", "pdf_hash": "82dabdb09e45f15a78b1874c4c79c71903a26791", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Three square wave waveforms where the first visualises the feedback given when a wrist extension was too fast, the second if it was too slow and the third if it was too fast. The first waveform is a straight line, indicating that there is no pulse. The second waveform has a medium pulse where the pulse is on for 50ms and off for 800ms, repeating. The last line is a fast pulse where the pulse is on for 50ms and off for 15ms, repeating.", "levels": null, "corpus_id": 233987465, "sentences": ["Three square wave waveforms where the first visualises the feedback given when a wrist extension was too fast, the second if it was too slow and the third if it was too fast.", "The first waveform is a straight line, indicating that there is no pulse.", "The second waveform has a medium pulse where the pulse is on for 50ms and off for 800ms, repeating.", "The last line is a fast pulse where the pulse is on for 50ms and off for 15ms, repeating."], "caption": "Figure 3: Visualisations of waveforms of a repeatedly puls- ing vibrations used to convey the speed of movement.", "local_uri": ["82dabdb09e45f15a78b1874c4c79c71903a26791_Image_006.png"], "annotated": false, "compound": false}
{"title": "HoloSound: Combining Speech and Sound Identification for Deaf or Hard of Hearing Users on a Head-mounted Display", "pdf_hash": "c1256d18050902f8ba95aac11e25780ae72cdc19", "year": 2020, "venue": "ASSETS", "alt_text": "Four screenshots of HoloSound in use in different context, showing the view as if looked by the user wearing the HoloLens.  In each image, a scene is shown (e.g., a group conversation, a person knocking on door) and the overlaid AR visualizations are shown: the recognized sounds are shown at the left bottom, captions are shown at the right bottom, and circle arcs indicating the sound source locations are shown in the middle of the screen. The arc and texts are in white color; texts use Arial 8-point font.  More specifically, for image 1, a person is chopping garlic, the identified sound on the bottom left shown is \"Chopping\", the circular arc is pointed towards the person's hands, and the captions show \"Yeah, the garlic looks well done now.\" (which is maybe spoken by the user wearing the HoloLens)  The next image shows a phone ringing on the table, identified sound is \"alarm\" , arc points towards the phone, and no captions are shown (probably because there is no conversation occurring).  The third image shows a 4-person family group conversation, one conversationalist is typing on a computer, and another is speaking to the person who is typing. The identified sound shown in AR is \"Typing\", the arc point towards the person who is talking, and the captioned text is \"You work too hard. Why not have dinner now, mom?\"  The final image shows a person knocking at the door, with sound \"knocking\", arc pointing towards the door, and the text.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 220978572, "sentences": ["Four screenshots of HoloSound in use in different context, showing the view as if looked by the user wearing the HoloLens.", "In each image, a scene is shown (e.g., a group conversation, a person knocking on door) and the overlaid AR visualizations are shown: the recognized sounds are shown at the left bottom, captions are shown at the right bottom, and circle arcs indicating the sound source locations are shown in the middle of the screen.", "The arc and texts are in white color; texts use Arial 8-point font.", "More specifically, for image 1, a person is chopping garlic, the identified sound on the bottom left shown is \"Chopping\", the circular arc is pointed towards the person's hands, and the captions show \"Yeah, the garlic looks well done now.\" (", "which is maybe spoken by the user wearing the HoloLens)  The next image shows a phone ringing on the table, identified sound is \"alarm\" , arc points towards the phone, and no captions are shown (probably because there is no conversation occurring).", "The third image shows a 4-person family group conversation, one conversationalist is typing on a computer, and another is speaking to the person who is typing.", "The identified sound shown in AR is \"Typing\", the arc point towards the person who is talking, and the captioned text is \"You work too hard.", "Why not have dinner now, mom?\"", "The final image shows a person knocking at the door, with sound \"knocking\", arc pointing towards the door, and the text."], "caption": "", "local_uri": ["c1256d18050902f8ba95aac11e25780ae72cdc19_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HoloSound: Combining Speech and Sound Identification for Deaf or Hard of Hearing Users on a Head-mounted Display", "pdf_hash": "c1256d18050902f8ba95aac11e25780ae72cdc19", "year": 2020, "venue": "ASSETS", "alt_text": "Three components of HoloSound system. Two closeup views from the front and the back of a user's head wearing HoloLens. Raspberry Pi is placed at the back of head and Microphone array is placed at the top of head.", "levels": null, "corpus_id": 220978572, "sentences": ["Three components of HoloSound system.", "Two closeup views from the front and the back of a user's head wearing HoloLens.", "Raspberry Pi is placed at the back of head and Microphone array is placed at the top of head."], "caption": "Figure 2: The three components of HoloSound system.", "local_uri": ["c1256d18050902f8ba95aac11e25780ae72cdc19_Image_002.png"], "annotated": false, "compound": false}
{"title": "HoloSound: Combining Speech and Sound Identification for Deaf or Hard of Hearing Users on a Head-mounted Display", "pdf_hash": "c1256d18050902f8ba95aac11e25780ae72cdc19", "year": 2020, "venue": "ASSETS", "alt_text": "Two system screenshots when looking at a man laughing as he watching video on a computer in front of him, showing two placements of captions: top and bottom, or windows view and subtitles view.", "levels": null, "corpus_id": 220978572, "sentences": ["Two system screenshots when looking at a man laughing as he watching video on a computer in front of him, showing two placements of captions: top and bottom, or windows view and subtitles view."], "caption": "Figure 3: Speech-transcription can either be (A) placed on top of the speakers in the 3D space (windows view) or", "local_uri": ["c1256d18050902f8ba95aac11e25780ae72cdc19_Image_003.png"], "annotated": false, "compound": false}
{"title": "Learning Cooperative Personalized Policies from Gaze Data", "pdf_hash": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d", "year": 2019, "venue": "UIST", "alt_text": "The left side of the figure shows a clipping of a virtual environment that shows various 3D primitives with different colors. Each object has a label displaying a number in the thousands. The right side shows a clipping of a virtual environment that contains spheres that show either a Q or an O. Again, each object possesses a label. This time labels show random strings.", "levels": null, "corpus_id": 202585377, "sentences": ["The left side of the figure shows a clipping of a virtual environment that shows various 3D primitives with different colors.", "Each object has a label displaying a number in the thousands.", "The right side shows a clipping of a virtual environment that contains spheres that show either a Q or an O. Again, each object possesses a label.", "This time labels show random strings."], "caption": "Figure 2. Virtual environment of the visual search task showing (a) the highest-number task with pre-attentive features and (b) the matching- strings task with attentive features.", "local_uri": ["1d4484c14344e5bf197b49e1f17fe696d2d4a79d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Learning Cooperative Personalized Policies from Gaze Data", "pdf_hash": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d", "year": 2019, "venue": "UIST", "alt_text": "This figure shows a gaze trajectory with different states and actions rendered over a clipping of the visual search environment showing two objects of the visual search task.", "levels": null, "corpus_id": 202585377, "sentences": ["This figure shows a gaze trajectory with different states and actions rendered over a clipping of the visual search environment showing two objects of the visual search task."], "caption": "Figure 6. A gaze trajectory with respect to a particular object (the green sphere) specifying the progression of states and the decision sequence of actions the agent is learning.", "local_uri": ["1d4484c14344e5bf197b49e1f17fe696d2d4a79d_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Learning Cooperative Personalized Policies from Gaze Data", "pdf_hash": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d", "year": 2019, "venue": "UIST", "alt_text": "Figure shows the progression of reward of our method and the SVM-baseline over percentage of training samples for the four conditions of the data collection study.", "levels": [[1]], "corpus_id": 202585377, "sentences": ["Figure shows the progression of reward of our method and the SVM-baseline over percentage of training samples for the four conditions of the data collection study."], "caption": "Figure 7. Performance comparison between ours (in purple) versus an", "local_uri": ["1d4484c14344e5bf197b49e1f17fe696d2d4a79d_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Learning Cooperative Personalized Policies from Gaze Data", "pdf_hash": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d", "year": 2019, "venue": "UIST", "alt_text": "Shows mean and 95%-confidence interval of the metrics tested in the user study. The results of task execution time are as follows: SA, MV: 38.2 sec, lower CI: 33.5 sec, upper CI: 42.9 sec; CO, MV: 39.6 sec, lower CI: 34.9 sec, upper CI: 44.3 sec; SL, MV: 38.6 sec, lower CI: 33.8 sec, upper CI: 43.3 sec; RL, MV: 34.9 sec, lower CI: 30.2 sec, upper CI: 39.6 sec. The results of perceived support are SA, MV: 4.3, lower CI: 3.7, upper CI: 4.9; CO, MV: 3.7, lower CI: 3.1, upper CI: 4.3; SL, MV: 3.8, lower CI: 3.2, upper CI: 4.4; RL, MV: 4.7, lower CI: 4.1, upper CI: 5.3. The results of perceived disruption are SA, MV: 2.9, lower CI: 2.3, upper CI: 3.4; CO, MV: 3.8, lower CI: 3.2, upper CI: 4.3; SL, MV: 4.0, lower CI: 3.4, upper CI: 4.5; RL, MV: 3.8, lower CI: 3.2, upper CI: 4.3. The fraction of labels shown in each condition are SA 100%, CO 4%, SL 9%, and RL 13%. Results for precision are SA, MV: 0.04, lower CI: 0.01, upper CI: 0.06; CO, MV: 0.4, lower CI: 0.38, upper CI: 0.43; SL, MV: 0.27, lower CI: 0.24, upper CI: 0.29; RL, MV: 0.2, lower CI: 0.18, upper CI: 0.23. Results for recall are SA, MV: 0.93, lower CI: 0.90, upper CI: 0.97; CO, MV: 0.04, lower CI: 0.01, upper CI: 0.08; SL, MV: 0.05, lower CI: 0.01, upper CI: 0.08; RL, MV: 0.27, lower CI: 0.23, upper CI: 0.30. Results for F1-score are SA, MV: 0.071, lower CI: 0.063, upper CI: 0.078; CO, MV: 0.075, lower CI: 0.068, upper CI: 0.083; SL, MV: 0.079, lower CI: 0.072, upper CI: 0.087; RL, MV: 0.085, lower CI: 0.078, upper CI: 0.093.", "levels": [[1], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 202585377, "sentences": ["Shows mean and 95%-confidence interval of the metrics tested in the user study.", "The results of task execution time are as follows: SA, MV: 38.2 sec, lower CI: 33.5 sec, upper CI: 42.9 sec; CO, MV: 39.6 sec, lower CI: 34.9 sec, upper CI: 44.3 sec; SL, MV: 38.6 sec, lower CI: 33.8 sec, upper CI: 43.3 sec; RL, MV: 34.9 sec, lower CI: 30.2 sec, upper CI: 39.6 sec.", "The results of perceived support are SA, MV: 4.3, lower CI: 3.7, upper CI: 4.9; CO, MV: 3.7, lower CI: 3.1, upper CI: 4.3; SL, MV: 3.8, lower CI: 3.2, upper CI: 4.4; RL, MV: 4.7, lower CI: 4.1, upper CI: 5.3.", "The results of perceived disruption are SA, MV: 2.9, lower CI: 2.3, upper CI: 3.4; CO, MV: 3.8, lower CI: 3.2, upper CI: 4.3; SL, MV: 4.0, lower CI: 3.4, upper CI: 4.5; RL, MV: 3.8, lower CI: 3.2, upper CI: 4.3.", "The fraction of labels shown in each condition are SA 100%, CO 4%, SL 9%, and RL 13%.", "Results for precision are SA, MV: 0.04, lower CI: 0.01, upper CI: 0.06; CO, MV: 0.4, lower CI: 0.38, upper CI: 0.43; SL, MV: 0.27, lower CI: 0.24, upper CI: 0.29; RL, MV: 0.2, lower CI: 0.18, upper CI: 0.23.", "Results for recall are SA, MV: 0.93, lower CI: 0.90, upper CI: 0.97; CO, MV: 0.04, lower CI: 0.01, upper CI: 0.08; SL, MV: 0.05, lower CI: 0.01, upper CI: 0.08; RL, MV: 0.27, lower CI: 0.23, upper CI: 0.30.", "Results for F1-score are SA, MV: 0.071, lower CI: 0.063, upper CI: 0.078; CO, MV: 0.075, lower CI: 0.068, upper CI: 0.083; SL, MV: 0.079, lower CI: 0.072, upper CI: 0.087; RL, MV: 0.085, lower CI: 0.078, upper CI: 0.093."], "caption": "Figure 9. Mean and 95%-confdence interval of (a) task execution time (in seconds), (b) perceived support and (c) disruption (Likert-item range was one to seven, higher number standing for more support / disruption),", "local_uri": ["1d4484c14344e5bf197b49e1f17fe696d2d4a79d_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Painting Portals: connecting homes through live paintings", "pdf_hash": "7658623ed89cfeb1c25d503d35dbb34d46973fbf", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Two frames with abstract geometric paintings digitally displayed on them. On the left, the frame is placed on a windowsill, with decorative flowers in front of it. On the right, the frame is placed on a table, with smaller decorative paintings next to it.", "levels": null, "corpus_id": 233987081, "sentences": ["Two frames with abstract geometric paintings digitally displayed on them.", "On the left, the frame is placed on a windowsill, with decorative flowers in front of it.", "On the right, the frame is placed on a table, with smaller decorative paintings next to it."], "caption": "Figure 1: Two Painting Portals incorporated into the decor of a home as framed abstract paintings that slowly change over time using the camera stream from a remote location.", "local_uri": ["7658623ed89cfeb1c25d503d35dbb34d46973fbf_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Painting Portals: connecting homes through live paintings", "pdf_hash": "7658623ed89cfeb1c25d503d35dbb34d46973fbf", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Four panels: the first panel shows the original image, which is a chair with a cushion on it. The next panel shows this image slightly geometrically abstracted, indicating that the user is at a distance of 0cm from the frame. The next panel shows this image more abstracted, indicating a distance of 20cm. The next panel shows this image very abstracted, indicating a distance of more than 40cm.", "levels": null, "corpus_id": 233987081, "sentences": ["Four panels: the first panel shows the original image, which is a chair with a cushion on it.", "The next panel shows this image slightly geometrically abstracted, indicating that the user is at a distance of 0cm from the frame.", "The next panel shows this image more abstracted, indicating a distance of 20cm.", "The next panel shows this image very abstracted, indicating a distance of more than 40cm."], "caption": "", "local_uri": ["7658623ed89cfeb1c25d503d35dbb34d46973fbf_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Tracko: Ad-hoc Mobile 3D Tracking Using Bluetooth Low Energy and Inaudible Signals for Cross-Device Interaction", "pdf_hash": "ddc754d5f35015ac998f7543ee4decae50cdc117", "year": 2015, "venue": "UIST", "alt_text": "We evaluated Tracko inside (5 ft)^3 space using 11 Optitrack cameras for millimeter 3D tracking ground-truth. (One camera is behind the photographer.)", "levels": [[-1], [-1]], "corpus_id": 16554471, "sentences": ["We evaluated Tracko inside (5 ft)^3 space using 11 Optitrack cameras for millimeter 3D tracking ground-truth. (", "One camera is behind the photographer.)"], "caption": "Tracko\u2019s receives tracking information from two sources, the BLE manager and the position manager, potentially at differ- ent update intervals. Therefore, Tracko implements a stand- ard linear Kalman Filter to fuse measurements from these multiple sources. From the algorithm we described above,", "local_uri": ["ddc754d5f35015ac998f7543ee4decae50cdc117_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Classroom Digital Twins with Instrumentation-Free Gaze Tracking", "pdf_hash": "c66d1c42eb90aef23c4813050c51e73f6da08e2a", "year": 2021, "venue": "CHI", "alt_text": "On the left is a web-based capture interface that detects ArUco markers and builds an inventory of important items (walls, whiteboards, etc.). On the right is an example digital twin output, also a web-based application.", "levels": null, "corpus_id": 233987882, "sentences": ["On the left is a web-based capture interface that detects ArUco markers and builds an inventory of important items (walls, whiteboards, etc.).", "On the right is an example digital twin output, also a web-based application."], "caption": "", "local_uri": ["c66d1c42eb90aef23c4813050c51e73f6da08e2a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Classroom Digital Twins with Instrumentation-Free Gaze Tracking", "pdf_hash": "c66d1c42eb90aef23c4813050c51e73f6da08e2a", "year": 2021, "venue": "CHI", "alt_text": "On the left is a figure showcasing the percentage of student gaze across various classroom foci (whiteboards, projector screens, lectern) at the end of a class session. In the middle is a figure of the heatmaps of students' gaze across the same foci and on the right is a heatmap of the instructor gaze aggregated across a class session.", "levels": null, "corpus_id": 233987882, "sentences": ["On the left is a figure showcasing the percentage of student gaze across various classroom foci (whiteboards, projector screens, lectern) at the end of a class session.", "In the middle is a figure of the heatmaps of students' gaze across the same foci and on the right is a heatmap of the instructor gaze aggregated across a class session."], "caption": "", "local_uri": ["c66d1c42eb90aef23c4813050c51e73f6da08e2a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Classroom Digital Twins with Instrumentation-Free Gaze Tracking", "pdf_hash": "c66d1c42eb90aef23c4813050c51e73f6da08e2a", "year": 2021, "venue": "CHI", "alt_text": "Figure showcasing a sample classroom scene with head pose annotated in a green box.", "levels": null, "corpus_id": 233987882, "sentences": ["Figure showcasing a sample classroom scene with head pose annotated in a green box."], "caption": "Figure 4: Sample scene from our controlled study with 3D gaze frustums overlaid in green.", "local_uri": ["c66d1c42eb90aef23c4813050c51e73f6da08e2a_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "The promise and peril of parallel chat in video meetings for work", "pdf_hash": "46d0712a63efe20498b6b963edbf4560dc76ebe6", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Screenshot of video meeting in Microsoft teams. People having a video call on the left. On the right within a yellow box is a list of chat messages, including images that meeting participants have shared with each other.", "levels": null, "corpus_id": 233987188, "sentences": ["Screenshot of video meeting in Microsoft teams.", "People having a video call on the left.", "On the right within a yellow box is a list of chat messages, including images that meeting participants have shared with each other."], "caption": "Figure 1: Parallel chat (yellow box) in a video meeting on Microsoft Teams.", "local_uri": ["46d0712a63efe20498b6b963edbf4560dc76ebe6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The promise and peril of parallel chat in video meetings for work", "pdf_hash": "46d0712a63efe20498b6b963edbf4560dc76ebe6", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A table showing the breakdown of agree-disagree responses to the six questions in our poll.", "levels": null, "corpus_id": 233987188, "sentences": ["A table showing the breakdown of agree-disagree responses to the six questions in our poll."], "caption": "", "local_uri": ["46d0712a63efe20498b6b963edbf4560dc76ebe6_Image_003.gif"], "annotated": false, "compound": false}
{"title": "Measuring text simplification with the crowd", "pdf_hash": "cbd7e92b327dc9e54024aaf0a1ff39a45dc20623", "year": 2015, "venue": "W4A", "alt_text": "Figure 2: Average worker ratings of all 10 trial sentences as dif- ferent canonical rules are applied. Different rules have differ- ent effects although generally the trend is to increase simplicity as expected.", "levels": [[1], [3]], "corpus_id": 16636296, "sentences": ["Figure 2: Average worker ratings of all 10 trial sentences as dif- ferent canonical rules are applied.", "Different rules have differ- ent effects although generally the trend is to increase simplicity as expected."], "caption": "", "local_uri": ["cbd7e92b327dc9e54024aaf0a1ff39a45dc20623_Image_003.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Measuring text simplification with the crowd", "pdf_hash": "cbd7e92b327dc9e54024aaf0a1ff39a45dc20623", "year": 2015, "venue": "W4A", "alt_text": "Figure 3: Average worker ratings (with standard deviation shown) of our 10 trial sentences, divided into two groups: high- change sentences (6) and low-change sentences (4) as different canonical rules are applied. A clear effect is observed when applying each rule in settings where there is a larger difference in the resulting sentence, suggesting workers are able to accurately pick up on successful rule-base simplicity changes.", "levels": [[1], [4, 3]], "corpus_id": 16636296, "sentences": ["Figure 3: Average worker ratings (with standard deviation shown) of our 10 trial sentences, divided into two groups: high- change sentences (6) and low-change sentences (4) as different canonical rules are applied.", "A clear effect is observed when applying each rule in settings where there is a larger difference in the resulting sentence, suggesting workers are able to accurately pick up on successful rule-base simplicity changes."], "caption": "", "local_uri": ["cbd7e92b327dc9e54024aaf0a1ff39a45dc20623_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3, 4], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Measuring text simplification with the crowd", "pdf_hash": "cbd7e92b327dc9e54024aaf0a1ff39a45dc20623", "year": 2015, "venue": "W4A", "alt_text": "Figure 5: Crowd sampling rate vs. difference from final answer. Even a small fraction of the 50 crowd workers polled for this study were able to approximate the final result from them all.", "levels": null, "corpus_id": 16636296, "sentences": ["Figure 5: Crowd sampling rate vs. difference from final answer.", "Even a small fraction of the 50 crowd workers polled for this study were able to approximate the final result from them all."], "caption": "", "local_uri": ["cbd7e92b327dc9e54024aaf0a1ff39a45dc20623_Image_006.png"], "annotated": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure. 1. DreamCatcher: A family sleep display.  Containts for people with rings that represent sleep and inside the ring is the mood reported.", "levels": null, "corpus_id": 218517784, "sentences": ["Figure. 1. DreamCatcher: A family sleep display.", "Containts for people with rings that represent sleep and inside the ring is the mood reported."], "caption": "Figure 1. The main home screen of DreamCatcher that was displayed on a tablet kiosk in a central location in the home. The sleep of each member of the family is represented as a blue ring. Inside each ring is an emoji chosen by family members to represent that person\u2019s daily mood.", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 3. Mood options to select from when reporting moood. There are 8 options, organized by two rows with four mood representation for every row. From left to right, the first row includes Rested, Sleepy, Nightmare, Happy. The second row inckludes OK, Hungry, Painful, Upset.", "levels": null, "corpus_id": 218517784, "sentences": ["Figure 3. Mood options to select from when reporting moood.", "There are 8 options, organized by two rows with four mood representation for every row.", "From left to right, the first row includes Rested, Sleepy, Nightmare, Happy.", "The second row inckludes OK, Hungry, Painful, Upset."], "caption": "Figure 3. Prototype mood options for self-report or for reporting on behalf of another family member.", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 4a Family Daily View showing an example of when one family members\u2019 sleep and mood is presented but information is missing for the rest of the family. The crescent-shape moon at the center fills in blue as more sleep and mood information is captured.", "levels": null, "corpus_id": 218517784, "sentences": ["Figure 4a Family Daily View showing an example of when one family members\u2019 sleep and mood is presented but information is missing for the rest of the family.", "The crescent-shape moon at the center fills in blue as more sleep and mood information is captured."], "caption": "", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 4c Single Daily View containing one day of data. Sleep is represented as a ring and a bar. The bar contains bedtime, waketime, and movement during the night.", "levels": [[-1], [-1], [-1]], "corpus_id": 218517784, "sentences": ["Figure 4c Single Daily View containing one day of data.", "Sleep is represented as a ring and a bar.", "The bar contains bedtime, waketime, and movement during the night."], "caption": "", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 4 d Single Weekly View showing a 7-day version of the Single Daily View.", "levels": null, "corpus_id": 218517784, "sentences": ["Figure 4 d Single Weekly View showing a 7-day version of the Single Daily View."], "caption": "Figure 4. Four views of DreamCatcher\u2019s high-fidelity prototypes used in feedback sessions.", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_010.png"], "annotated": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 5c. DreamCatcher view of a single person with sleep and mood data for a week.", "levels": [[0], [1]], "corpus_id": 218517784, "sentences": ["Figure 5c.", "DreamCatcher view of a single person with sleep and mood data for a week."], "caption": "Figure 5. Screens in the final design of DreamCatcher. Figure 1 also shows the Family Daily View. Each example also includes a reflective prompt and an indicator that audio is being recorded.", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 7. Heatmap of interactions for each family. Rows represent the type of interaction (from top to bottom): audio reflection, mood reporting, and views. Each column represents a family, from right to left, Fam 1 represents Family F1 to Fam 10 representing Family10. The x-axis in each family column represents family members and the y-axis represents the day of the study. Dark green bars represent the days on which children interacted, light green represents days children did not interact, dark red represents days parents interacted, and light red represents days parents did not interact.", "levels": [[0], [1], [1], [1], [1], [1]], "corpus_id": 218517784, "sentences": ["Figure 7.", "Heatmap of interactions for each family.", "Rows represent the type of interaction (from top to bottom): audio reflection, mood reporting, and views.", "Each column represents a family, from right to left, Fam 1 represents Family F1 to Fam 10 representing Family10.", "The x-axis in each family column represents family members and the y-axis represents the day of the study.", "Dark green bars represent the days on which children interacted, light green represents days children did not interact, dark red represents days parents interacted, and light red represents days parents did not interact."], "caption": "", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_016.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "pdf_hash": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 8. Heatmap of sleep data captured. Dark green represents days children interacted, light green represents days children did not interact, dark red represents days parents interacted, light orange represents days parents did not interact.", "levels": [[0], [1], [1]], "corpus_id": 218517784, "sentences": ["Figure 8.", "Heatmap of sleep data captured.", "Dark green represents days children interacted, light green represents days children did not interact, dark red represents days parents interacted, light orange represents days parents did not interact."], "caption": "Figure 8. Heatmap of sleep data captured. Dark green represents days when children interacted, light green represents days children did not interact, dark red represents days parents interacted, and light red represents days parents did not interact.", "local_uri": ["373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_017.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Description of how sentences would be constructed for quizzes on case grammar, given the object labels and their spatial relationships: Object 1 (subject of sentence) + Verb (indicating position) + Preposition and Article (multiple-choice options) + Object 2 (object following the preposition)", "levels": null, "corpus_id": 218482946, "sentences": ["Description of how sentences would be constructed for quizzes on case grammar, given the object labels and their spatial relationships: Object 1 (subject of sentence) + Verb (indicating position) + Preposition and Article (multiple-choice options) + Object 2 (object following the preposition)"], "caption": "", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Screenshot of the Augmented-Reality (AR) App, depicting objects and their labels virtual labels in a common office scene (an apple, a bottle, a keyboard, a telephone)", "levels": null, "corpus_id": 218482946, "sentences": ["Screenshot of the Augmented-Reality (AR) App, depicting objects and their labels virtual labels in a common office scene (an apple, a bottle, a keyboard, a telephone)"], "caption": "Figure 2. Screenshot of the Augmented-Reality (AR) App, depicting vir- tual labels of objects in a common of\ufb01ce scene.", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Screen shot of the Snapshot app, depicting virtual labels of a screen and a telephone in a common office scene", "levels": null, "corpus_id": 218482946, "sentences": ["Screen shot of the Snapshot app, depicting virtual labels of a screen and a telephone in a common office scene"], "caption": "Figure 3. Screenshot of the Snapshot app, depicting virtual labels of objects in a common of\ufb01ce scene.", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Example image that was used for the transfer test, depicting two fictitious objects", "levels": null, "corpus_id": 218482946, "sentences": ["Example image that was used for the transfer test, depicting two fictitious objects"], "caption": "Figure 5. Example image that was used for the transfer test, depicting \ufb01ctitious objects.", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Prior and posterior distributions for the likelihood ratio of H0 and H+, and corresponding Bayes Factors, based on performance improvement on the transfer test and learnt vocabulary after one week compared to performance before the study. Relative likelihoods are illustrated by the pie-chart. Created with JASP", "levels": [[1], [1], [1]], "corpus_id": 218482946, "sentences": ["Prior and posterior distributions for the likelihood ratio of H0 and H+, and corresponding Bayes Factors, based on performance improvement on the transfer test and learnt vocabulary after one week compared to performance before the study.", "Relative likelihoods are illustrated by the pie-chart.", "Created with JASP"], "caption": "(a) Transfer Improvement (b) Additional Words Recalled", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_009.jpg", "80183519a1e9c67b6996bea274cd5e6c251e6683_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Box plots of SUS Scores, indicating the median, upper and lower quartiles, minimum and maximum values, and outliers. The median of Snapshot is higher than for AR and the interquartile range is larger, but AR has two low outliers and Snapshot only 1.", "levels": [[1], [3, 2]], "corpus_id": 218482946, "sentences": ["Box plots of SUS Scores, indicating the median, upper and lower quartiles, minimum and maximum values, and outliers.", "The median of Snapshot is higher than for AR and the interquartile range is larger, but AR has two low outliers and Snapshot only 1."], "caption": "", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "pdf_hash": "80183519a1e9c67b6996bea274cd5e6c251e6683", "year": 2020, "venue": "CHI", "alt_text": "Summary of potential use-locations and the number of participants who indicated they would use either the AR or Snapshot app in these locations. The number of users is lower for Snapshot in all cases except 'at home' and 'on public transport'.", "levels": null, "corpus_id": 218482946, "sentences": ["Summary of potential use-locations and the number of participants who indicated they would use either the AR or Snapshot app in these locations.", "The number of users is lower for Snapshot in all cases except 'at home' and 'on public transport'."], "caption": "methods (i.e., Snapshot), which elicited more consistent meth- ods. In contrast, AR allows for more diverse self-learning and might have bene\ufb01ted from a more guided experience, at least until users are more familiar with this medium.", "local_uri": ["80183519a1e9c67b6996bea274cd5e6c251e6683_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Five hexaganol Lanterns made out of cardboard. Each has a different means of construction. Elastic bands are on some Lanterns.", "levels": [[-1], [-1], [-1]], "corpus_id": 233987746, "sentences": ["Five hexaganol Lanterns made out of cardboard.", "Each has a different means of construction.", "Elastic bands are on some Lanterns."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Left image - child holding a Lantern that is hanging from the end of a stick by string. The Lantern is glowing green.  Right image - a cardboard hexaganol Lantern that is openend at one end. Inside there are electrical components. To the right of the Lantern are elastic bands and small bulldog clips.", "levels": null, "corpus_id": 233987746, "sentences": ["Left image - child holding a Lantern that is hanging from the end of a stick by string.", "The Lantern is glowing green.  Right image - a cardboard hexaganol Lantern that is openend at one end.", "Inside there are electrical components.", "To the right of the Lantern are elastic bands and small bulldog clips."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Left image - a Lantern hanging from a stick structure under a tree in the woods that is glowing red.  Right image - a wicker basked filled with Lanterns that are all glowing red.", "levels": null, "corpus_id": 233987746, "sentences": ["Left image - a Lantern hanging from a stick structure under a tree in the woods that is glowing red.  Right image - a wicker basked filled with Lanterns that are all glowing red."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "A child walking toward Lanterns in the woods. The child is bending over and holding his hand towards the Lantern, which is glowing red.", "levels": null, "corpus_id": 233987746, "sentences": ["A child walking toward Lanterns in the woods.", "The child is bending over and holding his hand towards the Lantern, which is glowing red."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Five Lanterns in a garden on tarmac. One Lantern is lit green and the other four are red.", "levels": null, "corpus_id": 233987746, "sentences": ["Five Lanterns in a garden on tarmac.", "One Lantern is lit green and the other four are red."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Two children in a woodland wearing jackets and hats. The children are holding Lanterns out towards Lanterns already in the woodland, as well as moving around the woodland to other Lanterns.", "levels": null, "corpus_id": 233987746, "sentences": ["Two children in a woodland wearing jackets and hats.", "The children are holding Lanterns out towards Lanterns already in the woodland, as well as moving around the woodland to other Lanterns."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Two children in a garden, face to face, holding Lanterns that are on the end of stick and glowing red. Both Lanterns are touching in front of them. In the background another child is holding a Lanterns on a stick that is glowing green.", "levels": null, "corpus_id": 233987746, "sentences": ["Two children in a garden, face to face, holding Lanterns that are on the end of stick and glowing red.", "Both Lanterns are touching in front of them.", "In the background another child is holding a Lanterns on a stick that is glowing green."], "caption": "Figure 8: Maddie provoking the children to come after her", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors", "pdf_hash": "ec7f3c105fcade8d212b85ac261c3935118d1099", "year": 2021, "venue": "CHI", "alt_text": "Two photos. The first shows children in a garden running on tarmac. On the left are four children with Lanterns on sticks that are glowing red. They are chasing another child on the right who is holding a Lantern that is glowing green. The second image shows three children standing over a fence holding Lanterns on sticks up in the air. Two Lanterns are glowing red and another is green.", "levels": null, "corpus_id": 233987746, "sentences": ["Two photos.", "The first shows children in a garden running on tarmac.", "On the left are four children with Lanterns on sticks that are glowing red.", "They are chasing another child on the right who is holding a Lantern that is glowing green.", "The second image shows three children standing over a fence holding Lanterns on sticks up in the air.", "Two Lanterns are glowing red and another is green."], "caption": "", "local_uri": ["ec7f3c105fcade8d212b85ac261c3935118d1099_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Towards the Prediction of Dyslexia by a Web-based Game with Musical Elements", "pdf_hash": "25b80e2c0254ff1d1cda02d132e161ebab1f8126", "year": 2017, "venue": "W4A", "alt_text": "Figure 1: Example of the game DysMusic for the rst two clicks on two sound cards (left) and then a pair of equal sounds is found (right).", "levels": null, "corpus_id": 1829122, "sentences": ["Figure 1: Example of the game DysMusic for the rst two clicks on two sound cards (left) and then a pair of equal sounds is found (right)."], "caption": "", "local_uri": ["25b80e2c0254ff1d1cda02d132e161ebab1f8126_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "pdf_hash": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "year": 2017, "venue": "", "alt_text": "The Group Spinner interface showing the radar chart for a group of students for the current and previous sessions. It also shows the indicators for the selected group so the teacher can update the graph based on the indicator values.", "levels": [[-1], [-1]], "corpus_id": 53632008, "sentences": ["The Group Spinner interface showing the radar chart for a group of students for the current and previous sessions.", "It also shows the indicators for the selected group so the teacher can update the graph based on the indicator values."], "caption": "Figure 1. Annotated crop of Group Spinner\u2019s interface showing current and previous session graphs along with some indicators.", "local_uri": ["e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "pdf_hash": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "year": 2017, "venue": "", "alt_text": "An empty radar chart with 10 axis: Outcome, Collaboration, Organization process, Classroom dynamics, Confidence, Behaviour, Motivation, Language, Skillfullness, and Thinking skills.", "levels": [[-1]], "corpus_id": 53632008, "sentences": ["An empty radar chart with 10 axis: Outcome, Collaboration, Organization process, Classroom dynamics, Confidence, Behaviour, Motivation, Language, Skillfullness, and Thinking skills."], "caption": "Figure 3. First version radar chart; superset of all possible axes as inspired by Activity Theory\u2019s activity triangle.", "local_uri": ["e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_008.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "pdf_hash": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "year": 2017, "venue": "", "alt_text": "The graph mode allows for manipulating the graph of any of the selected groups through moving the buble marks for each of the axes. It also shows the graph for the previous session for comparison. The user can show/hide the graph for the previous session and the graph of the students' self assessment.", "levels": [[-1], [-1], [-1]], "corpus_id": 53632008, "sentences": ["The graph mode allows for manipulating the graph of any of the selected groups through moving the buble marks for each of the axes.", "It also shows the graph for the previous session for comparison.", "The user can show/hide the graph for the previous session and the graph of the students' self assessment."], "caption": "", "local_uri": ["e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "pdf_hash": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "year": 2017, "venue": "", "alt_text": "In the indicators mode, it is possible to show a description of any of the indicators, to scroll to any of the indicators using the bookmarks toolbar and to increase/decrease the value of any of the indicators.", "levels": [[-1]], "corpus_id": 53632008, "sentences": ["In the indicators mode, it is possible to show a description of any of the indicators, to scroll to any of the indicators using the bookmarks toolbar and to increase/decrease the value of any of the indicators."], "caption": "teachers, except for T6 (a member of the research team) who provided her feedback in written form. Interviews were audio recorded and transcribed for analysis.", "local_uri": ["e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "pdf_hash": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "year": 2017, "venue": "", "alt_text": "The radar chart from T1 shows plots from 6 previous sessions as well as the yet unmodified plot for the 7th session. It shows how the teachers' focus may change from session to session.", "levels": [[1], [1]], "corpus_id": 53632008, "sentences": ["The radar chart from T1 shows plots from 6 previous sessions as well as the yet unmodified plot for the 7th session.", "It shows how the teachers' focus may change from session to session."], "caption": "Figure 6. Graphs of 6 sessions by T1 with the yet unmodified graph of session 7.", "local_uri": ["e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "(a) Rendering of the pavilion structure, (b) Photo of the partially-completed pavilion, (c) Closeup photo of watch screen displaying instructions, (d) Photo of a participant attaching a part to the structure, (e) Photo of a participant attaching a glowing LED node to a part on the structure.", "levels": null, "corpus_id": 18150448, "sentences": ["(a) Rendering of the pavilion structure, (b) Photo of the partially-completed pavilion, (c) Closeup photo of watch screen displaying instructions, (d) Photo of a participant attaching a part to the structure, (e) Photo of a participant attaching a glowing LED node to a part on the structure."], "caption": "Figure 1. We explore crowdsourced fabrication through the collaborative construction of a 12-foot tall bamboo pavilion (a). The pavilion was built with the assistance of more than one hundred untrained volunteers over a 3-day exhibition, enhanced and enabled by an intelligent construction space (b). Workers were guided by smartwatch devices (c), wireless LED modules embedded in building materials (d, e), and a backend engine that coordinated the overall build process.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_001.png"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "(a) Watch screen with text: \"Lets try it now! Go to STATION 1: TRAINING (look for the PINK light), (b) Watch screen with text: \"Next, tie off the thread to the bottom tip with a double knot.\" and a small photo showing a knot on a stick endcap.", "levels": null, "corpus_id": 18150448, "sentences": ["(a) Watch screen with text: \"Lets try it now! Go to STATION 1: TRAINING (look for the PINK light), (b) Watch screen with text: \"Next, tie off the thread to the bottom tip with a double knot.\" and a small photo showing a knot on a stick endcap."], "caption": "Figure 4. Example guidance screens from the smartwatch app.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "Labeled photo of a robot arm attached to a table, holding a completed tensegrity module. Also labeled is the end effector, a webcam and lightbox connected to the table, and the stick loading platform.", "levels": null, "corpus_id": 18150448, "sentences": ["Labeled photo of a robot arm attached to a table, holding a completed tensegrity module.", "Also labeled is the end effector, a webcam and lightbox connected to the table, and the stick loading platform."], "caption": "Figure 6. Individual Hive components are made up of three bamboo sticks, with a thread winding holding the module together. The winding is performed by a 6-axis robotic arm.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_007.png"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "Screenshot of the dashboard, showing a rendering of the structure with completed nodes highlighted and a big \"65% Complete (146 of 224 units)\" heading at the top. Also includes smaller areas displaying the status of Human Workers, Robot Workers, social media posts, and a system log.", "levels": null, "corpus_id": 18150448, "sentences": ["Screenshot of the dashboard, showing a rendering of the structure with completed nodes highlighted and a big \"65% Complete (146 of 224 units)\" heading at the top.", "Also includes smaller areas displaying the status of Human Workers, Robot Workers, social media posts, and a system log."], "caption": "Figure 7. The foreman engine dashboard, displayed at the entrance to the exhibit.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "Diagram of the main components of the system and the connections and protocols used for communication between individual components.", "levels": null, "corpus_id": 18150448, "sentences": ["Diagram of the main components of the system and the connections and protocols used for communication between individual components."], "caption": "Figure 8. The system architecture.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "(a) Diagram of the end effector with a bolt highlighted. (b) Diagram showing how the clamp channel should be aligned. (c) Diagram showing a stick in the clamp, positioned so that its colored end effector matches the color of a glowing light on the loading platform.", "levels": null, "corpus_id": 18150448, "sentences": ["(a) Diagram of the end effector with a bolt highlighted. (b) Diagram showing how the clamp channel should be aligned. (c) Diagram showing a stick in the clamp, positioned so that its colored end effector matches the color of a glowing light on the loading platform."], "caption": "Figure 9. Instruction provided for loading the bamboo sticks. The user loosened a bolt (a) and then adjusted the angle of a clamp (b). (c) An LED on the loading platform indicates which stick to insert and the proper orientation.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "(a) Diagram showing glowing blue and yellow connector nodes on the structure, and a part with corresponding colored end caps lined up with the glowing nodes. (b) Diagram showing a part attached to the structure with green and pink endcaps at the bottom, and two glowing connector nodes lined up with these endcaps but not yet connected.", "levels": null, "corpus_id": 18150448, "sentences": ["(a) Diagram showing glowing blue and yellow connector nodes on the structure, and a part with corresponding colored end caps lined up with the glowing nodes. (b) Diagram showing a part attached to the structure with green and pink endcaps at the bottom, and two glowing connector nodes lined up with these endcaps but not yet connected."], "caption": "Figure 10. (a) Connector nodes on the structure pulse in colors that indicate the location and orientation to attach a part. (b) The worker\u2019s connector nodes pulse in colors that indicate the end caps they should be attached to.", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "Three charts, one for each of the three days of the conference. Each chart shows time (x-axis) and part number (y-axis), and indicates the start through end time for each part with a line. Labels on the charts indicate when individual rings were completed, and show two periods on Day 2 where no building occurred, labeled \"Replacing modules\" and \"Repairing structure\".", "levels": [[1], [1], [1]], "corpus_id": 18150448, "sentences": ["Three charts, one for each of the three days of the conference.", "Each chart shows time (x-axis) and part number (y-axis), and indicates the start through end time for each part with a line.", "Labels on the charts indicate when individual rings were completed, and show two periods on Day 2 where no building occurred, labeled \"Replacing modules\" and \"Repairing structure\"."], "caption": "", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "Timeline showing the median duration taken by volunteer participants to complete individual steps in the buld process. Training (66s), Stick Gathering (43s), End Effector Adjustment (107s), Loading Stick 1 (68s), Loading Stick 2 (51s), Loading Stick 3 (43s), Stick Measuring (61s), Tying String (84s), Winding (184s), Node Gathering (146s), Removing Module (151s), Finding Location (20s), Attaching Module (95s), Attaching Connectors (72s).", "levels": null, "corpus_id": 18150448, "sentences": ["Timeline showing the median duration taken by volunteer participants to complete individual steps in the buld process.", "Training (66s), Stick Gathering (43s), End Effector Adjustment (107s), Loading Stick 1 (68s), Loading Stick 2 (51s), Loading Stick 3 (43s), Stick Measuring (61s), Tying String (84s), Winding (184s), Node Gathering (146s), Removing Module (151s), Finding Location (20s), Attaching Module (95s), Attaching Connectors (72s)."], "caption": "Figure 13. Median times for individual steps in the build process (volunteer participants).", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_014.gif"], "annotated": false, "compound": false}
{"title": "Crowdsourced Fabrication", "pdf_hash": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "year": 2016, "venue": "UIST", "alt_text": "Charts showing results for the question \"Please rate how challenging or straightforward you found each of the following steps in the build process\", for each of Registration, Collecting sticks, Loading sticks into the robot, String winding, Unloading the completed part, Locating the correct position and orientation for the part on the pavilion, and Attaching the part to the pavilion.", "levels": [[1]], "corpus_id": 18150448, "sentences": ["Charts showing results for the question \"Please rate how challenging or straightforward you found each of the following steps in the build process\", for each of Registration, Collecting sticks, Loading sticks into the robot, String winding, Unloading the completed part, Locating the correct position and orientation for the part on the pavilion, and Attaching the part to the pavilion."], "caption": "", "local_uri": ["9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_015.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HomeSound: An Iterative Field Deployment of an In-Home Sound Awareness System for Deaf or Hard of Hearing Users", "pdf_hash": "df8b4524d4612960264644d0df9e5c229466e17e", "year": 2020, "venue": "CHI", "alt_text": "HomeSound Prototype 1 interface showing the floorplan (top) and history (bottom) view with a bookmark button to take screenshot and type feedback.", "levels": null, "corpus_id": 209332578, "sentences": ["HomeSound Prototype 1 interface showing the floorplan (top) and history (bottom) view with a bookmark button to take screenshot and type feedback."], "caption": "Figure 2: HomeSound Prototype 1 interface showing the floorplan view (top half) and history view (bottom half).", "local_uri": ["df8b4524d4612960264644d0df9e5c229466e17e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "HomeSound: An Iterative Field Deployment of an In-Home Sound Awareness System for Deaf or Hard of Hearing Users", "pdf_hash": "df8b4524d4612960264644d0df9e5c229466e17e", "year": 2020, "venue": "CHI", "alt_text": "HomeSound Prototype 2 that extends Prototype 1 interface with showing the sound (in this case, \"Speech\") the confidence level (\"87%\") and a settings button to select the sounds to show.", "levels": null, "corpus_id": 209332578, "sentences": ["HomeSound Prototype 2 that extends Prototype 1 interface with showing the sound (in this case, \"Speech\") the confidence level (\"87%\") and a settings button to select the sounds to show."], "caption": "Figure 4: HomeSound Prototype 2 interface for displays", "local_uri": ["df8b4524d4612960264644d0df9e5c229466e17e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Blending into Everyday Life: Designing a Social Media-Based Peer Support System", "pdf_hash": "3da7758cb720d42753947abe208b1d1c2aa42b55", "year": 2021, "venue": "CHI", "alt_text": "8 example cards with bright cartoon illustrations and text. From left to right:  Encouraging others to share their stories in a group. The image is of a mobile phone.  Welcoming new people to a group. The image is of a welcome sign.  Sending reminders to people about the clinical tests. The image is a hypodermic needle.  Asking if people need help using WhatsApp. The image is the green WhatsApp logo.  Sending individual messages if you've noticed somone has not contributed for a few days. The image is a large question mark.  Recommending diet friendly places to eat out in a group. The image is of a menu.  Give tips on how to say no to people who are offering you food, without being rude. The image is of a beef burger with a strikethrough line.  Replying quickly to people who ask questions. The image is of a stopwatch.", "levels": null, "corpus_id": 233987740, "sentences": ["8 example cards with bright cartoon illustrations and text.", "From left to right:  Encouraging others to share their stories in a group.", "The image is of a mobile phone.", "Welcoming new people to a group.", "The image is of a welcome sign.", "Sending reminders to people about the clinical tests.", "The image is a hypodermic needle.", "Asking if people need help using WhatsApp.", "The image is the green WhatsApp logo.", "Sending individual messages if you've noticed somone has not contributed for a few days.", "The image is a large question mark.", "Recommending diet friendly places to eat out in a group.", "The image is of a menu.", "Give tips on how to say no to people who are offering you food, without being rude.", "The image is of a beef burger with a strikethrough line.", "Replying quickly to people who ask questions.", "The image is of a stopwatch."], "caption": "", "local_uri": ["3da7758cb720d42753947abe208b1d1c2aa42b55_Image_002.jpg", "3da7758cb720d42753947abe208b1d1c2aa42b55_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Blending into Everyday Life: Designing a Social Media-Based Peer Support System", "pdf_hash": "3da7758cb720d42753947abe208b1d1c2aa42b55", "year": 2021, "venue": "CHI", "alt_text": "On the left is a large sheet of paper divided into a 6 frame storyboard. Each frame of the storyboard is illustrated with cut out props of people talking and using WhatsApp. Words accompany describe using whatsapp to provide other options for socializing away from unhealthy food.  On the right is another large piece of paper. This one is divided into 4 large quadrants, by a central cross axis. There are a number of cards placed on the paper, the majority of which are in the top right quadrant, indicating that HAs are happy to do this, and need no help.", "levels": null, "corpus_id": 233987740, "sentences": ["On the left is a large sheet of paper divided into a 6 frame storyboard.", "Each frame of the storyboard is illustrated with cut out props of people talking and using WhatsApp.", "Words accompany describe using whatsapp to provide other options for socializing away from unhealthy food.", "On the right is another large piece of paper.", "This one is divided into 4 large quadrants, by a central cross axis.", "There are a number of cards placed on the paper, the majority of which are in the top right quadrant, indicating that HAs are happy to do this, and need no help."], "caption": "", "local_uri": ["3da7758cb720d42753947abe208b1d1c2aa42b55_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Say It All: Feedback for Improving Non-Visual Presentation Accessibility", "pdf_hash": "a15cd58325eb7334786ba0de65f961f66965fd44", "year": 2021, "venue": "CHI", "alt_text": "Figure 6: A bar graph comparing Presentation A11y and the Default Interface. The y axis is out of 7 points. The comparisons are on the subjects of self-perceived accessibility, helpfulness as reminding for mentioning visuals, and causing distraction. Self-perceived accessibility: Presentation A11y scored 5.06, while Default Interface scored 4.00. Helpfulness as reminding for mentioning visuals: Presentation A11y scored 5.56, while Default Interface scored 2.94. Causing distraction: Presentation A11y scored 2.56, while Default Interface scored 2.06.", "levels": [[1], [1], [1], [2], [2], [2]], "corpus_id": 232380154, "sentences": ["Figure 6: A bar graph comparing Presentation A11y and the Default Interface.", "The y axis is out of 7 points.", "The comparisons are on the subjects of self-perceived accessibility, helpfulness as reminding for mentioning visuals, and causing distraction.", "Self-perceived accessibility: Presentation A11y scored 5.06, while Default Interface scored 4.00.", "Helpfulness as reminding for mentioning visuals: Presentation A11y scored 5.56, while Default Interface scored 2.94.", "Causing distraction: Presentation A11y scored 2.56, while Default Interface scored 2.06."], "caption": "", "local_uri": ["a15cd58325eb7334786ba0de65f961f66965fd44_Image_031.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Demonstration of G-ID: Identifying 3D Prints Using Slicing Parameters", "pdf_hash": "e1326d4dc023f90f06131767d737e10d8eeeb4ba", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "(a) Three input fields for labeling the sliced key covers in the software user interface.  (b) A rotoscope of 3D printer laying down traces. (c) The mobile app showing two detected results: \"Daphne\" on one callout, \"Miranda\" on the other. The bottom surface of the key cover, which has traces, is highlighted in another callout.", "levels": null, "corpus_id": 218483329, "sentences": ["(a) Three input fields for labeling the sliced key covers in the software user interface.", "(b) A rotoscope of 3D printer laying down traces.", "(c) The mobile app showing two detected results: \"Daphne\" on one callout, \"Miranda\" on the other.", "The bottom surface of the key cover, which has traces, is highlighted in another callout."], "caption": "Figure 1: 3D printed objects inherently possess surface patterns due to the angle of the print path and the thickness of the trace the 3D printer lays down. G-ID provides (a) a user interface for slicing individual instances of the same object with different settings and assigning labels to them. After (b) 3D printing, users can (c) identify each instance using the G-ID mobile app that uses image processing techniques to detect the slicing parameters and retrieve the associated labels.", "local_uri": ["e1326d4dc023f90f06131767d737e10d8eeeb4ba_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study", "pdf_hash": "9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "(View from inside HoloLens) Two people engaged in a conversation with each other. A caption window is placed on top of each speaker.", "levels": null, "corpus_id": 47018792, "sentences": ["(View from inside HoloLens) Two people engaged in a conversation with each other.", "A caption window is placed on top of each speaker."], "caption": "Figure 1a. Prototype 1: AR Windows displays captions in a HoloLens web browser window. Caption windows can be placed close to speakers or visual materials, such as lecture slides in 3D space.", "local_uri": ["9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study", "pdf_hash": "9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "(View from inside HoloLens) A conversational partner is pointing to an equation on the white board. Captions appear in white text at the bottom of the picture. Captions read \"So this hear x plus y equals..\"", "levels": null, "corpus_id": 47018792, "sentences": ["(View from inside HoloLens) A conversational partner is pointing to an equation on the white board.", "Captions appear in white text at the bottom of the picture.", "Captions read \"So this hear x plus y equals..\""], "caption": "", "local_uri": ["9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study", "pdf_hash": "9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "The first author and their conversational partner are engaged in a 1:1 conversation. The first author is wearing a HoloLens and is writing on a notepad while looking down.", "levels": null, "corpus_id": 47018792, "sentences": ["The first author and their conversational partner are engaged in a 1:1 conversation.", "The first author is wearing a HoloLens and is writing on a notepad while looking down."], "caption": "(a)", "local_uri": ["9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study", "pdf_hash": "9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "Three conversational partners are sitting in a lounge including the first author. The first author is wearing a HoloLens and has their hand up, in an attempt to position the captions in space.", "levels": null, "corpus_id": 47018792, "sentences": ["Three conversational partners are sitting in a lounge including the first author.", "The first author is wearing a HoloLens and has their hand up, in an attempt to position the captions in space."], "caption": "(b)", "local_uri": ["9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study", "pdf_hash": "9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "(View from inside HoloLens) The first author has their hand up and is trying to position a caption browser window near a conversational partner.", "levels": null, "corpus_id": 47018792, "sentences": ["(View from inside HoloLens) The first author has their hand up and is trying to position a caption browser window near a conversational partner."], "caption": "(c)", "local_uri": ["9230baf1c24c7c4dd0da5774cc0569ade8a8fdfe_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "The top is the text being typed, the middle is the candidate list, the bottom is the keyboard interface", "levels": null, "corpus_id": 233987652, "sentences": ["The top is the text being typed, the middle is the candidate list, the bottom is the keyboard interface"], "caption": "", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "Left is the three visual effects on correction, the text is \"she is coming home\"; the middle is different style of text markers, the text is \"will you go to\"; the right is the suggestion list interface.", "levels": null, "corpus_id": 233987652, "sentences": ["Left is the three visual effects on correction, the text is \"she is coming home\"; the middle is different style of text markers, the text is \"will you go to\"; the right is the suggestion list interface."], "caption": "", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_004.png"], "annotated": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "The text editor is an android application. The top is showing the presented text, the middle is the text editing area, beheath there are two buttons to undo or proceed to next prhase, and on the bottom there is the keyboard", "levels": null, "corpus_id": 233987652, "sentences": ["The text editor is an android application.", "The top is showing the presented text, the middle is the text editing area, beheath there are two buttons to undo or proceed to next prhase, and on the bottom there is the keyboard"], "caption": "", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_012.png"], "annotated": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "The PhraseFlow keyboard interface with buffer commiting method. Top is the text being typed, bottom is the suggestion list.", "levels": null, "corpus_id": 233987652, "sentences": ["The PhraseFlow keyboard interface with buffer commiting method.", "Top is the text being typed, bottom is the suggestion list."], "caption": "", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_015.png"], "annotated": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "Bar charts of word per minute, character error rate, word error rate, in-word IKI and between-word IKI of Gboard and Phraseflow. The statistics are reported in section 6.4", "levels": [[1], [0]], "corpus_id": 233987652, "sentences": ["Bar charts of word per minute, character error rate, word error rate, in-word IKI and between-word IKI of Gboard and Phraseflow.", "The statistics are reported in section 6.4"], "caption": "Figure 6: The in-lab study results of Gboard and PhraseFlow. Error bars are one standard deviation", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_016.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "Boxplot of SUS and TLX score on Gboard and PhraseFlow. Scores are reported in section 6.4", "levels": [[1], [0]], "corpus_id": 233987652, "sentences": ["Boxplot of SUS and TLX score on Gboard and PhraseFlow.", "Scores are reported in section 6.4"], "caption": "7.1 Preparation\u200c", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_017.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "Perceived speed bar chart of phraseflow in day 2,4,and 6 of the deployment study. Results are reported in section 7.4", "levels": [[1], [0]], "corpus_id": 233987652, "sentences": ["Perceived speed bar chart of phraseflow in day 2,4,and 6 of the deployment study.", "Results are reported in section 7.4"], "caption": "", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_019.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "pdf_hash": "1b2d659346931daf853e0640d874557a86b4eb2a", "year": 2021, "venue": "CHI", "alt_text": "Perceived accuracy bar chart of phraseflow in day 2,4,and 6 of the deployment study. Results are reported in section 7.4", "levels": [[1], [0]], "corpus_id": 233987652, "sentences": ["Perceived accuracy bar chart of phraseflow in day 2,4,and 6 of the deployment study.", "Results are reported in section 7.4"], "caption": "", "local_uri": ["1b2d659346931daf853e0640d874557a86b4eb2a_Image_020.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Technology Adoption and Learning Preferences for Older Adults:: Evolving Perceptions, Ongoing Challenges, and Emerging Design Opportunities", "pdf_hash": "471fbf15c2ccb59fa88ec08f55f495cd8518098d", "year": 2021, "venue": "CHI", "alt_text": "Figure 1 shows the two phases of the research study. Phase 1 included online questionnaire with N=42, followed by individual semi-structured interviews with N=19 and and dyadic semi-structured interviews with N=8. Phase 2 included a video prototype development and design probe interviews with N=13.", "levels": null, "corpus_id": 233987001, "sentences": ["Figure 1 shows the two phases of the research study.", "Phase 1 included online questionnaire with N=42, followed by individual semi-structured interviews with N=19 and and dyadic semi-structured interviews with N=8.", "Phase 2 included a video prototype development and design probe interviews with N=13."], "caption": "", "local_uri": ["471fbf15c2ccb59fa88ec08f55f495cd8518098d_Image_002.png"], "annotated": false, "compound": false}
{"title": "\u201cIt\u2019s Complicated\u201d: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability", "pdf_hash": "938061e2073bef9286e65120ff062259f25f8f4e", "year": 2021, "venue": "CHI", "alt_text": "Leila, a black, non-binary person with a filtering face mask walks down a neighborhood street with one hand in their pocket and the other hand on their cane. They have a short mohawk and are wearing a jacket, shorts, tennis shoes, and glasses.", "levels": null, "corpus_id": 232084526, "sentences": ["Leila, a black, non-binary person with a filtering face mask walks down a neighborhood street with one hand in their pocket and the other hand on their cane.", "They have a short mohawk and are wearing a jacket, shorts, tennis shoes, and glasses."], "caption": "", "local_uri": ["938061e2073bef9286e65120ff062259f25f8f4e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Don\u2019t You Know That You\u2019re Toxic: Normalization of Toxicity in Online Gaming", "pdf_hash": "ca0fb3862482f03c365c0788cfba4afa077fc0c1", "year": 2021, "venue": "CHI", "alt_text": "Histograms that show participants largely skew towards identifying as gamers, a varied spread of Overwatch knowledge, and a varied spread of Overwatch skill (although there are more low skilled players than high skilled players).", "levels": null, "corpus_id": 233987538, "sentences": ["Histograms that show participants largely skew towards identifying as gamers, a varied spread of Overwatch knowledge, and a varied spread of Overwatch skill (although there are more low skilled players than high skilled players)."], "caption": "", "local_uri": ["ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_005.jpg", "ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_006.jpg", "ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Don\u2019t You Know That You\u2019re Toxic: Normalization of Toxicity in Online Gaming", "pdf_hash": "ca0fb3862482f03c365c0788cfba4afa077fc0c1", "year": 2021, "venue": "CHI", "alt_text": "Histograms showing that the audio clips are generally perceived by participants to be as toxic as intended (LowTox is perceived as the least toxic, MidTox as medium toxic, and HighTox as the most toxic). A median split shows that participants are more likely to report the HighTox clip if the more toxic they rate it.", "levels": null, "corpus_id": 233987538, "sentences": ["Histograms showing that the audio clips are generally perceived by participants to be as toxic as intended (LowTox is perceived as the least toxic, MidTox as medium toxic, and HighTox as the most toxic).", "A median split shows that participants are more likely to report the HighTox clip if the more toxic they rate it."], "caption": "", "local_uri": ["ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_008.jpg", "ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_009.jpg", "ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_010.jpg", "ca0fb3862482f03c365c0788cfba4afa077fc0c1_Image_011.jpg"], "annotated": false, "compound": true}
{"title": "Power Play: How the Need to Empower or Overpower Other Players Predicts Preferences in League of Legends", "pdf_hash": "de99d2e06c5b8761ecc3fbccb094ec234c4144c4", "year": 2020, "venue": "CHI", "alt_text": "This image shows what the Summoner's Rift map in League of Legends looks like. On the lower right corner we see the base of the red team and 5 arrows marking the way of the five players towards the lanes and jungle and the same from the base of the blue team (lower left corner)", "levels": null, "corpus_id": 218482695, "sentences": ["This image shows what the Summoner's Rift map in League of Legends looks like. On the lower right corner we see the base of the red team and 5 arrows marking the way of the five players towards the lanes and jungle and the same from the base of the blue team (lower left corner)"], "caption": "Figure 1. Schematic outline of the League of Legends 5v5 map Summoner\u2019s Rift. Each arrow represents one player. Jungler lines are not representative of routes. The Jungle is the entire area between the three lanes.", "local_uri": ["de99d2e06c5b8761ecc3fbccb094ec234c4144c4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Power Play: How the Need to Empower or Overpower Other Players Predicts Preferences in League of Legends", "pdf_hash": "de99d2e06c5b8761ecc3fbccb094ec234c4144c4", "year": 2020, "venue": "CHI", "alt_text": "This table shows that in study 1, study 3 and study 4 there are no significant associations between the overall power motive and preferences for roles and champion types. Cronbach's alpha for all role and champion type scales used in the paper are displayed and range from .70 for the fighter scale in study 4 to .96 for the jungle scale in study 1.", "levels": null, "corpus_id": 218482695, "sentences": ["This table shows that in study 1, study 3 and study 4 there are no significant associations between the overall power motive and preferences for roles and champion types.", "Cronbach's alpha for all role and champion type scales used in the paper are displayed and range from .70 for the fighter scale in study 4 to .96 for the jungle scale in study 1."], "caption": "Table 1. Regressions between the explicit power motive and role as well as champion type preferences in League of Legends, when not distinguishing facets of power motivation. None of these results were statistically significant.", "local_uri": ["de99d2e06c5b8761ecc3fbccb094ec234c4144c4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Power Play: How the Need to Empower or Overpower Other Players Predicts Preferences in League of Legends", "pdf_hash": "de99d2e06c5b8761ecc3fbccb094ec234c4144c4", "year": 2020, "venue": "CHI", "alt_text": "The Eigenvale of the scales are 2.457 for prosociality and 1.824 for dominance. Cronbach's Alpha is .802 for prosociality and .780 for dominance. For prosociality we see a mean of 4.971 and a standard deviation of 1.22; for dominance the mean is 4.440 and the standard deviation is 1.47. All scale items and factor loadings of the two scales are presented, e.g., \"I enjoy other players when they need help\" for prosociality and \"I enjoy dominating others in a game\" for dominance.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218482695, "sentences": ["The Eigenvale of the scales are 2.457 for prosociality and 1.824 for dominance.", "Cronbach's Alpha is .802 for prosociality and .780 for dominance.", "For prosociality we see a mean of 4.971 and a standard deviation of 1.22; for dominance the mean is 4.440 and the standard deviation is 1.47.", "All scale items and factor loadings of the two scales are presented, e.g., \"I enjoy other players when they need help\" for prosociality and \"I enjoy dominating others in a game\" for dominance."], "caption": "", "local_uri": ["de99d2e06c5b8761ecc3fbccb094ec234c4144c4_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Power Play: How the Need to Empower or Overpower Other Players Predicts Preferences in League of Legends", "pdf_hash": "de99d2e06c5b8761ecc3fbccb094ec234c4144c4", "year": 2020, "venue": "CHI", "alt_text": "This image shows the distribution of our participants on our dominance and prosocial scales. Few people score low in both domains and we see that most participants are relatively high in dominance motivation, most participants score high on both scales.", "levels": [[1], [3, 2]], "corpus_id": 218482695, "sentences": ["This image shows the distribution of our participants on our dominance and prosocial scales.", "Few people score low in both domains and we see that most participants are relatively high in dominance motivation, most participants score high on both scales."], "caption": "Dominance versus Prosociality.", "local_uri": ["de99d2e06c5b8761ecc3fbccb094ec234c4144c4_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Power Play: How the Need to Empower or Overpower Other Players Predicts Preferences in League of Legends", "pdf_hash": "de99d2e06c5b8761ecc3fbccb094ec234c4144c4", "year": 2020, "venue": "CHI", "alt_text": "This table shows R\u00b2 and beta values for prosociality, dominance and their interaction (moderation) for the roles Top Lane, Mid Lane, Jungle, AD Carry, Support as well as the three support subtypes in study 3. The text highlights which of these are significant on a 5% alpha error level.", "levels": [[-1], [-1]], "corpus_id": 218482695, "sentences": ["This table shows R\u00b2 and beta values for prosociality, dominance and their interaction (moderation) for the roles Top Lane, Mid Lane, Jungle, AD Carry, Support as well as the three support subtypes in study 3.", "The text highlights which of these are significant on a 5% alpha error level."], "caption": "Table 3. Multiple regression models of the explicit power motive facets; N=133. The first column shows how much variance both facets predict together. Significant results are displayed in bold; *p<.05, **p<.01.", "local_uri": ["de99d2e06c5b8761ecc3fbccb094ec234c4144c4_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Power Play: How the Need to Empower or Overpower Other Players Predicts Preferences in League of Legends", "pdf_hash": "de99d2e06c5b8761ecc3fbccb094ec234c4144c4", "year": 2020, "venue": "CHI", "alt_text": "This table shows R\u00b2 and beta values for prosociality, dominance and their interaction (moderation) for the champion types tank, fighter, slayer, mage, controller, marksman in study 4. The text highlights which of these are significant on a 5% alpha error level.", "levels": [[-1], [-1]], "corpus_id": 218482695, "sentences": ["This table shows R\u00b2 and beta values for prosociality, dominance and their interaction (moderation) for the champion types tank, fighter, slayer, mage, controller, marksman in study 4.", "The text highlights which of these are significant on a 5% alpha error level."], "caption": "Table 4. Multiple regression models consisting of power facets; N=108. The first column shows how much variance both facets predict together. Significant results are displayed in bold;", "local_uri": ["de99d2e06c5b8761ecc3fbccb094ec234c4144c4_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 1: (a) VR involves interactions with upper limbs, which can lead to discomfort (b) Out approach retains visual objects in their location, but users can reach them from more comfortable positions (c). Our approach is based on defining two space partitioning trees, and using optimization approaches to look for most comfortable retargetings (visual to physical positions).", "levels": null, "corpus_id": 12742899, "sentences": ["Figure 1: (a) VR involves interactions with upper limbs, which can lead to discomfort (b) Out approach retains visual objects in their location, but users can reach them from more comfortable positions (c).", "Our approach is based on defining two space partitioning trees, and using optimization approaches to look for most comfortable retargetings (visual to physical positions)."], "caption": "", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 2: Comparison of different ergonomic metrics (RULA, CE). These all identify regions of space for more comfortable interaction with observable similarities.", "levels": null, "corpus_id": 12742899, "sentences": ["Figure 2: Comparison of different ergonomic metrics (RULA, CE).", "These all identify regions of space for more comfortable interaction with observable similarities."], "caption": "Figure 2: Comparison of different ergonomic metrics (RULA, CE). These all identify regions of space for more comfortable interaction with observable similarities.", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 3: Example of a tetrahedron pair, defining a volume in space P and its equivalent (slightly different) volume in V. Vertex O and edges are used to define their local systems of reference. This allows mappings any point in tetrahedron P to a single point in tetrahedron V, enabling retargeting.", "levels": null, "corpus_id": 12742899, "sentences": ["Figure 3: Example of a tetrahedron pair, defining a volume in space P and its equivalent (slightly different) volume in V. Vertex O and edges are used to define their local systems of reference.", "This allows mappings any point in tetrahedron P to a single point in tetrahedron V, enabling retargeting."], "caption": "These matrices allow us to directly map any physical point", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 4. Summary of our manipulation technique. (A&B) Side 3D view of the boundary space enclosing user\u2019s interactive range. (C&D) Tetrahedron-based partitioning of the physical and visual space (simplified 2D view). Matching tetrahedrons highlighted on same colours. (E) An interactive element inside a tetrahedron will cause it to be sub-divided in four tetrahedrons. (F) Continuity of interaction is assured when hand moves across tetrahedrons, but the direction and speed of motion can be affected.", "levels": null, "corpus_id": 12742899, "sentences": ["Figure 4.", "Summary of our manipulation technique. (A&B) Side 3D view of the boundary space enclosing user\u2019s interactive range.", "(C&D) Tetrahedron-based partitioning of the physical and visual space (simplified 2D view).", "Matching tetrahedrons highlighted on same colours. (E) An interactive element inside a tetrahedron will cause it to be sub-divided in four tetrahedrons. (F) Continuity of interaction is assured when hand moves across tetrahedrons, but the direction and speed of motion can be affected."], "caption": "Figure 4. Summary of our manipulation technique. (A&B) Side 3D view of the boundary space enclosing user\u2019s interactive range. (C&D) Tetrahedron-based partitioning of the physical and visual space (simplified 2D view). Matching tetrahedrons highlighted on same colours. (E) An interactive element inside a tetrahedron will cause it to be sub-divided in four tetrahedrons. (F) Continuity of interaction is assured when hand moves across tetrahedrons, but the direction and speed of motion can be affected.", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 5: User displacements will change the mapping of interactive elements, causing the hierarchical tree structure to be recomputed.", "levels": null, "corpus_id": 12742899, "sentences": ["Figure 5: User displacements will change the mapping of interactive elements, causing the hierarchical tree structure to be recomputed."], "caption": "Figure 5: User displacements will change the mapping of interactive elements, causing the hierarchical tree structure to be recomputed.", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 7: (A) Screenshot showing the selection task implemented in out testing environment and (B) third layout tested, with visual elements anchored to the world in two planes, and forcing users to walk in order to reach them.", "levels": null, "corpus_id": 12742899, "sentences": ["Figure 7: (A) Screenshot showing the selection task implemented in out testing environment and (B) third layout tested, with visual elements anchored to the world in two planes, and forcing users to walk in order to reach them."], "caption": "", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 9: Average measurements per strategy and layout: (a) Time completion task and (b) RULA scores during the task. Significant difference between retargeting strategies for each layout are represented by \u2018*\u2019.", "levels": [[1], [1]], "corpus_id": 12742899, "sentences": ["Figure 9: Average measurements per strategy and layout: (a) Time completion task and (b) RULA scores during the task.", "Significant difference between retargeting strategies for each layout are represented by \u2018*\u2019."], "caption": "", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "pdf_hash": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "year": 2017, "venue": "UIST", "alt_text": "Figure 8: Box plots for the results of our questionnaires. Horizontal red bars represent medians, and boxes represent the interquartile ranges (IQRs). Whiskers stretch to the data points that are within the median \u00b1 1.5 IQR.", "levels": [[1], [1], [2]], "corpus_id": 12742899, "sentences": ["Figure 8: Box plots for the results of our questionnaires.", "Horizontal red bars represent medians, and boxes represent the interquartile ranges (IQRs).", "Whiskers stretch to the data points that are within the median \u00b1 1.5 IQR."], "caption": "Figure 8: Box plots for the results of our questionnaires. Horizontal red bars represent medians, and boxes represent the interquartile ranges (IQRs). Whiskers stretch to the data points that are within the median \u00b1 1.5 IQR.", "local_uri": ["7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "pdf_hash": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "year": 2021, "venue": "ACM Trans. Comput. Hum. Interact.", "alt_text": "Caption: Fig 2. For each sensor, each feature was extracted from 45 time slices. First, raw data from the device sensor was preprocessed and then filtered by an epoch and a days-of-the-week option. Features (let NS be number of features derived from each sensor) were then extracted from the selected raw data according to 3 levels of granularity - per semester (NS features), per half-semester (2*NS features), and per week (16*NS features). Description: The figure shows  raw data being filtered by epoch (5 options), then by day of week (3 options) and then selection granularity (3 options) leading to feature extraction. Flowchart for temporal slicing is explained in section 4.1.8.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 222113385, "sentences": ["Caption: Fig 2.", "For each sensor, each feature was extracted from 45 time slices.", "First, raw data from the device sensor was preprocessed and then filtered by an epoch and a days-of-the-week option.", "Features (let NS be number of features derived from each sensor) were then extracted from the selected raw data according to 3 levels of granularity - per semester (NS features), per half-semester (2*NS features), and per week (16*NS features).", "Description: The figure shows  raw data being filtered by epoch (5 options), then by day of week (3 options) and then selection granularity (3 options) leading to feature extraction.", "Flowchart for temporal slicing is explained in section 4.1.8."], "caption": "", "local_uri": ["fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "pdf_hash": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "year": 2021, "venue": "ACM Trans. Comput. Hum. Interact.", "alt_text": "Caption: Figure 3 shows how depression status (\"no dep\" vs \"dep\") changed from pre to post semester. Description: Diagram showing that only 20 out of 138 students had depressive symptoms pre-semester. However, 56 out of 138 students had depressive symptoms post-semester. These 56 students included the 20 students who had depressive symptoms pre-semester, and 36 students who did not have depressive symptoms pre-semester.", "levels": null, "corpus_id": 222113385, "sentences": ["Caption: Figure 3 shows how depression status (\"no dep\" vs \"dep\") changed from pre to post semester.", "Description: Diagram showing that only 20 out of 138 students had depressive symptoms pre-semester.", "However, 56 out of 138 students had depressive symptoms post-semester.", "These 56 students included the 20 students who had depressive symptoms pre-semester, and 36 students who did not have depressive symptoms pre-semester."], "caption": "", "local_uri": ["fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "pdf_hash": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "year": 2021, "venue": "ACM Trans. Comput. Hum. Interact.", "alt_text": "Bar plot of Accuracy and F1 for detecting post-semester depression. Baseline accuracy was 59.4%. Bluetooth accuracy was 69.3% and F1 was 0.64. Calls accuracy was 68.5% and F1 was 0.59. Campus Map accuracy was 68.2% and F1 was 0.66. Location accuracy was 69.5% and F1 was 0.62. Phone Usage accuracy was 70.3% and F1 was 0.75. Sleep accuracy was 69.2% and F1 was 0.66. Steps accuracy was 63.6% and F1 was 0.53. All-7 accuracy was 82.3% and F1 was 0.78. Best set accuracy was 85.7% and F1 was 0.82.", "levels": [[1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 222113385, "sentences": ["Bar plot of Accuracy and F1 for detecting post-semester depression.", "Baseline accuracy was 59.4%.", "Bluetooth accuracy was 69.3% and F1 was 0.64.", "Calls accuracy was 68.5% and F1 was 0.59.", "Campus Map accuracy was 68.2% and F1 was 0.66.", "Location accuracy was 69.5% and F1 was 0.62.", "Phone Usage accuracy was 70.3% and F1 was 0.75.", "Sleep accuracy was 69.2% and F1 was 0.66.", "Steps accuracy was 63.6% and F1 was 0.53.", "All-7 accuracy was 82.3% and F1 was 0.78.", "Best set accuracy was 85.7% and F1 was 0.82."], "caption": "", "local_uri": ["fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "pdf_hash": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "year": 2021, "venue": "ACM Trans. Comput. Hum. Interact.", "alt_text": "Bar plot of Accuracy and F1 for detecting change in depression. Baseline accuracy was 65.9%. Bluetooth accuracy was 65.8% and F1 was 0.55. Calls accuracy was 64.8% and F1 was 0.50. Campus Map accuracy was 79.1% and F1 was 0.80. Location accuracy was 74.3% and F1 was 0.73. Phone Usage accuracy was 73.9% and F1 was 0.68. Sleep accuracy was 73.8% and F1 was 0.66. Steps accuracy was 68.2% and F1 was 0.56. All-7 accuracy was 75.9% and F1 was 0.67. Best set accuracy was 85.4% and F1 was 0.80.", "levels": [[1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 222113385, "sentences": ["Bar plot of Accuracy and F1 for detecting change in depression.", "Baseline accuracy was 65.9%.", "Bluetooth accuracy was 65.8% and F1 was 0.55.", "Calls accuracy was 64.8% and F1 was 0.50.", "Campus Map accuracy was 79.1% and F1 was 0.80.", "Location accuracy was 74.3% and F1 was 0.73.", "Phone Usage accuracy was 73.9% and F1 was 0.68.", "Sleep accuracy was 73.8% and F1 was 0.66.", "Steps accuracy was 68.2% and F1 was 0.56.", "All-7 accuracy was 75.9% and F1 was 0.67.", "Best set accuracy was 85.4% and F1 was 0.80."], "caption": "", "local_uri": ["fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "pdf_hash": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "year": 2021, "venue": "ACM Trans. Comput. Hum. Interact.", "alt_text": "Trend mostly upwards. Accuracy is 85.7% for N=91 in week 9. Accuracy is 84.6% for N=91 in week 10. Accuracy is 87.0% for N=92 in week 11. Accuracy is 87.1% for N=93 in week 12. Accuracy is 84.9% for N=93 in week 15. Accuracy is 83.9% for N=93 for all weeks.", "levels": null, "corpus_id": 222113385, "sentences": ["Trend mostly upwards.", "Accuracy is 85.7% for N=91 in week 9.", "Accuracy is 84.6% for N=91 in week 10.", "Accuracy is 87.0% for N=92 in week 11.", "Accuracy is 87.1% for N=93 in week 12.", "Accuracy is 84.9% for N=93 in week 15.", "Accuracy is 83.9% for N=93 for all weeks."], "caption": "", "local_uri": ["fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "pdf_hash": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "year": 2021, "venue": "ACM Trans. Comput. Hum. Interact.", "alt_text": "Trend mostly upwards, but clear dip seen at week 7. Accuracy is 89.0% for N=91 in week 8. Accuracy is 89.0% for N=91 in week 10. Accuracy is 89.2% for N=93 in week 12. Accuracy is 88.2% for N=93 in week 13. Accuracy is 89.2% for N=93 in week 14. Accuracy is 87.1% for N=93 for all weeks.", "levels": null, "corpus_id": 222113385, "sentences": ["Trend mostly upwards, but clear dip seen at week 7.", "Accuracy is 89.0% for N=91 in week 8.", "Accuracy is 89.0% for N=91 in week 10.", "Accuracy is 89.2% for N=93 in week 12.", "Accuracy is 88.2% for N=93 in week 13.", "Accuracy is 89.2% for N=93 in week 14.", "Accuracy is 87.1% for N=93 for all weeks."], "caption": "", "local_uri": ["fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Conversational and Expressive Style in a Multimodal Embodied Conversational Agent", "pdf_hash": "0c0c81e47f346fbc3a4c1e8e88ad70f8e39f40e2", "year": 2021, "venue": "CHI", "alt_text": "SIVA's Architecture: The system uses a fusion of both microphone and webcam inputs to drive the agent. Audio is segmented based on voice activity detection, and then speech and prosody recognition are performed. Generated dialogue is passed on to the conversational style manager, and along with the prosody style variables, conversational style aligned speech is synthesized, which is used for recognizing phonemes. The video pipeline detects the face to perform expression recognition and head pose tracking. The output from the text sentiment recognizer, along with emotional expression probabilities, is used to generate expressions. Finally, the outputs from both the pipelines are composited together to synthesize SIVA's expressive and lip-syncing behaviors.", "levels": null, "corpus_id": 233987526, "sentences": ["SIVA's Architecture: The system uses a fusion of both microphone and webcam inputs to drive the agent.", "Audio is segmented based on voice activity detection, and then speech and prosody recognition are performed.", "Generated dialogue is passed on to the conversational style manager, and along with the prosody style variables, conversational style aligned speech is synthesized, which is used for recognizing phonemes.", "The video pipeline detects the face to perform expression recognition and head pose tracking.", "The output from the text sentiment recognizer, along with emotional expression probabilities, is used to generate expressions.", "Finally, the outputs from both the pipelines are composited together to synthesize SIVA's expressive and lip-syncing behaviors."], "caption": "", "local_uri": ["0c0c81e47f346fbc3a4c1e8e88ad70f8e39f40e2_Image_003.png"], "annotated": false, "compound": false}
{"title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "pdf_hash": "fbd915aa1143821e150396f6e9ac20f2134c400d", "year": 2019, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Fig.1  The left most side of the figure shows the data collection stage, including seven type of data: bluetooth, call logs, screen status, location coordination, campus map, sleep logs and step logs. The grounded truth is from the BDI-II questionnaire. The middle part of the figure shows the feature extraction from the raw data. Then the features is fed into association rule minings. The output rules are fed back on the features to compute contextually filtered features. Finally, the right side of the figure shows the training stage.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 202159041, "sentences": ["Fig.1  The left most side of the figure shows the data collection stage, including seven type of data: bluetooth, call logs, screen status, location coordination, campus map, sleep logs and step logs.", "The grounded truth is from the BDI-II questionnaire.", "The middle part of the figure shows the feature extraction from the raw data.", "Then the features is fed into association rule minings.", "The output rules are fed back on the features to compute contextually filtered features.", "Finally, the right side of the figure shows the training stage."], "caption": "", "local_uri": ["fbd915aa1143821e150396f6e9ac20f2134c400d_Image_004.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "pdf_hash": "fbd915aa1143821e150396f6e9ac20f2134c400d", "year": 2019, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Fig.2  A complex flowchart of the detailed pipeline as described in Section 4.3. The upper part of the flow chart described the procedure of feature selection, rule mining and rule selection (applied on common rules and unique rules respectively.) of the RuleGenerateSet. Then, the output rules are applied on the TrainTestSet to extract contextually filtered features for machine learning training.  The rule selection and the contextually filtered features are the key contributions of this paper.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 202159041, "sentences": ["Fig.2  A complex flowchart of the detailed pipeline as described in Section 4.3.", "The upper part of the flow chart described the procedure of feature selection, rule mining and rule selection (applied on common rules and unique rules respectively.)", "of the RuleGenerateSet.", "Then, the output rules are applied on the TrainTestSet to extract contextually filtered features for machine learning training.", "The rule selection and the contextually filtered features are the key contributions of this paper."], "caption": "Fig. 2. The detailed pipeline of rule mining, pairing, selecting and models training. The dashed frame highlights the novel procedures in the pipeline.", "local_uri": ["fbd915aa1143821e150396f6e9ac20f2134c400d_Image_020.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "pdf_hash": "fbd915aa1143821e150396f6e9ac20f2134c400d", "year": 2019, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Fig.3.a  Two heatmaps of the top weekday rules selected by metrics. The color is mapped to the percentage of the students who have this behavior rule in that day. The left is for non-depression group and the right is for depression group. For each heatmap, the x-axis is the time of the study, and every rule is lined up on the y-axis. The color pattern in the middle term (in the middle of the x-axis) is different from other time.", "levels": null, "corpus_id": 202159041, "sentences": ["Fig.3.a  Two heatmaps of the top weekday rules selected by metrics.", "The color is mapped to the percentage of the students who have this behavior rule in that day.", "The left is for non-depression group and the right is for depression group.", "For each heatmap, the x-axis is the time of the study, and every rule is lined up on the y-axis.", "The color pattern in the middle term (in the middle of the x-axis) is different from other time."], "caption": "", "local_uri": ["fbd915aa1143821e150396f6e9ac20f2134c400d_Image_021.gif"], "annotated": false, "compound": false}
{"title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "pdf_hash": "fbd915aa1143821e150396f6e9ac20f2134c400d", "year": 2019, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Fig.3.b  Two heatmaps of the top weekend rules selected by metrics. The color is mapped to the percentage of the students who have this behavior rule in that day. The left is for non-depression group and the right is for depression group. For each heatmap, the x-axis is the time of the study, and every rule is lined up on the y-axis. The color pattern in the middle term (in the middle of the x-axis) is different from other time.", "levels": [[1], [1], [1], [1], [3]], "corpus_id": 202159041, "sentences": ["Fig.3.b  Two heatmaps of the top weekend rules selected by metrics.", "The color is mapped to the percentage of the students who have this behavior rule in that day.", "The left is for non-depression group and the right is for depression group.", "For each heatmap, the x-axis is the time of the study, and every rule is lined up on the y-axis.", "The color pattern in the middle term (in the middle of the x-axis) is different from other time."], "caption": "Weekend Rules (Lef: Students with no depressive symptoms, Right: depressive symptoms)Fig. 3. Heatmaps of prevalence of the top 105 rules among students with and without depressive symptoms, for weekends and weekdays. X axis is day of semester, Y axis shows rules aligned from morning to night epochs. Color indicates the proportion of students in a class that fulfill a particular rule. The brighter the color, the larger proportion of students having the patern. The abnormal vertical color paterns in the middle of both figures correspond to the mid-term examines and the break period, indicating that the rules can capture people\u2019s routine behavior. Rule names on the lef indicate some example rules that are significantly diferent between the two classes of participants (see Table 3).break period, some rules, which otherwise match many students, match very few students (represented by dark areas in the middle-top of the heatmap). This is positive evidence that contextually fltered features capture routine behavior, unlike their unimodal counterparts.We further investigate the rules that capture diferent behavior patterns between students with depressive symptoms and students without depressive symptoms. We used a paired t-test on every rule to identify the rules that were signifcantly diferent between the two groups. Table 3 summarizes a subset of the top 20 rules in weekday/weekend rules that show the strongest signifcant diference (see the full list of top rules in Appendix). (a) A Rule Common in Groups. More in Non-dep Group          (b) A Rule that is Unique to Non-dep Group (c) A Rule Common in Groups. More in Dep Group              (d) A Rule that is Unique to in Dep Group (e) A Similar Rule with Same Context Unique To          (f) A Similar Rule with Same Context Unique To Non-dep Group                                    Dep GroupFig. 4. Heatmaps of example rules that captures behavior diferences between depression group and non-depression group. The percentage indicates the average proportion of students fulfilling the rule throughout the study period. Color indicates proportion of students on each day (X axis) Table 3 summarizes the details of the rules.Weekday night rule No.5 (the frst row in Table 3) indicates that students are be likely to have good sleep quality when they are on-campus and have low co-location (i.e., the number of Bluetooth encounters) during weekday nights. This rule is present in both groups, but appears signifcantly more in the non-depression group (t75 = 3.99, p < 0.001, see Figure 4a). Weekday night rule No.9 (the second row in Table 3) indicates students\u2019 sleep bouts (periods of continuous sleep) are likely to be longer when they are on-campus and sleep efciency is high. This rule is unique to the non-depression group (t75 = 2.88, p < 0.01, see Figure 4b).\u2212Weekend morning rule No.3 (the third row in Table 3) indicates that when students have poor sleep (the sleep is intermittent), they are more likely to have low mobility (few location transitions) during weekend mornings (6am - 12pm). This rule is found in both groups, with signifcantly more students in the depression group (t29 = 2.54, p < 0.05, see Figure 4c) experiencing this. The CondDisc also shows that students with depressive symptoms are more like to match this rule\u2019s context (0.36 vs. 0.27), indicating worse sleep quality.Another example rule refects the relationship of mobile phone usage, sleep duration and depression. Weekend night rule No. 19 (the fourth row in Table 3) is only ranked high enough to be selected in the depression group. This suggests the potential efect of phone usage on sleep quality for depressive students. We discuss these fndings more in Section 6.We also fnd interesting unique rules in each group that refected the diferences between the two groups. Weekend morning rules No. 10 and No.11 (the last two rows in Table 3) share the same context set X , but have diferent Y s. Rule No. 10 is unique to the non-depression group and rule No. 11 is unique to the depression group. They indicated that students without depressive symptoms are more likely to have a high sleep efciency when their location movement is low during weekend morning periods, but students with depressive symptoms are more likely to only have a medium sleep efciency for the same context (see Figure 4e and 4f).Table 3. Examples of rules that capture behavior diference between students with and without depressive symptoms. We tested a rule\u2019s ability to diferentiate between classes using a paired t-test; significance level is indicated in the Rule column. We selected rules for this table that show the strongest significant diference. All top 20 weekday/weekend rules can be found in the appendix. Type is the method by which the rule was found. Prop in Non-dep and Prop in Dep are the proportion of students in a class that fulfill the rule, averaged over days in the study. Note that M varies between diferent epochs (people can have diferent behavior patern during the day) as well as their types (i.e., common or unique). E.g., M of a weekday night rule can be much bigger than that of a weekday morning rule.Rule                X                                            Y                           TypeProp in     Prop in    Ctx     Conf  CondNon-dep       Dep      Spec   Dif       Disc    MWkdy Night No.5\u2217\u2217\u2217[CampusMap] Percentage oftime of-campus (low)                [Sleep] Sleep[Bluetooth] Number of            efciency (high) unique device of others (low)Common        11.4%     8.5%     2   0.137 0.094 0.031Wkdy Night No.9\u2217\u2217Wkend[CampusMap] Percentage of time of-campus (low)[Sleep] Sleep efciency (high)[Sleep] Number of[Sleep] Maximum length ofasleep bouts (high)Unique (Non-dep)10.1%     7.9%     2   0.535 0.335 0.453Morning No.3\u2217bouts being asleep (high)           [Location] Number of[Sleep] Number of                    location transition (low) bouts being restless (high)Common         9.2%     12.3%     2   0.054 0.081 0.007Wkend     - [Screen] Mean length of          [Sleep] Mean length of     UniqueNight No.19\u2217screen being unlock (high)         being asleep (low)              (Dep)[Location] Number of7.8%     10.3%     1   0.387 0.374 0.147Wkend     location transition (low)             [Sleep] Sleep                      UniqueMorning     - [CampusMap] Number of No.10\u2217\u2217\u2217     building transitionon-campus (low)- [Location] Number ofefciency (high)               (Non-dep)11.2%     5.5%     2   0.456 0.469 0.422Wkend     location transition (low)             [Sleep] Sleep                      UniqueMorning     - [CampusMap] Number of No.11\u2217\u2217\u2217     building transitionon-campus (low)efciency (medium)          (Dep)9.8%     16.6%     2   0.435 0.483 0.398* indicates p < 0.05, \u2217\u2217 indicates p < 0.01, \u2217\u2217\u2217 indicates p < 0.001", "local_uri": ["fbd915aa1143821e150396f6e9ac20f2134c400d_Image_022.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "pdf_hash": "fbd915aa1143821e150396f6e9ac20f2134c400d", "year": 2019, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Fig.4.ab  Two heatmaps of two rules, respectively. For each heatmaps, the x-asix is the time of the study, and the y-axis only have two points: depression group and non-depression group. The color is mapped to the percentage of the students who have this behavior rule in that day.  The left figure is for weekday night rule No.5, which is common in both groups but more in non-depression group. Non-depression group have an average of 11.35 percentage of students having this rules during the study. And depression group only have 8.50 percentages.   The right figure is for weekday night rule No.9, which is unique in non-depression group. Non-depression group have an average of 10.14 percentage. And depression group only have 7.93 percentages.     Fig.4.cd  The left figure is for weekend morning rule No.3, which is common in both groups but more in depression group. Depression group have an average of 12.25 percentage. And non-depression group only have 9.22 percentages.   The right figure is for weekend night rule No.19, which is unique in depression group. Depression group have an average of 10.27 percentage. And non-depression group only have 7.84 percentages.", "levels": [[1], [1], [1], [2], [2, 1], [2], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 202159041, "sentences": ["Fig.4.ab  Two heatmaps of two rules, respectively.", "For each heatmaps, the x-asix is the time of the study, and the y-axis only have two points: depression group and non-depression group.", "The color is mapped to the percentage of the students who have this behavior rule in that day.", "The left figure is for weekday night rule No.5, which is common in both groups but more in non-depression group.", "Non-depression group have an average of 11.35 percentage of students having this rules during the study.", "And depression group only have 8.50 percentages.", "The right figure is for weekday night rule No.9, which is unique in non-depression group.", "Non-depression group have an average of 10.14 percentage.", "And depression group only have 7.93 percentages.", "Fig.4.cd  The left figure is for weekend morning rule No.3, which is common in both groups but more in depression group.", "Depression group have an average of 12.25 percentage.", "And non-depression group only have 9.22 percentages.", "The right figure is for weekend night rule No.19, which is unique in depression group.", "Depression group have an average of 10.27 percentage.", "And non-depression group only have 7.84 percentages."], "caption": "(a) A Rule Common in Groups. More in Non-dep Group          (b) A Rule that is Unique to Non-dep Group", "local_uri": ["fbd915aa1143821e150396f6e9ac20f2134c400d_Image_023.gif", "fbd915aa1143821e150396f6e9ac20f2134c400d_Image_024.gif", "fbd915aa1143821e150396f6e9ac20f2134c400d_Image_025.gif", "fbd915aa1143821e150396f6e9ac20f2134c400d_Image_026.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "pdf_hash": "fbd915aa1143821e150396f6e9ac20f2134c400d", "year": 2019, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Fig.4.ef  The left figure is for weekend morning rule No.10,  and the right is for weekend morning rule No.11. They have the same X but different Y.  Weekend morning rule No.10 is unique in non-depression group. Non-depression group have an average of 11.18 percentage. And depression group only have 5.50 percentages.   Weekend morning rule No.11 is unique in depression group. Depression group have an average of 16.58 percentage. And non-depression group only have 9.80 percentages.", "levels": [[1], [1], [2], [2], [2], [2], [2]], "corpus_id": 202159041, "sentences": ["Fig.4.ef  The left figure is for weekend morning rule No.10,  and the right is for weekend morning rule No.11.", "They have the same X but different Y.  Weekend morning rule No.10 is unique in non-depression group.", "Non-depression group have an average of 11.18 percentage.", "And depression group only have 5.50 percentages.", "Weekend morning rule No.11 is unique in depression group.", "Depression group have an average of 16.58 percentage.", "And non-depression group only have 9.80 percentages."], "caption": "(e) A Similar Rule with Same Context Unique To          (f) A Similar Rule with Same Context Unique To Non-dep Group                                    Dep Group", "local_uri": ["fbd915aa1143821e150396f6e9ac20f2134c400d_Image_027.gif", "fbd915aa1143821e150396f6e9ac20f2134c400d_Image_028.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "Sensing Tablet Grasp + Micro-mobility for Active Reading", "pdf_hash": "93847034441c8c0d5000b1dc614171c020bddb7c", "year": 2015, "venue": "UIST", "alt_text": "This illustration shows another multi-tablet usage scenario, where a single user has three tablets arranged on a desk in front of himself. And the steps of the interaction are numbered and highlighted with arrows showing how information flows across the tablets, as follows: 1. Finger (on outer edge of screen) indicates part of page. 2. Content echoed (on the second device). This is identified as the Fine-Grained Reference technique in the figure. The third tablet is not used but simply makes it clear that the tablets involved are the one being touched (by a finger along the outer edge to indicate part of a page) and the one being interacted with, and where the indicated content is echoed. The user is shown about to select part of the echoed content with a pen on the second tablet.", "levels": null, "corpus_id": 16950793, "sentences": ["This illustration shows another multi-tablet usage scenario, where a single user has three tablets arranged on a desk in front of himself. And the steps of the interaction are numbered and highlighted with arrows showing how information flows across the tablets, as follows: 1.", "Finger (on outer edge of screen) indicates part of page.", "2. Content echoed (on the second device).", "This is identified as the Fine-Grained Reference technique in the figure.", "The third tablet is not used but simply makes it clear that the tablets involved are the one being touched (by a finger along the outer edge to indicate part of a page) and the one being interacted with, and where the indicated content is echoed.", "The user is shown about to select part of the echoed content with a pen on the second tablet."], "caption": "Figure 8. Fine-grained reference to part of a document.", "local_uri": ["93847034441c8c0d5000b1dc614171c020bddb7c_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Assessing Social Anxiety Through Digital Biomarkers Embedded in a Gaming Task", "pdf_hash": "2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "year": 2021, "venue": "CHI", "alt_text": "Left: The layout of the task room, which is divided into an exposure room and an assessment room. Marker on the map show the position of the NPC and target position. The right image shows two versions of the NPC: One angry looking white male in a suit and one neutral looking male in a casual outfit.", "levels": null, "corpus_id": 233987701, "sentences": ["Left: The layout of the task room, which is divided into an exposure room and an assessment room.", "Marker on the map show the position of the NPC and target position.", "The right image shows two versions of the NPC: One angry looking white male in a suit and one neutral looking male in a casual outfit."], "caption": "", "local_uri": ["2edfa40cbe5e9843a2a7ff198ed0df59a6269865_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Assessing Social Anxiety Through Digital Biomarkers Embedded in a Gaming Task", "pdf_hash": "2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "year": 2021, "venue": "CHI", "alt_text": "The four steps of the movement task: Image 1: A third person perspective of the player's avatar standing in front of a mirror. Image 2: The player approaches the NPC. Image 3: The attention task is shown. A text box on the screen shows text: \"What is the hair colour of the agent?\" and different answer options are displayed. Image 4: The player's avatar walking past the NPC.", "levels": null, "corpus_id": 233987701, "sentences": ["The four steps of the movement task: Image 1: A third person perspective of the player's avatar standing in front of a mirror.", "Image 2: The player approaches the NPC.", "Image 3: The attention task is shown.", "A text box on the screen shows text: \"What is the hair colour of the agent?\" and different answer options are displayed.", "Image 4: The player's avatar walking past the NPC."], "caption": "", "local_uri": ["2edfa40cbe5e9843a2a7ff198ed0df59a6269865_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Assessing Social Anxiety Through Digital Biomarkers Embedded in a Gaming Task", "pdf_hash": "2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "year": 2021, "venue": "CHI", "alt_text": "Left: The image shows the gaming task from the first person perspective, in which the player only sees the room but not the own avatar; the second image shows the third person perspective, in which the player sees the world and their own avatar. Right: Images on the right show the avatar creator for predefined avatars and the one for customizing the own avatar.", "levels": null, "corpus_id": 233987701, "sentences": ["Left: The image shows the gaming task from the first person perspective, in which the player only sees the room but not the own avatar; the second image shows the third person perspective, in which the player sees the world and their own avatar.", "Right: Images on the right show the avatar creator for predefined avatars and the one for customizing the own avatar."], "caption": "", "local_uri": ["2edfa40cbe5e9843a2a7ff198ed0df59a6269865_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Assessing Social Anxiety Through Digital Biomarkers Embedded in a Gaming Task", "pdf_hash": "2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "year": 2021, "venue": "CHI", "alt_text": "This image visualizes the steps of the process of calculating the skew and kurtosis of the distribution for the movement around the NPC.", "levels": [[-1]], "corpus_id": 233987701, "sentences": ["This image visualizes the steps of the process of calculating the skew and kurtosis of the distribution for the movement around the NPC."], "caption": "", "local_uri": ["2edfa40cbe5e9843a2a7ff198ed0df59a6269865_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Assessing Social Anxiety Through Digital Biomarkers Embedded in a Gaming Task", "pdf_hash": "2edfa40cbe5e9843a2a7ff198ed0df59a6269865", "year": 2021, "venue": "CHI", "alt_text": "This Figure shows the relationsip between the digital biomarkers and LSAS for the four conditions; We see that Third Person Perspective and Customized Characters have the most relationships between the digital biomarkers in the game and LSAS", "levels": null, "corpus_id": 233987701, "sentences": ["This Figure shows the relationsip between the digital biomarkers and LSAS for the four conditions; We see that Third Person Perspective and Customized Characters have the most relationships between the digital biomarkers in the game and LSAS"], "caption": "", "local_uri": ["2edfa40cbe5e9843a2a7ff198ed0df59a6269865_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "\u201cI just went into it assuming that I wouldn't be able to have the full experience\u201d: Understanding the Accessibility of Virtual Reality for People with Limited Mobility", "pdf_hash": "d401ea7b58bb6b333bbee309a6e498313c4820b4", "year": 2020, "venue": "ASSETS", "alt_text": "(left) a participant wearing a VR headset with the image focusing on the adjustment knob located on the back. (center) a participant sitting in a wheelchair using their left arm using their right hand to steady a VR controller. (right) a participant using two hands to press and hold down a controller joystick.", "levels": null, "corpus_id": 221408980, "sentences": ["(left) a participant wearing a VR headset with the image focusing on the adjustment knob located on the back. (center) a participant sitting in a wheelchair using their left arm using their right hand to steady a VR controller. (right) a participant using two hands to press and hold down a controller joystick."], "caption": "", "local_uri": ["d401ea7b58bb6b333bbee309a6e498313c4820b4_Image_002.png"], "annotated": false, "compound": false}
{"title": "ReFind: Design, Lived Experience and Ongoingness in Bereavement", "pdf_hash": "2eb0deaa1d909d64f8370673e7a38fa7925377e0", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: A hand holding the ReFind object which is circular, white and has a circular small screen in the centre surrounded by a brass ring. On the screen is a woman seated and a young child sat on a table next to her.", "levels": null, "corpus_id": 218482813, "sentences": ["Figure 1: A hand holding the ReFind object which is circular, white and has a circular small screen in the centre surrounded by a brass ring.", "On the screen is a woman seated and a young child sat on a table next to her."], "caption": "Figure 1. ReFind \u00a9 Jayne Wallace.", "local_uri": ["2eb0deaa1d909d64f8370673e7a38fa7925377e0_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "ReFind: Design, Lived Experience and Ongoingness in Bereavement", "pdf_hash": "2eb0deaa1d909d64f8370673e7a38fa7925377e0", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: A photograph showing foam models being made on a lathe in a workshop. Components of the piece before construction laying on a workbench.", "levels": [[-1], [-1]], "corpus_id": 218482813, "sentences": ["Figure 2: A photograph showing foam models being made on a lathe in a workshop.", "Components of the piece before construction laying on a workbench."], "caption": "Figure 2. Design Development Components.", "local_uri": ["2eb0deaa1d909d64f8370673e7a38fa7925377e0_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ReFind: Design, Lived Experience and Ongoingness in Bereavement", "pdf_hash": "2eb0deaa1d909d64f8370673e7a38fa7925377e0", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: Photographs of ReFind being held in someones hands, showing the object from the front and the back.", "levels": [[-1]], "corpus_id": 218482813, "sentences": ["Figure 3: Photographs of ReFind being held in someones hands, showing the object from the front and the back."], "caption": "Figure 3. ReFind Artefact \u00a9 Jayne Wallace.", "local_uri": ["2eb0deaa1d909d64f8370673e7a38fa7925377e0_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ReFind: Design, Lived Experience and Ongoingness in Bereavement", "pdf_hash": "2eb0deaa1d909d64f8370673e7a38fa7925377e0", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: A close up of a small sketchbook, open to show that each account captured a small drawing of the image sent to the piece, written description of the five images that came back - along with written tags, experience of use, robustness of the system and any changes over time.", "levels": null, "corpus_id": 218482813, "sentences": ["Figure 5: A close up of a small sketchbook, open to show that each account captured a small drawing of the image sent to the piece, written description of the five images that came back - along with written tags, experience of use, robustness of the system and any changes over time."], "caption": "Figure 5. Diary of Use", "local_uri": ["2eb0deaa1d909d64f8370673e7a38fa7925377e0_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "pdf_hash": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "year": 2021, "venue": "CHI", "alt_text": "Figure 1 is a chart illustrating participants' ratings of the usefulness of the information they verbally shared while working. The chart has 6 mini bar charts. On the left are 3 mini bar charts for the concurrent think-aloud condition, and on the right are 3 mini bar charts for the retrospective think-aloud condition. For each condition, there is a mini bar chart per domain (i.e., coding, models, and slides). First we describe the concurrent think-aloud ratings: For concurrent think-aloud for the coding domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 3 (i.e., 3 participants out of 4) for \"moderately useful\", and a blue bar of height 1 for \"very useful\". For concurrent think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 4 for \"moderately useful\", and a bar of height 0 for \"very useful\". For concurrent think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 2 for \"moderately useful\", and a blue bar of height 2 for \"very useful\". Next we describe the retrospective think-aloud ratings: For retrospective think-aloud for the coding domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 3 for \"moderately useful\", and a blue bar of height 1 for \"very useful\". For retrospective think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 4 for \"moderately useful\", and a bar of height 0 for \"very useful\". For retrospective think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 1 for \"moderately useful\", and a blue bar of height 3 for \"very useful\". Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Not Useful, and 7-Extremely Useful). Ratings (1, 2) have been aggregated into \"not useful\", ratings (3, 4, 5) have been aggregated into \"moderately useful\", and ratings (6, 7) have been aggregated into \"very useful\".", "levels": [[1], [1], [1], [1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [1], [1]], "corpus_id": 231993774, "sentences": ["Figure 1 is a chart illustrating participants' ratings of the usefulness of the information they verbally shared while working.", "The chart has 6 mini bar charts.", "On the left are 3 mini bar charts for the concurrent think-aloud condition, and on the right are 3 mini bar charts for the retrospective think-aloud condition.", "For each condition, there is a mini bar chart per domain (i.e., coding, models, and slides).", "First we describe the concurrent think-aloud ratings: For concurrent think-aloud for the coding domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 3 (i.e., 3 participants out of 4) for \"moderately useful\", and a blue bar of height 1 for \"very useful\".", "For concurrent think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 4 for \"moderately useful\", and a bar of height 0 for \"very useful\".", "For concurrent think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 2 for \"moderately useful\", and a blue bar of height 2 for \"very useful\".", "Next we describe the retrospective think-aloud ratings: For retrospective think-aloud for the coding domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 3 for \"moderately useful\", and a blue bar of height 1 for \"very useful\".", "For retrospective think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 4 for \"moderately useful\", and a bar of height 0 for \"very useful\".", "For retrospective think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 1 for \"moderately useful\", and a blue bar of height 3 for \"very useful\".", "Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Not Useful, and 7-Extremely Useful).", "Ratings (1, 2) have been aggregated into \"not useful\", ratings (3, 4, 5) have been aggregated into \"moderately useful\", and ratings (6, 7) have been aggregated into \"very useful\"."], "caption": "", "local_uri": ["8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "pdf_hash": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "year": 2021, "venue": "CHI", "alt_text": "Figure 2 is a chart illustrating participants' ratings of their confidence in capturing all the important information while speaking while working. The chart has 6 mini bar charts. On the left are 3 mini bar charts for the concurrent think-aloud condition, and on the right are 3 mini bar charts for the retrospective think-aloud condition. For each condition, there is a mini bar chart per domain (i.e., coding, models, and slides). First we describe the concurrent think-aloud ratings: For concurrent think-aloud for the coding domain, there are 3 bars, from left: an orange bar of height 1 (i.e., 1 participant out of 4) for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 3 for \"confident\". For concurrent think-aloud for the models domain, there are 3 bars, from left: an orange bar of height 2 for \"not confident\", a yellow bar of height 2 for \"neutral\", and a bar of height 0 for \"confident\". For concurrent think-aloud for the slides domain, there are 3 bars, from left: an orange bar of height 1 for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 3 for \"confident\". For retrospective think-aloud for the coding domain, there are 3 bars, from left: an orange bar of height 2 for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 2 for \"confident\". For retrospective think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not confident\", a yellow bar of height 1 for \"neutral\", and a green bar of height 3 for \"confident\". For retrospective think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not confident\", a yellow bar of height 1 for \"neutral\", and a green bar of height 3 for \"confident\". Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Very Not Confident, and 7-Very Confident). Ratings (1, 2, 3) have been aggregated into \"Not Confident\", rating (4) is \"Neutral\", and ratings (5, 6, 7) have been aggregated into \"Confident\".", "levels": [[1], [1], [1], [1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [1], [1]], "corpus_id": 231993774, "sentences": ["Figure 2 is a chart illustrating participants' ratings of their confidence in capturing all the important information while speaking while working.", "The chart has 6 mini bar charts.", "On the left are 3 mini bar charts for the concurrent think-aloud condition, and on the right are 3 mini bar charts for the retrospective think-aloud condition.", "For each condition, there is a mini bar chart per domain (i.e., coding, models, and slides).", "First we describe the concurrent think-aloud ratings: For concurrent think-aloud for the coding domain, there are 3 bars, from left: an orange bar of height 1 (i.e., 1 participant out of 4) for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 3 for \"confident\".", "For concurrent think-aloud for the models domain, there are 3 bars, from left: an orange bar of height 2 for \"not confident\", a yellow bar of height 2 for \"neutral\", and a bar of height 0 for \"confident\".", "For concurrent think-aloud for the slides domain, there are 3 bars, from left: an orange bar of height 1 for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 3 for \"confident\".", "For retrospective think-aloud for the coding domain, there are 3 bars, from left: an orange bar of height 2 for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 2 for \"confident\".", "For retrospective think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not confident\", a yellow bar of height 1 for \"neutral\", and a green bar of height 3 for \"confident\".", "For retrospective think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not confident\", a yellow bar of height 1 for \"neutral\", and a green bar of height 3 for \"confident\".", "Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Very Not Confident, and 7-Very Confident).", "Ratings (1, 2, 3) have been aggregated into \"Not Confident\", rating (4) is \"Neutral\", and ratings (5, 6, 7) have been aggregated into \"Confident\"."], "caption": "", "local_uri": ["8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "pdf_hash": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "year": 2021, "venue": "CHI", "alt_text": "Figure 4: In the top left there are two versions of the visual widget presented in Figure 5. The one of the left is identical to the one in Figure 5, and the one on the right has 2 additional smaller wedges, one on the far left and one on the far right. Beneath these visual widgets are three yellow rubber ducks with varying fill-levels; the far-left one is at a low fill-level, with opaque yellow at the bottom but transparent above; the duck in the middle is half-filled; and the duck on the right is completely filled. In the top-right of the figure is the Apple menu bar, of notice on the left side are red, blue, and orange circles with different fill-levels (low fill level means mostly transparent except for the very bottom). In the bottom-right of the figure is a check-list kind of interface, where each item has icons representing the design intent (lightbulb), process (tools), important (star), and problem (alert) items.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 231993774, "sentences": ["Figure 4: In the top left there are two versions of the visual widget presented in Figure 5.", "The one of the left is identical to the one in Figure 5, and the one on the right has 2 additional smaller wedges, one on the far left and one on the far right.", "Beneath these visual widgets are three yellow rubber ducks with varying fill-levels; the far-left one is at a low fill-level, with opaque yellow at the bottom but transparent above; the duck in the middle is half-filled; and the duck on the right is completely filled.", "In the top-right of the figure is the Apple menu bar, of notice on the left side are red, blue, and orange circles with different fill-levels (low fill level means mostly transparent except for the very bottom).", "In the bottom-right of the figure is a check-list kind of interface, where each item has icons representing the design intent (lightbulb), process (tools), important (star), and problem (alert) items."], "caption": "", "local_uri": ["8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "pdf_hash": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "year": 2021, "venue": "CHI", "alt_text": "Figure 5: Visual widget with 3 wedges at the top, short yellow on the left (with a lightbulb icon) representing \"design intent\", tall blue in the middle (with a person speaking icon) representing \"all utterances\", and medium red on the right (with a tool icon) representing \"process\". Each wedge also has a dotted line representing the wedge fill-goal. Below the wedges is a circle with 3 parts representing buttons: the top half has a star in a center and represents \"important\", the bottom left quadrant has a checklist and represents \"to-do item\", and the bottom right quadrant has an alert icon and represents \"problem\". Each wedge or button has associated keywords: for design intent: \"this is meant to\", \"because\", \"goal\", for process: \"this tool is for\", \"this tool is useful for\", for to-do item: \"i still need to\", \"i still have to\", for problem: \"issue\", \"problem\", for important: \"it is important\"", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 231993774, "sentences": ["Figure 5: Visual widget with 3 wedges at the top, short yellow on the left (with a lightbulb icon) representing \"design intent\", tall blue in the middle (with a person speaking icon) representing \"all utterances\", and medium red on the right (with a tool icon) representing \"process\".", "Each wedge also has a dotted line representing the wedge fill-goal.", "Below the wedges is a circle with 3 parts representing buttons: the top half has a star in a center and represents \"important\", the bottom left quadrant has a checklist and represents \"to-do item\", and the bottom right quadrant has an alert icon and represents \"problem\".", "Each wedge or button has associated keywords: for design intent: \"this is meant to\", \"because\", \"goal\", for process: \"this tool is for\", \"this tool is useful for\", for to-do item: \"i still need to\", \"i still have to\", for problem: \"issue\", \"problem\", for important: \"it is important\""], "caption": "", "local_uri": ["8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "pdf_hash": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "year": 2021, "venue": "CHI", "alt_text": "Figure 6: A screenshot of a rectangular computer window. The window has 2 sections, each one having rows of speech utterances on the left, YouTube video clips in the center, and Fusion 360 command names on the right. Each section has a gray background.", "levels": null, "corpus_id": 231993774, "sentences": ["Figure 6: A screenshot of a rectangular computer window.", "The window has 2 sections, each one having rows of speech utterances on the left, YouTube video clips in the center, and Fusion 360 command names on the right.", "Each section has a gray background."], "caption": "A live archive window ( ) presents transcribed utterances and their real-time labels alongside corresponding video capture and software commands used. This serves both as an initial presentation interface for people who want to leverage the spoken information and its context, as well as a post hoc editing interface for creators.", "local_uri": ["8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "pdf_hash": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "year": 2021, "venue": "CHI", "alt_text": "Figure 8 is a chart illustrating participants' ratings of how much effort it took to document knowledge for think-aloud and traditional documentation techniques. The chart has 2 mini bar charts. On the left is the bar chart for the think-aloud condition. It has 3 bars, from left: a red bar of height 3 (i.e., 3 participants out of 12) for the category of \"high effort\", a yellow bar of height 3 for \"medium effort\", and a green bar of height 6 for \"low effort\". On the right is the bar chart for the traditional documentation condition. It has 3 bars, from left: a red bar of height 4 for the \"high effort\", a yellow bar of height 2 for \"medium effort\", and a green bar of height 6 for \"low effort\". Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Very Low Effort, and 7-Very High Effort). Ratings (1, 2, 3) have been aggregated into \"low effort\", rating (4) is \"medium effort\", and ratings (5, 6, 7) have been aggregated into \"high effort\".", "levels": [[1], [1], [1], [2, 1], [1], [2, 1], [1], [1]], "corpus_id": 231993774, "sentences": ["Figure 8 is a chart illustrating participants' ratings of how much effort it took to document knowledge for think-aloud and traditional documentation techniques.", "The chart has 2 mini bar charts.", "On the left is the bar chart for the think-aloud condition.", "It has 3 bars, from left: a red bar of height 3 (i.e., 3 participants out of 12) for the category of \"high effort\", a yellow bar of height 3 for \"medium effort\", and a green bar of height 6 for \"low effort\".", "On the right is the bar chart for the traditional documentation condition.", "It has 3 bars, from left: a red bar of height 4 for the \"high effort\", a yellow bar of height 2 for \"medium effort\", and a green bar of height 6 for \"low effort\".", "Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Very Low Effort, and 7-Very High Effort).", "Ratings (1, 2, 3) have been aggregated into \"low effort\", rating (4) is \"medium effort\", and ratings (5, 6, 7) have been aggregated into \"high effort\"."], "caption": "", "local_uri": ["8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_018.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 1 shows the workflow of our design tool. Figure 1.a shows an input 3D model of a seahorse. Figure 1.b shows that the user can select the seahorse\u2019s body and convert it into two nesting springs. The user also can change the length of the springs. Figure 1.c shows that the user can control the spring stiffness by dragging sliders and the springs will updated with the new stiffness. Figure 1.d shows that the user also can customize the spring deformation behavior by adding and parameterizing a mechanical joint. Figure 1.e shows that the 3D-printed design which is a seahorse with embedded springs and joint.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 1 shows the workflow of our design tool.", "Figure 1.a shows an input 3D model of a seahorse.", "Figure 1.b shows that the user can select the seahorse\u2019s body and convert it into two nesting springs.", "The user also can change the length of the springs.", "Figure 1.c shows that the user can control the spring stiffness by dragging sliders and the springs will updated with the new stiffness.", "Figure 1.d shows that the user also can customize the spring deformation behavior by adding and parameterizing a mechanical joint.", "Figure 1.e shows that the 3D-printed design which is a seahorse with embedded springs and joint."], "caption": ". We introduce Ondul\u00e9, an interactive tool that allows designers to create and control deformable objects with embedded springs and joints. Above, a workflow shows how to make a solid seahorse body bendable and twistable: (a) select a seahorse body; (b) change the spring length and regenerate the spring directly on the model; (c) control spring stiffness; (d) parameterize spring deformation behaviors by adding additional joints; and", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 2 lists all spring configurations and spring deformation behaviors. Figure 2.a shows that a spring can be compressed. The spring parameters, including wire thickness, spring diameter, and number of coil turns, are also marked in the spring. Figure 2.b shows that a spring can be extended. Figure 2.c shows that a spring can be rotated/twisted. Figure 2.d shows that a spring can be laterally bent.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 2 lists all spring configurations and spring deformation behaviors.", "Figure 2.a shows that a spring can be compressed.", "The spring parameters, including wire thickness, spring diameter, and number of coil turns, are also marked in the spring.", "Figure 2.b shows that a spring can be extended.", "Figure 2.c shows that a spring can be rotated/twisted.", "Figure 2.d shows that a spring can be laterally bent."], "caption": ". Basic helical spring deformation behaviors: (a) compress, (b) extend, (c) twist, and (d) laterally bend.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 3 presents the photos of the setups in mechanical experiments. Figure 3.a shows the load frame and the extensometers in Experiment 1. A green 3D-printed rod is installed in and stretched by the load frame grips. Two extensometers are attached on the rod to measure the displacement change in the rod under load in two perpendicular directions. Figure 3.b shows a red 3D-printed helical spring is stretched by the load frame grip in Experiment 2. Figure 3.c shows a white 3D-printed helical spring is connected to a torque sensor on one end and a stepper motor on the other end in the Experiment 3 setup. A protractor on the motor side is used to calibrate the rotation angle readings.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 3 presents the photos of the setups in mechanical experiments.", "Figure 3.a shows the load frame and the extensometers in Experiment 1.", "A green 3D-printed rod is installed in and stretched by the load frame grips.", "Two extensometers are attached on the rod to measure the displacement change in the rod under load in two perpendicular directions.", "Figure 3.b shows a red 3D-printed helical spring is stretched by the load frame grip in Experiment 2.", "Figure 3.c shows a white 3D-printed helical spring is connected to a torque sensor on one end and a stepper motor on the other end in the Experiment 3 setup.", "A protractor on the motor side is used to calibrate the rotation angle readings."], "caption": ". Mechanical experiment setups: (a) the load frame stretches a 3D-printed rod; (b) the load frame stretches a 3D-printed helical; and (c) the motor rotates a helical spring and torque is measured.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 4 details the dimensions of the test rod for Experiment 1 and the printing settings for all tests. In the top-left corner, a diagram of the sideview of the rod is annotated with an 8mm diameter and a 40mm gauge length. In the top-right corner, it showcases the cross-sectional view of the rods with line vs triangle infill patterns and 20% infill density. The bottom part shows five different infill density examples using the cross-sectional view of the rod, as well as the five printing orientations, where vertical is 0 degree and horizontal is 90 degree.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 4 details the dimensions of the test rod for Experiment 1 and the printing settings for all tests.", "In the top-left corner, a diagram of the sideview of the rod is annotated with an 8mm diameter and a 40mm gauge length.", "In the top-right corner, it showcases the cross-sectional view of the rods with line vs triangle infill patterns and 20% infill density.", "The bottom part shows five different infill density examples using the cross-sectional view of the rod, as well as the five printing orientations, where vertical is 0 degree and horizontal is 90 degree."], "caption": ". The 3D-printed solid rods in Experiment 1 and three varied test conditions: infill density, infill pattern, and print orientation.\u200c", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 5 presents four line charts of the results of Experiment 1. Blue hollow circles represent Young\u2019s modulus E, orange hollow rectangles mean shear modulus G, and green hollow triangle indicate tensile strength. From left to right, it shows the distribution of E and G with different infill densities, the distribution of E and G with infill patterns, the distribution of tensile strength with different printing orientations, and the distribution of E and G with different printing orientations.", "levels": [[1], [1], [1]], "corpus_id": 199501237, "sentences": ["Figure 5 presents four line charts of the results of Experiment 1.", "Blue hollow circles represent Young\u2019s modulus E, orange hollow rectangles mean shear modulus G, and green hollow triangle indicate tensile strength.", "From left to right, it shows the distribution of E and G with different infill densities, the distribution of E and G with infill patterns, the distribution of tensile strength with different printing orientations, and the distribution of E and G with different printing orientations."], "caption": ". Experiment 1 results showing that E and G increase with infill density as well as more robust infill patterns. Tensile strength increases as printing angle increases; however, shear stress is highest at 45\u00b0.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 6 presents four line charts of the results of Experiment 2 (tensile tests). Blue hollow circles represent the experimental results and orange hollow rectangles indicate theoretical predictions. From left to right, it shows the difference between experimental results and theoretical predictions in terms of varied spring wire thickness, spring diameter, number of coil turn, and spring length.", "levels": [[1], [1], [1]], "corpus_id": 199501237, "sentences": ["Figure 6 presents four line charts of the results of Experiment 2 (tensile tests).", "Blue hollow circles represent the experimental results and orange hollow rectangles indicate theoretical predictions.", "From left to right, it shows the difference between experimental results and theoretical predictions in terms of varied spring wire thickness, spring diameter, number of coil turn, and spring length."], "caption": "", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_016.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 7 presents four line charts of the results of Experiment 3 (torsion tests). Blue hollow circles represent the experimental results and orange hollow rectangles indicate theoretical predictions. From left to right, it shows the difference between experimental results and theoretical predictions in terms of varied spring wire thickness, spring diameter, number of coil turn, and spring length.", "levels": [[1], [1], [1]], "corpus_id": 199501237, "sentences": ["Figure 7 presents four line charts of the results of Experiment 3 (torsion tests).", "Blue hollow circles represent the experimental results and orange hollow rectangles indicate theoretical predictions.", "From left to right, it shows the difference between experimental results and theoretical predictions in terms of varied spring wire thickness, spring diameter, number of coil turn, and spring length."], "caption": "", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_018.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 8 shows the prismatic joint structure and how it works. From left to right, it shows a seahorse model with blue arrows that indicate the seahorse only can compress and extend, a red 3D model with annotations explaining each part in the joint, and three diagrams illustrating that the prismatic joint can be compressed and extended and the slide moves inside the shaft.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 8 shows the prismatic joint structure and how it works.", "From left to right, it shows a seahorse model with blue arrows that indicate the seahorse only can compress and extend, a red 3D model with annotations explaining each part in the joint, and three diagrams illustrating that the prismatic joint can be compressed and extended and the slide moves inside the shaft."], "caption": ". A prismatic joint is used for a linear-only deformation.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 9 shows the revolute joint structure and how it works. From left to right, it shows a seahorse model with blue arrows that indicate the seahorse only can twist, a red 3D model with annotations explaining each part in the joint, and two diagrams illustrating that the revolute joint can be rotated.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 9 shows the revolute joint structure and how it works.", "From left to right, it shows a seahorse model with blue arrows that indicate the seahorse only can twist, a red 3D model with annotations explaining each part in the joint, and two diagrams illustrating that the revolute joint can be rotated."], "caption": ". A revolute joint is used for a twist-only deformation.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_022.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 10 shows the chain of knuckle joints and how they work. From left to right, it shows a seahorse model with blue arrows that indicate the seahorse only can bend in one plane, a red 3D model with annotations explaining each part in the joint, and two diagrams illustrating that the chain can be bent in one direction.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 10 shows the chain of knuckle joints and how they work.", "From left to right, it shows a seahorse model with blue arrows that indicate the seahorse only can bend in one plane, a red 3D model with annotations explaining each part in the joint, and two diagrams illustrating that the chain can be bent in one direction."], "caption": "", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_024.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 11 shows two joint designs that support compound spring deformation behaviors. Figure 11.a shows the cylindrical joint structure and how it works. From left to right, it shows a seahorse model with blue arrows that indicate the seahorse can compress, extend, and twist, a red 3D model with annotations explaining each part in the joint, and a diagram illustrating that the joint can compress, extend, and twist. Figure 11.b shows the chain of ball joints and how they work. From left to right, it shows a seahorse model with blue arrows that indicate the seahorse can twist and bend in many directions, a red 3D model with annotations explaining each part in the joint, and a diagram illustrating that the joint can twist and bend in any directions.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 11 shows two joint designs that support compound spring deformation behaviors.", "Figure 11.a shows the cylindrical joint structure and how it works.", "From left to right, it shows a seahorse model with blue arrows that indicate the seahorse can compress, extend, and twist, a red 3D model with annotations explaining each part in the joint, and a diagram illustrating that the joint can compress, extend, and twist.", "Figure 11.b shows the chain of ball joints and how they work. From left to right, it shows a seahorse model with blue arrows that indicate the seahorse can twist and bend in many directions, a red 3D model with annotations explaining each part in the joint, and a diagram illustrating that the joint can twist and bend in any directions."], "caption": ". (a) A cylindrical joint is used for linear+twist deformations and (b) a chain of ball joints is used for twist+bend deformations.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_026.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 12 shows the interface of our design tool. The figure has two parts. On the left side, the figure shows the Rhino edit scene on the left and the Rhino plugin on the right. The plugin includes three parts. From top to bottom, they are spring generation panel, spring stiffness control panel, and spring behavior design panel. In the spring generation panel, there is a \u201cControl to spring\u201d button and a \u201cChange spring length\u201d button. All the edited springs are recorded in a horizontal box beneath the two buttons. In the spring stiffness control panel, the basic stiffness control allows the user to drag a stiffness level bar to make the change and the advanced stiffness control allows the user to change spring wire diameters and turn gap. In the spring behavior design panel, the user can add different joint design by checking the ratio button. The user can parameterize each joint design in a graphical interface. On the right side of the figure, from top to bottom, it shows the workflows of all three panels. For the top spring generation panel, the user can first select the model body, then generate the spring, and can change the spring length. With spring stiffness control panel, the user can drag the bar to make the change and the new spring with updated stiffness is rendered in the Rhino modeling scene. With the spring behavior design panel, the user can add a joint to the model and the join is rendered in red in the model. By changing the parameters, in this case, changing the compression and extension displacement, the joint is updated directly in the 3D model.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 199501237, "sentences": ["Figure 12 shows the interface of our design tool.", "The figure has two parts.", "On the left side, the figure shows the Rhino edit scene on the left and the Rhino plugin on the right.", "The plugin includes three parts.", "From top to bottom, they are spring generation panel, spring stiffness control panel, and spring behavior design panel.", "In the spring generation panel, there is a \u201cControl to spring\u201d button and a \u201cChange spring length\u201d button.", "All the edited springs are recorded in a horizontal box beneath the two buttons.", "In the spring stiffness control panel, the basic stiffness control allows the user to drag a stiffness level bar to make the change and the advanced stiffness control allows the user to change spring wire diameters and turn gap.", "In the spring behavior design panel, the user can add different joint design by checking the ratio button.", "The user can parameterize each joint design in a graphical interface.", "On the right side of the figure, from top to bottom, it shows the workflows of all three panels.", "For the top spring generation panel, the user can first select the model body, then generate the spring, and can change the spring length.", "With spring stiffness control panel, the user can drag the bar to make the change and the new spring with updated stiffness is rendered in the Rhino modeling scene.", "With the spring behavior design panel, the user can add a joint to the model and the join is rendered in red in the model.", "By changing the parameters, in this case, changing the compression and extension displacement, the joint is updated directly in the 3D model."], "caption": ". The Ondul\u00e9 spring design tool interface (left) has four parts: Rhino modeling environment, a spring generation panel, a spring stiffness control panel, and a spring behavior design panel. The workflow for each design panel is shown on the right.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_028.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 13 shows how the medial axis is used to check if the selected body is printable with springs and has complex shape. From left to right, we take the discontinuous points on the medial axis, sample the curve segment between two discontinuous points, and calculate the distance between sample points and the surface.", "levels": [[-1], [-1]], "corpus_id": 199501237, "sentences": ["Figure 13 shows how the medial axis is used to check if the selected body is printable with springs and has complex shape.", "From left to right, we take the discontinuous points on the medial axis, sample the curve segment between two discontinuous points, and calculate the distance between sample points and the surface."], "caption": ". Generating the medial axis, calculating the size of the selected body, and evaluating the printability of an embedded spring.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_030.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 14 shows how to generate the deformation spring. From left to right, the media axis is generated, a spiral around the medial axis is generated, and a solid yellow spring is generated using the generated spiral.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 14 shows how to generate the deformation spring. From left to right, the media axis is generated, a spiral around the medial axis is generated, and a solid yellow spring is generated using the generated spiral."], "caption": ". Generating the deformation spring using the generated medial axis and RhinoCommon functions.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_032.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 15 shows how the decorative spring is generated. From left to right, we sample on the medial axis and on the generated spiral curve around the medial axis separately, create ray using corresponding sample points on the media axis and the spiral curve and intersect with the surface, retrace from the intersecting points on the surface with a fixed distance and generate the solid decorative spring.", "levels": [[-1], [-1]], "corpus_id": 199501237, "sentences": ["Figure 15 shows how the decorative spring is generated.", "From left to right, we sample on the medial axis and on the generated spiral curve around the medial axis separately, create ray using corresponding sample points on the media axis and the spiral curve and intersect with the surface, retrace from the intersecting points on the surface with a fixed distance and generate the solid decorative spring."], "caption": ". Generating the decorative spring.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_034.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 16 demonstrates the exploded view of two joints: a prismatic joint and a chain of three ball joints. Both joints use the medial axis for generation in our implementation. Figure 16.a shows a red 3D mode of a prismatic joint and all parts are shown in the exploded view (from left to right): a rod, a cap, a shaft, and a slider. Figure 16.b shows a red 3D model of a chain of three ball joints and all part are shown in the exploded vide (from left to right): a cylindrical rod, a sphere ball joint, a socket.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 16 demonstrates the exploded view of two joints: a prismatic joint and a chain of three ball joints.", "Both joints use the medial axis for generation in our implementation.", "Figure 16.a shows a red 3D mode of a prismatic joint and all parts are shown in the exploded view (from left to right): a rod, a cap, a shaft, and a slider.", "Figure 16.b shows a red 3D model of a chain of three ball joints and all part are shown in the exploded vide (from left to right): a cylindrical rod, a sphere ball joint, a socket."], "caption": "", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_036.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 17 shows the application of jack-in-the-box. Figure 17.a shows the two spring designs in the application. A freeform spring is at the top and a spring with a cylindrical joint is at the bottom. Figure 17.b shows the cat figure is compressed and sits inside a laser-cut box and the user is cranking the jack-in-the-box toy. The cat faces toward left. Figure 17.c shows that the cat figure pops up and turns to right.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 17 shows the application of jack-in-the-box.", "Figure 17.a shows the two spring designs in the application.", "A freeform spring is at the top and a spring with a cylindrical joint is at the bottom.", "Figure 17.b shows the cat figure is compressed and sits inside a laser-cut box and the user is cranking the jack-in-the-box toy.", "The cat faces toward left.", "Figure 17.c shows that the cat figure pops up and turns to right."], "caption": ". A jack-in-the-box spring mechanism generated by Ondul\u00e9: (a) two spring designs are embedded; (b) the cat can be fully compressed and locked inside a laser-cut box; and (c) the cat pops out following a path as a surprise.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_038.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 18 showcases the launching rocket application. Figure 18.a shows that the rocket sits on top of a compressed smoke model. The smoke has a spring with a prismatic joint. An external latch is used to lock the smoke. Figure 18.b shows the user is pulling the latch. The compressed spring releases and pushes the rocket to fly. Figure 18.c shows that the smoke extends fully, and the rocket falls back to the table.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 18 showcases the launching rocket application.", "Figure 18.a shows that the rocket sits on top of a compressed smoke model.", "The smoke has a spring with a prismatic joint.", "An external latch is used to lock the smoke.", "Figure 18.b shows the user is pulling the latch.", "The compressed spring releases and pushes the rocket to fly.", "Figure 18.c shows that the smoke extends fully, and the rocket falls back to the table."], "caption": ". A launching rocket application: (a) a rocket sits on top of a compressed \u201csmoke\u201d spring, which is locked by an external latch; (b) the user can launch the rocket by pulling the latch; and (c) the smoke is in its full extension.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_040.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 19 presents the hand exercisers. Figure 19.a shows that a hand is holding an off-the-shelf hand exerciser. Figure 19.b shows that a hand is holding a green 3D-printed replication of the hand exerciser. Figure 19.c shows that a hand is holding a red 3D-printed hand exerciser which has different spring stiffness for fingers. Figure 19.d shows that a hand is holding a white custom blowfish shape hand exerciser. Four fingers can touch the body surface and press individually. The palm can also push.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 19 presents the hand exercisers.", "Figure 19.a shows that a hand is holding an off-the-shelf hand exerciser.", "Figure 19.b shows that a hand is holding a green 3D-printed replication of the hand exerciser.", "Figure 19.c shows that a hand is holding a red 3D-printed hand exerciser which has different spring stiffness for fingers.", "Figure 19.d shows that a hand is holding a white custom blowfish shape hand exerciser.", "Four fingers can touch the body surface and press individually.", "The palm can also push."], "caption": "", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_042.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 20 shows the setup of the tangible storytelling prop and how the user can interact with it. Figure 20.a shows the overview of the setup including a 3D-printed storytelling animal prop, attached sensors, microcontroller and circuitry, and a storytelling authoring tool running on a laptop. Figure 20.b shows that a horse character appears when the user presses the animal prop\u2019s neck. Figure 20.c shows that a giraffe character appears when the user stretches the animal prop\u2019s neck. Figure 20.d shows that the horse character is running when the user taps the animal prop\u2019s body. Figure 20.e shows that the giraffe character is talking when the user moves the animal prop\u2019s head up and down.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 20 shows the setup of the tangible storytelling prop and how the user can interact with it.", "Figure 20.a shows the overview of the setup including a 3D-printed storytelling animal prop, attached sensors, microcontroller and circuitry, and a storytelling authoring tool running on a laptop.", "Figure 20.b shows that a horse character appears when the user presses the animal prop\u2019s neck.", "Figure 20.c shows that a giraffe character appears when the user stretches the animal prop\u2019s neck.", "Figure 20.d shows that the horse character is running when the user taps the animal prop\u2019s body.", "Figure 20.e shows that the giraffe character is talking when the user moves the animal prop\u2019s head up and down."], "caption": "", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_044.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 21 shows another three applications made by our design tool. Figure 21.a shows that two red springs are customized and embedded in a cutting device. The cutting machine is cutting a bread by a user who has muscle weakness. Figure 21.b shows that a Halloween mask which has bendable elephant trunk, created by the design tool. Figure 21.c shows that six different spring designs are embedded inside a white 3D-pirnted snake. Two designs are annotated: a bendable part close to the snake head and a twistable part in the middle of the body.", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 21 shows another three applications made by our design tool.", "Figure 21.a shows that two red springs are customized and embedded in a cutting device.", "The cutting machine is cutting a bread by a user who has muscle weakness.", "Figure 21.b shows that a Halloween mask which has bendable elephant trunk, created by the design tool.", "Figure 21.c shows that six different spring designs are embedded inside a white 3D-pirnted snake.", "Two designs are annotated: a bendable part close to the snake head and a twistable part in the middle of the body."], "caption": ". Other applications that Ondul\u00e9 can support: (a) an accessible cutting device, (b) an elephant mask with a bendable trunk, and (c) a snake body with multiple spring deformation behaviors.", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_046.jpg"], "annotated": false, "compound": false}
{"title": "Ondul\u00e9: Designing and Controlling 3D Printable Springs", "pdf_hash": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "year": 2019, "venue": "UIST", "alt_text": "Figure 22 explains the two material properties: Young\u2019s modulus E and shear modulus G. It also shows how to derive G using E and Poisson ratio v. Figure 22.a shows that an object is stretched vertically with an axial force. The Young\u2019s modulus is calculated using uniaxial stress to and strain. E equals to stress divided by strain and the formula is shown in equation 3. Figure 22.b shows that an object is pulled horizontally with a shear force. The shear modulus is calculated using shear stress to and shear strain. G equals to shear stress divided by shear strain and the formula is shown in equation 4. Figure 22.c shows equation 4, which is about how to calculate G using E and v. G equals to E divided by 2*(1+v).", "levels": null, "corpus_id": 199501237, "sentences": ["Figure 22 explains the two material properties: Young\u2019s modulus E and shear modulus G. It also shows how to derive G using E and Poisson ratio v. Figure 22.a shows that an object is stretched vertically with an axial force.", "The Young\u2019s modulus is calculated using uniaxial stress to and strain.", "E equals to stress divided by strain and the formula is shown in equation 3.", "Figure 22.b shows that an object is pulled horizontally with a shear force.", "The shear modulus is calculated using shear stress to and shear strain.", "G equals to shear stress divided by shear strain and the formula is shown in equation 4.", "Figure 22.c shows equation 4, which is about how to calculate G using E and v. G equals to E divided by 2*(1+v)."], "caption": ". Material properties (a) Young\u2019s modulus E and (b) shear modulus G. G can be derived using E and Poisson ratio \u03c5 (c).", "local_uri": ["21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_048.jpg"], "annotated": false, "compound": false}
{"title": "TacTILE: A Preliminary Toolchain for Creating Accessible Graphics with 3D-Printed Overlays and Auditory Annotations", "pdf_hash": "a4e3fa4a1eb2de08bbb23d2574e23ec2bff3462a", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 2. Converting a floor plan from 2D graphic to 3D-printed title. On the left side it shows the annotation tool and on the right side it shows a 3D-printed tactile overlay.", "levels": [[-1], [-1], [-1]], "corpus_id": 29371850, "sentences": ["Figure 2.", "Converting a floor plan from 2D graphic to 3D-printed title.", "On the left side it shows the annotation tool and on the right side it shows a 3D-printed tactile overlay."], "caption": "Figure 2. Converting a floor plan from 2D graphic to 3D-printed tile:", "local_uri": ["a4e3fa4a1eb2de08bbb23d2574e23ec2bff3462a_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "What Happens After Death? Using a Design Workbook to Understand User Expectations for Preparing their Data", "pdf_hash": "12072eb4647be6e0d46ea6e940933a412f8a61ab", "year": 2021, "venue": "CHI", "alt_text": "A digital illustration of design concept, Memory Swipe. It shows a game being played on TV with 3 players. Each player can choose accept (a check mark) or reject (an X). The text to the side reads: Choose which memories are worth keeping after your death by playing Memory Swipe! Your selected photos, recipes, games, and certificates will show on the screen as the audience votes on which ones to keep.", "levels": null, "corpus_id": 233987700, "sentences": ["A digital illustration of design concept, Memory Swipe.", "It shows a game being played on TV with 3 players.", "Each player can choose accept (a check mark) or reject (an X).", "The text to the side reads: Choose which memories are worth keeping after your death by playing Memory Swipe! Your selected photos, recipes, games, and certificates will show on the screen as the audience votes on which ones to keep."], "caption": "Blast from the Past", "local_uri": ["12072eb4647be6e0d46ea6e940933a412f8a61ab_Image_008.jpg", "12072eb4647be6e0d46ea6e940933a412f8a61ab_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "Helping students keep up with real-time captions by pausing and highlighting", "pdf_hash": "1c039ecfbef9bc19146b87920a0561570ff72732", "year": 2014, "venue": "W4A", "alt_text": "A workflow diagram of Scribe: On the left, audio from a lecture is captured by a smart phone. The smart phone splits the processed speech and feeds it to multiple typists. A merging server combined individual fragments into a single whole transcript and sends it out. It can be displayed, crowd corrected or fed back to the audio capture device.", "levels": null, "corpus_id": 15755658, "sentences": ["A workflow diagram of Scribe: On the left, audio from a lecture is captured by a smart phone.", "The smart phone splits the processed speech and feeds it to multiple typists.", "A merging server combined individual fragments into a single whole transcript and sends it out.", "It can be displayed, crowd corrected or fed back to the audio capture device."], "caption": "Figure 3. Legion:Scribe system. Audio is streamed from a user\u2019s mobile device to a server that divides it into pieces. Workers use the captionist interface (Figure 2) to type what they can of the content they are asked to. Multiple workers\u2019 inputs can be combined to increase coverage and decrease latency, resulting in better captions than any of the workers could produce individually.", "local_uri": ["1c039ecfbef9bc19146b87920a0561570ff72732_Image_003.png"], "annotated": false, "compound": false}
{"title": "Helping students keep up with real-time captions by pausing and highlighting", "pdf_hash": "1c039ecfbef9bc19146b87920a0561570ff72732", "year": 2014, "venue": "W4A", "alt_text": "A series of snapshots of transcripts from each kind of caption approach (Scribe, CART and ASR). The series show the transcripts at 10s, 15s, 20s and 25s.", "levels": [[-1], [-1]], "corpus_id": 15755658, "sentences": ["A series of snapshots of transcripts from each kind of caption approach (Scribe, CART and ASR).", "The series show the transcripts at 10s, 15s, 20s and 25s."], "caption": "Figure 4. An example of the captions produced by professional captionists (CART), automatic speech recognition (ASR), and Scribe, over time. This shows that in addition to errors, captions can also come in \u201cbursts\u201d (especially with ASR), making them even more dif\ufb01cult for users to read.", "local_uri": ["1c039ecfbef9bc19146b87920a0561570ff72732_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Helping students keep up with real-time captions by pausing and highlighting", "pdf_hash": "1c039ecfbef9bc19146b87920a0561570ff72732", "year": 2014, "venue": "W4A", "alt_text": "Screenshots of the baseline view of a lecture is shown It consists of two views side by side on a screen. The left view shows the chemistry periodic table with drawings over it by the teacher. The right view shows the real-time captions (visual translation of the audio).", "levels": null, "corpus_id": 15755658, "sentences": ["Screenshots of the baseline view of a lecture is shown It consists of two views side by side on a screen.", "The left view shows the chemistry periodic table with drawings over it by the teacher.", "The right view shows the real-time captions (visual translation of the audio)."], "caption": "Figure 6. Pausing caption tool user interface. Users are shown a tran\u00ad script that is initially synchronized with the video shown and updated in real-time as new captions arrive, and given a set of playback controls for the captions that allow them to pause (using hold-to-pause or toggle), fast-forward, and skip directly back to the current point in the video.", "local_uri": ["1c039ecfbef9bc19146b87920a0561570ff72732_Image_007.jpg", "1c039ecfbef9bc19146b87920a0561570ff72732_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Helping students keep up with real-time captions by pausing and highlighting", "pdf_hash": "1c039ecfbef9bc19146b87920a0561570ff72732", "year": 2014, "venue": "W4A", "alt_text": "A bar graph of the comprehension scores is shown. It shows a plot of the baseline and tool scores. The tools improve the performance in both cases, but the difference between the highlighting tool and the baseline is larger than the different between the pausing tool and the baseline.", "levels": [[1], [1], [3]], "corpus_id": 15755658, "sentences": ["A bar graph of the comprehension scores is shown.", "It shows a plot of the baseline and tool scores.", "The tools improve the performance in both cases, but the difference between the highlighting tool and the baseline is larger than the different between the pausing tool and the baseline."], "caption": "Figure 8. The results of the comprehension tests used in our study with 95% con\ufb01dence intervals shown. There was a positive improvement seen in students scores using both the pausing and highlighting players (7.32% and 14.56% respectively), however, only the highlighting player was signi\ufb01cant (p < 0.001). The improvement seen between the pausing and highlighting players was also signi\ufb01cant (p < 0.01). While there is a difference in the score of the baseline as well, this effect was not signi\ufb01cant (p > 0.05).", "local_uri": ["1c039ecfbef9bc19146b87920a0561570ff72732_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Empirical Analysis of Data Deletion and Opt-Out Choices on 150 Websites", "pdf_hash": "221e1708f8244eee9819d585be32f95bf850e51d", "year": 2019, "venue": "SOUPS @ USENIX Security Symposium", "alt_text": "https://lh5.googleusercontent.com/93L7BI9MjOrKIO9qnhXwFS2lRlaGBmMN-WVWa9XCcj3rhnIXybKA_FejJJBNEpRZ_eYb2_gs1GpAZ8ylvzW8_80hNqnl-jMYjGKkNYK1DM83uPhN5nph87ZCg8-9DKf7yiQH00FwAoI", "levels": null, "corpus_id": 201803625, "sentences": ["https://lh5.googleusercontent.com/93L7BI9MjOrKIO9qnhXwFS2lRlaGBmMN-WVWa9XCcj3rhnIXybKA_FejJJBNEpRZ_eYb2_gs1GpAZ8ylvzW8_80hNqnl-jMYjGKkNYK1DM83uPhN5nph87ZCg8-9DKf7yiQH00FwAoI"], "caption": "Yixin Zou", "local_uri": ["221e1708f8244eee9819d585be32f95bf850e51d_Image_002.jpg", "221e1708f8244eee9819d585be32f95bf850e51d_Image_092.jpg"], "annotated": false, "compound": true}
{"title": "An Empirical Analysis of Data Deletion and Opt-Out Choices on 150 Websites", "pdf_hash": "221e1708f8244eee9819d585be32f95bf850e51d", "year": 2019, "venue": "SOUPS @ USENIX Security Symposium", "alt_text": "https://lh3.googleusercontent.com/B6ELIuYM8zUIw84r-5JXmHKI0cfjm453cjExqR6miM7UAo02mO0lw6A74Dux7MX2w1Fijoq74HDXNgsk7KXSBH39pXTShlQv4uDDbhnRSL7Jd2FIes8ogXbQt5dNBM8EoelO3hiJScc", "levels": null, "corpus_id": 201803625, "sentences": ["https://lh3.googleusercontent.com/B6ELIuYM8zUIw84r-5JXmHKI0cfjm453cjExqR6miM7UAo02mO0lw6A74Dux7MX2w1Fijoq74HDXNgsk7KXSBH39pXTShlQv4uDDbhnRSL7Jd2FIes8ogXbQt5dNBM8EoelO3hiJScc"], "caption": "", "local_uri": ["221e1708f8244eee9819d585be32f95bf850e51d_Image_003.png", "221e1708f8244eee9819d585be32f95bf850e51d_Image_093.png"], "annotated": false, "compound": true}
{"title": "One does not Simply RSVP: Mental Workload to Select Speed Reading Parameters using Electroencephalography", "pdf_hash": "3c1ff82b7498d26ebf9b30c64a7ce0e29886ccc4", "year": 2020, "venue": "CHI", "alt_text": "Decorative teaser figure. User reading with RSVP. EEG determines changes from the brain resting state through the individual alpha frequency (IAF) and task engagement through the frontal theta frequency for different RSVP Text Alignments and Presentations Speeds in Words per Minute (WPM).", "levels": null, "corpus_id": 218482487, "sentences": ["Decorative teaser figure.", "User reading with RSVP.", "EEG determines changes from the brain resting state through the individual alpha frequency (IAF) and task engagement through the frontal theta frequency for different RSVP Text Alignments and Presentations Speeds in Words per Minute (WPM)."], "caption": "(a)                                                              (b)\u200c", "local_uri": ["3c1ff82b7498d26ebf9b30c64a7ce0e29886ccc4_Image_001.png", "3c1ff82b7498d26ebf9b30c64a7ce0e29886ccc4_Image_002.png"], "annotated": false, "compound": true}
{"title": "Mole Messenger: Pushable Interfaces for Connecting Family at a Distance", "pdf_hash": "69b77958a7dad2f8492dab4c10481a0c8ca5a72a", "year": 2019, "venue": "Tangible and Embedded Interaction", "alt_text": "https://lh5.googleusercontent.com/WpuOHw5AwJWnGXigp6MJbNi0nU9LIHJVFpG0SHOeV-8A4Y9VdbJ9jBCuDzA_9WuQCHt7CTbDKviswxlv4xD2E_xIScPjP5AF8sY_w4aYsZcvEkjaBVlEb3hIOnvd48aru_V1Tjy9", "levels": null, "corpus_id": 83458754, "sentences": ["https://lh5.googleusercontent.com/WpuOHw5AwJWnGXigp6MJbNi0nU9LIHJVFpG0SHOeV-8A4Y9VdbJ9jBCuDzA_9WuQCHt7CTbDKviswxlv4xD2E_xIScPjP5AF8sY_w4aYsZcvEkjaBVlEb3hIOnvd48aru_V1Tjy9"], "caption": "Figure 1: Mole Messenger Prototype", "local_uri": ["69b77958a7dad2f8492dab4c10481a0c8ca5a72a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Mole Messenger: Pushable Interfaces for Connecting Family at a Distance", "pdf_hash": "69b77958a7dad2f8492dab4c10481a0c8ca5a72a", "year": 2019, "venue": "Tangible and Embedded Interaction", "alt_text": "https://lh5.googleusercontent.com/7yD_n0ekpYXagb7YFkdw_Tv631D4HuLbNfRaaSJQuxDLpt2KkG3LbQBH7Xvtzd8oznRgsQ3a63M6r66JxVeVJmIUsrihzt2dutho98eMrfVFu3zkxAwimReN9iO0DqVB1blcia7W", "levels": null, "corpus_id": 83458754, "sentences": ["https://lh5.googleusercontent.com/7yD_n0ekpYXagb7YFkdw_Tv631D4HuLbNfRaaSJQuxDLpt2KkG3LbQBH7Xvtzd8oznRgsQ3a63M6r66JxVeVJmIUsrihzt2dutho98eMrfVFu3zkxAwimReN9iO0DqVB1blcia7W"], "caption": "", "local_uri": ["69b77958a7dad2f8492dab4c10481a0c8ca5a72a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Mole Messenger: Pushable Interfaces for Connecting Family at a Distance", "pdf_hash": "69b77958a7dad2f8492dab4c10481a0c8ca5a72a", "year": 2019, "venue": "Tangible and Embedded Interaction", "alt_text": "https://lh3.googleusercontent.com/iltBs9W1TJ6u4Y2-X_2Gsz81uNlhAetX_7yggd-JjEks9AKO7uE-WwQ6EmP4hAAo_Z6qi5e9jupXDv5M798176JfChxHoDw9byT9RImFAiajwiT5NQc6ULK1_jnLzt-Kz34NjuwB", "levels": null, "corpus_id": 83458754, "sentences": ["https://lh3.googleusercontent.com/iltBs9W1TJ6u4Y2-X_2Gsz81uNlhAetX_7yggd-JjEks9AKO7uE-WwQ6EmP4hAAo_Z6qi5e9jupXDv5M798176JfChxHoDw9byT9RImFAiajwiT5NQc6ULK1_jnLzt-Kz34NjuwB"], "caption": "Figure 4: The two messengers with color synchronization. The left mole is pushed down and the right mole is up.", "local_uri": ["69b77958a7dad2f8492dab4c10481a0c8ca5a72a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Towards Language Independent Detection of Dyslexia with a Web-based Game", "pdf_hash": "5091808675dbf9c89abcddca4b5ac34d42f34c98", "year": 2018, "venue": "W4A", "alt_text": "Figure 3: Waveform for the order of intervals for one musical element of the stage Rise Time. The exam- ple starts with a 0.025s fade-in interval and then a 0.250s interval followed by a 0.250s fade-in interval.", "levels": null, "corpus_id": 49870313, "sentences": ["Figure 3: Waveform for the order of intervals for one musical element of the stage Rise Time.", "The exam- ple starts with a 0.025s fade-in interval and then a 0.250s interval followed by a 0.250s fade-in interval."], "caption": "", "local_uri": ["5091808675dbf9c89abcddca4b5ac34d42f34c98_Image_002.png"], "annotated": false, "compound": false}
{"title": "Towards Language Independent Detection of Dyslexia with a Web-based Game", "pdf_hash": "5091808675dbf9c89abcddca4b5ac34d42f34c98", "year": 2018, "venue": "W4A", "alt_text": "Figure 2: Example of the musical part from the game MusVis for the first two clicks on two sound cards (left) and then a pair of equal sounds is found (right). The participant is asked to find two equal musical elements by clicking on sound cards in a row.", "levels": null, "corpus_id": 49870313, "sentences": ["Figure 2: Example of the musical part from the game MusVis for the first two clicks on two sound cards (left) and then a pair of equal sounds is found (right).", "The participant is asked to find two equal musical elements by clicking on sound cards in a row."], "caption": "Figure 2: Example of the musical part from the game MusVis for the \ufb01rst two clicks on two sound cards (left) and then a pair of equal sounds is found (right). The participant is asked to \ufb01nd two equal musical elements by clicking on sound cards in a row.", "local_uri": ["5091808675dbf9c89abcddca4b5ac34d42f34c98_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Towards Language Independent Detection of Dyslexia with a Web-based Game", "pdf_hash": "5091808675dbf9c89abcddca4b5ac34d42f34c98", "year": 2018, "venue": "W4A", "alt_text": "Figure 5: Overview of the designed visual elements. The figure shows the target element (top) and dis- tractor elements (below) for the four different stages (z, symbol, rectangle, face) of the visual part of the game MusVis.", "levels": null, "corpus_id": 49870313, "sentences": ["Figure 5: Overview of the designed visual elements.", "The figure shows the target element (top) and dis- tractor elements (below) for the four different stages (z, symbol, rectangle, face) of the visual part of the game MusVis."], "caption": "", "local_uri": ["5091808675dbf9c89abcddca4b5ac34d42f34c98_Image_004.png"], "annotated": false, "compound": false}
{"title": "Towards Language Independent Detection of Dyslexia with a Web-based Game", "pdf_hash": "5091808675dbf9c89abcddca4b5ac34d42f34c98", "year": 2018, "venue": "W4A", "alt_text": "Figure 4: Example of the visual part of the game MusVis with the priming of the target element sym- bol (left) and then the nine-squared design including the distractors for each symbol (right).", "levels": null, "corpus_id": 49870313, "sentences": ["Figure 4: Example of the visual part of the game MusVis with the priming of the target element sym- bol (left) and then the nine-squared design including the distractors for each symbol (right)."], "caption": "Figure 4: Example of the visual part of the game MusVis with the priming of the target element sym- bol (left) and then the nine-squared design including the distractors for each symbol (right).", "local_uri": ["5091808675dbf9c89abcddca4b5ac34d42f34c98_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops", "pdf_hash": "10d88a7c302852e1e11ee06be027e7957bdd74a1", "year": 2018, "venue": "UIST", "alt_text": "Overview of Maestro\u2019s interface. (A) a recommended 3D navigation intervention; (B) system highlights extended periods without 3D navigation; (C) a peer helper recommendation; (D) system highlights multiple consecutive undo/erase actions; (E) inactive student indicator; (F) a recommended \u201cattention\u201d intervention; (G) 3D navigation reminder on student\u2019s screen.", "levels": null, "corpus_id": 51999993, "sentences": ["Overview of Maestro\u2019s interface. (A) a recommended 3D navigation intervention; (B) system highlights extended periods without 3D navigation; (C) a peer helper recommendation; (D) system highlights multiple consecutive undo/erase actions; (E) inactive student indicator; (F) a recommended \u201cattention\u201d intervention; (G) 3D navigation reminder on student\u2019s screen."], "caption": "Figure 1. Overview of Maestro\u2019s interface. (A) a recommended 3D navigation intervention; (B) system highlights extended periods without 3D navigation; (C) a peer helper recommendation; (D) system highlights multiple consecutive undo/erase actions;", "local_uri": ["10d88a7c302852e1e11ee06be027e7957bdd74a1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops", "pdf_hash": "10d88a7c302852e1e11ee06be027e7957bdd74a1", "year": 2018, "venue": "UIST", "alt_text": "A student widget. (A) live view of the student\u2019s Tinkercad window; (B) timelines of command activity; (C) the interventions menu; (D) the interventions menu (open)", "levels": null, "corpus_id": 51999993, "sentences": ["A student widget. (A) live view of the student\u2019s Tinkercad window; (B) timelines of command activity; (C) the interventions menu; (D) the interventions menu (open)"], "caption": "Figure 2. A student widget. (A) live view of the student\u2019s Tinkercad window; (B) timelines of command activity; (C) the interventions menu; (D) the interventions menu (open)", "local_uri": ["10d88a7c302852e1e11ee06be027e7957bdd74a1_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops", "pdf_hash": "10d88a7c302852e1e11ee06be027e7957bdd74a1", "year": 2018, "venue": "UIST", "alt_text": "Maestro indicating which students have followed the instructor\u2019s demonstration (in green) and who hasn\u2019t followed yet (in red). (A) A recommended \u201cencouragement\u201d intervention; (B) a recommended \u201cpeer help\u201d intervention.", "levels": null, "corpus_id": 51999993, "sentences": ["Maestro indicating which students have followed the instructor\u2019s demonstration (in green) and who hasn\u2019t followed yet (in red). (A) A recommended \u201cencouragement\u201d intervention; (B) a recommended \u201cpeer help\u201d intervention."], "caption": "Figure 3. Maestro indicating which students have followed the instructor\u2019s demonstration (in green) and who hasn\u2019t followed yet (in red). (A) A recommended \u201cencouragement\u201d intervention; (B) a recommended \u201cpeer help\u201d intervention.", "local_uri": ["10d88a7c302852e1e11ee06be027e7957bdd74a1_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops", "pdf_hash": "10d88a7c302852e1e11ee06be027e7957bdd74a1", "year": 2018, "venue": "UIST", "alt_text": "Interventions as they appear in the student\u2019s Tinkercad window. (A) \u201cpoint to\u201d intervention; (B) \u201cthumbs up\u201d intervention; (C) \u201ctext message\u201d intervention; (D) \u201c3D navigation reminder\u201d intervention; (E) \u201cattention\u201d intervention", "levels": null, "corpus_id": 51999993, "sentences": ["Interventions as they appear in the student\u2019s Tinkercad window. (A) \u201cpoint to\u201d intervention; (B) \u201cthumbs up\u201d intervention; (C) \u201ctext message\u201d intervention; (D) \u201c3D navigation reminder\u201d intervention; (E) \u201cattention\u201d intervention"], "caption": "Figure 4. Interventions as they appear in the student\u2019s Tinkercad window. (A) \u201cpoint to\u201d intervention; (B) \u201cthumbs up\u201d intervention; (C) \u201ctext message\u201d intervention; (D) \u201c3D navigation reminder\u201d intervention; (E) \u201cattention\u201d intervention", "local_uri": ["10d88a7c302852e1e11ee06be027e7957bdd74a1_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops", "pdf_hash": "10d88a7c302852e1e11ee06be027e7957bdd74a1", "year": 2018, "venue": "UIST", "alt_text": "Timelines for the instructor and each of the participants, with dots representing each command invocation.", "levels": [[-1]], "corpus_id": 51999993, "sentences": ["Timelines for the instructor and each of the participants, with dots representing each command invocation."], "caption": "Figure 5. Workshop command log data, showing the instructor\u2019s activity (top) and that of participants (below). Each command is assigned a unique color.", "local_uri": ["10d88a7c302852e1e11ee06be027e7957bdd74a1_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops", "pdf_hash": "10d88a7c302852e1e11ee06be027e7957bdd74a1", "year": 2018, "venue": "UIST", "alt_text": "Timelines for each participant, with dots representing use of 3D navigation commands, 3D navigation interventions recommended to the instructor, and 3D navigation interventions sent by the instructor.", "levels": null, "corpus_id": 51999993, "sentences": ["Timelines for each participant, with dots representing use of 3D navigation commands, 3D navigation interventions recommended to the instructor, and 3D navigation interventions sent by the instructor."], "caption": "Figure 7. Timeline of participant 3D navigation (blue), recommended 3D navigation interventions (green), and 3D navigation interventions sent by the instructor (red).", "local_uri": ["10d88a7c302852e1e11ee06be027e7957bdd74a1_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "The Image of the Interface: How People Use Landmarks to Develop Spatial Memory of Commands in Graphical Interfaces", "pdf_hash": "820496a2ef073a3607eef619bd9e48d3f18b24ec", "year": 2021, "venue": "CHI", "alt_text": "Part of the washed-out image of the Home ribbon of Word interface used in the study. Washed-out images of the GUIs were created by removing the commands from a screen snapshot, keeping the outline and boundaries intact.", "levels": null, "corpus_id": 233987000, "sentences": ["Part of the washed-out image of the Home ribbon of Word interface used in the study.", "Washed-out images of the GUIs were created by removing the commands from a screen snapshot, keeping the outline and boundaries intact."], "caption": "", "local_uri": ["820496a2ef073a3607eef619bd9e48d3f18b24ec_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Dueto: Accessible, Gaze-Operated Musical Expression", "pdf_hash": "e3a76e6e18d94354263702dc882cf6418b351e30", "year": 2019, "venue": "ASSETS", "alt_text": "Figure 1, right. Diagram showing the shape of the eye gestures needed to create minor and major triad chords. The eye gestures follow a triangular pattern with 3 fixation points. The first step is to look at the root key and then look up and down to complete a mountain shape gesture to play a minor chord or look down and then up to complete a V-shape gesture that will play a major chord. The major chord example is labelled with the notes C-E-G.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 202286483, "sentences": ["Figure 1, right.", "Diagram showing the shape of the eye gestures needed to create minor and major triad chords.", "The eye gestures follow a triangular pattern with 3 fixation points.", "The first step is to look at the root key and then look up and down to complete a mountain shape gesture to play a minor chord or look down and then up to complete a V-shape gesture that will play a major chord.", "The major chord example is labelled with the notes C-E-G."], "caption": "Figure 1. Dueto explores multimodality combined with gaze-based interactions for the creation of accessible musical interfaces. Dueto can be played with eye-gaze alone, gaze and switch, or gaze and multi-touch, where multi-touch input is carried out by a partner. The harmony ladder is located on the lower part of Dueto\u2019s UI (left) showing notes corresponding to the C major key. The small circles that connect the main notes appear and disappear in a cascade animation, hinting to the user how to perform the shape of the gestures that enable playing multiple notes at the same time (chords) with just the eyes (right).", "local_uri": ["e3a76e6e18d94354263702dc882cf6418b351e30_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Lightwear: An Exploration in Wearable Light Therapy", "pdf_hash": "992b6d5ea3b2da2d7b98a48b86e011b067a7c679", "year": 2015, "venue": "Tangible and Embedded Interaction", "alt_text": "Graph detailing the action spectrum for the light wavelength determined to be most effective for treating Seasonal Affective Disorder-related symptoms.", "levels": [[1]], "corpus_id": 14999899, "sentences": ["Graph detailing the action spectrum for the light wavelength determined to be most effective for treating Seasonal Affective Disorder-related symptoms."], "caption": "Figure 2. Action spectrum-blue light vs. other light sources [18].", "local_uri": ["992b6d5ea3b2da2d7b98a48b86e011b067a7c679_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Lightwear: An Exploration in Wearable Light Therapy", "pdf_hash": "992b6d5ea3b2da2d7b98a48b86e011b067a7c679", "year": 2015, "venue": "Tangible and Embedded Interaction", "alt_text": "Images of all 6 wearable light-emitting prototypes \u2013 Starting from the top left and moving clockwise: Glasses (1), Fiber Optic Scarf (2), Teal Cowl (3), Fascinator Hat (4), Hood (5), and Classic Golfer\u2019s Hat (6).", "levels": null, "corpus_id": 14999899, "sentences": ["Images of all 6 wearable light-emitting prototypes \u2013 Starting from the top left and moving clockwise: Glasses (1), Fiber Optic Scarf (2), Teal Cowl (3), Fascinator Hat (4), Hood (5), and Classic Golfer\u2019s Hat (6)."], "caption": "Figure 5. Wearable light-emitting prototypes \u2013 Glasses (1), Fiber Optic Scarf (2), Teal Cowl (3), Fascinator Hat (4), Hood (5), and Classic Golfer\u2019s Hat (6).", "local_uri": ["992b6d5ea3b2da2d7b98a48b86e011b067a7c679_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students", "pdf_hash": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "scatter plot and correlation line between social support and depression for people who have reported discrimination and those who have not", "levels": [[1]], "corpus_id": 207960577, "sentences": ["scatter plot and correlation line between social support and depression for people who have reported discrimination and those who have not"], "caption": "", "local_uri": ["fa83fb659bbf9efb4b4a56e6a933c155c87c201e_Image_013.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students", "pdf_hash": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "ratings of depression and frustration on days before and after the discrimination", "levels": null, "corpus_id": 207960577, "sentences": ["ratings of depression and frustration on days before and after the discrimination"], "caption": "", "local_uri": ["fa83fb659bbf9efb4b4a56e6a933c155c87c201e_Image_014.png"], "annotated": false, "compound": false}
{"title": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students", "pdf_hash": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "bar chart for level of significance from the day before to two days after the discrimination events", "levels": [[1]], "corpus_id": 207960577, "sentences": ["bar chart for level of significance from the day before to two days after the discrimination events"], "caption": "", "local_uri": ["fa83fb659bbf9efb4b4a56e6a933c155c87c201e_Image_015.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Family Group Chat: Family Needs to Manage Contact and Conflict", "pdf_hash": "e47602e15c2719d6107b2b4ca70395c3ca56f4e9", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "P9's messages and their expected responses from family members in different situations.", "levels": null, "corpus_id": 218483444, "sentences": ["P9's messages and their expected responses from family members in different situations."], "caption": "", "local_uri": ["e47602e15c2719d6107b2b4ca70395c3ca56f4e9_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "A Multi-Modal Approach for Blind and Visually Impaired Developers to Edit Webpage Designs", "pdf_hash": "1c4f68393faa7bdeabb08b77a6ffdc1710e82d3d", "year": 2019, "venue": "ASSETS", "alt_text": "The system consists of three modules: the accessible canvas on the left, the code editor on the right, and the controller (server) in the middle. The UI updates made on the canvas are sent to the controller and the controller uses diff processor and validator components to check if the proposed updates violate the design guidelines. If passes, validated updates are sent back to the canvas. Otherwise, the canvas remains unchanged. When the changed code is saved in the code editor, the controller is notified and check the design guideline violations. The updates will be announced by the built-in screen reader in the code editor.", "levels": null, "corpus_id": 204917531, "sentences": ["The system consists of three modules: the accessible canvas on the left, the code editor on the right, and the controller (server) in the middle.", "The UI updates made on the canvas are sent to the controller and the controller uses diff processor and validator components to check if the proposed updates violate the design guidelines.", "If passes, validated updates are sent back to the canvas.", "Otherwise, the canvas remains unchanged.", "When the changed code is saved in the code editor, the controller is notified and check the design guideline violations.", "The updates will be announced by the built-in screen reader in the code editor."], "caption": "Figure 1. Multi-modal system overview. See video demo.", "local_uri": ["1c4f68393faa7bdeabb08b77a6ffdc1710e82d3d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "SpaceBot: Towards Participatory Evaluation of Smart Buildings", "pdf_hash": "c2bfd438040954b5c1980338b96b997d4fd4ef71", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "An artist's render of the Urban Sciences Building showing the 6 floors and atrium space, including a courtyard in front with people.", "levels": null, "corpus_id": 5085250, "sentences": ["An artist's render of the Urban Sciences Building showing the 6 floors and atrium space, including a courtyard in front with people."], "caption": "", "local_uri": ["c2bfd438040954b5c1980338b96b997d4fd4ef71_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SpaceBot: Towards Participatory Evaluation of Smart Buildings", "pdf_hash": "c2bfd438040954b5c1980338b96b997d4fd4ef71", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A data visualisation showing a 3d model of the building, with red cubes representing data nodes. Clicking a node reveals the data for that space, in this case, Floor 2, Room 2.015.", "levels": [[-1], [-1]], "corpus_id": 5085250, "sentences": ["A data visualisation showing a 3d model of the building, with red cubes representing data nodes.", "Clicking a node reveals the data for that space, in this case, Floor 2, Room 2.015."], "caption": "", "local_uri": ["c2bfd438040954b5c1980338b96b997d4fd4ef71_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SpaceBot: Towards Participatory Evaluation of Smart Buildings", "pdf_hash": "c2bfd438040954b5c1980338b96b997d4fd4ef71", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A Twitter screenshot showing the @NUEstates twitter feed. The description reads: \u201cNewcastle University. Estate Support Service. Delivering an outstanding estate.\u201d", "levels": null, "corpus_id": 5085250, "sentences": ["A Twitter screenshot showing the @NUEstates twitter feed.", "The description reads: \u201cNewcastle University. Estate Support Service.", "Delivering an outstanding estate.\u201d"], "caption": "", "local_uri": ["c2bfd438040954b5c1980338b96b997d4fd4ef71_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Gender Equity in Author Order Assignment", "pdf_hash": "4882d3bc4028cb245ae7549c6c72081897c72bfb", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "(Left) Percentage of women at ranks 1 through 6 in CSCW, UIST, and ICML. 17% of women in CSCW are rank 1, 29% of women in UIST are rank 1, and 12% of women in ICML are rank 1. 30-35% of women in all three conferences are rank 2. 17-22% of women are rank 3. 12% of women in CSCW are rank 4, 9% of women in UIST are rank 4, and 18% of women in ICML are rank 4. 6-8% of women are rank 5. 6-8% of women are rank 6. (Right) Percentage of men at ranks 1 through 6 in CSCW, UIST, and ICML. 11% of men in CSCW are rank 1, 16% of men in UIST are rank 1, and 8% of men in ICML are rank 1. 26-20% of men in all three conferences are rank 2. 21-27% of men are rank 3. 13-16% of men in are rank 4. 9-11% of men are rank 5. 10-13% of men are rank 6.", "levels": null, "corpus_id": 53247654, "sentences": ["(Left) Percentage of women at ranks 1 through 6 in CSCW, UIST, and ICML.", "17% of women in CSCW are rank 1, 29% of women in UIST are rank 1, and 12% of women in ICML are rank 1. 30-35% of women in all three conferences are rank 2. 17-22% of women are rank 3.", "12% of women in CSCW are rank 4, 9% of women in UIST are rank 4, and 18% of women in ICML are rank 4. 6-8% of women are rank 5. 6-8% of women are rank 6. (Right) Percentage of men at ranks 1 through 6 in CSCW, UIST, and ICML.", "11% of men in CSCW are rank 1, 16% of men in UIST are rank 1, and 8% of men in ICML are rank 1. 26-20% of men in all three conferences are rank 2. 21-27% of men are rank 3. 13-16% of men in are rank 4. 9-11% of men are rank 5. 10-13% of men are rank 6."], "caption": "", "local_uri": ["4882d3bc4028cb245ae7549c6c72081897c72bfb_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Gender Equity in Author Order Assignment", "pdf_hash": "4882d3bc4028cb245ae7549c6c72081897c72bfb", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "(Left) For CSCW, 18% more papers than expected have no women authors, 14% more for UIST, and 5% more for ICML. These differences are all significant.  (Middle) For CSCW, 18% fewer papers than expected have one woman author, 14% fewer for UIST, and 11% fewer for ICML. These differences are also all significant.  (Right) For CSCW, 7% more papers than expected have at least two women (not significant). For UIST, 25% fewer papers than expected have no women (significant), and 14% fewer for ICML (not significant).", "levels": null, "corpus_id": 53247654, "sentences": ["(Left) For CSCW, 18% more papers than expected have no women authors, 14% more for UIST, and 5% more for ICML.", "These differences are all significant.", "(Middle) For CSCW, 18% fewer papers than expected have one woman author, 14% fewer for UIST, and 11% fewer for ICML.", "These differences are also all significant.", "(Right) For CSCW, 7% more papers than expected have at least two women (not significant).", "For UIST, 25% fewer papers than expected have no women (significant), and 14% fewer for ICML (not significant)."], "caption": "", "local_uri": ["4882d3bc4028cb245ae7549c6c72081897c72bfb_Image_019.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Gender Equity in Author Order Assignment", "pdf_hash": "4882d3bc4028cb245ae7549c6c72081897c72bfb", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "(Left) First author slots are most prevalent for authors of rank 2 or 3.  (Middle) Middle author slots are most prevalent for authors of ranks 1-4.  (Right) Last author slots are most prevalent for authors of ranks 4-6. Among these high-rank authors, a higher proportion of men than women hold the last author slot.", "levels": null, "corpus_id": 53247654, "sentences": ["(Left) First author slots are most prevalent for authors of rank 2 or 3.", "(Middle) Middle author slots are most prevalent for authors of ranks 1-4.", "(Right) Last author slots are most prevalent for authors of ranks 4-6.", "Among these high-rank authors, a higher proportion of men than women hold the last author slot."], "caption": "", "local_uri": ["4882d3bc4028cb245ae7549c6c72081897c72bfb_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Urban Accessibility as a Socio-Political Problem", "pdf_hash": "249599b991052f90d48141883b074bcb229fa8c3", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure with four sub-figures showing missing curb ramp, utility pole as sidewalk obstacle, broken sidewalk as surface problem, missing sidewalks, and an intersection with multiple barriers.", "levels": [[-1]], "corpus_id": 230717562, "sentences": ["Figure with four sub-figures showing missing curb ramp, utility pole as sidewalk obstacle, broken sidewalk as surface problem, missing sidewalks, and an intersection with multiple barriers."], "caption": "Figure 1. Examples of sidewalk accessibility barriers. The last sub-figure shows multiple barriers present together: missing ramp, no sidewalk, and uneven surface. Images from Project Sidewalk [78].", "local_uri": ["249599b991052f90d48141883b074bcb229fa8c3_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Urban Accessibility as a Socio-Political Problem", "pdf_hash": "249599b991052f90d48141883b074bcb229fa8c3", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure showing two sub-figures. Figure A shows the design of sidewalks with clear dilineated zones from the front of the building to the curb side. Figure B shows a detailed diagram of an accessible curb ramp, annotated with specific recommended measurements such as 36 inches min for the top landing space.", "levels": null, "corpus_id": 230717562, "sentences": ["Figure showing two sub-figures.", "Figure A shows the design of sidewalks with clear dilineated zones from the front of the building to the curb side.", "Figure B shows a detailed diagram of an accessible curb ramp, annotated with specific recommended measurements such as 36 inches min for the top landing space."], "caption": "Figure 2. Illustrations of sidewalk and curb ramp design. Figure A shows City of Montgomery\u2019s Urban Design Guidelines on designing sidewalks [134]. US Access Board requires a minimum of 5ft width for Pedestrian Zones. Figure B shows the design of an accessible curb ramp [135].", "local_uri": ["249599b991052f90d48141883b074bcb229fa8c3_Image_006.png"], "annotated": false, "compound": false}
{"title": "Urban Accessibility as a Socio-Political Problem", "pdf_hash": "249599b991052f90d48141883b074bcb229fa8c3", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 3 shows the three stakeholders annotated with the individual roles and the common goals across all groups. Two-way arrows indicate interactions between each stakeholder pair", "levels": null, "corpus_id": 230717562, "sentences": ["Figure 3 shows the three stakeholders annotated with the individual roles and the common goals across all groups.", "Two-way arrows indicate interactions between each stakeholder pair"], "caption": "Figure 3. Roles and Interactions between groups involved in city-scale decision-making.", "local_uri": ["249599b991052f90d48141883b074bcb229fa8c3_Image_009.png"], "annotated": false, "compound": false}
{"title": "Urban Accessibility as a Socio-Political Problem", "pdf_hash": "249599b991052f90d48141883b074bcb229fa8c3", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "2-part diagram. Left half shows the four stakeholder groups laid along a diamond shape with arrows showing interactions. Each edge is also annotated by smaller arrows indicating the amount of interactions occuring between stakeholders.  Right half shows a list of interaction goals that occur on each edge of the graph.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 230717562, "sentences": ["2-part diagram.", "Left half shows the four stakeholder groups laid along a diamond shape with arrows showing interactions.", "Each edge is also annotated by smaller arrows indicating the amount of interactions occuring between stakeholders.", "Right half shows a list of interaction goals that occur on each edge of the graph."], "caption": "Figure 4. Civic Interaction Space. Illustrates the different civic interactions between the primary stakeholder groups and identifies six points of interactions. Groups are denoted by: CM=community includes, MI/caregivers and general public, A=advocates (and activists), D=department officials, and PM=policymakers. The perceived number of interactions between stakeholders is represented by the weight of the arrows. For example, high interactions between policymakers and department officials due to interdependent roles vs relatively less interactions between government officials and citizens.", "local_uri": ["249599b991052f90d48141883b074bcb229fa8c3_Image_010.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EduFeed: A Social Feed to Engage Preliterate Children in Educational Activities", "pdf_hash": "411ca948c7ba610b7dd34353d948618a9ecc29ba", "year": 2017, "venue": "CSCW", "alt_text": "The sharescreen shows a small, round speaker icon at the top; next to the icon it says, \"Share this with one friend.\" There are five circles that have images of students. Underneath is a green circular button that says, \"Done.\"", "levels": null, "corpus_id": 16353505, "sentences": ["The sharescreen shows a small, round speaker icon at the top; next to the icon it says, \"Share this with one friend.\" There are five circles that have images of students.", "Underneath is a green circular button that says, \"Done.\""], "caption": "", "local_uri": ["411ca948c7ba610b7dd34353d948618a9ecc29ba_Image_001.png"], "annotated": false, "compound": false}
{"title": "EduFeed: A Social Feed to Engage Preliterate Children in Educational Activities", "pdf_hash": "411ca948c7ba610b7dd34353d948618a9ecc29ba", "year": 2017, "venue": "CSCW", "alt_text": "The image is of the social feed. It has three speech bubbles with the activities for addition, fill-in-the-blank, and word-typing. The addition activity is 1 plus blank equals two. It has yellow and green bars stacked in the bubble. The fill-in-the-blank activity is, \"My favorite animal is the blank.\" The word-typing activity is for the word 'cat'. It has a picture of a cat and the word spelled out. Underneath the bubbles are thumbnails of a robot, a student, and a teacher. Smaller thumbnails of students under the addition activity shows that they completed it.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 16353505, "sentences": ["The image is of the social feed.", "It has three speech bubbles with the activities for addition, fill-in-the-blank, and word-typing.", "The addition activity is 1 plus blank equals two.", "It has yellow and green bars stacked in the bubble.", "The fill-in-the-blank activity is, \"My favorite animal is the blank.\"", "The word-typing activity is for the word 'cat'.", "It has a picture of a cat and the word spelled out.", "Underneath the bubbles are thumbnails of a robot, a student, and a teacher.", "Smaller thumbnails of students under the addition activity shows that they completed it."], "caption": "Figure 1. The social feed displays thumbnails previewing different types of educational exercises, along with thumbnails of the poster and users who have completed it. A speech bubble metaphor is used to convey the metaphor of social sharing to a preliterate audience. The robot icon indicates an item inserted in the feed algorithmically by the system, rather than one shared explicitly by a classmate.", "local_uri": ["411ca948c7ba610b7dd34353d948618a9ecc29ba_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EduFeed: A Social Feed to Engage Preliterate Children in Educational Activities", "pdf_hash": "411ca948c7ba610b7dd34353d948618a9ecc29ba", "year": 2017, "venue": "CSCW", "alt_text": "There the three different feeds shown here from left to right: AS, the activity- and sharer-driven feed; A, the activity-driven feed; and S, the sharer-driven feed. The AS feed is the same as figure 1. There are speech bubbles with peers and the robot underneath. In the A version their is only the robot thumbnail underneath the speech bubble. Finally, in the S version, there are no speech bubbles. Instead, the bubbles are replaced by large squares that show the sharer (which can be the robot, peer, and teacher).", "levels": null, "corpus_id": 16353505, "sentences": ["There the three different feeds shown here from left to right: AS, the activity- and sharer-driven feed; A, the activity-driven feed; and S, the sharer-driven feed.", "The AS feed is the same as figure 1.", "There are speech bubbles with peers and the robot underneath.", "In the A version their is only the robot thumbnail underneath the speech bubble.", "Finally, in the S version, there are no speech bubbles.", "Instead, the bubbles are replaced by large squares that show the sharer (which can be the robot, peer, and teacher)."], "caption": "", "local_uri": ["411ca948c7ba610b7dd34353d948618a9ecc29ba_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "EduFeed: A Social Feed to Engage Preliterate Children in Educational Activities", "pdf_hash": "411ca948c7ba610b7dd34353d948618a9ecc29ba", "year": 2017, "venue": "CSCW", "alt_text": "A boy on the left is kneeling while the boy on the right holds both tablets in his lap. They are sitting on the floor.", "levels": null, "corpus_id": 16353505, "sentences": ["A boy on the left is kneeling while the boy on the right holds both tablets in his lap.", "They are sitting on the floor."], "caption": "", "local_uri": ["411ca948c7ba610b7dd34353d948618a9ecc29ba_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "EduFeed: A Social Feed to Engage Preliterate Children in Educational Activities", "pdf_hash": "411ca948c7ba610b7dd34353d948618a9ecc29ba", "year": 2017, "venue": "CSCW", "alt_text": "A girl on the left completes a word typing activity for the girl on the right. They are sitting at a table.", "levels": null, "corpus_id": 16353505, "sentences": ["A girl on the left completes a word typing activity for the girl on the right. They are sitting at a table."], "caption": "Figure 10. A student helps his peer find an activity in his feed.", "local_uri": ["411ca948c7ba610b7dd34353d948618a9ecc29ba_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "DytectiveU: A Game to Train the Difficulties and the Strengths of Children with Dyslexia", "pdf_hash": "97e2b28eaa7df00806bb4f283c105a76d91d1317", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 1: Characters of DytectiveU (up); and an  example of an exercise targeting the executive func-  tion of activation and attention and the linguistic  skills of phonological and lexical awareness (down).", "levels": null, "corpus_id": 29908793, "sentences": ["Figure 1: Characters of DytectiveU (up); and an  example of an exercise targeting the executive func-  tion of activation and attention and the linguistic  skills of phonological and lexical awareness (down)."], "caption": "", "local_uri": ["97e2b28eaa7df00806bb4f283c105a76d91d1317_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "DytectiveU: A Game to Train the Difficulties and the Strengths of Children with Dyslexia", "pdf_hash": "97e2b28eaa7df00806bb4f283c105a76d91d1317", "year": 2017, "venue": "ASSETS", "alt_text": "Figure 2: Example of an evaluation report of the language skills after playing a challenge. English translation in blue.", "levels": null, "corpus_id": 29908793, "sentences": ["Figure 2: Example of an evaluation report of the language skills after playing a challenge.", "English translation in blue."], "caption": "Figure 2: Example of an evaluation report of the language skills after playing a challenge. English translation in blue.", "local_uri": ["97e2b28eaa7df00806bb4f283c105a76d91d1317_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "On-the-job Learning for Real-time", "pdf_hash": "05e1ac3144f378261c557932e89340a2a474080d", "year": 2017, "venue": "", "alt_text": "That is, given a dictionary, g(\u2219), we add 1 for every word, wi, found in the dictionary, and 0 otherwise. The total count is then divided by the total number of words.", "levels": [[-1], [-1]], "corpus_id": 46437810, "sentences": ["That is, given a dictionary, g(\u2219), we add 1 for every word, wi, found in the dictionary, and 0 otherwise.", "The total count is then divided by the total number of words."], "caption": "The scoring system distinguishes between stenotype and qwerty by exploiting the fact that the chorded qwerty keys are not often English words; either by themselves, or when combined with the subsequently pressed keys. The algorithm uses H(hi) to distinguish these cases because the boundary between chords and qwerty keys is not marked in the keyset since the spacebar is not used in stenotype to separate words.", "local_uri": ["05e1ac3144f378261c557932e89340a2a474080d_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "On-the-job Learning for Real-time", "pdf_hash": "05e1ac3144f378261c557932e89340a2a474080d", "year": 2017, "venue": "", "alt_text": "Please type the following in the box below. Animated example. Please type the following according to the rules above.", "levels": null, "corpus_id": 46437810, "sentences": ["Please type the following in the box below.", "Animated example.", "Please type the following according to the rules above."], "caption": "Figure 1. The webpage shown above collected keysets from crowdworkers containing a mixture of touch-typing and stenotype. The example phrase \u2018so we need energy miracles now drni use op\u2019 shows how we substituted in the qwerty keys corresponding to the needed stenotype chords to collect the test keysets from crowdworkers. In this example, a worker would need to chord the key combinations d-r-n and o-p.", "local_uri": ["05e1ac3144f378261c557932e89340a2a474080d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "On-the-job Learning for Real-time", "pdf_hash": "05e1ac3144f378261c557932e89340a2a474080d", "year": 2017, "venue": "", "alt_text": "Interface contains a visualiation of the keyboard with both QWERTY and steno keys on the left, and a transcription box on the right. The keyboard visualization shows the keys necessary to trigger the chord for the word that is being taught, which is \"something\"", "levels": null, "corpus_id": 46437810, "sentences": ["Interface contains a visualiation of the keyboard with both QWERTY and steno keys on the left, and a transcription box on the right.", "The keyboard visualization shows the keys necessary to trigger the chord for the word that is being taught, which is \"something\""], "caption": "Figure 3. The user interface for Scopist highlights stenotype keys on a typical qwerty layout to teach a crowdworker the appropriate hand posture for chording. The highlighted keys are for the prompted word underlined at the top of the page, and a reminder of the link between the chord keys and the sounds of the prompted word are provided in parentheses.", "local_uri": ["05e1ac3144f378261c557932e89340a2a474080d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Turning off-the-shelf games into biofeedback games", "pdf_hash": "6100fbab9dc9e9fa8d6387b91ac58bf3a15731e5", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure shows levels of texture-based biofeedback for two games (Portal, Nail'd).", "levels": null, "corpus_id": 18710961, "sentences": ["Figure shows levels of texture-based biofeedback for two games (Portal, Nail'd)."], "caption": "Figure 1. Columns show levels of texture-based biofeedback. Rows show customizations of an effect for two games (Portal, Nail\u2019d).", "local_uri": ["6100fbab9dc9e9fa8d6387b91ac58bf3a15731e5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Introduction to Intelligent User Interfaces", "pdf_hash": "bf833bed703f8960c99b329d10cebf67c14981d3", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: Examples of intelligent user interfaces in everyday life. Left: More and more people use smart speakers and voice assistants at home. Center: Recommendations have become a ubiquitous element in many user interfaces, for example, on movie platforms. Right: Modern smartphone keyboards decode even sloppy typing, correct words, and suggest next words. The course covers concepts and techniques relevant to these and further interfaces and examples.", "levels": null, "corpus_id": 233987051, "sentences": ["Figure 1: Examples of intelligent user interfaces in everyday life.", "Left: More and more people use smart speakers and voice assistants at home.", "Center: Recommendations have become a ubiquitous element in many user interfaces, for example, on movie platforms.", "Right: Modern smartphone keyboards decode even sloppy typing, correct words, and suggest next words.", "The course covers concepts and techniques relevant to these and further interfaces and examples."], "caption": "", "local_uri": ["bf833bed703f8960c99b329d10cebf67c14981d3_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SmellControl: The Study of Sense of Agency in Smell", "pdf_hash": "4b763962607ce472bff32e4ef5dea670315ab0e3", "year": 2020, "venue": "ICMI", "alt_text": "This figure shows how the scent delivery system is composed. It consisted of a tank of compressed air, 3 valves, 3 jars containing the scents and the output nozzle. The valves were controlled by an Arduino board connected to a computer.", "levels": null, "corpus_id": 224818469, "sentences": ["This figure shows how the scent delivery system is composed.", "It consisted of a tank of compressed air, 3 valves, 3 jars containing the scents and the output nozzle.", "The valves were controlled by an Arduino board connected to a computer."], "caption": "", "local_uri": ["4b763962607ce472bff32e4ef5dea670315ab0e3_Image_003.png"], "annotated": false, "compound": false}
{"title": "SmellControl: The Study of Sense of Agency in Smell", "pdf_hash": "4b763962607ce472bff32e4ef5dea670315ab0e3", "year": 2020, "venue": "ICMI", "alt_text": "A picture containing screen, clock, computer  Description automatically generated", "levels": [[-1]], "corpus_id": 224818469, "sentences": ["A picture containing screen, clock, computer  Description automatically generated"], "caption": "Figure 8. (Top) Mean action, outcome and total binding in milliseconds with \u00b1 SD in brackets. (Middle) A positive value represents a delayed awareness (action binding) while a negative value an early awareness (outcome binding). (Bottom) Total binding (Action Binding \u2013 Outcome Binding). Error bars represent SEM. * = p<0.05.", "local_uri": ["4b763962607ce472bff32e4ef5dea670315ab0e3_Image_008.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "StreamWiki: Enabling Viewers of Knowledge Sharing Live Streams to Collaboratively Generate Archival Documentation for Effective In-Stream and Post Hoc Learning", "pdf_hash": "9f4b2b3bc454840e5d013d316f1eedf1edb4017f", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A screenshot of the main interface of StreamWiki, showing the video stream player, the comment window, the card widget, and the archived cards.", "levels": null, "corpus_id": 53056195, "sentences": ["A screenshot of the main interface of StreamWiki, showing the video stream player, the comment window, the card widget, and the archived cards."], "caption": "", "local_uri": ["9f4b2b3bc454840e5d013d316f1eedf1edb4017f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "StreamWiki: Enabling Viewers of Knowledge Sharing Live Streams to Collaboratively Generate Archival Documentation for Effective In-Stream and Post Hoc Learning", "pdf_hash": "9f4b2b3bc454840e5d013d316f1eedf1edb4017f", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A screenshot of the card interface and the list of archived comments, and a screenshot of the comment volume and keywords visualizations and the previously added cards.", "levels": null, "corpus_id": 53056195, "sentences": ["A screenshot of the card interface and the list of archived comments, and a screenshot of the comment volume and keywords visualizations and the previously added cards."], "caption": "", "local_uri": ["9f4b2b3bc454840e5d013d316f1eedf1edb4017f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "StreamWiki: Enabling Viewers of Knowledge Sharing Live Streams to Collaboratively Generate Archival Documentation for Effective In-Stream and Post Hoc Learning", "pdf_hash": "9f4b2b3bc454840e5d013d316f1eedf1edb4017f", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Survey responses about the experiences of writing summaries using StreamWiki. Few people thought it time consuming, hard or distracting. Most people thought it made them feel more engaged, helped them understand the content, helped them answer questions raised in the stream, increased their interest in the topic, made them feel a stronger sense of social presence, and gave them a good overview of the stream.", "levels": null, "corpus_id": 53056195, "sentences": ["Survey responses about the experiences of writing summaries using StreamWiki.", "Few people thought it time consuming, hard or distracting.", "Most people thought it made them feel more engaged, helped them understand the content, helped them answer questions raised in the stream, increased their interest in the topic, made them feel a stronger sense of social presence, and gave them a good overview of the stream."], "caption": "Fig. 4 . Survey responses about the experience of writing summaries using StreamWiki.", "local_uri": ["9f4b2b3bc454840e5d013d316f1eedf1edb4017f_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 1 illustrates a hierarchical menu selection sequence of StickyPie, and how it is different to the traditional pie menu using eye gaze input modality. Key feature of StickyPie is the supporting scale-invariant gaze marking input.", "levels": [[-1], [-1]], "corpus_id": 233987560, "sentences": ["Figure 1 illustrates a hierarchical menu selection sequence of StickyPie, and how it is different to the traditional pie menu using eye gaze input modality.", "Key feature of StickyPie is the supporting scale-invariant gaze marking input."], "caption": "Figure 1: StickyPie mitigates overshoot errors when performing a border crossing selection on a gaze-based marking menu: (a) Example item selection sequence with a gaze-based marking menu. (b) Example of overshoot error which frequently occurs with a regular gaze-based marking menu (RegularPie) because of the scale-variance of gaze marking input. (c) We present StickyPie, adopting the concept of scale-invariant marking input toward better support of gaze gestures in marking menus.", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 2 illustrates the four representative types of selection errors of hierarchical pie menu interface using eye gaze input.     Figure 2 b, c, and ,d illustrates each of three types of errors which affected by size of pie menu. Those figures show how the size affect each errors.     Figure 2 f illustrates the content bound rule which consider balancing the overshoot errors and the incorrect selection errors.", "levels": [[1], [1], [1], [1]], "corpus_id": 233987560, "sentences": ["Figure 2 illustrates the four representative types of selection errors of hierarchical pie menu interface using eye gaze input.", "Figure 2 b, c, and ,d illustrates each of three types of errors which affected by size of pie menu.", "Those figures show how the size affect each errors.", "Figure 2 f illustrates the content bound rule which consider balancing the overshoot errors and the incorrect selection errors."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 3 a illustrates the experiments set-up. A participant wear Vive Pro Eye and hold a hand-held controller.", "levels": null, "corpus_id": 233987560, "sentences": ["Figure 3 a illustrates the experiments set-up.", "A participant wear Vive Pro Eye and hold a hand-held controller."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 4 illustrates the results of experiment 1. Information Trasfer Rate tend to increase as enlarging the size of pie menu in experienced trials, but, in novice trials, it is tend to decrease as enlarging the size of pie.     Completion time tend to increase as enlarging the size of pie menu in novice trials, but, in experienced trials, the size of pie did not affect the completion time.     Error rate tend to decreas as enlarging the size of pie in both novice and experienced trials. However, in novice tirals of 4x4x4 layout and 8x8 layout, the largest pie generated more errors than the smaller sized pies. It is because of the misread errors caused by poor visual acuity of the peripheral regions of the display.", "levels": [[1], [3], [3], [3], [3], [4]], "corpus_id": 233987560, "sentences": ["Figure 4 illustrates the results of experiment 1.", "Information Trasfer Rate tend to increase as enlarging the size of pie menu in experienced trials, but, in novice trials, it is tend to decrease as enlarging the size of pie.", "Completion time tend to increase as enlarging the size of pie menu in novice trials, but, in experienced trials, the size of pie did not affect the completion time.", "Error rate tend to decreas as enlarging the size of pie in both novice and experienced trials.", "However, in novice tirals of 4x4x4 layout and 8x8 layout, the largest pie generated more errors than the smaller sized pies.", "It is because of the misread errors caused by poor visual acuity of the peripheral regions of the display."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3, 4], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 5 shows the total number of errors per different size of pies. Major error types are the overshoot errors and the incorrect selection errors.    In novice trials,   76 overshoot errros, 26 incorrect selection errors, 30 False Activation errors, 4 misread errors, and 5 blinking errors occur in small size pie condition.    49 overshoot errros, 17 incorrect selection errors, 16 False Activation errors, 14 misread errors, and 2 blinking errors occur in medium size pie condition.    31 overshoot errros, 37 incorrect selection errors, 11 False Activation errors, 35 misread errors, and 7 blinking errors occur in large size pie condition.    In experienced trials,   62 overshoot errros, 30 incorrect selection errors, 21 False Activation errors, 2 misread errors, and 3 blinking errors occur in small size pie condition.    37 overshoot errros, 19 incorrect selection errors, 7 False Activation errors, 1 misread errors, and 3 blinking errors occur in medium size pie condition.    20 overshoot errros, 18 incorrect selection errors, 5 False Activation errors, 0 misread errors, and 2 blinking errors occur in large size pie condition.", "levels": [[1], [3], [2], [2], [2], [2], [2], [2]], "corpus_id": 233987560, "sentences": ["Figure 5 shows the total number of errors per different size of pies.", "Major error types are the overshoot errors and the incorrect selection errors.", "In novice trials,   76 overshoot errros, 26 incorrect selection errors, 30 False Activation errors, 4 misread errors, and 5 blinking errors occur in small size pie condition.", "49 overshoot errros, 17 incorrect selection errors, 16 False Activation errors, 14 misread errors, and 2 blinking errors occur in medium size pie condition.", "31 overshoot errros, 37 incorrect selection errors, 11 False Activation errors, 35 misread errors, and 7 blinking errors occur in large size pie condition.", "In experienced trials,   62 overshoot errros, 30 incorrect selection errors, 21 False Activation errors, 2 misread errors, and 3 blinking errors occur in small size pie condition.", "37 overshoot errros, 19 incorrect selection errors, 7 False Activation errors, 1 misread errors, and 3 blinking errors occur in medium size pie condition.", "20 overshoot errros, 18 incorrect selection errors, 5 False Activation errors, 0 misread errors, and 2 blinking errors occur in large size pie condition."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 6 illustrates two possible options to mitigate the overshoot errors. First option is placing the next pie at a fixed distance. It could mitigate the overshoot errors but could generate the undershoot errors. The other option is placing the next pie at the saccade landing position. It could mitigate the overshoot errors and free to undershoot errors.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987560, "sentences": ["Figure 6 illustrates two possible options to mitigate the overshoot errors.", "First option is placing the next pie at a fixed distance.", "It could mitigate the overshoot errors but could generate the undershoot errors.", "The other option is placing the next pie at the saccade landing position.", "It could mitigate the overshoot errors and free to undershoot errors."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 7 illustrates how we detect saccade landing position. The saccade landing event is detected by observing eye movement velocity after a user performing border crossing gesture.     If we detect saccade eye movement when a user performing border crossing gesture, then we monitor the eye movement velocity until detecting saccade landing event. Once detecting saccade landing event, we determine the saccade is blinking or not. If it is blinking, we cancel the selection by the border crossing gesture. If not, the next pie appears at the position of saccade landing position.", "levels": null, "corpus_id": 233987560, "sentences": ["Figure 7 illustrates how we detect saccade landing position.", "The saccade landing event is detected by observing eye movement velocity after a user performing border crossing gesture.", "If we detect saccade eye movement when a user performing border crossing gesture, then we monitor the eye movement velocity until detecting saccade landing event.", "Once detecting saccade landing event, we determine the saccade is blinking or not. If it is blinking, we cancel the selection by the border crossing gesture.", "If not, the next pie appears at the position of saccade landing position."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 8 illustrates the size of pies which tested in experiment 2. All the pies set the 2.5 degree of content bound. As following the content bound rule, the diameters of 4, 6, and 8 granularity pies are 12.07, 15.00, and 18.07 degree, respectively.", "levels": null, "corpus_id": 233987560, "sentences": ["Figure 8 illustrates the size of pies which tested in experiment 2.", "All the pies set the 2.5 degree of content bound.", "As following the content bound rule, the diameters of 4, 6, and 8 granularity pies are 12.07, 15.00, and 18.07 degree, respectively."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR", "pdf_hash": "6ec41b668075f3053375de627be63b5661867880", "year": 2021, "venue": "CHI", "alt_text": "Figure 9 illustrates the results of experiment 2.     Information Trasfer Rates were equivalent across the different depth and the granularity in novice trials. In experienced trials, those are also equivalent across the different granularities with 3 depth hierarchy. however, the information transfer rate tend to increase as increasing the granularity with 2 depth hierachy.     Completion time tend to increase as increasing both the granularity and the depth in both novice and experienced trials.    Error rate tend to increase as increasing both the granularity and the depth in both novice and experienced trials. Figure 9 c show that the overshoot errors not much occur while using StickyPie.", "levels": [[1], [3], [3], [3], [3], [3], [3, 1]], "corpus_id": 233987560, "sentences": ["Figure 9 illustrates the results of experiment 2.", "Information Trasfer Rates were equivalent across the different depth and the granularity in novice trials.", "In experienced trials, those are also equivalent across the different granularities with 3 depth hierarchy.", "however, the information transfer rate tend to increase as increasing the granularity with 2 depth hierachy.", "Completion time tend to increase as increasing both the granularity and the depth in both novice and experienced trials.", "Error rate tend to increase as increasing both the granularity and the depth in both novice and experienced trials.", "Figure 9 c show that the overshoot errors not much occur while using StickyPie."], "caption": "", "local_uri": ["6ec41b668075f3053375de627be63b5661867880_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Patching Physical Objects", "pdf_hash": "15940c3cf0526cb7904981b5e2a2000a96e06548", "year": 2015, "venue": "UIST", "alt_text": "To minimize material consumption and to reduce waste during design iteration, we propose patching the existing object rather than reprinting it from scratch. (a) First, our software calculates which part changed, then (b) a mill removes outdated geometry, followed by a print head that prints the new geometry.", "levels": null, "corpus_id": 10192677, "sentences": ["To minimize material consumption and to reduce waste during design iteration, we propose patching the existing object rather than reprinting it from scratch.", "(a) First, our software calculates which part changed, then (b) a mill removes outdated geometry, followed by a print head that prints the new geometry."], "caption": "Figure 1: To minimize material consumption and to reduce waste during design iteration, we propose patch- ing the existing object rather than reprinting it from scratch. (a) First, our software calculates which part changed, then (b) a mill removes outdated geometry, followed by a print head that prints the new geometry.", "local_uri": ["15940c3cf0526cb7904981b5e2a2000a96e06548_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Patching Physical Objects", "pdf_hash": "15940c3cf0526cb7904981b5e2a2000a96e06548", "year": 2015, "venue": "UIST", "alt_text": "(a) Failed print. (b) The user selects the failed part with the repair brush. (c) Patching.", "levels": null, "corpus_id": 10192677, "sentences": ["(a) Failed print. (b) The user selects the failed part with the repair brush.", "(c) Patching."], "caption": "Figure 3: (a) Failed print. (b) The user selects the failed part with the repair brush. (c) Patching.", "local_uri": ["15940c3cf0526cb7904981b5e2a2000a96e06548_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Patching Physical Objects", "pdf_hash": "15940c3cf0526cb7904981b5e2a2000a96e06548", "year": 2015, "venue": "UIST", "alt_text": "(a) The print head causes a collision. (b) The system removes additional geometry, to (c) then reprint.", "levels": null, "corpus_id": 10192677, "sentences": ["(a) The print head causes a collision. (b) The system removes additional geometry, to (c) then reprint."], "caption": "Figure 7: (a) The print head causes a collision. (b) The system removes additional geometry, to (c) then reprint.", "local_uri": ["15940c3cf0526cb7904981b5e2a2000a96e06548_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Patching Physical Objects", "pdf_hash": "15940c3cf0526cb7904981b5e2a2000a96e06548", "year": 2015, "venue": "UIST", "alt_text": "(a) In this setup, the mill bit causes a collision. (b) Correct alignment of mill and print head.", "levels": null, "corpus_id": 10192677, "sentences": ["(a) In this setup, the mill bit causes a collision.", "(b) Correct alignment of mill and print head."], "caption": "Figure 8: (a) In this setup, the mill bit causes a collision.", "local_uri": ["15940c3cf0526cb7904981b5e2a2000a96e06548_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Patching Physical Objects", "pdf_hash": "15940c3cf0526cb7904981b5e2a2000a96e06548", "year": 2015, "venue": "UIST", "alt_text": "To avoid collision of the print head with existing geometry, we lower the base plate accordingly.", "levels": null, "corpus_id": 10192677, "sentences": ["To avoid collision of the print head with existing geometry, we lower the base plate accordingly."], "caption": "Figure 13: In the example shown, the optimal solution to minimize waste and material consumption is to rotate the object 90\u00b0.", "local_uri": ["15940c3cf0526cb7904981b5e2a2000a96e06548_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "The Crowd Work Accessibility Problem", "pdf_hash": "104eff3cd27e37977aa04cf930b58084fee726b2", "year": 2017, "venue": "W4A", "alt_text": "Two examples of inaccessible elements of other kinds of tasks: a graphical task and a scale question.", "levels": [[-1]], "corpus_id": 24893988, "sentences": ["Two examples of inaccessible elements of other kinds of tasks: a graphical task and a scale question."], "caption": "", "local_uri": ["104eff3cd27e37977aa04cf930b58084fee726b2_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dytective: Toward a Game to Detect Dyslexia", "pdf_hash": "c77eef6c6f716ec67e5e9e66d4946829f842a2f8", "year": 2015, "venue": "ASSETS", "alt_text": "Figure 1: The first level of Dytective. Players hear the character that they should click and then click it quickly within the time limit (shown in the green circle).", "levels": null, "corpus_id": 17761923, "sentences": ["Figure 1: The first level of Dytective.", "Players hear the character that they should click and then click it quickly within the time limit (shown in the green circle)."], "caption": "", "local_uri": ["c77eef6c6f716ec67e5e9e66d4946829f842a2f8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Stitching Together the Experiences of Disabled Knitters", "pdf_hash": "cec4695536ed17351d9f2d5b70ef8d4416461e5c", "year": 2021, "venue": "CHI", "alt_text": "a: A green round plastic loom with many evenly spaced pegs. A purple knitted object is visible inside, and the knitter's hand is resting on the loom. b: A cardigan with a dark plum neckline and large round buttons. The colorwork is a grid of squares with stars inside connected by diagonal lines. c: A purple knit object is resting on a table next to a ball of yarn. It has evenly spaced orange triangular stitch markers on the needles, which are capped with rubber end caps. d: A yellow hat is resting on a grey surface. It has k1 p1 ribbing followed by approximately 4x4 squares of knit and purl for five rows. The top decreases rapidly in diameter and is all knit.", "levels": null, "corpus_id": 233987604, "sentences": ["a: A green round plastic loom with many evenly spaced pegs.", "A purple knitted object is visible inside, and the knitter's hand is resting on the loom.", "b: A cardigan with a dark plum neckline and large round buttons.", "The colorwork is a grid of squares with stars inside connected by diagonal lines.", "c: A purple knit object is resting on a table next to a ball of yarn.", "It has evenly spaced orange triangular stitch markers on the needles, which are capped with rubber end caps.", "d: A yellow hat is resting on a grey surface.", "It has k1 p1 ribbing followed by approximately 4x4 squares of knit and purl for five rows.", "The top decreases rapidly in diameter and is all knit."], "caption": "", "local_uri": ["cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Stitching Together the Experiences of Disabled Knitters", "pdf_hash": "cec4695536ed17351d9f2d5b70ef8d4416461e5c", "year": 2021, "venue": "CHI", "alt_text": "A printed knitting pattern with handwritten annotations.]{A zoomed in part of a knitting pattern printed out on paper with penciled in circles around stitch counts, underlines for instructions, check marks for counting, and notes between printed lines.", "levels": null, "corpus_id": 233987604, "sentences": ["A printed knitting pattern with handwritten annotations.]{A zoomed in part of a knitting pattern printed out on paper with penciled in circles around stitch counts, underlines for instructions, check marks for counting, and notes between printed lines."], "caption": "Figure 2: A portion of P16\u2019s written pattern", "local_uri": ["cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Stitching Together the Experiences of Disabled Knitters", "pdf_hash": "cec4695536ed17351d9f2d5b70ef8d4416461e5c", "year": 2021, "venue": "CHI", "alt_text": "a: A light green knitted robot with a dark green heart over where it's heart would be and dark green stripes over it's arms and legs. It also has removeable headphones over it's ears. b: A green robot with a dark green knit heart as well as dark green stripes over the arms and legs. This robot has a limb difference in it's left limb. c: Light pink robot with a dark pink heart and stripes over it's arms and legs. One leg is a prosthetic and is detachable and re-attachable by a clip.", "levels": null, "corpus_id": 233987604, "sentences": ["a: A light green knitted robot with a dark green heart over where it's heart would be and dark green stripes over it's arms and legs.", "It also has removeable headphones over it's ears.", "b: A green robot with a dark green knit heart as well as dark green stripes over the arms and legs.", "This robot has a limb difference in it's left limb.", "c: Light pink robot with a dark pink heart and stripes over it's arms and legs.", "One leg is a prosthetic and is detachable and re-attachable by a clip."], "caption": "(a)                                         (b)                                         (c)", "local_uri": ["cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_008.jpg", "cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_009.jpg", "cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_010.jpg"], "annotated": false, "compound": true}
{"title": "Stitching Together the Experiences of Disabled Knitters", "pdf_hash": "cec4695536ed17351d9f2d5b70ef8d4416461e5c", "year": 2021, "venue": "CHI", "alt_text": "An oval loom with white hooks attached underneath the loom and stitches in yarn around some of the pegs.]{The rectangular loom has ten white hook purl attachments meant for yarn to loop around to aid in the execution of purl stitches, and a small rectangle of cardboard next to the hooks on either ends of the loom. The cardboard has a slit cut in it that the yarn feeds through for tension.", "levels": [[-1], [-1]], "corpus_id": 233987604, "sentences": ["An oval loom with white hooks attached underneath the loom and stitches in yarn around some of the pegs.]{The rectangular loom has ten white hook purl attachments meant for yarn to loop around to aid in the execution of purl stitches, and a small rectangle of cardboard next to the hooks on either ends of the loom.", "The cardboard has a slit cut in it that the yarn feeds through for tension."], "caption": "", "local_uri": ["cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_011.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Stitching Together the Experiences of Disabled Knitters", "pdf_hash": "cec4695536ed17351d9f2d5b70ef8d4416461e5c", "year": 2021, "venue": "CHI", "alt_text": "Images of the needle, tensioner, final iteration and first iteration of the one handed needle knitting device]{(a) The needle tip on the left becomes smaller then larger at the tip with the vertical indentation from the tip to partway down the needle. (b) The tensioner has two feed guides and a pair of posts which yarn can be wrapped around. (c) A knitting device that holds two needles by the back with the points angled toward each other. On a stick pointing away from the needle tips, there is a stick with the tensioner and next to the device is a latch needle with a metal hook and swinging component near the tip. All of this is mounted on a black lap table. (d) a person is knitting with purple yarn. They are a holding the hook in their left hand and using it to grab yarn coming off the tensioner to complete a stitch.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987604, "sentences": ["Images of the needle, tensioner, final iteration and first iteration of the one handed needle knitting device]{(a) The needle tip on the left becomes smaller then larger at the tip with the vertical indentation from the tip to partway down the needle. (", "b) The tensioner has two feed guides and a pair of posts which yarn can be wrapped around. (", "c) A knitting device that holds two needles by the back with the points angled toward each other.", "On a stick pointing away from the needle tips, there is a stick with the tensioner and next to the device is a latch needle with a metal hook and swinging component near the tip.", "All of this is mounted on a black lap table. (", "d) a person is knitting with purple yarn.", "They are a holding the hook in their left hand and using it to grab yarn coming off the tensioner to complete a stitch."], "caption": "", "local_uri": ["cec4695536ed17351d9f2d5b70ef8d4416461e5c_Image_012.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing video games for older adults and caregivers", "pdf_hash": "a5b078260bddc6569886accc4e1a6e0199f11d2b", "year": 2014, "venue": "", "alt_text": "A screenshot showing the first game, Candy Kids, where candy flows in the top and middle parts of the screen and need to be picked up by the player avatar (represented by a hand) to be fed to a girl displayed at the bottom of the screen.", "levels": null, "corpus_id": 17341401, "sentences": ["A screenshot showing the first game, Candy Kids, where candy flows in the top and middle parts of the screen and need to be picked up by the player avatar (represented by a hand) to be fed to a girl displayed at the bottom of the screen."], "caption": "", "local_uri": ["a5b078260bddc6569886accc4e1a6e0199f11d2b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Designing video games for older adults and caregivers", "pdf_hash": "a5b078260bddc6569886accc4e1a6e0199f11d2b", "year": 2014, "venue": "", "alt_text": "A screenshot of the second game, Prairie Hunter, which shows animals (e.g., a bear, and a bird) in front of a forest scenery which can be hunted by the player. The player avatar is represented by a black crosshair.", "levels": null, "corpus_id": 17341401, "sentences": ["A screenshot of the second game, Prairie Hunter, which shows animals (e.g., a bear, and a bird) in front of a forest scenery which can be hunted by the player.", "The player avatar is represented by a black crosshair."], "caption": "Figure 1. Candy Kids.                                          Figure 2. Prairie Hunter.", "local_uri": ["a5b078260bddc6569886accc4e1a6e0199f11d2b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Designing video games for older adults and caregivers", "pdf_hash": "a5b078260bddc6569886accc4e1a6e0199f11d2b", "year": 2014, "venue": "", "alt_text": "Results for the player experience questionnaire showing above-average experience on all subscales, and showing that the average experience for older adults and caregivers revealed no significant differences.", "levels": [[2, 1]], "corpus_id": 17341401, "sentences": ["Results for the player experience questionnaire showing above-average experience on all subscales, and showing that the average experience for older adults and caregivers revealed no significant differences."], "caption": "", "local_uri": ["a5b078260bddc6569886accc4e1a6e0199f11d2b_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Designing video games for older adults and caregivers", "pdf_hash": "a5b078260bddc6569886accc4e1a6e0199f11d2b", "year": 2014, "venue": "", "alt_text": "The player experience profile for Emma and Andrew shows that it is imbalanced, with the caregiver Andrew reporting higher levels of ease of use and competence, but low levels for communication and partner preference. In contrast, results for Emma, the older adult, show that she thought that Andrew and her communicated well and that she preferred playing with him, but that she did not perceive the game easy to use, and that she did not feel competent.", "levels": null, "corpus_id": 17341401, "sentences": ["The player experience profile for Emma and Andrew shows that it is imbalanced, with the caregiver Andrew reporting higher levels of ease of use and competence, but low levels for communication and partner preference.", "In contrast, results for Emma, the older adult, show that she thought that Andrew and her communicated well and that she preferred playing with him, but that she did not perceive the game easy to use, and that she did not feel competent."], "caption": "", "local_uri": ["a5b078260bddc6569886accc4e1a6e0199f11d2b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Designing video games for older adults and caregivers", "pdf_hash": "a5b078260bddc6569886accc4e1a6e0199f11d2b", "year": 2014, "venue": "", "alt_text": "Player experience profile for Jim and Sarah, showing very balanced results for both older adult and caregiver, with all ratings being above three on a five point Likert scale except for competence result of caregiver Sarah, which is lower.", "levels": null, "corpus_id": 17341401, "sentences": ["Player experience profile for Jim and Sarah, showing very balanced results for both older adult and caregiver, with all ratings being above three on a five point Likert scale except for competence result of caregiver Sarah, which is lower."], "caption": "", "local_uri": ["a5b078260bddc6569886accc4e1a6e0199f11d2b_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Authentication Beyond Desktops and Smartphones: Novel Approaches for Smart Devices and Environments", "pdf_hash": "63201a586cc636e850070e1829c52d7157b00583", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: A smart environment in which users are authenticated through multiple devices.", "levels": null, "corpus_id": 218482841, "sentences": ["Figure 1: A smart environment in which users are authenticated through multiple devices."], "caption": "of our everyday life creates new challenges for authenti- cation, in particular since many of those devices are not designed and developed with authentication in mind. Ex- amples include but are not limited to wearables, AR and VR glasses, devices in smart homes, and public displays. The goal of this workshop is to develop a common under-", "local_uri": ["63201a586cc636e850070e1829c52d7157b00583_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users", "pdf_hash": "243a694be1b17c4018272ecce46685ad24b079a6", "year": 2020, "venue": "ASSETS", "alt_text": "A user standing in a front of a car, and the wrist-worn watch shows \"Car Honk, 98%\".", "levels": null, "corpus_id": 220727229, "sentences": ["A user standing in a front of a car, and the wrist-worn watch shows \"Car Honk, 98%\"."], "caption": "", "local_uri": ["243a694be1b17c4018272ecce46685ad24b079a6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users", "pdf_hash": "243a694be1b17c4018272ecce46685ad24b079a6", "year": 2020, "venue": "ASSETS", "alt_text": "A user standing outdoors and looking at a watch worn on the left wrist while holding a phone in the right hand.", "levels": null, "corpus_id": 220727229, "sentences": ["A user standing outdoors and looking at a watch worn on the left wrist while holding a phone in the right hand."], "caption": "Figure 1: SoundWatch uses a deep-CNN based sound classifier to classify and provide feedback about environmental sounds on a smartwatch in real-time. Images show different use cases of the app and one of the four architectures we built (watch+phone).", "local_uri": ["243a694be1b17c4018272ecce46685ad24b079a6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users", "pdf_hash": "243a694be1b17c4018272ecce46685ad24b079a6", "year": 2020, "venue": "ASSETS", "alt_text": "Watch-only architecture uses watch for all four steps (record audio, compute features, predict sound, and display information)  Watch+cloud architecture uses watch for the first two steps (record audio and compute features), the cloud for the predict sound step, and the watch again for the display notification.  Watch+phone uses watch for the record audio and display notification, but phone for the compute features and predict sounds.  Finally, watch+phone+cloud architecture uses watch for the record audio and display notification, phone to compute features, and the cloud to predict sound.", "levels": null, "corpus_id": 220727229, "sentences": ["Watch-only architecture uses watch for all four steps (record audio, compute features, predict sound, and display information)  Watch+cloud architecture uses watch for the first two steps (record audio and compute features), the cloud for the predict sound step, and the watch again for the display notification.", "Watch+phone uses watch for the record audio and display notification, but phone for the compute features and predict sounds.", "Finally, watch+phone+cloud architecture uses watch for the record audio and display notification, phone to compute features, and the cloud to predict sound."], "caption": "", "local_uri": ["243a694be1b17c4018272ecce46685ad24b079a6_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users", "pdf_hash": "243a694be1b17c4018272ecce46685ad24b079a6", "year": 2020, "venue": "ASSETS", "alt_text": "A table with four models with their accuracies presented across all sounds, med. priority sounds, high priority sounds, as well as for home, office and outdoor contexts. Values given in text.", "levels": null, "corpus_id": 220727229, "sentences": ["A table with four models with their accuracies presented across all sounds, med.", "priority sounds, high priority sounds, as well as for home, office and outdoor contexts.", "Values given in text."], "caption": "", "local_uri": ["243a694be1b17c4018272ecce46685ad24b079a6_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users", "pdf_hash": "243a694be1b17c4018272ecce46685ad24b079a6", "year": 2020, "venue": "ASSETS", "alt_text": "A bar graph comparing the model performance of the four models across the there sound categories. Values given in text.", "levels": [[1], [0]], "corpus_id": 220727229, "sentences": ["A bar graph comparing the model performance of the four models across the there sound categories.", "Values given in text."], "caption": "", "local_uri": ["243a694be1b17c4018272ecce46685ad24b079a6_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing IoT Resources to Support Outdoor Play for Children", "pdf_hash": "301de4ab3e4377191b55884ca1724428771054e1", "year": 2020, "venue": "CHI", "alt_text": "Figure 1. (a) Photo of the Play Poles prototype outside in a tarmac yard. The prototype is in the middle of the photo and has a wooden base that holds a copper pipe with a circular coloured disk on top.  Figure 1. (b) Photo of the top of the Play Poles prototype with a copper pipe and coloured disk that is orange.  Figure 1. (c) Photo of the Play Poles controller that is wooden, rectuangular and has six buttons in two linear rows of three.  These buttons are different colours: blue, pink, orange, purple, green and red.", "levels": null, "corpus_id": 218483193, "sentences": ["Figure 1. (a) Photo of the Play Poles prototype outside in a tarmac yard.", "The prototype is in the middle of the photo and has a wooden base that holds a copper pipe with a circular coloured disk on top.", "Figure 1. (b) Photo of the top of the Play Poles prototype with a copper pipe and coloured disk that is orange.", "Figure 1. (c) Photo of the Play Poles controller that is wooden, rectuangular and has six buttons in two linear rows of three.", "These buttons are different colours: blue, pink, orange, purple, green and red."], "caption": "Figure 1. Play Poles: (a) at Birch Tree, (b) coloured disc detail and (c) controller.", "local_uri": ["301de4ab3e4377191b55884ca1724428771054e1_Image_001.jpg", "301de4ab3e4377191b55884ca1724428771054e1_Image_002.jpg", "301de4ab3e4377191b55884ca1724428771054e1_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "Designing IoT Resources to Support Outdoor Play for Children", "pdf_hash": "301de4ab3e4377191b55884ca1724428771054e1", "year": 2020, "venue": "CHI", "alt_text": "Figure 2. (a) Photo showing a tin can with a hand that is pulling out an internal wooden structure housing electronics.  Figure 2. (b) Photo of a Play Can showing tin can with a red lid and out of the top sits a bull dog clip with a coloured disk attached.  Figure 2. (c) Photo showing a copper pipe attached to the bottom of a tin can.", "levels": null, "corpus_id": 218483193, "sentences": ["Figure 2. (a) Photo showing a tin can with a hand that is pulling out an internal wooden structure housing electronics.", "Figure 2. (b) Photo of a Play Can showing tin can with a red lid and out of the top sits a bull dog clip with a coloured disk attached.", "Figure 2. (c) Photo showing a copper pipe attached to the bottom of a tin can."], "caption": "", "local_uri": ["301de4ab3e4377191b55884ca1724428771054e1_Image_004.jpg", "301de4ab3e4377191b55884ca1724428771054e1_Image_005.jpg", "301de4ab3e4377191b55884ca1724428771054e1_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Designing IoT Resources to Support Outdoor Play for Children", "pdf_hash": "301de4ab3e4377191b55884ca1724428771054e1", "year": 2020, "venue": "CHI", "alt_text": "Figure 3. (a) Photo of the Beacon Box prototype with two black rectangular boxes.  One has a hinge with a lid that is open revealing the inside of the box.  Figure 3. (b) Photo of the light meter prototype which consists of a circular ring of red light.", "levels": null, "corpus_id": 218483193, "sentences": ["Figure 3. (a) Photo of the Beacon Box prototype with two black rectangular boxes.", "One has a hinge with a lid that is open revealing the inside of the box.", "Figure 3. (b) Photo of the light meter prototype which consists of a circular ring of red light."], "caption": "Figure 3. (a) Beacon Boxes (b) Light Meter", "local_uri": ["301de4ab3e4377191b55884ca1724428771054e1_Image_007.jpg", "301de4ab3e4377191b55884ca1724428771054e1_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Designing IoT Resources to Support Outdoor Play for Children", "pdf_hash": "301de4ab3e4377191b55884ca1724428771054e1", "year": 2020, "venue": "CHI", "alt_text": "Figure 4. Photo of the Play Watch.  It shows a rectangular electronic component with a small LED display and two buttons.  This is on the wrist of a child who is holding it to the camera.", "levels": null, "corpus_id": 218483193, "sentences": ["Figure 4.", "Photo of the Play Watch.", "It shows a rectangular electronic component with a small LED display and two buttons.", "This is on the wrist of a child who is holding it to the camera."], "caption": "Figure 4. Play Watch", "local_uri": ["301de4ab3e4377191b55884ca1724428771054e1_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Voice Games: Investigation Into the Use of Non-speech Voice Input for Making Computer Games More Accessible", "pdf_hash": "28636cf804c2520de41a7a7ff4392d173ba98a7b", "year": 2011, "venue": "INTERACT", "alt_text": "A bar graph showing the timing results for the NMI group from the categorical input experiment.", "levels": [[1]], "corpus_id": 2114983, "sentences": ["A bar graph showing the timing results for the NMI group from the categorical input experiment."], "caption": "Figure 4: Summary of the aggregate results from the categorical input experiment for the NMI group. The results are grouped by task type. Each bar shows the contribution from reaction time and system time towards the total input time, as defined in Figure 2. Error bars show 95% confidence intervals for total input times.", "local_uri": ["28636cf804c2520de41a7a7ff4392d173ba98a7b_Image_006.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Making GIFs Accessible", "pdf_hash": "71219f2574c484c41e730c3ab758af92ac942400", "year": 2020, "venue": "ASSETS", "alt_text": "A histogram of unique popular GIFs we saw in our Twitter sample. The most popular GIF on the left exceeds 1000 uses, and the graph quickly tapers off after around 200 GIFs. After that, most are below 50 uses. The graph is displayed in a logarithmic scale, yet still tapers off quickly.", "levels": [[1], [3, 2], [3, 2], [3, 1]], "corpus_id": 226068842, "sentences": ["A histogram of unique popular GIFs we saw in our Twitter sample.", "The most popular GIF on the left exceeds 1000 uses, and the graph quickly tapers off after around 200 GIFs.", "After that, most are below 50 uses.", "The graph is displayed in a logarithmic scale, yet still tapers off quickly."], "caption": "", "local_uri": ["71219f2574c484c41e730c3ab758af92ac942400_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Making GIFs Accessible", "pdf_hash": "71219f2574c484c41e730c3ab758af92ac942400", "year": 2020, "venue": "ASSETS", "alt_text": "Four frames from a GIF of Oprah. In the first she looks off camera. Second, she looks towards the camera, out of the side of her eye. Third, she looks directly at the camera with a slight smile. Fourth, she shrugs with her palm up and head tilted slightly.", "levels": null, "corpus_id": 226068842, "sentences": ["Four frames from a GIF of Oprah.", "In the first she looks off camera.", "Second, she looks towards the camera, out of the side of her eye.", "Third, she looks directly at the camera with a slight smile.", "Fourth, she shrugs with her palm up and head tilted slightly."], "caption": "", "local_uri": ["71219f2574c484c41e730c3ab758af92ac942400_Image_006.jpg", "71219f2574c484c41e730c3ab758af92ac942400_Image_007.jpg", "71219f2574c484c41e730c3ab758af92ac942400_Image_008.jpg", "71219f2574c484c41e730c3ab758af92ac942400_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "Making GIFs Accessible", "pdf_hash": "71219f2574c484c41e730c3ab758af92ac942400", "year": 2020, "venue": "ASSETS", "alt_text": "Still images of all the 15 GIFs we used in the study. The alternative text for the GIFs are included here: 01: Spongebob and Patrick Starr run back and forth across the screen frantically screaming with their arms raised. 02: Sheldon from the Big Bang Theory hyperventilates into a paper bag 03: Judge Judy sitting at the bench exageratedly rolls her eyes 04: Micheal Scott from The Office standing in the office yells \"No! God! No! God! Please! No! No! No! Nooooooooooo!\" 05: Detective Jake Peralta from Brooklynn 99 stands up from sitting while saying \"Alright, I'm going to go Cry\". 06: Stringer Bell from The Wire, gives another character a side eye, a second glance, then a small laugh 07: Basketball player Donovan Mitchell talks to a reporter, and teammate Rudy Gobert walks by and rubs his head 08: Elissa Slater from Big Brother is drinking water from a coffee cup, then she starts laughing and spits water everywhere. 09: Michelle Tanner from Full House blows a kiss with a smile. Text says \"You da best!\" 10: Wavy colorful text reads \"oh look I made it worse\" 11: A crowd claps and rises from their seats to give a standing ovation. 12: Ryan Gosling puts his head in both hands and rubs his eyes in frustration. 13: Michael Scott and Erin Hannon from The Office celebrate by showering the office with champagne and dancing wildly. 14: A gray cartoon cat pokes another cat on the cheek before kissing it. A heart floats upwards. 15: Oprah Winfrey turns to look straight at the camera, shifts her eyes sideways and then back to center, then shrugs.", "levels": null, "corpus_id": 226068842, "sentences": ["Still images of all the 15 GIFs we used in the study.", "The alternative text for the GIFs are included here: 01: Spongebob and Patrick Starr run back and forth across the screen frantically screaming with their arms raised.", "02: Sheldon from the Big Bang Theory hyperventilates into a paper bag 03: Judge Judy sitting at the bench exageratedly rolls her eyes 04: Micheal Scott from The Office standing in the office yells \"No! God! No! God! Please! No! No! No! Nooooooooooo!\" 05: Detective Jake Peralta from Brooklynn 99 stands up from sitting while saying \"Alright, I'm going to go Cry\".", "06: Stringer Bell from The Wire, gives another character a side eye, a second glance, then a small laugh 07: Basketball player Donovan Mitchell talks to a reporter, and teammate Rudy Gobert walks by and rubs his head 08: Elissa Slater from Big Brother is drinking water from a coffee cup, then she starts laughing and spits water everywhere.", "09: Michelle Tanner from Full House blows a kiss with a smile.", "Text says \"You da best!\" 10: Wavy colorful text reads \"oh look I made it worse\" 11: A crowd claps and rises from their seats to give a standing ovation.", "12: Ryan Gosling puts his head in both hands and rubs his eyes in frustration. 13: Michael Scott and Erin Hannon from The Office celebrate by showering the office with champagne and dancing wildly.", "14: A gray cartoon cat pokes another cat on the cheek before kissing it.", "A heart floats upwards.", "15: Oprah Winfrey turns to look straight at the camera, shifts her eyes sideways and then back to center, then shrugs."], "caption": "", "local_uri": ["71219f2574c484c41e730c3ab758af92ac942400_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing", "pdf_hash": "56f28fa646fe7fc259ec368577db46f0acc317a5", "year": 2021, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Three diagrams are arranged in a row. The first diagram for Part 1 is titled \"(a) sound recognizer demo during the initial interview.\" A screenshot of an online interactive sound recognizer is in the background with four sections labeled. The first label is \"sound classes\" and points to areas on the left titled \"background noise\", \"clapping\", and \"paper crumpling\"; the second label is \"training samples with spectrogram\" and points to small audio visualizations with in the sound class areas; the third label is \"real-time input\" and points to a larger spectrogram visualization on the upper-right; and the fourth label is \"model results\" and points to percentage bars in the lower right. A person in a white shirt is shown in the foreground holding paper with the caption \"P3 tests his recognizer by crumpling paper.\" The second diagram for Part 2 is titled \"(b) sample recording during the field study\". A black stove top is shown in the background where a silver tea kettle is producing steam. A hand holding a phone is in the foreground; the phone screen says \"recording\" on top and shows a waveform with a label added that says \"real-time waveform\". At the bottom of the screen is a \"pause\" button that is labeled \"toggle recording\".         The third diagram for part 3 is titled \"(c) design brainstorming during the follow-up interview\" with a smaller caption above that says \"P9 suggests and enhanced waveform with individual sounds accentuated\". A person in a black shirt holds up a yellow sheet of paper with a rectangle drawn on it. Inside there is a rough sketch of a waveform labeled \"waveform underlay\" and rectangular shapes that are sketched in and labeled \"sound annotations\".", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 235175667, "sentences": ["Three diagrams are arranged in a row.", "The first diagram for Part 1 is titled \"(a) sound recognizer demo during the initial interview.\"", "A screenshot of an online interactive sound recognizer is in the background with four sections labeled.", "The first label is \"sound classes\" and points to areas on the left titled \"background noise\", \"clapping\", and \"paper crumpling\"; the second label is \"training samples with spectrogram\" and points to small audio visualizations with in the sound class areas; the third label is \"real-time input\" and points to a larger spectrogram visualization on the upper-right; and the fourth label is \"model results\" and points to percentage bars in the lower right.", "A person in a white shirt is shown in the foreground holding paper with the caption \"P3 tests his recognizer by crumpling paper.\"", "The second diagram for Part 2 is titled \"(b) sample recording during the field study\".", "A black stove top is shown in the background where a silver tea kettle is producing steam.", "A hand holding a phone is in the foreground; the phone screen says \"recording\" on top and shows a waveform with a label added that says \"real-time waveform\".", "At the bottom of the screen is a \"pause\" button that is labeled \"toggle recording\".", "The third diagram for part 3 is titled \"(c) design brainstorming during the follow-up interview\" with a smaller caption above that says \"P9 suggests and enhanced waveform with individual sounds accentuated\".", "A person in a black shirt holds up a yellow sheet of paper with a rectangle drawn on it.", "Inside there is a rough sketch of a waveform labeled \"waveform underlay\" and rectangular shapes that are sketched in and labeled \"sound annotations\"."], "caption": "", "local_uri": ["56f28fa646fe7fc259ec368577db46f0acc317a5_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing", "pdf_hash": "56f28fa646fe7fc259ec368577db46f0acc317a5", "year": 2021, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Five video screens grouped together; each has a title at the top, a real-life video at left, and a large waveform and spectrogram visualization at right.        The first is titled \"Tea kettle whistle in a quiet home\"; the video shows a yellow kettle sitting on a stovetop; the visualizations show quiet sound activity increasing in loudness to the right.       The second is titled \"Baby crying during a thunderstorm\"; the video shows a small baby laying on a white blanket and crying; the visualizations show loud, intermittent sound activity.       The third is titled \"emergency sirens passing on a busy street\"; the video shows an ambulance with sirens on driving through a large city; the visualizations show loud, constant sound activity.       The fourth is titled \"Dog barking on a summer day\"; the video shows a brown dog with mouth open in a wooded area; the visualizations show very sharp peaks for loud, intermittent sound activity.       The fifth is titled \"Door knock at a small party\"; the video shows a hand clenched in a fist hovering over a wooden door; the visualizations show constant, medium sound activity with sharp peaks for the knocking.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 235175667, "sentences": ["Five video screens grouped together; each has a title at the top, a real-life video at left, and a large waveform and spectrogram visualization at right.", "The first is titled \"Tea kettle whistle in a quiet home\"; the video shows a yellow kettle sitting on a stovetop; the visualizations show quiet sound activity increasing in loudness to the right.", "The second is titled \"Baby crying during a thunderstorm\"; the video shows a small baby laying on a white blanket and crying; the visualizations show loud, intermittent sound activity.", "The third is titled \"emergency sirens passing on a busy street\"; the video shows an ambulance with sirens on driving through a large city; the visualizations show loud, constant sound activity.", "The fourth is titled \"Dog barking on a summer day\"; the video shows a brown dog with mouth open in a wooded area; the visualizations show very sharp peaks for loud, intermittent sound activity.", "The fifth is titled \"Door knock at a small party\"; the video shows a hand clenched in a fist hovering over a wooden door; the visualizations show constant, medium sound activity with sharp peaks for the knocking."], "caption": "", "local_uri": ["56f28fa646fe7fc259ec368577db46f0acc317a5_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing", "pdf_hash": "56f28fa646fe7fc259ec368577db46f0acc317a5", "year": 2021, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "Six sound features, labled A through G. A shows the loudness of a sound with a bar meter and dynamic wave design, B provides feedback on the level of background noise with a text status or bar, C shows how many sounds are co-occurring with a text status, D identifies if a recording has background noise with a text status or waveform overlay, E identifies the presences of co-occuring sounds with a text status or waveform overlay, F assesses the quality of a sound with a text status.", "levels": [[-1]], "corpus_id": 235175667, "sentences": ["Six sound features, labled A through G. A shows the loudness of a sound with a bar meter and dynamic wave design, B provides feedback on the level of background noise with a text status or bar, C shows how many sounds are co-occurring with a text status, D identifies if a recording has background noise with a text status or waveform overlay, E identifies the presences of co-occuring sounds with a text status or waveform overlay, F assesses the quality of a sound with a text status."], "caption": "", "local_uri": ["56f28fa646fe7fc259ec368577db46f0acc317a5_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing", "pdf_hash": "56f28fa646fe7fc259ec368577db46f0acc317a5", "year": 2021, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "A stacked bar graph with each participant placed on the x-axis and the number of sound classes on the y-axis. P9 and P4 each recorded between 25-30 sound classes; eight participants each recorded between 15-20; and P10 and P11 recorded between 10-15. 103 classes were indoor mechanical sounds, 59 were presence of people or pets, 30 were non-urgent alerts, 22 were outdoor background, 11 were urgent alerts, and 18 were other sounds.", "levels": [[1], [2], [2]], "corpus_id": 235175667, "sentences": ["A stacked bar graph with each participant placed on the x-axis and the number of sound classes on the y-axis.", "P9 and P4 each recorded between 25-30 sound classes; eight participants each recorded between 15-20; and P10 and P11 recorded between 10-15.", "103 classes were indoor mechanical sounds, 59 were presence of people or pets, 30 were non-urgent alerts, 22 were outdoor background, 11 were urgent alerts, and 18 were other sounds."], "caption": "", "local_uri": ["56f28fa646fe7fc259ec368577db46f0acc317a5_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "HCI Interventions for Science Communication", "pdf_hash": "70ec7206fd2e30632b368abd107af7075f4a42c5", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../Documents/100_Interactionart/400_PhDthesis/200_Images/Figure%205.14%20Physical%20and%20Virtual%20Plot%20Points,%20Twitter,%202014%2", "levels": [[-1]], "corpus_id": 5038663, "sentences": ["../Documents/100_Interactionart/400_PhDthesis/200_Images/Figure%205.14%20Physical%20and%20Virtual%20Plot%20Points,%20Twitter,%202014%2"], "caption": "Figure 2. Screenshot of found chronofact \u00a9 FurtureCoast.org, 2014", "local_uri": ["70ec7206fd2e30632b368abd107af7075f4a42c5_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure1\nThere are three visualizations of bi-manual interactions. The one on the left shows a left hand with the index finger and the thumb stretched to create a horizontal Plane. A cube is then being placed against that Plane by the right hand. The middle image depicts scaling a cube up by creating a Plane at its bottom with left hand, and then grabbing the object and pointing up with the thumb to 1D scale it. The right-most figure is a left hand pointing up (with the thumb) which creates a visible vertical Ray. A cube is then rotated around the Ray with the right hand by pointing in direction around the Ray with the index finger.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure1\nThere are three visualizations of bi-manual interactions.", "The one on the left shows a left hand with the index finger and the thumb stretched to create a horizontal Plane.", "A cube is then being placed against that Plane by the right hand.", "The middle image depicts scaling a cube up by creating a Plane at its bottom with left hand, and then grabbing the object and pointing up with the thumb to 1D scale it.", "The right-most figure is a left hand pointing up (with the thumb) which creates a visible vertical Ray.", "A cube is then rotated around the Ray with the right hand by pointing in direction around the Ray with the index finger."], "caption": "Figure 1. A user can create shape constraints such as Plane (left) or Ray (right) through gestures with their non-dominant hand. The same gestures can be used by the dominant hand to control the manipulation degrees of freedom (middle, right).", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure2\nFigure2a shows a hand in resting position\nFigure2b shows a hand with the thumb pointing up\nFigure2c shows a hand with the index pointing forward\nFigure2d shows a hand with the index pointing forward and the thumb pointing up\nFigure2e shows the left hand pointing up to create a vertical Ray, and then the right hand grabbing and a tethered cube and pointing up (in the direction of Ray) to translate the cube along the Ray.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure2\nFigure2a shows a hand in resting position\nFigure2b shows a hand with the thumb pointing up\nFigure2c shows a hand with the index pointing forward\nFigure2d shows a hand with the index pointing forward and the thumb pointing up\nFigure2e shows the left hand pointing up to create a vertical Ray, and then the right hand grabbing and a tethered cube and pointing up (in the direction of Ray) to translate the cube along the Ray."], "caption": "Figure 2. Hand gestures for creating a Point (a), creating an upward Ray (b), creating a forward Ray (c), and creating a Plane (d). The same hand gestures are also used on the dominant hand to further specify the DOF (e).", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure3\nThis figure contains three parts. The left part shows a open hand with a cursor. The middle part shows that as the index and thumb come together (to grab an object), then they meet at the position of the cursor. The right-most part shows an example of this interaction, a hand is grabbing a cube by bringing their index and thumb together when the cursor is inside the cube.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure3\nThis figure contains three parts.", "The left part shows a open hand with a cursor.", "The middle part shows that as the index and thumb come together (to grab an object), then they meet at the position of the cursor.", "The right-most part shows an example of this interaction, a hand is grabbing a cube by bringing their index and thumb together when the cursor is inside the cube."], "caption": "Figure 3. A white cursor located near the virtual hand (left) indicates where the hand will grasp an object (middle). If the cursor is inside the object, then the user can grab it (right).", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure4\nFigure4a shows a Ray, and a cube next to it\nFigure4b shows the hand has clicked on the cube, and now there is a visible line (tether) connecting the cube and the Ray.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure4\nFigure4a shows a Ray, and a cube next to it\nFigure4b shows the hand has clicked on the cube, and now there is a visible line (tether) connecting the cube and the Ray."], "caption": "Figure 4. Tethering an object to a Ray by clicking on it.", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_004.gif"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure5\nAll parts of this figure share a horizontal Plane that is created at the bottom by the left hand. \nFigure5a shows a box being aligned along a Plane by grabbing it and moving it against the Plane. \nFigure5b shows a box already on the Plane being 1D scaled vertically by grabbing it and pointing up (ie. perpendicular to the horizontal plane) with right hand. \nFigure5c shows a tethered cube above the Plane being translated right by grabbing it and pointing right with right hand. \nFigure5d shows a tethered cube above the Plane translated along the Plane by pointing forward (with index) and right (with thumb) at the same time using right hand.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure5\nAll parts of this figure share a horizontal Plane that is created at the bottom by the left hand.", "Figure5a shows a box being aligned along a Plane by grabbing it and moving it against the Plane.", "Figure5b shows a box already on the Plane being 1D scaled vertically by grabbing it and pointing up (ie. perpendicular to the horizontal plane) with right hand.", "Figure5c shows a tethered cube above the Plane being translated right by grabbing it and pointing right with right hand.", "Figure5d shows a tethered cube above the Plane translated along the Plane by pointing forward (with index) and right (with thumb) at the same time using right hand."], "caption": "Figure 5. Interactions with an active Plane. (a) Objects collide against the Plane and move on it. (b) When an object was left on the Plane, re-grasping and moving the object scales it relative to the Plane. Tethered objects can (c) move along one of Plane\u2019s directional axes or (d) move parallel to the Plane.", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure6 \nAll parts of this figure have a vertical Ray that is created with the left hand by pointing up. \nFigure6a shows the right hand grabbing a tethered cube and pointing up (parallel to vertical Ray) to translate the cube along the Ray. \nFigure6b shows the right hand grabbing a tethered cube and pointing the index around the Ray to rotate rotate the cube around the Ray. \nFigure6c shows the right hand grabbing a tethered cube and pointing thumb away from the Ray to change the distance between the cube and the Ray. \nFigure6d shows the right hand grabbing an intersecting cube and pointing thumb away from the Ray to 2D scale it about the Ray.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure6 \nAll parts of this figure have a vertical Ray that is created with the left hand by pointing up.", "Figure6a shows the right hand grabbing a tethered cube and pointing up (parallel to vertical Ray) to translate the cube along the Ray. \nFigure6b shows the right hand grabbing a tethered cube and pointing the index around the Ray to rotate rotate the cube around the Ray.", "Figure6c shows the right hand grabbing a tethered cube and pointing thumb away from the Ray to change the distance between the cube and the Ray. \nFigure6d shows the right hand grabbing an intersecting cube and pointing thumb away from the Ray to 2D scale it about the Ray."], "caption": "Figure 6. Tethered objects can (a) move along the Ray, (b) rotate around the Ray, (c) move away from the Ray. Intersecting objects can also (d) 2D uniform scale out from the Ray.", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_006.gif"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure7 All parts of this figure have a Point that is created by the left hand. \nFigure7a shows a tethered cube changing its distance from the Point by grabbing it with the right hand pointing the right hand away from the Point. \nFigure7b shows a tethered cube rotating about the Point by grabbing it with the right hand and pointing with both index and thumb. \nFigure7c shows a tethered cube rotating on a concentric circle about the Point, by pointing forward with just the index in the desired direction.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure7 All parts of this figure have a Point that is created by the left hand.", "Figure7a shows a tethered cube changing its distance from the Point by grabbing it with the right hand pointing the right hand away from the Point.", "Figure7b shows a tethered cube rotating about the Point by grabbing it with the right hand and pointing with both index and thumb.", "Figure7c shows a tethered cube rotating on a concentric circle about the Point, by pointing forward with just the index in the desired direction."], "caption": "Figure 7. An object that is (a) changing its distance from the Point, (b) rotating around the Point, and (c) rotating around a concentric circle using hand gestures", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_007.gif"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure8 \nThis figure contains three parts. The left-most part shows a Plane being activated, there is a visible net connecting the Plane and the hand. The middle part shows the hand moving away from the activated Plane, the net between the hand and the Plane begins to stretch. The right-most part shows the Plane independently placed in the world, with nothing connecting the hand the Plane anymore.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure8 \nThis figure contains three parts.", "The left-most part shows a Plane being activated, there is a visible net connecting the Plane and the hand.", "The middle part shows the hand moving away from the activated Plane, the net between the hand and the Plane begins to stretch.", "The right-most part shows the Plane independently placed in the world, with nothing connecting the hand the Plane anymore."], "caption": "Figure 8. Elastic bands stretch as the hand moves further away from the constraint, until eventually breaking and leaving the constraint permanently in the world.", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure9\nFigure9a shows a Point with two cubes tethered to it. \nFigure9b shows a hand moving the points, and the same motion being applied to the two cubes.", "levels": [[-1], [-1]], "corpus_id": 204811916, "sentences": ["Figure9\nFigure9a shows a Point with two cubes tethered to it.", "Figure9b shows a hand moving the points, and the same motion being applied to the two cubes."], "caption": "Figure 9. A Point acts as a natural metaphor for grouping objects, with a persistent Point that has objects tethered to it (a), the user can grasp the Point with her index (b) to move all tethered objects around like a group.", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_009.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure10\nFigure10a shows a point that has two orthogonal planes tethered to it\nFigure10b shows a cube being placed against the two orthogonal planes from part (a)", "levels": null, "corpus_id": 204811916, "sentences": ["Figure10\nFigure10a shows a point that has two orthogonal planes tethered to it\nFigure10b shows a cube being placed against the two orthogonal planes from part (a)"], "caption": "Figure 10. To create and use a corner, we first activate each of the associated walls using a Point (a), and then can grab any object to place it against the corner (b).", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "pdf_hash": "6302d830ea2c73d6e5c70724f3ea3f276d552a1c", "year": 2019, "venue": "UIST", "alt_text": "Figure11\nThis figure shows a hand with a glove holding a controller. It indicates that the glove contains two Flex sensors, one along the index, and another on the thumb. They are connected to a Adafruit Huzzah32 Board on the wrist of the person. The board is powered by a Li-Po Battery.", "levels": null, "corpus_id": 204811916, "sentences": ["Figure11\nThis figure shows a hand with a glove holding a controller.", "It indicates that the glove contains two Flex sensors, one along the index, and another on the thumb.", "They are connected to a Adafruit Huzzah32 Board on the wrist of the person.", "The board is powered by a Li-Po Battery."], "caption": "Figure 11. Gesture glove for detecting the gestures, while holding the controller in hand.", "local_uri": ["6302d830ea2c73d6e5c70724f3ea3f276d552a1c_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Accessible Video Calling", "pdf_hash": "f9d006f7aa7a956734fa92f74a5b633dcd6fc3bb", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "In parallel with exchanging video and speech between a Sighted and BLV user in video calling, inserting the NAVC prototype between the users enables it to detect conversational cues in the video and speech and generate corresponding audio cues for the BLV user", "levels": null, "corpus_id": 204873425, "sentences": ["In parallel with exchanging video and speech between a Sighted and BLV user in video calling, inserting the NAVC prototype between the users enables it to detect conversational cues in the video and speech and generate corresponding audio cues for the BLV user"], "caption": "", "local_uri": ["f9d006f7aa7a956734fa92f74a5b633dcd6fc3bb_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Accessible Video Calling", "pdf_hash": "f9d006f7aa7a956734fa92f74a5b633dcd6fc3bb", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Picture from user study where experimenter holds up a note for the confederate participant to see during the video call", "levels": null, "corpus_id": 204873425, "sentences": ["Picture from user study where experimenter holds up a note for the confederate participant to see during the video call"], "caption": "", "local_uri": ["f9d006f7aa7a956734fa92f74a5b633dcd6fc3bb_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "Photograph of a rectangular swatch of fabric with four square areas which are thicker than the rest. Superimposed, an image of the pattern used to generate the swatch.", "levels": [[-1], [-1]], "corpus_id": 233987322, "sentences": ["Photograph of a rectangular swatch of fabric with four square areas which are thicker than the rest.", "Superimposed, an image of the pattern used to generate the swatch."], "caption": "180\u200c", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_018.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "Photograph: side view of spacer fabric swatch lightly clamped in the bending rig.", "levels": [[-1]], "corpus_id": 233987322, "sentences": ["Photograph: side view of spacer fabric swatch lightly clamped in the bending rig."], "caption": "", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_022.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "A photograph of a folded piece of knit spacer fabric held gently in a hand, with the fingers of the hand above the structure and the thumb of the hand below. The fabric structure is flat. Next to this, a photograph of the same fabric, but the hand is squeezing. The fabric structure remains flat. Below this, a diagrammatic drawing of this spacer fabric shows the orientation of the filler yarn in both the un-squeezed and squeezed states.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987322, "sentences": ["A photograph of a folded piece of knit spacer fabric held gently in a hand, with the fingers of the hand above the structure and the thumb of the hand below.", "The fabric structure is flat.", "Next to this, a photograph of the same fabric, but the hand is squeezing.", "The fabric structure remains flat.", "Below this, a diagrammatic drawing of this spacer fabric shows the orientation of the filler yarn in both the un-squeezed and squeezed states."], "caption": "", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_025.jpg", "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_026.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "A photograph of the same folded knit spacer fabric as in the previous images. In this image the spacer fabric has been folded opposite to the previous figures, such that the faces that were on the outside of the folded fabric are now on the inside. The fabric piece is again held gently in a hand, in a similar pose as the previous image. The fabric structure is flat. Next to this, a photograph of the same folded knit spacer fabric as in the previous image, but the hand is squeezing the fabric. In this configuration, the squeezing pressure has caused the fabric to open into a v shape rather than remaining flat. Below this, a diagrammatic drawing shows how the different orientation of the filler yarn in this configuration leads to the observed opening behavior.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987322, "sentences": ["A photograph of the same folded knit spacer fabric as in the previous images.", "In this image the spacer fabric has been folded opposite to the previous figures, such that the faces that were on the outside of the folded fabric are now on the inside.", "The fabric piece is again held gently in a hand, in a similar pose as the previous image.", "The fabric structure is flat.", "Next to this, a photograph of the same folded knit spacer fabric as in the previous image, but the hand is squeezing the fabric.", "In this configuration, the squeezing pressure has caused the fabric to open into a v shape rather than remaining flat.", "Below this, a diagrammatic drawing shows how the different orientation of the filler yarn in this configuration leads to the observed opening behavior."], "caption": "", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_029.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "Image composition showing motion paths for six bristlebots as described in the caption; photos are taken from above. Each bot starts in the center of the frame. The \"neutral\" face order bots and one of the \"front priority\" bots all traveled forward, to the left side of the frame. One front priority bot moved sideways, downward in the frame, and the last one move backwards, rightward in the frame.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987322, "sentences": ["Image composition showing motion paths for six bristlebots as described in the caption; photos are taken from above.", "Each bot starts in the center of the frame.", "The \"neutral\" face order bots and one of the \"front priority\" bots all traveled forward, to the left side of the frame.", "One front priority bot moved sideways, downward in the frame, and the last one move backwards, rightward in the frame."], "caption": "(b)Figure 13: Varying the face knitting order and bristle height can produce bristle-bots with different travel directions. Paths are plotted over the first 1 second of travel. Dots are placed every 30th of a second. Blue paths marked with a circle indicate swatches knit with alternating face knitting order; green paths marked with a square indicate swatches with entirely front face priority knitting order. Within each color, the bristle length is shown as shades from dark (shortest bristles, tuck distance 1) to light (longest bristles, tuck distance 3).while the back slides forward. This \u201ccaterpillar\u201d robot is soft and lightweight.\u201cBristlebots\u201d use springy bristles to turn vibration forces into net motion [41, 42]. Because our \u201cbristle\u201d structure can have knit- in bias, we can produce fabrics which encourage linear motion to a greater or lesser extent (Figure 13). As noted in Section 6.3, this behavior shows some variation: the behavior of the front-face- priority bristlebots is somewhat inconsistent.", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_033.jpg", "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_034.jpg"], "annotated": false, "compound": true}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "A photograph of a knit control pad. The control pad is a curved spacer fabric sheet with five thicker areas. Each of the thicker areas includes four knit-in stripes of conductive yarn.", "levels": [[-1], [-1], [-1]], "corpus_id": 233987322, "sentences": ["A photograph of a knit control pad.", "The control pad is a curved spacer fabric sheet with five thicker areas.", "Each of the thicker areas includes four knit-in stripes of conductive yarn."], "caption": "", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_038.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Engineering Multifunctional Spacer Fabrics Through Machine Knitting", "pdf_hash": "88d1d38b237d966ca84c8d3e94dbaae03a87f4c1", "year": 2021, "venue": "CHI", "alt_text": "A close-up photograph of a monofilament bristle structure. The face fabric is white and the bristles are a blue monofilament.", "levels": [[-1], [-1]], "corpus_id": 233987322, "sentences": ["A close-up photograph of a monofilament bristle structure.", "The face fabric is white and the bristles are a blue monofilament."], "caption": "", "local_uri": ["88d1d38b237d966ca84c8d3e94dbaae03a87f4c1_Image_040.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems", "pdf_hash": "b5cff6a8e605b87e828405a2bb7cbdd0275470d0", "year": 2020, "venue": "UIST", "alt_text": "Example figure showcasing how addressability would work in a smart home setting.", "levels": null, "corpus_id": 222804576, "sentences": ["Example figure showcasing how addressability would work in a smart home setting."], "caption": "Figure 2. Future smart homes and offices are envisioned to contain many \u201csmart\u201d devices able to respond to voice commands. How- ever, without device-specific wakewords, multiple devices may try to respond to generic queries (left). Ideally, users would be able to face and speak to a device, more akin to human-human interaction (center & right). Thus, there is a need for Direction-of-Voice estimation approaches, especially those than can run locally on self-contained devices, without having to install extra sensors in the environment or rely on multi-device interoperability, which does not appear to be forthcoming in the near future.", "local_uri": ["b5cff6a8e605b87e828405a2bb7cbdd0275470d0_Image_003.png"], "annotated": false, "compound": false}
{"title": "Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems", "pdf_hash": "b5cff6a8e605b87e828405a2bb7cbdd0275470d0", "year": 2020, "venue": "UIST", "alt_text": "Example plot showcasing the waveforms for facing and not facing, the FFT with HLBR features and the cross correlation between the wave forms.", "levels": [[1]], "corpus_id": 222804576, "sentences": ["Example plot showcasing the waveforms for facing and not facing, the FFT with HLBR features and the cross correlation between the wave forms."], "caption": "Figure 3. Example waveform, FFT, and cross correlation for an example \u201cfacing\u201d and \u201cnot facing\u201d trial (all other factors same).", "local_uri": ["b5cff6a8e605b87e828405a2bb7cbdd0275470d0_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems", "pdf_hash": "b5cff6a8e605b87e828405a2bb7cbdd0275470d0", "year": 2020, "venue": "UIST", "alt_text": "Effect of distance is seen here in the LDA plots. As the distance increases the difference between the two components decreases.", "levels": [[1], [3]], "corpus_id": 222804576, "sentences": ["Effect of distance is seen here in the LDA plots.", "As the distance increases the difference between the two components decreases."], "caption": "Figure 9. LDA at 1, 3 and 5m speaker distances.", "local_uri": ["b5cff6a8e605b87e828405a2bb7cbdd0275470d0_Image_033.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Bespoke Reflections: Creating a One-Handed Braille Keyboard", "pdf_hash": "6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 1. Braille Context  Figure 1 a) The numbering of braille dots  - A rectangle with 2 rows of three dots to represent a braille cell the dots are number 1,2,3 down the left and 4,5,6, down the right  Figure 1 b) The layout of letters used in combination for braille dots  - A rectangle with 2 rows of three dots to represent a braille cell the dots are number F,D,S down the left and J,K,L, down the right  Figure 1 c) A perkins brailler  - A black and white illustration  of a perking brailler  Figure 1 d) Braille Display - A black and white image of a 14 cells braille display", "levels": null, "corpus_id": 225957997, "sentences": ["Figure 1. Braille Context  Figure 1 a) The numbering of braille dots  - A rectangle with 2 rows of three dots to represent a braille cell the dots are number 1,2,3 down the left and 4,5,6, down the right  Figure 1 b) The layout of letters used in combination for braille dots  - A rectangle with 2 rows of three dots to represent a braille cell the dots are number F,D,S down the left and J,K,L, down the right  Figure 1 c) A perkins brailler  - A black and white illustration  of a perking brailler  Figure 1 d) Braille Display - A black and white image of a 14 cells braille display"], "caption": "", "local_uri": ["6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Bespoke Reflections: Creating a One-Handed Braille Keyboard", "pdf_hash": "6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 2. Prototype keyboards  Figure 2 Left) Prototype 1 Piano style - A wood laser cut seven keys with copper tape on the keys and a hand resting on the keys  Figure 2 Right) Prototype 2 cups style - Layered laser cut ply wood in the shape of a cup with buttons down either side", "levels": null, "corpus_id": 225957997, "sentences": ["Figure 2.", "Prototype keyboards  Figure 2 Left) Prototype 1 Piano style - A wood laser cut seven keys with copper tape on the keys and a hand resting on the keys  Figure 2 Right) Prototype 2 cups style - Layered laser cut ply wood in the shape of a cup with buttons down either side"], "caption": "", "local_uri": ["6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37_Image_004.png"], "annotated": false, "compound": false}
{"title": "Bespoke Reflections: Creating a One-Handed Braille Keyboard", "pdf_hash": "6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 3. Prototype keyboards on a desk  Figure 3 Left) Raising the braille display on a box to streamline with the cup tilted at an angle. \u2026 a work desk with a laptop and braille display with a person holding the cup at an angel near the braille display which is on a lunchbox.  Figure 3 Right) The bridge style keyboard that was implemented to streamline the keys with the braille display. \u2026 a work desk with a laptop and braille display with the bride keyboard in place holding the braille display.", "levels": null, "corpus_id": 225957997, "sentences": ["Figure 3.", "Prototype keyboards on a desk  Figure 3 Left) Raising the braille display on a box to streamline with the cup tilted at an angle.", "\u2026 a work desk with a laptop and braille display with a person holding the cup at an angel near the braille display which is on a lunchbox.", "Figure 3 Right) The bridge style keyboard that was implemented to streamline the keys with the braille display.", "\u2026 a work desk with a laptop and braille display with the bride keyboard in place holding the braille display."], "caption": "", "local_uri": ["6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37_Image_005.png"], "annotated": false, "compound": false}
{"title": "Bespoke Reflections: Creating a One-Handed Braille Keyboard", "pdf_hash": "6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 4. Prototype keyboard refinements  Figure 4. Left) Wooden bridge style prototype with no holes and fingers drawn \u2026 on the left-hand end. It is held together with rubber bands  Figure 4 Right) Acrylic bridge with holes for button placement \u2026 There are holes where the buttons would go, it is held together with rubber bands", "levels": null, "corpus_id": 225957997, "sentences": ["Figure 4. Prototype keyboard refinements  Figure 4.", "Left) Wooden bridge style prototype with no holes and fingers drawn \u2026 on the left-hand end.", "It is held together with rubber bands  Figure 4 Right) Acrylic bridge with holes for button placement \u2026 There are holes where the buttons would go, it is held together with rubber bands"], "caption": "", "local_uri": ["6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37_Image_006.png"], "annotated": false, "compound": false}
{"title": "Bespoke Reflections: Creating a One-Handed Braille Keyboard", "pdf_hash": "6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 5. Prototype keyboard developed  Figure 5. Left) Acrylic bridge with buttons \u2026 There are five buttons on the left-hand end and five eon the front of the bridge  Figure 5 Right) Third round prototype with membrane switch as smooth silver cover \u2026 There are five buttons running up the slope on the left-hand end and two buttons behind them and three on the front.", "levels": null, "corpus_id": 225957997, "sentences": ["Figure 5.", "Prototype keyboard developed  Figure 5.", "Left) Acrylic bridge with buttons \u2026 There are five buttons on the left-hand end and five eon the front of the bridge  Figure 5 Right) Third round prototype with membrane switch as smooth silver cover \u2026 There are five buttons running up the slope on the left-hand end and two buttons behind them and three on the front."], "caption": "", "local_uri": ["6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37_Image_007.png"], "annotated": false, "compound": false}
{"title": "Bespoke Reflections: Creating a One-Handed Braille Keyboard", "pdf_hash": "6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 6. Final Keyboard  Figure 6: The latest version of the keyboard with a braille display \u2026 A Grey bridge with four recessed plastic buttons on running up the left-hand side with three buttons behind. There are three buttons on the front of the bridge on the left-hand side under the edge of the braille display.", "levels": null, "corpus_id": 225957997, "sentences": ["Figure 6. Final Keyboard  Figure 6: The latest version of the keyboard with a braille display \u2026 A Grey bridge with four recessed plastic buttons on running up the left-hand side with three buttons behind.", "There are three buttons on the front of the bridge on the left-hand side under the edge of the braille display."], "caption": "", "local_uri": ["6da5c1f9726bd6c5ce1adb8ae9231fe00795ae37_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts", "pdf_hash": "fdc69738f68745f1b11b269d30f9482fb428480b", "year": 2020, "venue": "CHI", "alt_text": "Three circular smartwatch screenshots are arranged in a row. The screenshot on the left says \"Door Knock\" with three arcs pointing upward. The one in the middle says \"Phone\" with two arcs pointing to the left. The one on the right says \"Name\" with one arc pointing to the right.", "levels": null, "corpus_id": 209428388, "sentences": ["Three circular smartwatch screenshots are arranged in a row.", "The screenshot on the left says \"Door Knock\" with three arcs pointing upward.", "The one in the middle says \"Phone\" with two arcs pointing to the left.", "The one on the right says \"Name\" with one arc pointing to the right."], "caption": "", "local_uri": ["fdc69738f68745f1b11b269d30f9482fb428480b_Image_002.png", "fdc69738f68745f1b11b269d30f9482fb428480b_Image_003.png"], "annotated": false, "compound": true}
{"title": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts", "pdf_hash": "fdc69738f68745f1b11b269d30f9482fb428480b", "year": 2020, "venue": "CHI", "alt_text": "Three diagrams are shown in a horizontal row. The left diagram is labelled \"direction\", with an icon of a person viewed from above in the center. Around the person are four sets of lines: two short lines in front of them, one short, one long to their right, three short behind, and one long one short to their left.     The middle diagram is labelled \"Loudness\" and shows three speaker icons with increasing volume arcs, each paired with sets of lines. The quiet speaker has one medium one short line, the moderate speaker has two medium lines, and the loud speaker has one medium one long line.    The right diagram shows a door, phone, and person icon in vertical order. To its right, he door icon has three short lines, the phone icon has three long lines, and the person icon has one short one long one short line.", "levels": null, "corpus_id": 209428388, "sentences": ["Three diagrams are shown in a horizontal row.", "The left diagram is labelled \"direction\", with an icon of a person viewed from above in the center.", "Around the person are four sets of lines: two short lines in front of them, one short, one long to their right, three short behind, and one long one short to their left.", "The middle diagram is labelled \"Loudness\" and shows three speaker icons with increasing volume arcs, each paired with sets of lines.", "The quiet speaker has one medium one short line, the moderate speaker has two medium lines, and the loud speaker has one medium one long line.", "The right diagram shows a door, phone, and person icon in vertical order.", "To its right, he door icon has three short lines, the phone icon has three long lines, and the person icon has one short one long one short line."], "caption": "(a) Direction                      (b) Loudness                (c) Identity", "local_uri": ["fdc69738f68745f1b11b269d30f9482fb428480b_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts", "pdf_hash": "fdc69738f68745f1b11b269d30f9482fb428480b", "year": 2020, "venue": "CHI", "alt_text": "A map of a bus stop is shown with a short scenario written at the top. A purple smiley face is in the center of the map, with red, numbered circles arranged around it. A numbered list labeled \"List of Sounds\", including \"bike bell\", \"bus arriving\", \"siren\" is shown on the right.", "levels": null, "corpus_id": 209428388, "sentences": ["A map of a bus stop is shown with a short scenario written at the top.", "A purple smiley face is in the center of the map, with red, numbered circles arranged around it.", "A numbered list labeled \"List of Sounds\", including \"bike bell\", \"bus arriving\", \"siren\" is shown on the right."], "caption": "", "local_uri": ["fdc69738f68745f1b11b269d30f9482fb428480b_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts", "pdf_hash": "fdc69738f68745f1b11b269d30f9482fb428480b", "year": 2020, "venue": "CHI", "alt_text": "Three column charts are shown horizontally. The first chart is labelled \"Complexity of Haptic w/ Visual\" and has shows decreasing utility moving right from Simple, to Tacton, to none.    The second chart is labelled Sound characteristics and shows decreasing utility moving right, from Identity, to Direction, to Loudness.     The last chart, labelled filtering options, shows decreasing utility moving from Identity to Direction to Loudness.", "levels": [[1], [3, 1], [3, 1], [3, 1]], "corpus_id": 209428388, "sentences": ["Three column charts are shown horizontally.", "The first chart is labelled \"Complexity of Haptic w/ Visual\" and has shows decreasing utility moving right from Simple, to Tacton, to none.", "The second chart is labelled Sound characteristics and shows decreasing utility moving right, from Identity, to Direction, to Loudness.", "The last chart, labelled filtering options, shows decreasing utility moving from Identity to Direction to Loudness."], "caption": "", "local_uri": ["fdc69738f68745f1b11b269d30f9482fb428480b_Image_008.jpg", "fdc69738f68745f1b11b269d30f9482fb428480b_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "PeopleLens", "pdf_hash": "6b66f0f16820f78fb938194b420375fde5638e1a", "year": 2021, "venue": "Interactions", "alt_text": "TH, a young boy in school uniform, is pictured in a classroom. He is wearing the PeopleLens head-mounted device and looking to the right, across the classroom. An overlay with three illustrations has been added to the photo. One illustration is of a walking, female line-figure and the figure's face is outlined a line-drawn frame (suggesting face recognition). A second illustration is of a male standing figure looking towards the PeopleLens camera Theo is wearing. Lines are drawn to emphasise this figure is looking towards the PeopleLens camera (suggesting gaze recognition). The third and final illustration is of a sitting female figure, reading a book. Lines are drawn through the figure emphasising her seated position (suggesting posture recognition).", "levels": null, "corpus_id": 233412262, "sentences": ["TH, a young boy in school uniform, is pictured in a classroom.", "He is wearing the PeopleLens head-mounted device and looking to the right, across the classroom.", "An overlay with three illustrations has been added to the photo.", "One illustration is of a walking, female line-figure and the figure's face is outlined a line-drawn frame (suggesting face recognition).", "A second illustration is of a male standing figure looking towards the PeopleLens camera Theo is wearing.", "Lines are drawn to emphasise this figure is looking towards the PeopleLens camera (suggesting gaze recognition).", "The third and final illustration is of a sitting female figure, reading a book.", "Lines are drawn through the figure emphasising her seated position (suggesting posture recognition)."], "caption": "PeopleLens", "local_uri": ["6b66f0f16820f78fb938194b420375fde5638e1a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics", "pdf_hash": "d1364b5bcc09ea2acc130f1258ac7192903b270a", "year": 2021, "venue": "CHI", "alt_text": "Figure of Pose-on-the-Go running on a commodity unmodified smartphone. On the left is a human walking, and on the right is the avatar of the human mimicking the pose of the person walking.", "levels": null, "corpus_id": 233987621, "sentences": ["Figure of Pose-on-the-Go running on a commodity unmodified smartphone.", "On the left is a human walking, and on the right is the avatar of the human mimicking the pose of the person walking."], "caption": "Figure 1: Running on an unmodifed smartphone held in the hand (left), Pose-on-the-Go produces an animated, full-body pose estimation (right) through sensor fusion.", "local_uri": ["d1364b5bcc09ea2acc130f1258ac7192903b270a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics", "pdf_hash": "d1364b5bcc09ea2acc130f1258ac7192903b270a", "year": 2021, "venue": "CHI", "alt_text": "Figure of person looking to the left with Pose-on-the-Go following their head and eye pose and creating an avatar following it", "levels": null, "corpus_id": 233987621, "sentences": ["Figure of person looking to the left with Pose-on-the-Go following their head and eye pose and creating an avatar following it"], "caption": "Figure 3: Head rotations are also refected by the avatar in- dependently from the chest normal.", "local_uri": ["d1364b5bcc09ea2acc130f1258ac7192903b270a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics", "pdf_hash": "d1364b5bcc09ea2acc130f1258ac7192903b270a", "year": 2021, "venue": "CHI", "alt_text": "Figure showcasing different orientations of the hand and the avatar rendered by Pose-on-the-Go following it.", "levels": null, "corpus_id": 233987621, "sentences": ["Figure showcasing different orientations of the hand and the avatar rendered by Pose-on-the-Go following it."], "caption": "", "local_uri": ["d1364b5bcc09ea2acc130f1258ac7192903b270a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics", "pdf_hash": "d1364b5bcc09ea2acc130f1258ac7192903b270a", "year": 2021, "venue": "CHI", "alt_text": "Figure showcasing a person holding the phone at different arm angles and the avatar following him via Pose-on-the-Go.", "levels": null, "corpus_id": 233987621, "sentences": ["Figure showcasing a person holding the phone at different arm angles and the avatar following him via Pose-on-the-Go."], "caption": "", "local_uri": ["d1364b5bcc09ea2acc130f1258ac7192903b270a_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics", "pdf_hash": "d1364b5bcc09ea2acc130f1258ac7192903b270a", "year": 2021, "venue": "CHI", "alt_text": "Pose-on-the-Go tracking finger position on the screen, which is reflected on the virtual phone.", "levels": null, "corpus_id": 233987621, "sentences": ["Pose-on-the-Go tracking finger position on the screen, which is reflected on the virtual phone."], "caption": "", "local_uri": ["d1364b5bcc09ea2acc130f1258ac7192903b270a_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics", "pdf_hash": "d1364b5bcc09ea2acc130f1258ac7192903b270a", "year": 2021, "venue": "CHI", "alt_text": "Figure of a person standing still, walking, sitting and standing on a block respectively, with the Pose-on-the-Go avatar reflecting their pose.", "levels": null, "corpus_id": 233987621, "sentences": ["Figure of a person standing still, walking, sitting and standing on a block respectively, with the Pose-on-the-Go avatar reflecting their pose."], "caption": "Figure 8: A user standing (far left), walking, (center left), sit- ting (center right) and standing on a box (far right) is mir- rored by the avatar.", "local_uri": ["d1364b5bcc09ea2acc130f1258ac7192903b270a_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement", "pdf_hash": "9e45bde76861fdd6fdee3443a9517ff323348410", "year": 2018, "venue": "UIST", "alt_text": "Figure 1: Missing and misleading labels are a common and important accessibility issue that can be addressed by new approaches to robust annotation for accessibility repair.", "levels": null, "corpus_id": 52981960, "sentences": ["Figure 1: Missing and misleading labels are a common and important accessibility issue that can be addressed by new approaches to robust annotation for accessibility repair."], "caption": "This interface includes 6 elements with missing or misleading labels for use by a screen reader.", "local_uri": ["9e45bde76861fdd6fdee3443a9517ff323348410_Image_001.png", "9e45bde76861fdd6fdee3443a9517ff323348410_Image_002.png", "9e45bde76861fdd6fdee3443a9517ff323348410_Image_003.png", "9e45bde76861fdd6fdee3443a9517ff323348410_Image_004.png"], "annotated": false, "compound": true}
{"title": "Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement", "pdf_hash": "9e45bde76861fdd6fdee3443a9517ff323348410", "year": 2018, "venue": "UIST", "alt_text": "Figure 2: We develop and evaluate new methods for robust annotation of mobile app interface elements appropriate for runtime accessibility repair, together with end-to-end tool support for developers implementing accessibility repairs.", "levels": null, "corpus_id": 52981960, "sentences": ["Figure 2: We develop and evaluate new methods for robust annotation of mobile app interface elements appropriate for runtime accessibility repair, together with end-to-end tool support for developers implementing accessibility repairs."], "caption": "Figure 2: We develop and evaluate new methods for robust annotation of mobile app interface elements appropriate for runtime accessibility repair, together with end-to-end tool support for developers implementing accessibility repairs.", "local_uri": ["9e45bde76861fdd6fdee3443a9517ff323348410_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Legion scribe: real-time captioning by non-experts", "pdf_hash": "3dd330b002ae1cdc7f82db64d01a0e01e377ec25", "year": 2014, "venue": "ASSETS", "alt_text": "Scribe captionist interface. Text is locked in as it is typed into a box. Below is a visual indicator that the worker should or should not type what they hear now. Points are animated and displayed to workers as words are validated by the system.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 16143256, "sentences": ["Scribe captionist interface.", "Text is locked in as it is typed into a box.", "Below is a visual indicator that the worker should or should not type what they hear now.", "Points are animated and displayed to workers as words are validated by the system."], "caption": "Figure 1: The worker interface encourages workers to type audio by locking in words soon after they are typed. To encourage typing specific segments, visual and audio cues are given, and the volume of the audio is reduced during off periods, while rewards are increased for on periods.", "local_uri": ["3dd330b002ae1cdc7f82db64d01a0e01e377ec25_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Demonstration of GestureCalc: An Eyes-Free Calculator for Touch Screens", "pdf_hash": "b0076a975f6051bf4b9c19ebf6d6cdebc0defd1d", "year": 2019, "venue": "ASSETS", "alt_text": "A visual representation of gesture code descriptions of digits from the \"Digits\" subsection. A table showing digits 0-9 in the first row and the visual representation of each digit's gesture code in the second row.", "levels": null, "corpus_id": 198908541, "sentences": ["A visual representation of gesture code descriptions of digits from the \"Digits\" subsection.", "A table showing digits 0-9 in the first row and the visual representation of each digit's gesture code in the second row."], "caption": "", "local_uri": ["b0076a975f6051bf4b9c19ebf6d6cdebc0defd1d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of GestureCalc: An Eyes-Free Calculator for Touch Screens", "pdf_hash": "b0076a975f6051bf4b9c19ebf6d6cdebc0defd1d", "year": 2019, "venue": "ASSETS", "alt_text": "A table of gestures. Columns include directional swipes (up, down, left, and right), tap, and long tap. Rows include 1-finger, 2-finger, and 3-finger. Each cell has a geometric visual representation of that gesture. All the gestures which are used in GestureCalc are in black, except 1-finger right swipe, 3-finger left and right swipes, and 2-finger and 3-finger long taps which are in gray because they are not used.", "levels": null, "corpus_id": 198908541, "sentences": ["A table of gestures.", "Columns include directional swipes (up, down, left, and right), tap, and long tap.", "Rows include 1-finger, 2-finger, and 3-finger.", "Each cell has a geometric visual representation of that gesture.", "All the gestures which are used in GestureCalc are in black, except 1-finger right swipe, 3-finger left and right swipes, and 2-finger and 3-finger long taps which are in gray because they are not used."], "caption": "Figure 2. Symbolic representations of a superset of our gestures, with a visual shortcut for each gesture. Gestures used in GestureCalc are marked in black, while currently unused gestures are marked in grey.", "local_uri": ["b0076a975f6051bf4b9c19ebf6d6cdebc0defd1d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of GestureCalc: An Eyes-Free Calculator for Touch Screens", "pdf_hash": "b0076a975f6051bf4b9c19ebf6d6cdebc0defd1d", "year": 2019, "venue": "ASSETS", "alt_text": "A visual representation of gesture code descriptions of operations from the \"Operators\" subsection. A table showing +, -, *, /, ., =, D, C in the first row and the visual representation of each operation's gesture code in the second row.", "levels": null, "corpus_id": 198908541, "sentences": ["A visual representation of gesture code descriptions of operations from the \"Operators\" subsection.", "A table showing +, -, *, /, ., =, D, C in the first row and the visual representation of each operation's gesture code in the second row."], "caption": "Codes for entering operations.", "local_uri": ["b0076a975f6051bf4b9c19ebf6d6cdebc0defd1d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Scopist: Building a Skill Ladder into Crowd Transcription", "pdf_hash": "6b872e274d2f771ac8910e11251403b41f066dc1", "year": 2017, "venue": "W4A", "alt_text": "That is, given a dictionary, g(\u2219), we add 1 for every word, wi, found in the dictionary, and 0 otherwise. The total count is then divided by the total number of words.", "levels": [[-1], [-1]], "corpus_id": 32190160, "sentences": ["That is, given a dictionary, g(\u2219), we add 1 for every word, wi, found in the dictionary, and 0 otherwise.", "The total count is then divided by the total number of words."], "caption": "The scoring system distinguishes between stenotype and qwerty by exploiting the fact that the chorded qwerty keys are not often English words; either by themselves, or when combined with the subsequently pressed keys. The algorithm uses H(hi) to distinguish these cases because the boundary between chords and qwerty keys is not marked in the keyset since the spacebar is not used in stenotype to separate words.", "local_uri": ["6b872e274d2f771ac8910e11251403b41f066dc1_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Scopist: Building a Skill Ladder into Crowd Transcription", "pdf_hash": "6b872e274d2f771ac8910e11251403b41f066dc1", "year": 2017, "venue": "W4A", "alt_text": "Please type the following in the box below. Animated example. Please type the following according to the rules above.", "levels": null, "corpus_id": 32190160, "sentences": ["Please type the following in the box below.", "Animated example.", "Please type the following according to the rules above."], "caption": "Figure 1. The webpage shown above collected keysets from crowdworkers containing a mixture of touch-typing and stenotype. The example phrase \u2018so we need energy miracles now drni use op\u2019 shows how we substituted in the qwerty keys corresponding to the needed stenotype chords to collect the test keysets from crowdworkers. In this example, a worker would need to chord the key combinations d-r-n and o-p.", "local_uri": ["6b872e274d2f771ac8910e11251403b41f066dc1_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Scopist: Building a Skill Ladder into Crowd Transcription", "pdf_hash": "6b872e274d2f771ac8910e11251403b41f066dc1", "year": 2017, "venue": "W4A", "alt_text": "Interface contains a visualiation of the keyboard with both QWERTY and steno keys on the left, and a transcription box on the right. The keyboard visualization shows the keys necessary to trigger the chord for the word that is being taught, which is \"something\"", "levels": null, "corpus_id": 32190160, "sentences": ["Interface contains a visualiation of the keyboard with both QWERTY and steno keys on the left, and a transcription box on the right.", "The keyboard visualization shows the keys necessary to trigger the chord for the word that is being taught, which is \"something\""], "caption": "Figure 3. The user interface for Scopist highlights stenotype keys on a typical qwerty layout to teach a crowdworker the appropriate hand posture for chording. The highlighted keys are for the prompted word underlined at the top of the page, and a reminder of the link between the chord keys and the sounds of the prompted word are provided in parentheses.", "local_uri": ["6b872e274d2f771ac8910e11251403b41f066dc1_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "A Feasibility Study of Using Google Street View and Computer Vision to Track the Evolution of Urban Accessibility", "pdf_hash": "e50f0bd082617ac9f85498cb995f6c9a7aa11e0d", "year": 2018, "venue": "ASSETS", "alt_text": "This image shows four historical Google Street View snapshots of a single location from 2007 to 2014 along with example results from our computer vision algorithm tracking the location of a physical obstacle (in this case, a light pole).", "levels": [[-1]], "corpus_id": 52200370, "sentences": ["This image shows four historical Google Street View snapshots of a single location from 2007 to 2014 along with example results from our computer vision algorithm tracking the location of a physical obstacle (in this case, a light pole)."], "caption": "Figure 1. In this paper, we examine the feasibility of using Google Street View\u2019s \u201ctime machine\u201d feature [4] and basic computer vision algorithms to track changes in urban accessibility over time. For each location, accessibility problems are manually labeled in the most recent Street View image (blue outline) then are automatically back propagated through time (red outlines) to track and discover potential changes. In the example here, an object in the pedestrian path has persisted over time to the most recent data (2014), while a sidewalk surface problem from 2007 was resolved by 2009.", "local_uri": ["e50f0bd082617ac9f85498cb995f6c9a7aa11e0d_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Feasibility Study of Using Google Street View and Computer Vision to Track the Evolution of Urban Accessibility", "pdf_hash": "e50f0bd082617ac9f85498cb995f6c9a7aa11e0d", "year": 2018, "venue": "ASSETS", "alt_text": "This figure shows example image patches from Stage 1 of our framework, including for missing curb ramps, surface problems, objects in path, accessible curb ramps, and accessible sidewalks.", "levels": null, "corpus_id": 52200370, "sentences": ["This figure shows example image patches from Stage 1 of our framework, including for missing curb ramps, surface problems, objects in path, accessible curb ramps, and accessible sidewalks."], "caption": "Figure 2. Example image patches extracted in Stage 1.", "local_uri": ["e50f0bd082617ac9f85498cb995f6c9a7aa11e0d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Overview of the ASSETS 2017 conference", "pdf_hash": "2ad2f47a60f86f83c542007fcda396195b8e0560", "year": 2018, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "Attendees wearing halloween costumes during poster session (photo by Luz Rello @luzrello)", "levels": null, "corpus_id": 24194910, "sentences": ["Attendees wearing halloween costumes during poster session (photo by Luz Rello @luzrello)"], "caption": "", "local_uri": ["2ad2f47a60f86f83c542007fcda396195b8e0560_Image_002.png"], "annotated": false, "compound": false}
{"title": "Overview of the ASSETS 2017 conference", "pdf_hash": "2ad2f47a60f86f83c542007fcda396195b8e0560", "year": 2018, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "Screen shot of a 3D computer aided design application, Blender, with an inset image of the cylindical Ollie robot held in the user's hand. The on-screen 3D cube model matches the orientation of the handheld robot.", "levels": [[-1], [-1]], "corpus_id": 24194910, "sentences": ["Screen shot of a 3D computer aided design application, Blender, with an inset image of the cylindical Ollie robot held in the user's hand.", "The on-screen 3D cube model matches the orientation of the handheld robot."], "caption": "In this article, I also presented the GUI Robots Software Toolkit which aimed to reduce the expertise and knowledge barriers to entry for tangible interface construction on the desktop. This toolkit was aimed to support early JavaScript developers, but will be extended to support a broader range of end-users.", "local_uri": ["2ad2f47a60f86f83c542007fcda396195b8e0560_Image_034.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Overview of the ASSETS 2017 conference", "pdf_hash": "2ad2f47a60f86f83c542007fcda396195b8e0560", "year": 2018, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "An alternative input technique to control Google Glass. This image shows the three sizes of wearable touchpads.", "levels": null, "corpus_id": 24194910, "sentences": ["An alternative input technique to control Google Glass.", "This image shows the three sizes of wearable touchpads."], "caption": "", "local_uri": ["2ad2f47a60f86f83c542007fcda396195b8e0560_Image_037.jpg"], "annotated": false, "compound": false}
{"title": "Overview of the ASSETS 2017 conference", "pdf_hash": "2ad2f47a60f86f83c542007fcda396195b8e0560", "year": 2018, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "A participant using the side of his fingers to tap on the touchpads he places on his wheelchair.", "levels": null, "corpus_id": 24194910, "sentences": ["A participant using the side of his fingers to tap on the touchpads he places on his wheelchair."], "caption": "Figure 1: We built an alternative input technique using wearable touchpads to control Google Glass: a", "local_uri": ["2ad2f47a60f86f83c542007fcda396195b8e0560_Image_038.jpg"], "annotated": false, "compound": false}
{"title": "Overview of the ASSETS 2017 conference", "pdf_hash": "2ad2f47a60f86f83c542007fcda396195b8e0560", "year": 2018, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "A blind student is exploring a 3D printed globe in front of a tablet. The tablet captures her hand gesture using its camera and speaks the continent she is touching.", "levels": null, "corpus_id": 24194910, "sentences": ["A blind student is exploring a 3D printed globe in front of a tablet.", "The tablet captures her hand gesture using its camera and speaks the continent she is touching."], "caption": "Figure 1. A blind student is exploring a 3D printed globe in front of a tablet. The tablet captures her hand gesture using its camera and speaks the continent she is touching.", "local_uri": ["2ad2f47a60f86f83c542007fcda396195b8e0560_Image_041.jpg"], "annotated": false, "compound": false}
{"title": "Overview of the ASSETS 2017 conference", "pdf_hash": "2ad2f47a60f86f83c542007fcda396195b8e0560", "year": 2018, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "Two images. The left image (a) shows the graphic user interface of Markit, where a user can select elements and enter annotations in textboxes. The right image (b) shows how a user interacts with a globe model using his finger. Talkit locates the position of the model and the finger. In the image, Talkit highlights the elements in the model and the identified finger for demonstration purposes.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 24194910, "sentences": ["Two images.", "The left image (a) shows the graphic user interface of Markit, where a user can select elements and enter annotations in textboxes.", "The right image (b) shows how a user interacts with a globe model using his finger.", "Talkit locates the position of the model and the finger.", "In the image, Talkit highlights the elements in the model and the identified finger for demonstration purposes."], "caption": "Figure 2. A maker creates an interactive model in Markit (a) and a blind user interacts with the model using Talkit (b).", "local_uri": ["2ad2f47a60f86f83c542007fcda396195b8e0560_Image_042.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EMPiano: Electromyographic Pitch Control on the Piano Keyboard", "pdf_hash": "a84e1a6db68133438c2b7d98a3c102f1bbc79ce9", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "EMPiano offers a seamless integration for piano playing to add a soft pitch vibrato to played notes. Electrodes (purple)  capture muscular activity allowing the system to recognize corresponding activation gestures.", "levels": null, "corpus_id": 233987729, "sentences": ["EMPiano offers a seamless integration for piano playing to add a soft pitch vibrato to played notes.", "Electrodes (purple)  capture muscular activity allowing the system to recognize corresponding activation gestures."], "caption": "", "local_uri": ["a84e1a6db68133438c2b7d98a3c102f1bbc79ce9_Image_001.png"], "annotated": false, "compound": false}
{"title": "EMPiano: Electromyographic Pitch Control on the Piano Keyboard", "pdf_hash": "a84e1a6db68133438c2b7d98a3c102f1bbc79ce9", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": ": Video calibration functionality of EMPiano. The  blue marker provides a playback orientation, while red  notes signal for the pianist when to use the activation gesture.", "levels": null, "corpus_id": 233987729, "sentences": [": Video calibration functionality of EMPiano.", "The  blue marker provides a playback orientation, while red  notes signal for the pianist when to use the activation gesture."], "caption": "Figure 4: Video calibration functionality of EMPiano. The blue marker provides a playback orientation, while red notes signal for the pianist when to use the activation ges- ture.", "local_uri": ["a84e1a6db68133438c2b7d98a3c102f1bbc79ce9_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Venous Materials: Towards Interactive Fluidic Mechanisms", "pdf_hash": "16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: Venous Materials: a novel interactive material with fluidic structure that responds to tangible interaction.", "levels": null, "corpus_id": 218483505, "sentences": ["Figure 1: Venous Materials: a novel interactive material with fluidic structure that responds to tangible interaction."], "caption": "Figure 1. Venous Materials: a novel interactive material with \ufb02uidic structure that responds to tangible interaction.", "local_uri": ["16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6_Image_001.png"], "annotated": false, "compound": false}
{"title": "Venous Materials: Towards Interactive Fluidic Mechanisms", "pdf_hash": "16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: Sample Geometric Patterns (FRACTAL, BRANCHES, VEINS, MESH and FREE LINE).", "levels": null, "corpus_id": 218483505, "sentences": ["Figure 3: Sample Geometric Patterns (FRACTAL, BRANCHES, VEINS, MESH and FREE LINE)."], "caption": "Figure 3. Sample Geometric Patterns (FRACTAL, BRANCHES, VEINS, MESH and FREE LINE).", "local_uri": ["16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6_Image_003.png"], "annotated": false, "compound": false}
{"title": "Venous Materials: Towards Interactive Fluidic Mechanisms", "pdf_hash": "16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: Two types of irreversible samples with a: MESH (Once the flow pass through the mesh, some of the fluid remains trapped in the mesh structure in three levels.), b: Color-Mixing (the mixed color is irreversible).", "levels": null, "corpus_id": 218483505, "sentences": ["Figure 4: Two types of irreversible samples with a: MESH (Once the flow pass through the mesh, some of the fluid remains trapped in the mesh structure in three levels.), b: Color-Mixing (the mixed color is irreversible)."], "caption": "Figure 5. Multi-Color Primitives. a: Gradation, and b: Overlay", "local_uri": ["16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6_Image_005.png"], "annotated": false, "compound": false}
{"title": "Venous Materials: Towards Interactive Fluidic Mechanisms", "pdf_hash": "16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6", "year": 2020, "venue": "CHI", "alt_text": "Figure 8: Simulation view and actual prototype of double layer linear prototype of Venous Materials. Fabricated prototype shows functionality of displaying two colors depending on the deformation.", "levels": null, "corpus_id": 218483505, "sentences": ["Figure 8: Simulation view and actual prototype of double layer linear prototype of Venous Materials.", "Fabricated prototype shows functionality of displaying two colors depending on the deformation."], "caption": "Figure 8. Simulation view and actual prototype of double layer linear prototype of Venous Materials. Fabricated prototype shows functionality of displaying two colors depending on the deformation.", "local_uri": ["16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6_Image_008.png"], "annotated": false, "compound": false}
{"title": "Venous Materials: Towards Interactive Fluidic Mechanisms", "pdf_hash": "16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6", "year": 2020, "venue": "CHI", "alt_text": "Figure 9: Design and simulation workflow example for the prototype on Figure 8.", "levels": null, "corpus_id": 218483505, "sentences": ["Figure 9: Design and simulation workflow example for the prototype on Figure 8."], "caption": "Figure 9. Design and simulation work\ufb02ow of the prototype on Figure 8.", "local_uri": ["16017fa12a9cc4aa14ebb67c519ed91f0d88e4d6_Image_009.png"], "annotated": false, "compound": false}
{"title": "AACrobat: Using Mobile Devices to Lower Communication Barriers and Provide Autonomy with Gaze-Based AAC", "pdf_hash": "0397dd030975410ee3c14e0b84f882abb9984008", "year": 2017, "venue": "CSCW", "alt_text": "The left hand side of the image displays 3 messages. The first is a \"communication preference\" message reading, \"Yes or no questions are easiest for me to answer quickly.\" The second message is a pre-composed block message reading, \"I have so much to tell you. I had a great visit with the doctor today.\" The third message is a multimedia message showing a picture of a beach. The center of the figure shows the full view of the AACrobat mobile app, with the real time text at the top and asynchronous messages at the bottom. The right side fo the figure shows the real time view over three time steps with additional characters appearing at each time step as the user is typing, \"Hi there.\"", "levels": null, "corpus_id": 14200587, "sentences": ["The left hand side of the image displays 3 messages.", "The first is a \"communication preference\" message reading, \"Yes or no questions are easiest for me to answer quickly.\" The second message is a pre-composed block message reading, \"I have so much to tell you. I had a great visit with the doctor today.\" The third message is a multimedia message showing a picture of a beach.", "The center of the figure shows the full view of the AACrobat mobile app, with the real time text at the top and asynchronous messages at the bottom.", "The right side fo the figure shows the real time view over three time steps with additional characters appearing at each time step as the user is typing, \"Hi there.\""], "caption": "", "local_uri": ["0397dd030975410ee3c14e0b84f882abb9984008_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "AACrobat: Using Mobile Devices to Lower Communication Barriers and Provide Autonomy with Gaze-Based AAC", "pdf_hash": "0397dd030975410ee3c14e0b84f882abb9984008", "year": 2017, "venue": "CSCW", "alt_text": "The left side of the image shows the AACrobat mobile app and is focused on the co-construction text input box. There is a \"send\" button to the right of the input box. The right side of the figure displays the co-constructed suggestion as the top suggestion in the eye-gaze keyboard after the partner hit the \"send\" button.", "levels": [[-1], [-1], [-1]], "corpus_id": 14200587, "sentences": ["The left side of the image shows the AACrobat mobile app and is focused on the co-construction text input box.", "There is a \"send\" button to the right of the input box.", "The right side of the figure displays the co-constructed suggestion as the top suggestion in the eye-gaze keyboard after the partner hit the \"send\" button."], "caption": "Figure 4. Example of co-construction functionality in the companion app inserting a suggestion into the prediction bar of the AAC application. When the communication partner sends a word or phrase suggestion, it appears in the AAC application\u2019s prediction bar among the system-generated predictions.", "local_uri": ["0397dd030975410ee3c14e0b84f882abb9984008_Image_011.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "AACrobat: Using Mobile Devices to Lower Communication Barriers and Provide Autonomy with Gaze-Based AAC", "pdf_hash": "0397dd030975410ee3c14e0b84f882abb9984008", "year": 2017, "venue": "CSCW", "alt_text": "The figure shows 7 different statuses. The first three statuses show that the keyboard app is not running, that the eye gaze sensor is being calibrated, and that there is no communication data to display. The next four statuses show that the user is typing, idle, speaking, and calibrating. Only one of these status messages is visible at any given time.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 14200587, "sentences": ["The figure shows 7 different statuses.", "The first three statuses show that the keyboard app is not running, that the eye gaze sensor is being calibrated, and that there is no communication data to display.", "The next four statuses show that the user is typing, idle, speaking, and calibrating.", "Only one of these status messages is visible at any given time."], "caption": "Figure 5. Indicators for device state (left) and AAC user state (right).", "local_uri": ["0397dd030975410ee3c14e0b84f882abb9984008_Image_012.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PERCs: Persistently Trackable Tangibles on Capacitive Multi-Touch Displays", "pdf_hash": "2fafd8b528727f4966cff89a1d5aaa0ec37231b7", "year": 2015, "venue": "UIST", "alt_text": "\"A table showing the average results of our experiment for each of the tested displays. On the MS table the average detection time was 50ms without the light sensor and 190ms with the light sensor. The position error was 1.5mm and the angle error was -0.78 degree. In 2.2% of all trials on the MS table only one marker was detected. On the PPI table, the average detection time was 65ms without the light sensor and 176ms with the light sensor. The position error was 2.1mm and the angle error was -1.84 degree. In 2.5% of all the trials on the PPI table only one marker was detected. On the iPad the average detection time was 55 without the light sensor and 167ms with the light sensor. The position error was 2.5mm and the angle error was -1.94 degree. In 3.5% of all the trials on the iPad only one marker was detected.\"", "levels": [[1], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 6138813, "sentences": ["\"A table showing the average results of our experiment for each of the tested displays.", "On the MS table the average detection time was 50ms without the light sensor and 190ms with the light sensor.", "The position error was 1.5mm and the angle error was -0.78 degree.", "In 2.2% of all trials on the MS table only one marker was detected.", "On the PPI table, the average detection time was 65ms without the light sensor and 176ms with the light sensor.", "The position error was 2.1mm and the angle error was -1.84 degree.", "In 2.5% of all the trials on the PPI table only one marker was detected.", "On the iPad the average detection time was 55 without the light sensor and 167ms with the light sensor.", "The position error was 2.5mm and the angle error was -1.94 degree.", "In 3.5% of all the trials on the iPad only one marker was detected.\""], "caption": "Number of Touches", "local_uri": ["2fafd8b528727f4966cff89a1d5aaa0ec37231b7_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Circumspect Users: Older Adults as Critical Adopters and Resistors of Technology", "pdf_hash": "3835f0f3052349d99baadbe7f4c460927a27fb81", "year": 2021, "venue": "CHI", "alt_text": "A photograph showing the printed image prompts used during the interviews scattered over the surface of a table.", "levels": [[-1]], "corpus_id": 233987501, "sentences": ["A photograph showing the printed image prompts used during the interviews scattered over the surface of a table."], "caption": "Figure 1: The printed image prompts used during the inter- views.", "local_uri": ["3835f0f3052349d99baadbe7f4c460927a27fb81_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Human-Centered Approach Evaluating Mobile Sign Language Video Communication", "pdf_hash": "c49e803279b709663c08c3acb491bd0183b9254b", "year": 2013, "venue": "", "alt_text": "Block diagram starting from the lower left, image of a person's head with a label 'sender's mind', arrow point up from that picture points to next box labeled 'articulation.' Inside the articulation box is a hand and a picture of an open mouth to indicate sound coming out. Pointing up from the articulation box is an arrow to a picture of a cell phone with the label 'information source.' The arrow pointing from the articulation box to the information source box goes through an oval labeled 'environment.' From the information source box, these is an arrow pointing right to a box labeled 'transmitter (encoder).' This arrow has a lavel called x sub n with a label 'message.' From the transmitter (encoder) box an arrow points from left to right labeled u sub n. There is a box called 'noise source' which points up to the signal u sub n. Output from these two signals is labeled u sub n hat. Ths u sub n hat labels a right pointing arrow which points to the 'receiver (decoder)'  square block. There is an arrow that points from the receiver to the destination box with a picture of a phons is labeled  x sub n hat. There is a down arrow that points from the destination block to a block called 'perception.' The perception block has a picture of an eye and an ear. The arrow pointing from the destination to the perception block goes through an oval labeled 'environment'. There are two 'environment' labels which are symmetrically placed in the diagram. Finally there is a downward pointing arrow from the perception box to the the 'receiver's mind' box with a picture of a head. There is a cloud in-between the sender's mind and receiver's mind blocks that is labeled 'knowledge store.' There are two-dashed arrows pointing outward to the sender's mind and receiver's mind blocks. The dashed lines have question marks underneath. The sender's mind and articulation block diagrams are grouped together and labeled 'sender'. The perception and receiver's mind blocks are grouped together and labeled receiver. There is red dashed line labeled 'signal intelligiblity' which includes all the blocks mentioned EXCEPT the sender's mind, knowledge store, and the receiver's mind. There is a purple dashed line labeled 'signal comprehension' which includes all the blocks circled by signal intelligibility and the receiver's mind block.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 14444227, "sentences": ["Block diagram starting from the lower left, image of a person's head with a label 'sender's mind', arrow point up from that picture points to next box labeled 'articulation.'", "Inside the articulation box is a hand and a picture of an open mouth to indicate sound coming out.", "Pointing up from the articulation box is an arrow to a picture of a cell phone with the label 'information source.'", "The arrow pointing from the articulation box to the information source box goes through an oval labeled 'environment.'", "From the information source box, these is an arrow pointing right to a box labeled 'transmitter (encoder).'", "This arrow has a lavel called x sub n with a label 'message.'", "From the transmitter (encoder) box an arrow points from left to right labeled u sub n. There is a box called 'noise source' which points up to the signal u sub n. Output from these two signals is labeled u sub n hat.", "Ths u sub n hat labels a right pointing arrow which points to the 'receiver (decoder)'  square block.", "There is an arrow that points from the receiver to the destination box with a picture of a phons is labeled  x sub n hat.", "There is a down arrow that points from the destination block to a block called 'perception.'", "The perception block has a picture of an eye and an ear.", "The arrow pointing from the destination to the perception block goes through an oval labeled 'environment'.", "There are two 'environment' labels which are symmetrically placed in the diagram.", "Finally there is a downward pointing arrow from the perception box to the the 'receiver's mind' box with a picture of a head.", "There is a cloud in-between the sender's mind and receiver's mind blocks that is labeled 'knowledge store.'", "There are two-dashed arrows pointing outward to the sender's mind and receiver's mind blocks.", "The dashed lines have question marks underneath.", "The sender's mind and articulation block diagrams are grouped together and labeled 'sender'.", "The perception and receiver's mind blocks are grouped together and labeled receiver.", "There is red dashed line labeled 'signal intelligiblity' which includes all the blocks mentioned EXCEPT the sender's mind, knowledge store, and the receiver's mind.", "There is a purple dashed line labeled 'signal comprehension' which includes all the blocks circled by signal intelligibility and the receiver's mind block."], "caption": "Figure 2: Block diagram of the Human Signal Intelligibility Model. Note that the components comprising signal intelligibility are a subset of signal comprehension, which is signal intelligibility plus the receiver\u2019s mind.", "local_uri": ["c49e803279b709663c08c3acb491bd0183b9254b_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Human-Centered Approach Evaluating Mobile Sign Language Video Communication", "pdf_hash": "c49e803279b709663c08c3acb491bd0183b9254b", "year": 2013, "venue": "", "alt_text": "This is a picture of the vertical Likert scale used in the survey. The question asks \u201chow easy was the video to understand?\u201d Underneath the question is 7 radio buttons, where 1-very easy, 2-easy, 3-somewhat easy, 4-neutral, 5- somewhat difficult, 6-difficult, 7-very difficult.", "levels": null, "corpus_id": 14444227, "sentences": ["This is a picture of the vertical Likert scale used in the survey.", "The question asks \u201chow easy was the video to understand?\u201d Underneath the question is 7 radio buttons, where 1-very easy, 2-easy, 3-somewhat easy, 4-neutral, 5- somewhat difficult, 6-difficult, 7-very difficult."], "caption": "Figure 4: Example of question 1 shown in web survey.", "local_uri": ["c49e803279b709663c08c3acb491bd0183b9254b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Human-Centered Approach Evaluating Mobile Sign Language Video Communication", "pdf_hash": "c49e803279b709663c08c3acb491bd0183b9254b", "year": 2013, "venue": "", "alt_text": "This is an example of the comprehension question asked about the video content shown. This example comprehension questions asks \u201cHow does Stephanie get to school?\u201d Below the question are four multiple choice answers displayed horizontally and each answer is accompanied by a picture. In this example, from left to right is a picture of a green car, yellow bus, a person walking, and a bicycle.", "levels": null, "corpus_id": 14444227, "sentences": ["This is an example of the comprehension question asked about the video content shown.", "This example comprehension questions asks \u201cHow does Stephanie get to school?\u201d Below the question are four multiple choice answers displayed horizontally and each answer is accompanied by a picture.", "In this example, from left to right is a picture of a green car, yellow bus, a person walking, and a bicycle."], "caption": "Figure 5: Multiple choice comprehension question example.", "local_uri": ["c49e803279b709663c08c3acb491bd0183b9254b_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "(a) Minecraft-like block world with two blocks created in the foreground; (b) Third-person perspective of the block world, with a cursor being used to select the location to stamp a tree shape in the world; (c) Third-person perspective of the block world, with the user manipulating the geometry of a 3D pyramid solid, as they would in TInkercad.", "levels": null, "corpus_id": 51995155, "sentences": ["(a) Minecraft-like block world with two blocks created in the foreground; (b) Third-person perspective of the block world, with a cursor being used to select the location to stamp a tree shape in the world; (c) Third-person perspective of the block world, with the user manipulating the geometry of a 3D pyramid solid, as they would in TInkercad."], "caption": "Figure 1. A cross-application bridge from Minecraft-style games to Tinkercad-style 3D solid modeling. (a) The player starts out in a Minecraft-like voxel world; (b) over time, tools are introduced which alter the interaction model and introduce 3D-modeling concepts; (c) eventually, the player transitions to Tinkercad-style 3D solid modeling.", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_001.png"], "annotated": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "(a) When the user switches to the new application, there is a dip in performance as they re-learn basic skills; (b) Instead of one major dip, a series of minor dips as each new feature is introduced.", "levels": null, "corpus_id": 51995155, "sentences": ["(a) When the user switches to the new application, there is a dip in performance as they re-learn basic skills; (b) Instead of one major dip, a series of minor dips as each new feature is introduced."], "caption": "Figure 2. Two models for switching between a known and target application. (a) Switch outright, which imposes a cost in re-learning basic skills. (b) Transition from the known application, through a series of motivated transition tasks.", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_002.png"], "annotated": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "(a) Screenshot from the Minecraft game; (b) Screenshot of Tinkercad, showing three basic shapes.", "levels": null, "corpus_id": 51995155, "sentences": ["(a) Screenshot from the Minecraft game; (b) Screenshot of Tinkercad, showing three basic shapes."], "caption": "Figure 3. The user interfaces of (a) Minecraft, (b) Tinkercad.", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_003.png"], "annotated": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "(a) Block world with three blocks in front of the player; (b) Third-person view of the block world with a mouse cursor selecting where to stamp a tree shape; (c) Third-person view with 3D navigation widgets visible; (d) Third-person view with three Tinkercad-style 3D primitives (cube, pyramid, and sphere) in the block world; (e) A 3D primitive being manipulated with the mouse cursor, in the block world; (f) The user setting a new workplane location in the block world.", "levels": null, "corpus_id": 51995155, "sentences": ["(a) Block world with three blocks in front of the player; (b) Third-person view of the block world with a mouse cursor selecting where to stamp a tree shape; (c) Third-person view with 3D navigation widgets visible; (d) Third-person view with three Tinkercad-style 3D primitives (cube, pyramid, and sphere) in the block world; (e) A 3D primitive being manipulated with the mouse cursor, in the block world; (f) The user setting a new workplane location in the block world."], "caption": "Figure 4. The six stages of functionality in the Blocks-to-CAD system. (a) Minecraft-style block tools; (b) Tree-stamp tool;", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_004.png"], "annotated": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "(a) Message displayed in corner of game screen: \"New tool available! Press the 'X' key to unlock.\"; (b) Dialog displaying a video of how to use the newly-unlocked Shapes tool.", "levels": null, "corpus_id": 51995155, "sentences": ["(a) Message displayed in corner of game screen: \"New tool available! Press the 'X' key to unlock.\"; (b) Dialog displaying a video of how to use the newly-unlocked Shapes tool."], "caption": "Figure 6. When the threshold for unlocking a tool is met, a notification is displayed in the corner of the screen (a). When the user unlocks the tool, a modal dialog with a short video demonstrating the tool is played (b).", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_006.png"], "annotated": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "Timelines for each participant, showing that their use of the TInkercad mode increases over the study session.", "levels": [[3]], "corpus_id": 51995155, "sentences": ["Timelines for each participant, showing that their use of the TInkercad mode increases over the study session."], "caption": "Figure 7. Timelines for participants in the experimental condition, indicating use of Tinkercad mode (blue) and points at which tools were unlocked (red dots). Participants are sorted by total use of the Tinkercad mode.", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_008.gif"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "Bar chart showing subjective ratings of the unlockable features on usefulness, fun, and how annoying or disruptive they were perceived to be.", "levels": [[1]], "corpus_id": 51995155, "sentences": ["Bar chart showing subjective ratings of the unlockable features on usefulness, fun, and how annoying or disruptive they were perceived to be."], "caption": "Figure 8. Subjective ratings of the unlockable features.", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_009.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "Bar charts showing that the control and experimental conditions were rated about the same in both fun and cognitive load.", "levels": [[3]], "corpus_id": 51995155, "sentences": ["Bar charts showing that the control and experimental conditions were rated about the same in both fun and cognitive load."], "caption": "Figure 9. Subjective ratings of the overall game experience, and the cognitive load (based on 5 NASA-TLX questions)", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_010.gif"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "pdf_hash": "148da4b210ceaa6d9699fd242a97d0739e460e12", "year": 2018, "venue": "UIST", "alt_text": "Bar charts comparing the time to complete the three transfer tasks in the control and experimental conditions.", "levels": [[1]], "corpus_id": 51995155, "sentences": ["Bar charts comparing the time to complete the three transfer tasks in the control and experimental conditions."], "caption": "Figure 10. Timing for transfer tasks.", "local_uri": ["148da4b210ceaa6d9699fd242a97d0739e460e12_Image_011.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision", "pdf_hash": "b977426b9fc5899db0ead908c6b0bbf241004148", "year": 2020, "venue": "CHI", "alt_text": "This figure shows the visual wayfinding guidance. It includes four sub-images, showing the virtual yellow Path, the Path with the Floating Window, the Path with the Anchored Signs, and the Path with an Action Sign.", "levels": null, "corpus_id": 218482994, "sentences": ["This figure shows the visual wayfinding guidance.", "It includes four sub-images, showing the virtual yellow Path, the Path with the Floating Window, the Path with the Anchored Signs, and the Path with an Action Sign."], "caption": "Figure 1. The visual wayfinding guidance: (A) Path (B) Path with the Floating Window (C) Path with Anchored Signs, including some Distance Signs and an Action Sign (D) an Action Sign.", "local_uri": ["b977426b9fc5899db0ead908c6b0bbf241004148_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision", "pdf_hash": "b977426b9fc5899db0ead908c6b0bbf241004148", "year": 2020, "venue": "CHI", "alt_text": "This figure shows the design of the Destination Star. It includes three sub-images, including an overview of the Path with the Anchored Signs and the Destination Star, an image of a Destination Star on the left side of the Path, and an image showing the Destination Indicator.", "levels": null, "corpus_id": 218482994, "sentences": ["This figure shows the design of the Destination Star.", "It includes three sub-images, including an overview of the Path with the Anchored Signs and the Destination Star, an image of a Destination Star on the left side of the Path, and an image showing the Destination Indicator."], "caption": "Figure 2. (A) An overview with the Path, Anchored Signs, and Destination Star; (B) a Destination Star on the left side of the Path; (C) Destination Indicator.", "local_uri": ["b977426b9fc5899db0ead908c6b0bbf241004148_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Let's Talk About X: Combining Image Recognition and Eye Gaze to Support Conversation for People with ALS", "pdf_hash": "0a7a169382aab1b39d339474728afd67c784cb39", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of an eye gaze-based keyboard device. A QWERTY keyboard covers the bottom two thirds of the screen. Word predictions and a text edit box cover the top portion of the screen.", "levels": [[-1], [-1], [-1]], "corpus_id": 9546625, "sentences": ["Screen shot of an eye gaze-based keyboard device.", "A QWERTY keyboard covers the bottom two thirds of the screen.", "Word predictions and a text edit box cover the top portion of the screen."], "caption": "Figure 1. SceneTalk is a gaze-based AAC system that uses the device\u2019s camera to recognize objects. Recognized objects are used to suggest relevant words and phrases as the user types. The prediction bar at the top of the keyboard combines contextual predictions detected in the image (in orange) with predictions from a language model (in white).", "local_uri": ["0a7a169382aab1b39d339474728afd67c784cb39_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Let's Talk About X: Combining Image Recognition and Eye Gaze to Support Conversation for People with ALS", "pdf_hash": "0a7a169382aab1b39d339474728afd67c784cb39", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of the pop-up window user interface. An image of the bus stop is shown; above the image are suggested words and phrases generated from that image.", "levels": null, "corpus_id": 9546625, "sentences": ["Screen shot of the pop-up window user interface.", "An image of the bus stop is shown; above the image are suggested words and phrases generated from that image."], "caption": "Figure 2. Suggested words (left) and phrases (right) are presented over the camera image in Pop-Up Menu mode.", "local_uri": ["0a7a169382aab1b39d339474728afd67c784cb39_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", "pdf_hash": "4f8c98fd17be479f444f490233f23182dcde7846", "year": 2017, "venue": "UIST", "alt_text": "An image showing various queries a person can ask with wearmail. In clockwise order, a) a person asking while filling a form, \"what is my acm membership number\" b) a person asks while walking to meeting room,  \"What room is the meeting with Jeff?\" c) a person asking while driving the car, \"What is the address of the accessibility meetup?\" d) a person asks in the kitchen, \"What is my moms lasagna recipe?\"", "levels": null, "corpus_id": 24786392, "sentences": ["An image showing various queries a person can ask with wearmail.", "In clockwise order, a) a person asking while filling a form, \"what is my acm membership number\" b) a person asks while walking to meeting room,  \"What room is the meeting with Jeff?\" c) a person asking while driving the car, \"What is the address of the accessibility meetup?\" d) a person asks in the kitchen, \"What is my moms lasagna recipe?\""], "caption": "", "local_uri": ["4f8c98fd17be479f444f490233f23182dcde7846_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", "pdf_hash": "4f8c98fd17be479f444f490233f23182dcde7846", "year": 2017, "venue": "UIST", "alt_text": "An image showing the screen capture of the gmail search application. In it contains all the search history of queries used to search the email like Virgin Atlantic, Mariott and Enterprise.", "levels": null, "corpus_id": 24786392, "sentences": ["An image showing the screen capture of the gmail search application.", "In it contains all the search history of queries used to search the email like Virgin Atlantic, Mariott and Enterprise."], "caption": "", "local_uri": ["4f8c98fd17be479f444f490233f23182dcde7846_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", "pdf_hash": "4f8c98fd17be479f444f490233f23182dcde7846", "year": 2017, "venue": "UIST", "alt_text": "Shows a system diagram of how wearmail works, first part a) shows a users query \"what is my passport number\" is passed to the hangout application running on smartwatch. After which, the figure has two parts c) shows the interface used by crowdworkers to create filterset emails based on where they think the information might lie and d) show the interface which crowd workers use to fill examples for a given query. In the middle of the figure is  web server which process all the information from crowdworkers and uses a Named entity recognizer and regex ranker to extract information from emails and pass it back to smart watch.", "levels": null, "corpus_id": 24786392, "sentences": ["Shows a system diagram of how wearmail works, first part a) shows a users query \"what is my passport number\" is passed to the hangout application running on smartwatch.", "After which, the figure has two parts c) shows the interface used by crowdworkers to create filterset emails based on where they think the information might lie and d) show the interface which crowd workers use to fill examples for a given query.", "In the middle of the figure is  web server which process all the information from crowdworkers and uses a Named entity recognizer and regex ranker to extract information from emails and pass it back to smart watch."], "caption": "", "local_uri": ["4f8c98fd17be479f444f490233f23182dcde7846_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", "pdf_hash": "4f8c98fd17be479f444f490233f23182dcde7846", "year": 2017, "venue": "UIST", "alt_text": "Shows the obsfucation interface the crowdworkers use to select emails. In the interface the words not appearing in 1000 most frequently used english words are blurred. Also highlights how the real names are substituted with pseudo-names to protect privacy.", "levels": null, "corpus_id": 24786392, "sentences": ["Shows the obsfucation interface the crowdworkers use to select emails.", "In the interface the words not appearing in 1000 most frequently used english words are blurred.", "Also highlights how the real names are substituted with pseudo-names to protect privacy."], "caption": "", "local_uri": ["4f8c98fd17be479f444f490233f23182dcde7846_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "WearMail: On-the-Go Access to Information in Your Email with a Privacy-Preserving Human Computation Workflow", "pdf_hash": "4f8c98fd17be479f444f490233f23182dcde7846", "year": 2017, "venue": "UIST", "alt_text": "Shows a user asking a question, \"What is my panera bread order number?\" Crowd generates 7 correct and 13 incorrect examples, the figure shows step2 where a list of regexes are generated based on the examples from crowd and the regexes are ranked based on the amount of characters they can match (least to most). In step3 of the figure shows an example email where context rank of 4 is calculated based on the frequency of occurrence of the query terms \"order\" and \"number\" near the match candidate answer from regex. In step4 of the figure explains how the final rank is calculated by dividing the context rank with regex rank, also illustrates the final possible matches for the query with correct and incorrect answers such as \"96822420\", \"855-3-Panera\", etc.", "levels": null, "corpus_id": 24786392, "sentences": ["Shows a user asking a question, \"What is my panera bread order number?\" Crowd generates 7 correct and 13 incorrect examples, the figure shows step2 where a list of regexes are generated based on the examples from crowd and the regexes are ranked based on the amount of characters they can match (least to most).", "In step3 of the figure shows an example email where context rank of 4 is calculated based on the frequency of occurrence of the query terms \"order\" and \"number\" near the match candidate answer from regex.", "In step4 of the figure explains how the final rank is calculated by dividing the context rank with regex rank, also illustrates the final possible matches for the query with correct and incorrect answers such as \"96822420\", \"855-3-Panera\", etc."], "caption": "", "local_uri": ["4f8c98fd17be479f444f490233f23182dcde7846_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "PizzaBlock: Designing Artefacts and Roleplay to Understand Decentralised Identity Management Systems", "pdf_hash": "3d59dd3f5d3df0ac930f7649feb3ad77bd0a63f3", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "a photo of the third iteration of the stickers used during the game for recording and proving of transactions between participants.", "levels": null, "corpus_id": 220324081, "sentences": ["a photo of the third iteration of the stickers used during the game for recording and proving of transactions between participants."], "caption": "", "local_uri": ["3d59dd3f5d3df0ac930f7649feb3ad77bd0a63f3_Image_057.png"], "annotated": false, "compound": false}
{"title": "PizzaBlock: Designing Artefacts and Roleplay to Understand Decentralised Identity Management Systems", "pdf_hash": "3d59dd3f5d3df0ac930f7649feb3ad77bd0a63f3", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "A photo of the final iteration of the stickers and all of the artefacts which they are used for in the game. The public ledger sheet, the private ledger, and the task sheets.", "levels": null, "corpus_id": 220324081, "sentences": ["A photo of the final iteration of the stickers and all of the artefacts which they are used for in the game.", "The public ledger sheet, the private ledger, and the task sheets."], "caption": "", "local_uri": ["3d59dd3f5d3df0ac930f7649feb3ad77bd0a63f3_Image_058.jpg"], "annotated": false, "compound": false}
{"title": "Self-Identifying Tactile Overlays", "pdf_hash": "fdec89fe49a19711d3787def734c7b806aae8de6", "year": 2018, "venue": "ASSETS", "alt_text": "Scheme of a coding pattern made of conductive adhesive tape and cardboard. It is a 4-bit binary code to identify an individual tactile overlay readable by the touch screen. Each bit is given by a strip of tape adhered on the overlay's backside.", "levels": null, "corpus_id": 52944564, "sentences": ["Scheme of a coding pattern made of conductive adhesive tape and cardboard.", "It is a 4-bit binary code to identify an individual tactile overlay readable by the touch screen.", "Each bit is given by a strip of tape adhered on the overlay's backside."], "caption": "Figure 1. Coding scheme of a with 4-bit binary code of tactile overlay readable by the touch screen. Every conductive tape represents 1 bit.", "local_uri": ["fdec89fe49a19711d3787def734c7b806aae8de6_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Self-Identifying Tactile Overlays", "pdf_hash": "fdec89fe49a19711d3787def734c7b806aae8de6", "year": 2018, "venue": "ASSETS", "alt_text": "An user with visual impairment reading content using the self-identifying tactile overlay made out of engraved paper. The tactile overlay is placed on a capacitive touch screen device displaying a matching GUI.", "levels": null, "corpus_id": 52944564, "sentences": ["An user with visual impairment reading content using the self-identifying tactile overlay made out of engraved paper.", "The tactile overlay is placed on a capacitive touch screen device displaying a matching GUI."], "caption": "Figure 2. A user with visual impairment handling the self-identi\ufb01ed tac\u00ad tile sheets application with a capacitive touch screen device.", "local_uri": ["fdec89fe49a19711d3787def734c7b806aae8de6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\u201cGrip-that-there\u201d: An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration", "pdf_hash": "d96c4a3b8447ba30118d33c289d272de11e6b079", "year": 2021, "venue": "CHI", "alt_text": "Detailed figures showing the steps involved in three of the explicit techniques, menu, subtle relocation, and fixed territories. For the Menu, users can touch or point at an object to bring up an allocation menu and then decide who should work on an object. For Subtle Relocation, users push an object to assign it to the robot. In Fixed Territories, users move objects to the robot's territory to allocate them.", "levels": null, "corpus_id": 231740680, "sentences": ["Detailed figures showing the steps involved in three of the explicit techniques, menu, subtle relocation, and fixed territories.", "For the Menu, users can touch or point at an object to bring up an allocation menu and then decide who should work on an object.", "For Subtle Relocation, users push an object to assign it to the robot.", "In Fixed Territories, users move objects to the robot's territory to allocate them."], "caption": "Figure 2: Explicit allocation techniques: a. Menu \u2013 touching and pointing to allocate objects; b. Subtle Relocation \u2013 pushing an object towards the robot to allocate it; c. Fixed Territories \u2013 moving objects into the robot\u2019s territory to allocate them.", "local_uri": ["d96c4a3b8447ba30118d33c289d272de11e6b079_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\u201cGrip-that-there\u201d: An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration", "pdf_hash": "d96c4a3b8447ba30118d33c289d272de11e6b079", "year": 2021, "venue": "CHI", "alt_text": "Figures depicting four implicit techniques. For the Proactive technique, the robot allocates objects that are closest to where the gripper is positioned. In the Distance technique, the robot assigns objects based on their distance from the robot's base, starting with the closest. For the Gaze technique, the robot picks up objects that are away from where the user is looking. In the Proximity technique, user activity in a regions causes objects in that region to be considered the user's and the robot allocates objects away from this region to work on.", "levels": null, "corpus_id": 231740680, "sentences": ["Figures depicting four implicit techniques.", "For the Proactive technique, the robot allocates objects that are closest to where the gripper is positioned.", "In the Distance technique, the robot assigns objects based on their distance from the robot's base, starting with the closest. For the Gaze technique, the robot picks up objects that are away from where the user is looking.", "In the Proximity technique, user activity in a regions causes objects in that region to be considered the user's and the robot allocates objects away from this region to work on."], "caption": "Figure 3: Implicit allocation techniques: a. Proactive \u2013 robot picks up the object labelled \u201c1\u201d and places it at the target. Then, it picks up the object closest to its gripper; b. Distance \u2013 robot picks up the object closest to its base, starting with the object labelled \u201c1\u201d and then \u201c2\u201d; c. Gaze - user looks at the robot\u2019s base, causing it to become red (user\u2019s territory) and the robot picks up object on the opposite side where the user isn\u2019t looking (robot\u2019s territory); d. Proximity - user picks up an object and moves it to a new location, causing its initial position to become the user\u2019s territory. Then, the robot avoids picking up any objects near the user\u2019s territory and instead picks up an object from the group\u2019s territory.", "local_uri": ["d96c4a3b8447ba30118d33c289d272de11e6b079_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "\u201cGrip-that-there\u201d: An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration", "pdf_hash": "d96c4a3b8447ba30118d33c289d272de11e6b079", "year": 2021, "venue": "CHI", "alt_text": "Figure depicting the implementation of the simulation. The Server consists of the ROS robot simulation while the Client consists of the Unity executable and the user's VR headset.", "levels": null, "corpus_id": 231740680, "sentences": ["Figure depicting the implementation of the simulation.", "The Server consists of the ROS robot simulation while the Client consists of the Unity executable and the user's VR headset."], "caption": "Figure 4: Evaluation testbed where: the Client executes a Unity-based simulation which the user interacts with through a VR headset; and the Server which receives information about objects on the workspace as well as user commands through ROS# which are processed by the Gazebo simulation engine and sent to MoveIt to prepare motion plans. The server and client can run on the same or different machines on different networks.", "local_uri": ["d96c4a3b8447ba30118d33c289d272de11e6b079_Image_004.png"], "annotated": false, "compound": false}
{"title": "\u201cGrip-that-there\u201d: An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration", "pdf_hash": "d96c4a3b8447ba30118d33c289d272de11e6b079", "year": 2021, "venue": "CHI", "alt_text": "Figure shows a bunch of yellow and black objects on a tabletop with the user and robot across from each other.", "levels": [[-1]], "corpus_id": 231740680, "sentences": ["Figure shows a bunch of yellow and black objects on a tabletop with the user and robot across from each other."], "caption": "Figure 5: The 2 x 2 experimental block participants completed when trying each interaction technique, in which the stacking task can be coupled (a, b) or decoupled (c, d), and objects are either scattered (a, c) or sorted (b, d). In coupled tasks, structures require the placement of both the user\u2019s and robot\u2019s blocks while decoupled tasks can be completed independently.", "local_uri": ["d96c4a3b8447ba30118d33c289d272de11e6b079_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Jellys: Towards a Videogame that Trains Rhythm and Visual Attention for Dyslexia", "pdf_hash": "84ba708bebcb46bf33ec71db2b9fca7c35d449de", "year": 2018, "venue": "ASSETS", "alt_text": "A child plays Jellys to test usability dimensions, such as learnability and engagement.", "levels": null, "corpus_id": 52942771, "sentences": ["A child plays Jellys to test usability dimensions, such as learnability and engagement."], "caption": "Figure 1. User study: A child plays Jellys to test usability dimensions, such as learnability and engagement.", "local_uri": ["84ba708bebcb46bf33ec71db2b9fca7c35d449de_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Jellys: Towards a Videogame that Trains Rhythm and Visual Attention for Dyslexia", "pdf_hash": "84ba708bebcb46bf33ec71db2b9fca7c35d449de", "year": 2018, "venue": "ASSETS", "alt_text": "Jellys example activities. First, rhythm training activities: tap a beat and rhythm patterns. Then, visual attention training activities: visual search and motion object training.", "levels": null, "corpus_id": 52942771, "sentences": ["Jellys example activities.", "First, rhythm training activities: tap a beat and rhythm patterns.", "Then, visual attention training activities: visual search and motion object training."], "caption": "Figure 2. Rhythm training activities: tap a beat (a) and rhythm patterns (b). Visual attention training activities: visual search (c) and motion object training (d)", "local_uri": ["84ba708bebcb46bf33ec71db2b9fca7c35d449de_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "ActPad\u2013 A Smart Desk Platform to Enable User Interaction with IoT Devices", "pdf_hash": "f6cb80a5a7a421ddaadae2d29cb34c02d6fac666", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A series of photos presenting our prototype.  From left to right: a-c) on the left show close-up captures of how to connect stationary and movable objects to our prototype. A large photo in the center shows a top-view of a desk setup including our prototype. d)-f) show example inputs: turning on light, touching a speaker, and the coffee cup.", "levels": null, "corpus_id": 233986991, "sentences": ["A series of photos presenting our prototype.", "From left to right: a-c) on the left show close-up captures of how to connect stationary and movable objects to our prototype.", "A large photo in the center shows a top-view of a desk setup including our prototype.", "d)-f) show example inputs: turning on light, touching a speaker, and the coffee cup."], "caption": "", "local_uri": ["f6cb80a5a7a421ddaadae2d29cb34c02d6fac666_Image_001.png"], "annotated": false, "compound": false}
{"title": "ActPad\u2013 A Smart Desk Platform to Enable User Interaction with IoT Devices", "pdf_hash": "f6cb80a5a7a421ddaadae2d29cb34c02d6fac666", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "CAD design of ActPad.  A screenshot of the CAD design of ActPad that illustrate 40 rectangular copper electrodes equally distributed on the desk pads surface. It additionally shows the inner wiring. Each copper electrode is individually connected to a central circuit, that provides a connector for a 40-pin bus.", "levels": null, "corpus_id": 233986991, "sentences": ["CAD design of ActPad.", "A screenshot of the CAD design of ActPad that illustrate 40 rectangular copper electrodes equally distributed on the desk pads surface.", "It additionally shows the inner wiring.", "Each copper electrode is individually connected to a central circuit, that provides a connector for a 40-pin bus."], "caption": "", "local_uri": ["f6cb80a5a7a421ddaadae2d29cb34c02d6fac666_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "ActPad\u2013 A Smart Desk Platform to Enable User Interaction with IoT Devices", "pdf_hash": "f6cb80a5a7a421ddaadae2d29cb34c02d6fac666", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "CAD design of the connectors.  A screenshot of the CAD design for the flexible connectors that enable the customization of object-to-sensor mapping for our prototype.", "levels": null, "corpus_id": 233986991, "sentences": ["CAD design of the connectors.", "A screenshot of the CAD design for the flexible connectors that enable the customization of object-to-sensor mapping for our prototype."], "caption": "", "local_uri": ["f6cb80a5a7a421ddaadae2d29cb34c02d6fac666_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "ActPad\u2013 A Smart Desk Platform to Enable User Interaction with IoT Devices", "pdf_hash": "f6cb80a5a7a421ddaadae2d29cb34c02d6fac666", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Screenshots of our GUI.  Two screenshots of the GUI that we provide with our prototype for users to be able to save custom input commands. The first screenshot shows the states of all touch points and allows to activate input-recording via a button click. The second screenshot shows the selectable actions.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233986991, "sentences": ["Screenshots of our GUI.", "Two screenshots of the GUI that we provide with our prototype for users to be able to save custom input commands.", "The first screenshot shows the states of all touch points and allows to activate input-recording via a button click.", "The second screenshot shows the selectable actions."], "caption": "Figure 3: The exploration interface visualizes the current state of all touch points and connected objects (O = con- nected to an object, _= not touched, X = touched). Users\u2019 sequential or simultaneous input can be recorded and as- signed to an action, e.g. controlling the light, turning on the monitor or the cofee-machine, open a webpage, play a song or switch to the next song.", "local_uri": ["f6cb80a5a7a421ddaadae2d29cb34c02d6fac666_Image_009.jpg", "f6cb80a5a7a421ddaadae2d29cb34c02d6fac666_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Medical Maker Response to COVID-19: Distributed Manufacturing Infrastructure for Stopgap Protective Equipment", "pdf_hash": "9bb586850112f505a4ab025f12486b462e46eee1", "year": 2021, "venue": "CHI", "alt_text": "Sample maker PPE displayed in two rows. First row shows a cloth mask, a tension reliving accessory to hold straps away from ears, two versions of face shields with different headbands. Second row shows a plastic sheet surgical gown, a 3D printed ventilator, Personal Air Purification Respirator (PAPR) head part, and Nasal Swab holders for COVID-19 test in collected from the NIH 3D Print Exchange.", "levels": null, "corpus_id": 233987545, "sentences": ["Sample maker PPE displayed in two rows.", "First row shows a cloth mask, a tension reliving accessory to hold straps away from ears, two versions of face shields with different headbands.", "Second row shows a plastic sheet surgical gown, a 3D printed ventilator, Personal Air Purification Respirator (PAPR) head part, and Nasal Swab holders for COVID-19 test in collected from the NIH 3D Print Exchange."], "caption": "", "local_uri": ["9bb586850112f505a4ab025f12486b462e46eee1_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Medical Maker Response to COVID-19: Distributed Manufacturing Infrastructure for Stopgap Protective Equipment", "pdf_hash": "9bb586850112f505a4ab025f12486b462e46eee1", "year": 2021, "venue": "CHI", "alt_text": "Process flow of Prototyping, Procuring, and Producing PPE: Intermediary Activities for Community Production shown in dotted boxes with arrows flowing from each stage to the next. The boxes split into artifacts for each stage where they prototype devices and protocols, procure materials and partners, then then they produce devices or parts. Repair work in each stage is shown by filled boxes in the diagram.", "levels": null, "corpus_id": 233987545, "sentences": ["Process flow of Prototyping, Procuring, and Producing PPE: Intermediary Activities for Community Production shown in dotted boxes with arrows flowing from each stage to the next.", "The boxes split into artifacts for each stage where they prototype devices and protocols, procure materials and partners, then then they produce devices or parts.", "Repair work in each stage is shown by filled boxes in the diagram."], "caption": "Figure 2: Intermediary Activities for Community Production take place across three main activities to (a) Prototype artifacts,", "local_uri": ["9bb586850112f505a4ab025f12486b462e46eee1_Image_005.png"], "annotated": false, "compound": false}
{"title": "A Pilot Deployment of an Online Tool for Large-Scale Virtual Auditing of Urban Accessibility", "pdf_hash": "353eff75823c9a433e27a6cddc09c7875265a461", "year": 2017, "venue": "ASSETS", "alt_text": "Figure showing three screens of the tool - first showing an example mission screen, second showing how to virtually walk and explore streets within the tool and third showing how to label", "levels": null, "corpus_id": 28955678, "sentences": ["Figure showing three screens of the tool - first showing an example mission screen, second showing how to virtually walk and explore streets within the tool and third showing how to label"], "caption": "Figure 1. Project Sidewalk uses basic game design principles to engage online volunteers in virtually auditing urban accessibility in Google Street View.", "local_uri": ["353eff75823c9a433e27a6cddc09c7875265a461_Image_001.png"], "annotated": false, "compound": false}
{"title": "Understanding the Needs of Prospective Tenants", "pdf_hash": "cb27e3a368203cd75f507660794d344bbba67b99", "year": 2018, "venue": "COMPASS", "alt_text": "An image of an eye shaped icon with a 15 next to it. This is an example of implicit observation.", "levels": [[-1], [-1]], "corpus_id": 49354802, "sentences": ["An image of an eye shaped icon with a 15 next to it.", "This is an example of implicit observation."], "caption": "Implicit Observation: eDigs exposes information already in web logs about how often a home is \u2018visited\u2019 virtually. Ultimately this could be extended to include other facts, such as the average time spent in a rental during a showing, how often a landlord updates their listing on the site, how many times the rental is \u2018shared,\u2019 and so on. This is an inexpensive way to add richer data that could help participants to select rentals.", "local_uri": ["cb27e3a368203cd75f507660794d344bbba67b99_Image_008.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Opportunities and Challenges in Involving Users in Project-Based HCI Education", "pdf_hash": "6d33e71dba82c07126e101ba5c3bb1982b7a174f", "year": 2020, "venue": "CHI", "alt_text": "Three adults drawing on a white board on the left and four children drawing on the right.", "levels": null, "corpus_id": 210177211, "sentences": ["Three adults drawing on a white board on the left and four children drawing on the right."], "caption": "Figure 2. Children and students during snack time (Session 3).", "local_uri": ["6d33e71dba82c07126e101ba5c3bb1982b7a174f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "VizWiz: nearly real-time answers to visual questions", "pdf_hash": "e0c668d1da866617ccfeee910d13eb14fa340bea", "year": 2010, "venue": "W4A", "alt_text": "A pictures shows an Android G1 phone connected by arrows to a Mechanical Turk interface on the web illustrating the example from the text - a hand-written note is pasted on top of some mailboxes, the question reads \"What does this say?\"   The option is given to play the speech.", "levels": null, "corpus_id": 207179082, "sentences": ["A pictures shows an Android G1 phone connected by arrows to a Mechanical Turk interface on the web illustrating the example from the text - a hand-written note is pasted on top of some mailboxes, the question reads \"What does this say?\"   The option is given to play the speech."], "caption": "", "local_uri": ["e0c668d1da866617ccfeee910d13eb14fa340bea_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding", "pdf_hash": "b95bc5a0a8199d4fb34dc67974fae85438c2037e", "year": 2015, "venue": "UIST", "alt_text": "LaserStacker produces laser cut objects consisting of multiple layers of acrylic without requiring manual assembly: It assembles the object by not only cutting with the laser, but also welding, and healing the cut. Three example objects: (a) a pinball table with spring (10 min), (b) an architectural model of our university campus (7 min), and (c) a simple but functional pair of scissors (3 min).", "levels": null, "corpus_id": 9452534, "sentences": ["LaserStacker produces laser cut objects consisting of multiple layers of acrylic without requiring manual assembly: It assembles the object by not only cutting with the laser, but also welding, and healing the cut.", "Three example objects: (a) a pinball table with spring (10 min), (b) an architectural model of our university campus (7 min), and (c) a simple but functional pair of scissors (3 min)."], "caption": "ABSTRACT", "local_uri": ["b95bc5a0a8199d4fb34dc67974fae85438c2037e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding", "pdf_hash": "b95bc5a0a8199d4fb34dc67974fae85438c2037e", "year": 2015, "venue": "UIST", "alt_text": "The cut/weld process illustrated at the example of the scissors. (a) To create the scissor outline in the bottom layer, LaserStacker first cuts through the top layer, then through the middle layer, and finally through the bottom layer. In this particular case of cutting the scissor outline, this happens to be a desirable effect in that it already cuts the scissor outline in the middle layer. We will remove the scissor outline in the top layer (surplus material) later. (b) LaserStacker now cuts the axle into the middle layer. This requires LaserStacker to cut the top layer as well, which in this case does not fit our intended design. LaserStacker will thus heal this cut in a later step.(c) LaserStacker welds the axle in the middle layer to the scissor outline in the bottom layer. Like any welding in a lower layer, this causes all layers above to be welded as well, causing the axle to be connected all the way through to the top layer. In this particular case, this is desired. In other cases, LaserStacker would release the weld later. We now have two scissor blades combined by a rotary joint; however, the rotary joint still lacks a cover. (d) LaserStacker cuts the axle cover in the top layer. This cut only touches the top layer, thus has no side effects on any of the other layers. (e) LaserStacker now heals the cut between the axle and the axle cover in the top layer that was caused in step (b). (f) We take the scissors out of the cutter. After removing the surplus material, the scissors are complete and we are ready to cut paper with them.", "levels": null, "corpus_id": 9452534, "sentences": ["The cut/weld process illustrated at the example of the scissors.", "(a) To create the scissor outline in the bottom layer, LaserStacker first cuts through the top layer, then through the middle layer, and finally through the bottom layer.", "In this particular case of cutting the scissor outline, this happens to be a desirable effect in that it already cuts the scissor outline in the middle layer.", "We will remove the scissor outline in the top layer (surplus material) later.", "(b) LaserStacker now cuts the axle into the middle layer.", "This requires LaserStacker to cut the top layer as well, which in this case does not fit our intended design.", "LaserStacker will thus heal this cut in a later step.(c) LaserStacker welds the axle in the middle layer to the scissor outline in the bottom layer.", "Like any welding in a lower layer, this causes all layers above to be welded as well, causing the axle to be connected all the way through to the top layer.", "In this particular case, this is desired.", "In other cases, LaserStacker would release the weld later.", "We now have two scissor blades combined by a rotary joint; however, the rotary joint still lacks a cover.", "(d) LaserStacker cuts the axle cover in the top layer.", "This cut only touches the top layer, thus has no side effects on any of the other layers.", "(e) LaserStacker now heals the cut between the axle and the axle cover in the top layer that was caused in step (b). (f) We take the scissors out of the cutter.", "After removing the surplus material, the scissors are complete and we are ready to cut paper with them."], "caption": "", "local_uri": ["b95bc5a0a8199d4fb34dc67974fae85438c2037e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding", "pdf_hash": "b95bc5a0a8199d4fb34dc67974fae85438c2037e", "year": 2015, "venue": "UIST", "alt_text": "Welding two sheets of acrylic: (a) the laser cuts through the top layer and into 30% of bottom layer, (b) the heat along the laser path welds both layers.", "levels": null, "corpus_id": 9452534, "sentences": ["Welding two sheets of acrylic: (a) the laser cuts through the top layer and into 30% of bottom layer, (b) the heat along the laser path welds both layers."], "caption": "", "local_uri": ["b95bc5a0a8199d4fb34dc67974fae85438c2037e_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding", "pdf_hash": "b95bc5a0a8199d4fb34dc67974fae85438c2037e", "year": 2015, "venue": "UIST", "alt_text": "Healing a cut: (a) creating a new cut in close proximity, (b) defocusing the laser and melting the piece to  (c) create a strong side-by-side weld.", "levels": null, "corpus_id": 9452534, "sentences": ["Healing a cut: (a) creating a new cut in close proximity, (b) defocusing the laser and melting the piece to  (c) create a strong side-by-side weld."], "caption": "Figure 6: Healing a cut: (a) creating a new cut in close proximity, (b) defocusing the laser and melting the piece to", "local_uri": ["b95bc5a0a8199d4fb34dc67974fae85438c2037e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding", "pdf_hash": "b95bc5a0a8199d4fb34dc67974fae85438c2037e", "year": 2015, "venue": "UIST", "alt_text": "Releasing a weld: (a) creating a new cut in very close proximity, (b) the material evaporates and releases the weld.", "levels": null, "corpus_id": 9452534, "sentences": ["Releasing a weld: (a) creating a new cut in very close proximity, (b) the material evaporates and releases the weld."], "caption": "Figure 7: Releasing a weld: (a) creating a new cut in very close proximity, (b) the material evaporates and releases the weld.", "local_uri": ["b95bc5a0a8199d4fb34dc67974fae85438c2037e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Rescribe: Authoring and Automatically Editing Audio Descriptions", "pdf_hash": "72d00737548215582bc4860ca26f04d6ccdbcdf2", "year": 2020, "venue": "UIST", "alt_text": "A graphic displaying inline, extended and extended-inline descriptions. The inline audio track shows the speech and music of the source audio and on a second row shows where the audio is placed relative to the speech (i.e. not on top of it). The extended descriptions copy this format but show that the source audio track is paused while the music plays back. Finally the inline extended description displays the music looped in the music track with the audio description under it. The subsequent speech is moved to the right to show that the space for descriptions has been extended.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 222208805, "sentences": ["A graphic displaying inline, extended and extended-inline descriptions.", "The inline audio track shows the speech and music of the source audio and on a second row shows where the audio is placed relative to the speech (i.e. not on top of it).", "The extended descriptions copy this format but show that the source audio track is paused while the music plays back.", "Finally the inline extended description displays the music looped in the music track with the audio description under it.", "The subsequent speech is moved to the right to show that the space for descriptions has been extended."], "caption": "", "local_uri": ["72d00737548215582bc4860ca26f04d6ccdbcdf2_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Rescribe: Authoring and Automatically Editing Audio Descriptions", "pdf_hash": "72d00737548215582bc4860ca26f04d6ccdbcdf2", "year": 2020, "venue": "UIST", "alt_text": "The Rescribe interface features a video . player on the far right labeled (A) showing a bright shot at an outdoor farmers market with a variety of eggplant types laying on a checkered table. Below the video is the timeline (B) that shows speech, descriptions, gaps, and extendable gaps along the timeline. (B) contains the source video transcript, then any description written in grey to designate its position. Finally, remaining gaps in the content are visualized in white spaces to show remaining space for descriptions. On the right is the description pane which features an empty text box (D) and a set of four descriptions that read \"A variety of eggplants sit on the table\", \"herbs and carrots\", \"text ojai valley inn\", and \"an expansive groomed green field with landerns hanging from trees. Next to each description is a representative keyframe to help users keep track of what shot they are describing.", "levels": null, "corpus_id": 222208805, "sentences": ["The Rescribe interface features a video .", "player on the far right labeled (A) showing a bright shot at an outdoor farmers market with a variety of eggplant types laying on a checkered table.", "Below the video is the timeline (B) that shows speech, descriptions, gaps, and extendable gaps along the timeline. (B) contains the source video transcript, then any description written in grey to designate its position.", "Finally, remaining gaps in the content are visualized in white spaces to show remaining space for descriptions.", "On the right is the description pane which features an empty text box (D) and a set of four descriptions that read \"A variety of eggplants sit on the table\", \"herbs and carrots\", \"text ojai valley inn\", and \"an expansive groomed green field with landerns hanging from trees.", "Next to each description is a representative keyframe to help users keep track of what shot they are describing."], "caption": "Figure 3. The interface features: 1) a video and timeline pane (left) with the source video speech in black and audio descriptions in grey (A,B); 2) a transcript pane (center) that visualizes the source video transcript (black), the audio descriptions (grey), and the gaps available (white) (C); and 3) the description pane (right) displaying the descriptions and thumbnails (D,E). As the video plays, the interface scrolls to the relevant point in the transcript (highlighted) and descriptions. Source video: Gaby\u2019s Guide to Ojai by What\u2019s Gaby Cooking [12].", "local_uri": ["72d00737548215582bc4860ca26f04d6ccdbcdf2_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Rescribe: Authoring and Automatically Editing Audio Descriptions", "pdf_hash": "72d00737548215582bc4860ca26f04d6ccdbcdf2", "year": 2020, "venue": "UIST", "alt_text": "A single description pane showing a user iterating between description options using a slider. The options are visualized by greying out unused text. Here the full description reads \"Thomas and Gaby are back on the couch in a bright living room\". If read without the grey text it reads \"Thomas and Gaby are back in a living room\".", "levels": null, "corpus_id": 222208805, "sentences": ["A single description pane showing a user iterating between description options using a slider.", "The options are visualized by greying out unused text.", "Here the full description reads \"Thomas and Gaby are back on the couch in a bright living room\".", "If read without the grey text it reads \"Thomas and Gaby are back in a living room\"."], "caption": "Figure 5. After Rescribe renders the new description, the user can access additional editing modalities through mouseover. While the original op- tions remain, users can re-record by clicking on the record button. The included text is black, and the original text not included is grey. Users can slide to select an option automatically or click words in order to tog- gle their inclusion. Users can rewrite by double clicking and the record- ing will be deleted.", "local_uri": ["72d00737548215582bc4860ca26f04d6ccdbcdf2_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Rescribe: Authoring and Automatically Editing Audio Descriptions", "pdf_hash": "72d00737548215582bc4860ca26f04d6ccdbcdf2", "year": 2020, "venue": "UIST", "alt_text": "Visualization of edit score. if \"|\" corresponds to an edit, \"A long bench with blue birds\" would be cut in the following places to shorten to \"A bench with birds\": \"A long | bench | with | blue | birds\" (4 edits), whereas to get \"A long bench\" it would be cut as follows: \"A long bench | with blue birds\" (1 edit).", "levels": null, "corpus_id": 222208805, "sentences": ["Visualization of edit score.", "if \"|\" corresponds to an edit, \"A long bench with blue birds\" would be cut in the following places to shorten to \"A bench with birds\": \"A long | bench | with | blue | birds\" (4 edits), whereas to get \"A long bench\" it would be cut as follows: \"A long bench | with blue birds\" (1 edit)."], "caption": "penalty of 20 (last word penalty) if the \ufb01nal word is removed from the description as the \ufb01nal word often contains downward intonation.", "local_uri": ["72d00737548215582bc4860ca26f04d6ccdbcdf2_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Rescribe: Authoring and Automatically Editing Audio Descriptions", "pdf_hash": "72d00737548215582bc4860ca26f04d6ccdbcdf2", "year": 2020, "venue": "UIST", "alt_text": "A figure with three subfigures displaying user agreement rates for extended, inline, and extended-inline descriptions. Responses are based on Likert scale statements for ease of use, distracting edits, and future use. The first subfigure highlights ratings for extended descriptions. Ease of understanding is rated 5 by 7 participants. Distracting edits are scored 5 by 4 participants, 3 by 2 participants, and 2 by 1 participant. Future use is rated 3 by 2 participants, 4 by 2 participants, and 5 by 1 participant. The second subfigure highlights rating for inline descriptions. Ease of understanding is rated 5 by 7 participants. Distracting edits are scored 3 by 3 participants, 2 by 2 participants, and 1 by 2 participants. Future use is rated 5 by 6 participants, and 4 by 1 participant. The third subfigure highlights rating for extended-inline descriptions. Ease of understanding is rated 5 by 7 participants. Distracting edits are scored 3 by 1 participant, and 1 by 6 participants. Future use is rated 5 by 5 participants, and 2 by 2 participants.", "levels": null, "corpus_id": 222208805, "sentences": ["A figure with three subfigures displaying user agreement rates for extended, inline, and extended-inline descriptions.", "Responses are based on Likert scale statements for ease of use, distracting edits, and future use.", "The first subfigure highlights ratings for extended descriptions.", "Ease of understanding is rated 5 by 7 participants.", "Distracting edits are scored 5 by 4 participants, 3 by 2 participants, and 2 by 1 participant.", "Future use is rated 3 by 2 participants, 4 by 2 participants, and 5 by 1 participant.", "The second subfigure highlights rating for inline descriptions.", "Ease of understanding is rated 5 by 7 participants.", "Distracting edits are scored 3 by 3 participants, 2 by 2 participants, and 1 by 2 participants.", "Future use is rated 5 by 6 participants, and 4 by 1 participant.", "The third subfigure highlights rating for extended-inline descriptions.", "Ease of understanding is rated 5 by 7 participants.", "Distracting edits are scored 3 by 1 participant, and 1 by 6 participants.", "Future use is rated 5 by 5 participants, and 2 by 2 participants."], "caption": "Figure 7. We played participants 3 audio description samples from the same draft description from the same video clip. We randomized the order of the descriptions and after each description asked users to rate their agreement with three Likert scale statements (1-Strongly Disagree to 5-Strongly Agree): I was able to understand the audio descriptions in this clip (\u201cUnderstand\u201d), I found the editing in this clip to be distracting (\u201cDistracting edits\u201d) and I would use this type of audio description in the future (\u201cFuture use\u201d)", "local_uri": ["72d00737548215582bc4860ca26f04d6ccdbcdf2_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "An Uninteresting Tour Through Why Our Research Papers Aren't Accessible", "pdf_hash": "0ae334ac9609022524aeaf74d62d83fb52648402", "year": 2016, "venue": "CHI Extended Abstracts", "alt_text": "This figure shows a screenshot of Adobe Acrobat reading order editor. It shows that a paragraph in the right column is incorrectly after the left column. This will cause a screenreader to jump between columns, which is very confusing.", "levels": [[-1], [-1], [-1]], "corpus_id": 2553610, "sentences": ["This figure shows a screenshot of Adobe Acrobat reading order editor.", "It shows that a paragraph in the right column is incorrectly after the left column.", "This will cause a screenreader to jump between columns, which is very confusing."], "caption": "", "local_uri": ["0ae334ac9609022524aeaf74d62d83fb52648402_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "An Uninteresting Tour Through Why Our Research Papers Aren't Accessible", "pdf_hash": "0ae334ac9609022524aeaf74d62d83fb52648402", "year": 2016, "venue": "CHI Extended Abstracts", "alt_text": "This figure shows a screenshot of Adobe Acrobat reading order editor. When trying to modify the table, its position is moved over the text, ruining the visual layout. Also, there's no undo function for this.", "levels": null, "corpus_id": 2553610, "sentences": ["This figure shows a screenshot of Adobe Acrobat reading order editor.", "When trying to modify the table, its position is moved over the text, ruining the visual layout.", "Also, there's no undo function for this."], "caption": "Figure 3: Making a table accessible in Adobe Acrobat often messes up its position in the paper. In this case, the table has been placed above text on the page. We are not sure why this happens, but with no \u2018undo\u2019 feature in Acrobat, we imagine many authors have simply given up in frustration after it happened. We now make a habit of saving our documents after every change so we can close and reopen the document upon hitting a bug.", "local_uri": ["0ae334ac9609022524aeaf74d62d83fb52648402_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Is Your Time Well Spent? Reflecting on Knowledge Work More Holistically", "pdf_hash": "d3fb03c18e115854d965b65db323dfacf787f04f", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: Five simple cartoon faces are arranged left to right above a slider. The face on the left is a sad face with an exaggerated frown. The face in the middle is neutral, the mouth is a straight line. The face on the right has an exaggerated smile.", "levels": null, "corpus_id": 218483541, "sentences": ["Figure 1: Five simple cartoon faces are arranged left to right above a slider.", "The face on the left is a sad face with an exaggerated frown.", "The face in the middle is neutral, the mouth is a straight line.", "The face on the right has an exaggerated smile."], "caption": "Figure 1. Five (1, 3, 5, 7, and 9) of the nine faces in the Likert scale, lined up with their position on the slider. In the survey, participants saw one face and the shape of the mouth changed with the slider.", "local_uri": ["d3fb03c18e115854d965b65db323dfacf787f04f_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Is Your Time Well Spent? Reflecting on Knowledge Work More Holistically", "pdf_hash": "d3fb03c18e115854d965b65db323dfacf787f04f", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: The visualization has 4 columns, one for each major theme, and 40 thin lines intersecting each column representing each participant. There is a circle on the intersection if the participant's definition contained a subtheme. The circle is bigger if it contained more than one subtheme in a certain theme. The participants are sorted from most subthemes (5) to least (1) and it is clear that each participant used a different combination of subthemes to define TWS.", "levels": null, "corpus_id": 218483541, "sentences": ["Figure 2: The visualization has 4 columns, one for each major theme, and 40 thin lines intersecting each column representing each participant.", "There is a circle on the intersection if the participant's definition contained a subtheme.", "The circle is bigger if it contained more than one subtheme in a certain theme.", "The participants are sorted from most subthemes (5) to least (1) and it is clear that each participant used a different combination of subthemes to define TWS."], "caption": "Figure 2. Participants (n=40) each gave a de\ufb01nition of TWS, which is represented by a line. It could be multi-faceted in terms of touching multiple major themes (multiple circles across the line) and multiple sub- themes within a theme (size of the circle). The number at the bottom gives the total number of participants (out of 40) who touched on the corresponding major theme.", "local_uri": ["d3fb03c18e115854d965b65db323dfacf787f04f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "FootNotes: Geo-referenced Audio Annotations for Nonvisual Exploration", "pdf_hash": "585b2052060367926b6ae2810a3f986124a41064", "year": 2018, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "A bronze statue of kids playing in the rain, situated in a local park, used in the user study. The caption contains the annotation text used.", "levels": [[-1], [-1]], "corpus_id": 52294684, "sentences": ["A bronze statue of kids playing in the rain, situated in a local park, used in the user study.", "The caption contains the annotation text used."], "caption": "FootNotes\u2019 functional annotations indicate how one should use a point of interest, such as hours for a busi- ness or accessibility information (e.g., describing the location of stairs or obstacles). The functional infor- mation can be gleaned from websites associated with certain points of interest, as well as added in situ by volunteer contributors.", "local_uri": ["585b2052060367926b6ae2810a3f986124a41064_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Enabling Data-Driven Reflections on Ideation Cards", "pdf_hash": "27e88ec01427cda9099cf69ee2cdf0f63e123b6e", "year": 2019, "venue": "", "alt_text": "The image shows an example car with it's various sections described. These include: it's type, an evocative image, a significant border colour, a memorable card title, a brief description of the concepts, and the catergory of the card.", "levels": null, "corpus_id": 211250646, "sentences": ["The image shows an example car with it's various sections described.", "These include: it's type, an evocative image, a significant border colour, a memorable card title, a brief description of the concepts, and the catergory of the card."], "caption": "Figure 1. Example Opportunity Card with labels.", "local_uri": ["27e88ec01427cda9099cf69ee2cdf0f63e123b6e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Charting the Unknown: Challenges in the Clinical Assessment of Patients\u2019 Technology Use Related to Eating Disorders", "pdf_hash": "ad3149f47d8b67d861d5b9290ee961c4da5ca82d", "year": 2021, "venue": "CHI", "alt_text": "Examples of Acute and long-term disease  support that includes community dieticians and counseling; parkview behavioral health; the parkview physicans group; and Parkview research center", "levels": null, "corpus_id": 233987394, "sentences": ["Examples of Acute and long-term disease  support that includes community dieticians and counseling; parkview behavioral health; the parkview physicans group; and Parkview research center"], "caption": "", "local_uri": ["ad3149f47d8b67d861d5b9290ee961c4da5ca82d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "A person is sitting on a couch speaking to a plant and a lamp. The plant is telling the person \"You haven't watered me in two weeks!\". The lamp is asking \"Would you like me to dim the lights?", "levels": null, "corpus_id": 218482500, "sentences": ["A person is sitting on a couch speaking to a plant and a lamp.", "The plant is telling the person \"You haven't watered me in two weeks!\".", "The lamp is asking \"Would you like me to dim the lights?"], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "An image of the Digital Ventriloquism proof of concept system with each component labeled.", "levels": null, "corpus_id": 218482500, "sentences": ["An image of the Digital Ventriloquism proof of concept system with each component labeled."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "A frequency response curve of the pre-adusted parametric speaker showing peaks at 4kHz and 14kHz with a dip around 7kHz.", "levels": null, "corpus_id": 218482500, "sentences": ["A frequency response curve of the pre-adusted parametric speaker showing peaks at 4kHz and 14kHz with a dip around 7kHz."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "A vector from the DOA output of the Respeaker with an output of the YOLO system identifying different objects such as chair, monitor, and potted plant.", "levels": null, "corpus_id": 218482500, "sentences": ["A vector from the DOA output of the Respeaker with an output of the YOLO system identifying different objects such as chair, monitor, and potted plant."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "A chart showing the different positions of microphone placement with respect to a placed object ranging from 0 to 180 degrees.", "levels": [[-1]], "corpus_id": 218482500, "sentences": ["A chart showing the different positions of microphone placement with respect to a placed object ranging from 0 to 180 degrees."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "8 geometry exemplars made from paper including convex, concave, egg crate, flat, wave, zig-zag, and square.", "levels": null, "corpus_id": 218482500, "sentences": ["8 geometry exemplars made from paper including convex, concave, egg crate, flat, wave, zig-zag, and square."], "caption": "Figure 7. Our eight surface geometry exemplars.", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "Image containing three different environments: office (showing printer, paper tray, monitor, plant, picture frame) domestic (showing diswasher, cabinet, coffeemaker, microwave, fridge) and workshop (showing trashcan, storage unit, laser cutter, fire extinguisher, and CNC) with a top down view for each environment including distances and angles.", "levels": null, "corpus_id": 218482500, "sentences": ["Image containing three different environments: office (showing printer, paper tray, monitor, plant, picture frame) domestic (showing diswasher, cabinet, coffeemaker, microwave, fridge) and workshop (showing trashcan, storage unit, laser cutter, fire extinguisher, and CNC) with a top down view for each environment including distances and angles."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "Confusion matrix for each of the three contexts. Paper tray did worst in office, coffee maker did worst in domestic, and laser cutter did worst in workshop.", "levels": null, "corpus_id": 218482500, "sentences": ["Confusion matrix for each of the three contexts.", "Paper tray did worst in office, coffee maker did worst in domestic, and laser cutter did worst in workshop."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Digital Ventriloquism: Giving Voice to Everyday Objects", "pdf_hash": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "year": 2020, "venue": "CHI", "alt_text": "Mockup that looks like a smart speaker with a \"skin\" of transducers for 360 degree beamforming.", "levels": null, "corpus_id": 218482500, "sentences": ["Mockup that looks like a smart speaker with a \"skin\" of transducers for 360 degree beamforming."], "caption": "", "local_uri": ["66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"A user grips a pen controller to performs (a) poke gestures,  and (b) tilt gestures.\"", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 1: \"A user grips a pen controller to performs (a) poke gestures,  and (b) tilt gestures.\""], "caption": "Figure 1. A user grips a pen controller to perform (a) poke gestures, and", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"Candidate grip postures: (a) tripod at front end, (b) tripod at  rear end, (c) quadropod at rear end, (d) pinch, and (e) overhand.\"", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 2: \"Candidate grip postures: (a) tripod at front end, (b) tripod at  rear end, (c) quadropod at rear end, (d) pinch, and (e) overhand.\""], "caption": "", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"(a) The 3D printed pen was used in the studies; (b) A user took  participant in Study 1.\"", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 3: \"(a) The 3D printed pen was used in the studies; (b) A user took  participant in Study 1.\""], "caption": "", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: \"The results of Study 1: (a) Pen tip travelled distance (mm); (b)  Pen shaft tilted angle (\u00b0).\"", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 4: \"The results of Study 1: (a) Pen tip travelled distance (mm); (b)  Pen shaft tilted angle (\u00b0).\""], "caption": "Figure 4. The results of Study 1: (a) Pen tip travelled distance (mm); (b) Pen shaft tilted angle (\u25e6).", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: \"Participant responses to the workload of different grip postures.  Graphs are centered around the neutral response, with the proportion  of positive and negative responses on the right and left side, respectively.\"", "levels": [[1], [1]], "corpus_id": 218482658, "sentences": ["Figure 5: \"Participant responses to the workload of different grip postures.", "Graphs are centered around the neutral response, with the proportion  of positive and negative responses on the right and left side, respectively.\""], "caption": "", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: \"(a) The user interface for selection with the poke; (b) A participant  uses the Pen Grip and selects with the poke in Study 2.", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 6: \"(a) The user interface for selection with the poke; (b) A participant  uses the Pen Grip and selects with the poke in Study 2."], "caption": "", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: \"(a) The user interface for selection with tilt; (b) A participant  uses the Palm Grip and selects with tilt in Study 2.\"", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 7: \"(a) The user interface for selection with tilt; (b) A participant  uses the Palm Grip and selects with tilt in Study 2.\""], "caption": "", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen", "pdf_hash": "7824023f283bc0823a431cf16ae25786ae3a598c", "year": 2020, "venue": "CHI", "alt_text": "Figure 13: \"(a) A participant can select an invisible object; (b) A participant  can see but cannot select the target.\"", "levels": null, "corpus_id": 218482658, "sentences": ["Figure 13: \"(a) A participant can select an invisible object; (b) A participant  can see but cannot select the target.\""], "caption": "", "local_uri": ["7824023f283bc0823a431cf16ae25786ae3a598c_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Framing Effects Influence Interface Feature Decisions", "pdf_hash": "750c3f939d0bd95b8ee9d68025ce4da7900b5c6b", "year": 2020, "venue": "CHI", "alt_text": "Two check boxes controlling interface options in Microsoft word. Above the checkboxes is the text \"Apply as you type\".", "levels": null, "corpus_id": 218483085, "sentences": ["Two check boxes controlling interface options in Microsoft word.", "Above the checkboxes is the text \"Apply as you type\"."], "caption": "Figure 1. A preference control supporting user decisions about interface features in Microsoft Word.", "local_uri": ["750c3f939d0bd95b8ee9d68025ce4da7900b5c6b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Framing Effects Influence Interface Feature Decisions", "pdf_hash": "750c3f939d0bd95b8ee9d68025ce4da7900b5c6b", "year": 2020, "venue": "CHI", "alt_text": "A screen capture image from the SwiftKey mobile keyboard, which includes a statement regarding the user's productivity stating \"I am 8% more productive with SwiftKey\"", "levels": null, "corpus_id": 218483085, "sentences": ["A screen capture image from the SwiftKey mobile keyboard, which includes a statement regarding the user's productivity stating \"I am 8% more productive with SwiftKey\""], "caption": "", "local_uri": ["750c3f939d0bd95b8ee9d68025ce4da7900b5c6b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Living in Augmented Reality: Ubiquitous Media and Reactive Environments Redux", "pdf_hash": "4ac029f4532cc8c9631185926060efc8306c1b52", "year": 2020, "venue": "", "alt_text": "A group of people sitting in front of a television  Description automatically generated", "levels": null, "corpus_id": 233229844, "sentences": ["A group of people sitting in front of a television  Description automatically generated"], "caption": "Figure 11 Both images show an example of the presentation being made remotely. In cases where no support materials are being used, the presenter appears on the large display. On the other hand, if slides or documents are included in the presentation, they appear on the large display, and the presenter appears on the smaller display to the left As in same-place meetings, people and presentation materials, appear in their own independent space \u2013 for local and remote attendees, alike. More on this below,", "local_uri": ["4ac029f4532cc8c9631185926060efc8306c1b52_Image_015.jpg", "4ac029f4532cc8c9631185926060efc8306c1b52_Image_016.jpg"], "annotated": false, "compound": true}
{"title": "Living in Augmented Reality: Ubiquitous Media and Reactive Environments Redux", "pdf_hash": "4ac029f4532cc8c9631185926060efc8306c1b52", "year": 2020, "venue": "", "alt_text": "A picture containing indoor, sitting, white, mirror  Description automatically generated", "levels": null, "corpus_id": 233229844, "sentences": ["A picture containing indoor, sitting, white, mirror  Description automatically generated"], "caption": "", "local_uri": ["4ac029f4532cc8c9631185926060efc8306c1b52_Image_024.jpg"], "annotated": false, "compound": false}
{"title": "Informing the Design of a Personalized Privacy Assistant for the Internet of Things", "pdf_hash": "7e5710e1c2fdd9616261832973f020ad2623f44b", "year": 2020, "venue": "CHI", "alt_text": "Diagram describing the different implementations we explored. In bold are the implementations we envisioned, in italic are the ones suggested by the users. Notification breaks into \"No control\" and \"Control\" (all bold) Recommendation (bold) breaks into  \"Previous behaviors\" (bold) and \"external sources\" (italic). External sources further breaks into \"experts\", \"crowd-sourced\", unknown\", \"multiple sources\" (all italic) \"Auto\" (bold): \"Automated \"(italic); \"Autonomous\" (bold).  Autonomous further breaks down into \"Previous behaviors\" (bold) and \"external sources (italic)", "levels": null, "corpus_id": 218482758, "sentences": ["Diagram describing the different implementations we explored.", "In bold are the implementations we envisioned, in italic are the ones suggested by the users.", "Notification breaks into \"No control\" and \"Control\" (all bold) Recommendation (bold) breaks into  \"Previous behaviors\" (bold) and \"external sources\" (italic).", "External sources further breaks into \"experts\", \"crowd-sourced\", unknown\", \"multiple sources\" (all italic) \"Auto\" (bold): \"Automated \"(italic); \"Autonomous\" (bold).", "Autonomous further breaks down into \"Previous behaviors\" (bold) and \"external sources (italic)"], "caption": "Figure 1. Diagram of the different implementations of PPA. In bold are the implementations we originally envisioned and in italic, the implemen- tations suggested by participants during the interviews.", "local_uri": ["7e5710e1c2fdd9616261832973f020ad2623f44b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Gathering Self-Report Data in Games Through NPC Dialogues: Effects on Data Quality, Data Quantity, Player Experience, and Information Intimacy", "pdf_hash": "f6adebe7eecae77ae926fe3b68c655318c15a8ef", "year": 2021, "venue": "CHI", "alt_text": "Figure 1a: A screenshot from the game showing a first person perspective view of the virtual environment. There are platforms divided by gaps and a target flag at the end of the room. Figure 1b: An in-game screenshot showing a dialogue. There is a text box at the bottom screen displaying the text \"Hi there, welcome to this task.\" and a header displaying the NPC's name \"Assist Cube\". The right side of the text box shows an image of the Assist Cube, a square with a smiling face. There are two buttons that display answer options, i.e., \"Why am I here?\" and \"Who are you?\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987327, "sentences": ["Figure 1a: A screenshot from the game showing a first person perspective view of the virtual environment.", "There are platforms divided by gaps and a target flag at the end of the room.", "Figure 1b: An in-game screenshot showing a dialogue.", "There is a text box at the bottom screen displaying the text \"Hi there, welcome to this task.\" and a header displaying the NPC's name \"Assist Cube\".", "The right side of the text box shows an image of the Assist Cube, a square with a smiling face.", "There are two buttons that display answer options, i.e., \"Why am I here?\" and \"Who are you?\""], "caption": "The 3D platformer game.                                                   (b) Exemplary dialogue.Figure 1: We used a 3D platformer game as a base for our study (a), featuring dialogues with the Assist Cube as an NPC, as a portrait and with a physical avatar (b).Multiple choice dialogue response options that are similar to Likert-type scale options. These questions comprised assessment of the states fun, boredom, and frustration, the players\u2019 attitude towards games (\"Do you like games?\"), their attitude towards chal- lenge in games (\"What do you think about challenge in games?\"), and their subjective performance (\"How do you feel about your per- formance so far?\"). They aimed to assess data quality by testing whether 5-point scale responses embedded in-game result in the same responses as assessed post-game, in particular to evaluate the validity to ground truth measurements that are commonly em- ployed in user studies. To account for the trade-of between suitable phrasing for dialogues and assessment (cf. Discussion on Dialogue Design), we chose a consistent prompt/answer format involving open questions instead of statements (e.g., \"How do you currently feel?\") and answer options including the construct to measure (e.g., \"I am very frustrated.\"). We chose to consistently measure strengths of constructs using the modifcations \"very\", \"quite\", \"moderate\", \"slight\", and \"not\", adapted to suit the specifc answer options. See Figure 2a for an example.We used open response formats to assess the players\u2019 will- ingness to provide information in a diferent format. We employed two text felds that asked to describe an embarrassing in-game fail- ure and their life situation\u2014to assess their willingness to describe game-relevant information and information that transcends the current play situation.We used repeated 5-point scales to assess the players\u2019 willing- ness to answer multiple single item questions in a row. We chose to use the Digital Games Motivation Scale (DGMS) [21] because of its length (36 items), which we deemed disproportionately long for in-game assessment, and its content (game motivations) to uphold the impression of relevance for this study. We did not evaluate the players\u2019 answers on this scale but rather were interested in the amount of questions players were willing to answer voluntarily. For that purpose, this multiple choice block of questions was employed at the end of the study. Players were transported to a fourth level to give the impression that the game continued. The user interface displayed a Continue Playing button similar to how players can skip in-game assessments in commercial games. We were interested in the amount of items players answered before wanting to continue play.Facilitating Afliation Through Dialogues. We employed diferent methods to facilitate afliation towards the Assist Cube in an efort to nudge them to be more cooperative. These methods comprised the Assist Cube providing information about itself (e.g., its age) before asking the players about their age. This was intended to trigger modeling behaviour in the players, i.e., that they similarly would be willing to disclose the same information. The Assist Cube had a smiling face design and used empathetic responses to foster the players\u2019 trust, e.g., by highlighting that the players\u2019 reported failure was nothing to feel bad about.", "local_uri": ["f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_002.jpg", "f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "Gathering Self-Report Data in Games Through NPC Dialogues: Effects on Data Quality, Data Quantity, Player Experience, and Information Intimacy", "pdf_hash": "f6adebe7eecae77ae926fe3b68c655318c15a8ef", "year": 2021, "venue": "CHI", "alt_text": "Figure 2a: A screenshot of the NPQ condition. A textbox shows a dialogue similar to the regular game dialogue. It shows the text \"This concludes this part. How do you feel?\", the Assist Cube's face, and it's name. There are five buttons displaying text: \"I am very frustrated\", \"I am quite frustrated.\", I am moderately frustrated.\", \"I am slightly frustrated.\", and \"I am not frustrated.\" In the background, a wall from the game's virtual environment is visible. Figure 2b: A screenshot from the TOQ condition. A fullscreen, single color panel is visible that is overlayed on the virtual environment. It shows the text \"How do you currently feel?\" at the top. There are five lines, each with a radio button and a line of text. The texts are: \"I am very frustrated\", \"I am quite frustrated.\", I am moderately frustrated.\", \"I am slightly frustrated.\", and \"I am not frustrated.\" At the bottom of the panel, there is a button with the text \"Submit\".", "levels": null, "corpus_id": 233987327, "sentences": ["Figure 2a: A screenshot of the NPQ condition.", "A textbox shows a dialogue similar to the regular game dialogue.", "It shows the text \"This concludes this part.", "How do you feel?\", the Assist Cube's face, and it's name.", "There are five buttons displaying text: \"I am very frustrated\", \"I am quite frustrated.\", I am moderately frustrated.\", \"I am slightly frustrated.\", and \"I am not frustrated.\" In the background, a wall from the game's virtual environment is visible.", "Figure 2b: A screenshot from the TOQ condition.", "A fullscreen, single color panel is visible that is overlayed on the virtual environment.", "It shows the text \"How do you currently feel?\" at the top.", "There are five lines, each with a radio button and a line of text.", "The texts are: \"I am very frustrated\", \"I am quite frustrated.\", I am moderately frustrated.\", \"I am slightly frustrated.\", and \"I am not frustrated.\" At the bottom of the panel, there is a button with the text \"Submit\"."], "caption": "NPQ condition.                                                                   (b) TOQ condition.", "local_uri": ["f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_004.jpg", "f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Gathering Self-Report Data in Games Through NPC Dialogues: Effects on Data Quality, Data Quantity, Player Experience, and Information Intimacy", "pdf_hash": "f6adebe7eecae77ae926fe3b68c655318c15a8ef", "year": 2021, "venue": "CHI", "alt_text": "Figure 3: Three images with blocks of barplots that show differences between conditions. The first image contrasts the PXI measures showing consistently higher average scores for the NPQ condition. The second image shows scores almost identical bars for Disclosure Failure WC, higher scores for Questionnaire on Disclosure Life WC, and higher scores for NPQ on QuestionsAnswered. The third image shows that Questionnaire had higher scores on objective intimacy while scores on perceived intimacy were substantially higher for NPQ.", "levels": [[1], [3, 1], [3, 1], [3, 1]], "corpus_id": 233987327, "sentences": ["Figure 3: Three images with blocks of barplots that show differences between conditions.", "The first image contrasts the PXI measures showing consistently higher average scores for the NPQ condition.", "The second image shows scores almost identical bars for Disclosure Failure WC, higher scores for Questionnaire on Disclosure Life WC, and higher scores for NPQ on QuestionsAnswered.", "The third image shows that Questionnaire had higher scores on objective intimacy while scores on perceived intimacy were substantially higher for NPQ."], "caption": "", "local_uri": ["f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_006.png", "f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_007.png", "f6adebe7eecae77ae926fe3b68c655318c15a8ef_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Input Accessibility: A Large Dataset and Summary Analysis of Age, Motor Ability and Input Performance", "pdf_hash": "a38749fbb390ae3939225efb60a72157cfe7c396", "year": 2020, "venue": "ASSETS", "alt_text": "The four tasks: Pointing shows a start circle that users click or tap first, followed by a target circle to click second. Dragging shows a start target that must be acquired and dragged to a second target circle. For crossing, users position their cursor or finger within a start circle, then move across a target line. For steering, a column is displayed with a start circle connected to one end; users position their cursor or finger within a start circle then move through the column and out the end.", "levels": null, "corpus_id": 226068671, "sentences": ["The four tasks: Pointing shows a start circle that users click or tap first, followed by a target circle to click second.", "Dragging shows a start target that must be acquired and dragged to a second target circle.", "For crossing, users position their cursor or finger within a start circle, then move across a target line.", "For steering, a column is displayed with a start circle connected to one end; users position their cursor or finger within a start circle then move through the column and out the end."], "caption": "", "local_uri": ["a38749fbb390ae3939225efb60a72157cfe7c396_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Input Accessibility: A Large Dataset and Summary Analysis of Age, Motor Ability and Input Performance", "pdf_hash": "a38749fbb390ae3939225efb60a72157cfe7c396", "year": 2020, "venue": "ASSETS", "alt_text": "Scatterplots showing the relationship between age and mean trial time for each of the four tasks.  For pointing, a fit line for participants with motor impairments goes from about 1 second at the younger end to about 1.2 seconds at the older end, and a fit line for participants without motor impairments goes from about 0.7 seconds at the younger end to about 1 second at the older end.  For dragging, a fit line for participants with motor impairments goes from about 1 second at the younger end to almost 2 seconds at the older end, and a fit line for participants without motor impairments goes from under 1 second at the younger end to about 1.5 at the older end.  For crossing, a fit line for participants with motor impairments goes from about 1 second at the younger end to about 1.2 seconds at the older end, and a fit line for participants without motor impairments goes from about 0.7 seconds at the younger end to about 1.2 seconds at the older end.  For steering, a fit line for participants with motor impairments goes from just under 2 seconds at the younger end to just over 2 seconds at the older end, and a fit line for participants without motor impairments goes from under 1.5 seconds at the younger end to about 2 seconds at the older end.", "levels": [[1], [3, 2], [3, 2], [3, 2], [3, 2]], "corpus_id": 226068671, "sentences": ["Scatterplots showing the relationship between age and mean trial time for each of the four tasks.", "For pointing, a fit line for participants with motor impairments goes from about 1 second at the younger end to about 1.2 seconds at the older end, and a fit line for participants without motor impairments goes from about 0.7 seconds at the younger end to about 1 second at the older end.", "For dragging, a fit line for participants with motor impairments goes from about 1 second at the younger end to almost 2 seconds at the older end, and a fit line for participants without motor impairments goes from under 1 second at the younger end to about 1.5 at the older end.", "For crossing, a fit line for participants with motor impairments goes from about 1 second at the younger end to about 1.2 seconds at the older end, and a fit line for participants without motor impairments goes from about 0.7 seconds at the younger end to about 1.2 seconds at the older end.", "For steering, a fit line for participants with motor impairments goes from just under 2 seconds at the younger end to just over 2 seconds at the older end, and a fit line for participants without motor impairments goes from under 1.5 seconds at the younger end to about 2 seconds at the older end."], "caption": "", "local_uri": ["a38749fbb390ae3939225efb60a72157cfe7c396_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Input Accessibility: A Large Dataset and Summary Analysis of Age, Motor Ability and Input Performance", "pdf_hash": "a38749fbb390ae3939225efb60a72157cfe7c396", "year": 2020, "venue": "ASSETS", "alt_text": "Scatterplots showing the relationship between age and error rate for each of the four tasks.  For pointing, a fit line for participants with motor impairments goes from about 4% to about 2.5% from the younger to older ends, and a fit line for participants without motor impairments goes from about 2.5% to about 2% from the younger to older ends.  For dragging, a fit line for participants with motor impairments goes from about 2.5% to about 2% from the younger to older ends, and a fit line for participants without motor impairments goes from about 1.5% to about 1% from the younger to older ends.  For crossing, a fit line for participants with motor impairments goes from about 10% to about 6% from the younger to older ends, and a fit line for participants without motor impairments goes from about 8.5% to about 2% from the younger to older ends.  For steering, a fit line for participants with motor impairments goes from about 15% to about 15% from the younger to older ends, and a fit line for participants without motor impairments goes from about 8% to about 10% from the younger to older ends.", "levels": [[1], [3, 2], [3, 2], [3, 2], [3, 2]], "corpus_id": 226068671, "sentences": ["Scatterplots showing the relationship between age and error rate for each of the four tasks.", "For pointing, a fit line for participants with motor impairments goes from about 4% to about 2.5% from the younger to older ends, and a fit line for participants without motor impairments goes from about 2.5% to about 2% from the younger to older ends.", "For dragging, a fit line for participants with motor impairments goes from about 2.5% to about 2% from the younger to older ends, and a fit line for participants without motor impairments goes from about 1.5% to about 1% from the younger to older ends.", "For crossing, a fit line for participants with motor impairments goes from about 10% to about 6% from the younger to older ends, and a fit line for participants without motor impairments goes from about 8.5% to about 2% from the younger to older ends.", "For steering, a fit line for participants with motor impairments goes from about 15% to about 15% from the younger to older ends, and a fit line for participants without motor impairments goes from about 8% to about 10% from the younger to older ends."], "caption": "", "local_uri": ["a38749fbb390ae3939225efb60a72157cfe7c396_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Input Accessibility: A Large Dataset and Summary Analysis of Age, Motor Ability and Input Performance", "pdf_hash": "a38749fbb390ae3939225efb60a72157cfe7c396", "year": 2020, "venue": "ASSETS", "alt_text": "Two scatterplots showing age and mean trial time relationship for the 32 fastest and 32 slowest participants. The fastest participants are mostly in the 18-40 range, with 3 people older than that, and include 7 participants who had reported motor impairments. The slowest participants are more dispersed in age, from just over 25 to almost 70, and include 5 participants without motor impairments.", "levels": [[1], [3, 2], [3, 2]], "corpus_id": 226068671, "sentences": ["Two scatterplots showing age and mean trial time relationship for the 32 fastest and 32 slowest participants.", "The fastest participants are mostly in the 18-40 range, with 3 people older than that, and include 7 participants who had reported motor impairments.", "The slowest participants are more dispersed in age, from just over 25 to almost 70, and include 5 participants without motor impairments."], "caption": "", "local_uri": ["a38749fbb390ae3939225efb60a72157cfe7c396_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "pdf_hash": "05300590913007eb710cd89f5d373e1ec0833bfa", "year": 2020, "venue": "CHI", "alt_text": "Illustration of our data as heatmaps of finger touchpoints. All present typing of the same sentence by a different participant: one (index) finger with no typing errors, two thumbs with no errors, one finger with errors, and two thumbs with errors. Glances at the text-entry area increase with the number of errors made, and error correction is visible as touches of Backspace. In two-thumb typing, visual guidance of the fingers is less in demand, so the gaze covers smaller areas of the keyboard.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218483471, "sentences": ["Illustration of our data as heatmaps of finger touchpoints.", "All present typing of the same sentence by a different participant: one (index) finger with no typing errors, two thumbs with no errors, one finger with errors, and two thumbs with errors.", "Glances at the text-entry area increase with the number of errors made, and error correction is visible as touches of Backspace.", "In two-thumb typing, visual guidance of the fingers is less in demand, so the gaze covers smaller areas of the keyboard."], "caption": "Figure 1. Illustration of our data as heatmaps of \ufb01nger touchpoints (blue for right index \ufb01nger or thumb, and red for left thumb) and eye movements (green). All present typing of the same sentence by a different participant: one (index) \ufb01nger with no typing errors, two thumbs with no errors, one \ufb01nger with errors, and two thumbs with errors. Glances at the text-entry area increase with the number of errors made, and error correction is visible as touches of Backspace. In two-thumb typing, visual guidance of the \ufb01ngers is less in demand, so the gaze covers smaller areas of the keyboard.", "local_uri": ["05300590913007eb710cd89f5d373e1ec0833bfa_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "pdf_hash": "05300590913007eb710cd89f5d373e1ec0833bfa", "year": 2020, "venue": "CHI", "alt_text": "Two images: a view for calibrating the eye tracker on the smartphone; user interface of the typing task. Shown also are calibration aids.", "levels": null, "corpus_id": 218483471, "sentences": ["Two images: a view for calibrating the eye tracker on the smartphone; user interface of the typing task.", "Shown also are calibration aids."], "caption": "Figure 2. Left: A view for calibrating the eye tracker. Right: User inter- face of the typing task. The green boxes are for eye-tracking purposes.", "local_uri": ["05300590913007eb710cd89f5d373e1ec0833bfa_Image_002.png"], "annotated": false, "compound": false}
{"title": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "pdf_hash": "05300590913007eb710cd89f5d373e1ec0833bfa", "year": 2020, "venue": "CHI", "alt_text": "Posture for holding the device and sitting during the experiment,  showing grips for one and two finger typing, and a wooden block on top of the phone for motion tracking purposes.", "levels": null, "corpus_id": 218483471, "sentences": ["Posture for holding the device and sitting during the experiment,  showing grips for one and two finger typing, and a wooden block on top of the phone for motion tracking purposes."], "caption": "Figure 3. Posture for holding the device and sitting during the experi- ment. Shown are grips for one- and two-\ufb01nger typing. The block above the device is for tracking the phone position.", "local_uri": ["05300590913007eb710cd89f5d373e1ec0833bfa_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "pdf_hash": "05300590913007eb710cd89f5d373e1ec0833bfa", "year": 2020, "venue": "CHI", "alt_text": "Heatmap showing finger-to-key mapping in two-thumb touch data of three participants (all sentences aggregated). These patters are representative of a tendency we found in the data for the right thumb to cover more keys than the left. The right hand was the dominant hand for most of the participants, but the same pattern was observed also for the left-handed participants.", "levels": [[-1], [-1], [-1]], "corpus_id": 218483471, "sentences": ["Heatmap showing finger-to-key mapping in two-thumb touch data of three participants (all sentences aggregated).", "These patters are representative of a tendency we found in the data for the right thumb to cover more keys than the left.", "The right hand was the dominant hand for most of the participants, but the same pattern was observed also for the left-handed participants."], "caption": "", "local_uri": ["05300590913007eb710cd89f5d373e1ec0833bfa_Image_005.jpg", "05300590913007eb710cd89f5d373e1ec0833bfa_Image_006.jpg", "05300590913007eb710cd89f5d373e1ec0833bfa_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "pdf_hash": "05300590913007eb710cd89f5d373e1ec0833bfa", "year": 2020, "venue": "CHI", "alt_text": "The impact of finger preparation on WPM by the task condition. The x-axis shows distance of finger from its next key, divided by the distance of the current and the next key (pressed by the same finger).", "levels": null, "corpus_id": 218483471, "sentences": ["The impact of finger preparation on WPM by the task condition.", "The x-axis shows distance of finger from its next key, divided by the distance of the current and the next key (pressed by the same finger)."], "caption": "Figure 5. The impact of \ufb01nger preparation on WPM by the task condi- tion. The x axis shows distance of \ufb01nger from its next key, divided by the distance of the current and the next key (pressed by the same \ufb01nger).", "local_uri": ["05300590913007eb710cd89f5d373e1ec0833bfa_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities", "pdf_hash": "eee3c529456963267f3fd804fbe7bd6883d3b299", "year": 2021, "venue": "CHI", "alt_text": "Figure 1: TapeBlocks: stacks of blocks, a  character, materials and components. Samples of TapeBlock LED and power block stack, a sample of a power button and vibration motor stack, in addition to blank blocks, components and a character.", "levels": null, "corpus_id": 233987118, "sentences": ["Figure 1: TapeBlocks: stacks of blocks, a  character, materials and components.", "Samples of TapeBlock LED and power block stack, a sample of a power button and vibration motor stack, in addition to blank blocks, components and a character."], "caption": "", "local_uri": ["eee3c529456963267f3fd804fbe7bd6883d3b299_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities", "pdf_hash": "eee3c529456963267f3fd804fbe7bd6883d3b299", "year": 2021, "venue": "CHI", "alt_text": "Types of TapeBlocks: power, button, light resistor, tilt switch, buzzer, fan, vibration motor, RGB colour change and a bicolour LEDs TapeBlock Types including power, sensor and output. Each of the individual blocks in standing upright in a line across the page", "levels": null, "corpus_id": 233987118, "sentences": ["Types of TapeBlocks: power, button, light resistor, tilt switch, buzzer, fan, vibration motor, RGB colour change and a bicolour LEDs TapeBlock Types including power, sensor and output.", "Each of the individual blocks in standing upright in a line across the page"], "caption": "", "local_uri": ["eee3c529456963267f3fd804fbe7bd6883d3b299_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities", "pdf_hash": "eee3c529456963267f3fd804fbe7bd6883d3b299", "year": 2021, "venue": "CHI", "alt_text": "Making an LED TapeBlocks by inserting the LED under the conductive tape} There are seven images of the process for making an LED TapeBlock starting from having a block and LED and wrapping the two pieces of tape around. The final image shows the LED with a power block working.", "levels": null, "corpus_id": 233987118, "sentences": ["Making an LED TapeBlocks by inserting the LED under the conductive tape} There are seven images of the process for making an LED TapeBlock starting from having a block and LED and wrapping the two pieces of tape around.", "The final image shows the LED with a power block working."], "caption": "", "local_uri": ["eee3c529456963267f3fd804fbe7bd6883d3b299_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities", "pdf_hash": "eee3c529456963267f3fd804fbe7bd6883d3b299", "year": 2021, "venue": "CHI", "alt_text": "Tools for the TapeBlock evaluations sessions Miro Board with yellow and blue sticky notes The board has a range of sticky notes with current making experience and creativity on blue and yellow sticky notes. A screenshot of the miroboard, an online, collaborative, virtual whiteboard. The researchers developed several of these with the NGO coaches and teachers to understand their current practices related to making, their thoughts on the specific TapeBlocks and their ideas about how TapeBlocks might fit into the programs that they offered.", "levels": null, "corpus_id": 233987118, "sentences": ["Tools for the TapeBlock evaluations sessions Miro Board with yellow and blue sticky notes The board has a range of sticky notes with current making experience and creativity on blue and yellow sticky notes.", "A screenshot of the miroboard, an online, collaborative, virtual whiteboard.", "The researchers developed several of these with the NGO coaches and teachers to understand their current practices related to making, their thoughts on the specific TapeBlocks and their ideas about how TapeBlocks might fit into the programs that they offered."], "caption": "", "local_uri": ["eee3c529456963267f3fd804fbe7bd6883d3b299_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities", "pdf_hash": "eee3c529456963267f3fd804fbe7bd6883d3b299", "year": 2021, "venue": "CHI", "alt_text": "Sample of Made TapeBlocks (a) Front and back of tilt switch sensor block Tilt switch block. The front of the tilt switch block has the line of tape with a gap and the switch and a second line of conductive tape. The back of the block has a gap and a second line of conductive tape  (b) Front and back of integrated block The integrated block. The front of the integrated block has the line of tape with and LED. The back of the block has a battery holder  (c) TapeBlocks stack TapeBlock Stack with the light one. Including a red power block a the bottom of the stack with a 3V battery, a yellow light sensor in the middle and a green LED block a the top of the stack.", "levels": null, "corpus_id": 233987118, "sentences": ["Sample of Made TapeBlocks (a) Front and back of tilt switch sensor block Tilt switch block.", "The front of the tilt switch block has the line of tape with a gap and the switch and a second line of conductive tape.", "The back of the block has a gap and a second line of conductive tape  (b) Front and back of integrated block The integrated block.", "The front of the integrated block has the line of tape with and LED.", "The back of the block has a battery holder  (c) TapeBlocks stack TapeBlock Stack with the light one.", "Including a red power block a the bottom of the stack with a 3V battery, a yellow light sensor in the middle and a green LED block a the top of the stack."], "caption": "", "local_uri": ["eee3c529456963267f3fd804fbe7bd6883d3b299_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities", "pdf_hash": "eee3c529456963267f3fd804fbe7bd6883d3b299", "year": 2021, "venue": "CHI", "alt_text": "Example of TapeBlock Character. A white furry toys. A White furry toy, with light up noses with cool haircut.", "levels": null, "corpus_id": 233987118, "sentences": ["Example of TapeBlock Character.", "A white furry toys.", "A White furry toy, with light up noses with cool haircut."], "caption": "", "local_uri": ["eee3c529456963267f3fd804fbe7bd6883d3b299_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "There are 4 figures in total.       In subfigure a, a 2D 6x6 color palette is attached to the left controller, and a brush is attached to the right controller pointing towards the palette.       In subfigure b, a 2D 4x4 menu container is attached to the left controller, and a ray from the right controller is pointing towards a function called ''Rainbow''.       In subfigure c, a music app (currently playing a song) and a running app (currently showing the runner's route on a map) are rendered on opposite sides of the user's left arm, and the user can switch between these apps by rotating her left arm.      In subfigure d, the user controls the music app by swiping her left arm with her right index finger, and the music app control buttons (previous song, pause, and next song) appear on the user's left arm.", "levels": null, "corpus_id": 233987272, "sentences": ["There are 4 figures in total.", "In subfigure a, a 2D 6x6 color palette is attached to the left controller, and a brush is attached to the right controller pointing towards the palette.", "In subfigure b, a 2D 4x4 menu container is attached to the left controller, and a ray from the right controller is pointing towards a function called ''Rainbow''.", "In subfigure c, a music app (currently playing a song) and a running app (currently showing the runner's route on a map) are rendered on opposite sides of the user's left arm, and the user can switch between these apps by rotating her left arm.", "In subfigure d, the user controls the music app by swiping her left arm with her right index finger, and the music app control buttons (previous song, pause, and next song) appear on the user's left arm."], "caption": "", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_002.png"], "annotated": false, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "There are 4 subfigures in total.       In subfigure a, two virtual hand models are rendered, with 3D printed pads wrapped around. There are 3x12 holes on the 3D printed pads so that retroreflective markers can be easily installed with different patterns.      For the left hand, one pad is on top of the arm, and the other is on the bottom.       For the right hand, one pad is on top of the hand, and the other is mounted on the first joint of the index finger.       An Oculus remote controller is held in the right palm.       In subfigure b, a participant is in a start posture, where her left arm is at her side in a green bounding box visible in VR, and her right hand touching a virtual start button.      In subfigure c, a participant is wearing these pads, as well as the Oculus headset, and she is attempting to click a virtual target.       Subfigure d shows the corresponding scene (subfigure c) captured in VR, where the user's right index finger intersects with a blue sphere on her left, and the sphere is now highlighted.", "levels": null, "corpus_id": 233987272, "sentences": ["There are 4 subfigures in total.", "In subfigure a, two virtual hand models are rendered, with 3D printed pads wrapped around.", "There are 3x12 holes on the 3D printed pads so that retroreflective markers can be easily installed with different patterns.", "For the left hand, one pad is on top of the arm, and the other is on the bottom.", "For the right hand, one pad is on top of the hand, and the other is mounted on the first joint of the index finger.", "An Oculus remote controller is held in the right palm.", "In subfigure b, a participant is in a start posture, where her left arm is at her side in a green bounding box visible in VR, and her right hand touching a virtual start button.", "In subfigure c, a participant is wearing these pads, as well as the Oculus headset, and she is attempting to click a virtual target.", "Subfigure d shows the corresponding scene (subfigure c) captured in VR, where the user's right index finger intersects with a blue sphere on her left, and the sphere is now highlighted."], "caption": "", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_003.png"], "annotated": false, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "There are 5 subfigures in total.       In subfigure a, there is a conical frustum simulating the virtual grid centered to the user's forearm.       Subfigure b shows the longitude dimension, where the longitude levels are surrounding the arm in cardinal and ordinal directions, i.e., N, NW, W, SW, S, SE, E, and NE for 0-7, respectively.      Subfigure c shows the latitude levels, wrist (0), forearm (1), and elbow (2), with 8cm gaps between levels.       Subfigure d shows the height levels, close (0), medium (1), and far (2), with 4cm, 12cm, and 20cm distance from the skin.      Subfigure e shows how to locate the 8 longitude levels given the tracking data of the 3D printed pads mounted to the hand. The two pads are mounted to the dorsal side and ventral side. If a vector is drawn perpendicular to these pads, then the longitude level 0 has an angle of 22.5 degrees with this aforementioned vector.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987272, "sentences": ["There are 5 subfigures in total.", "In subfigure a, there is a conical frustum simulating the virtual grid centered to the user's forearm.", "Subfigure b shows the longitude dimension, where the longitude levels are surrounding the arm in cardinal and ordinal directions, i.e., N, NW, W, SW, S, SE, E, and NE for 0-7, respectively.", "Subfigure c shows the latitude levels, wrist (0), forearm (1), and elbow (2), with 8cm gaps between levels.", "Subfigure d shows the height levels, close (0), medium (1), and far (2), with 4cm, 12cm, and 20cm distance from the skin.", "Subfigure e shows how to locate the 8 longitude levels given the tracking data of the 3D printed pads mounted to the hand.", "The two pads are mounted to the dorsal side and ventral side.", "If a vector is drawn perpendicular to these pads, then the longitude level 0 has an angle of 22.5 degrees with this aforementioned vector."], "caption": "Figure 3: (a) An illustration of the \u2018conical frustum\u2019 grids and depictions of how the (b) longitude, (c) latitude, and (d) height target locations were situated around and along the arm. (e) An illustration of the mapping of the eight longitude levels to the tracking results of the left arm, when the arm was held forward with the 3D printed pads on the (i) dorsal side and (ii) ventral side parallel to each other. The longitude level 0 (N) has an angle of 22.5 degrees perpendicular to the two pads.", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "The first subfigure shows bar charts of Latitude levels wrist, forearm, and elbow combined with awareness schemes known and unknown.       The height of the bar chart represents the throughput result of each combination:       wrist x known: 3.047, wrist x unknown: 2.697, forearm x known: 3.020, forearm x unknown: 2.584, elbow x known: 2.822, elbow x unknown: 2.265.       Gray lines indicate significant pairs between wrist x known vs. elbow x known, wrist x unknown vs. forearm x unknown, wrist x unknown vs. elbow x unknown, forearm x known vs. elbow x known, and forearm x unknown vs. elbow x unknown.      The second subfigure shows bar charts of Height levels close, medium, and far combined with awareness schemes known and unknown.       The height of the bar chart represents the throughput result of each combination:       close x known: 3.059, close x unknown: 2.660, medium x known: 3.014, medium x unknown: 2.565, far x known: 2.815, far x unknown: 2.321.      Gray lines indicate significant pairs between close x known vs. far x known, close x unknown vs. medium x unknown, close x unknown vs. far x unknown, medium x known vs. far x known, and medium x unknown vs. far x unknown.", "levels": [[1], [2, 1], [1], [1], [2, 1], [1]], "corpus_id": 233987272, "sentences": ["The first subfigure shows bar charts of Latitude levels wrist, forearm, and elbow combined with awareness schemes known and unknown.", "The height of the bar chart represents the throughput result of each combination:       wrist x known: 3.047, wrist x unknown: 2.697, forearm x known: 3.020, forearm x unknown: 2.584, elbow x known: 2.822, elbow x unknown: 2.265.", "Gray lines indicate significant pairs between wrist x known vs. elbow x known, wrist x unknown vs. forearm x unknown, wrist x unknown vs. elbow x unknown, forearm x known vs. elbow x known, and forearm x unknown vs. elbow x unknown.", "The second subfigure shows bar charts of Height levels close, medium, and far combined with awareness schemes known and unknown.", "The height of the bar chart represents the throughput result of each combination:       close x known: 3.059, close x unknown: 2.660, medium x known: 3.014, medium x unknown: 2.565, far x known: 2.815, far x unknown: 2.321.", "Gray lines indicate significant pairs between close x known vs. far x known, close x unknown vs. medium x unknown, close x unknown vs. far x unknown, medium x known vs. far x known, and medium x unknown vs. far x unknown."], "caption": "", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "The bar chart shows the throughput result of each combination of longitude levels (0-7) and latitude levels (0-2).       The values are (ordered in wrist, forearm, and elbow):       (N) 2.873, 2.827, 2.644;       (NW) 2.677, 2.673, 2.503;       (W) 2.765, 2.649, 2.386;       (SW) 2.880, 2.766, 2.373;      (S) 2.921, 2.790, 2.422;      (SE) 2.965, 2.809, 2.561;      (E) 2.951, 2.936, 2.704;      (NE) 2.943, 2.964, 2.752.      Gray lines indicate significant pairs between W x Wrist vs. W x Elbow, W x Forearm vs. W x Elbow, SW x Wrist vs. SW x Elbow, SW x Forearm vs. SW x Elbow, S x Wrist vs. S x Elbow, S x Forearm vs. S x Elbow, and SE x Wrist vs. SE x Elbow.", "levels": [[1], [2], [1]], "corpus_id": 233987272, "sentences": ["The bar chart shows the throughput result of each combination of longitude levels (0-7) and latitude levels (0-2).", "The values are (ordered in wrist, forearm, and elbow):       (N) 2.873, 2.827, 2.644;       (NW) 2.677, 2.673, 2.503;       (W) 2.765, 2.649, 2.386;       (SW) 2.880, 2.766, 2.373;      (S) 2.921, 2.790, 2.422;      (SE) 2.965, 2.809, 2.561;      (E) 2.951, 2.936, 2.704;      (NE) 2.943, 2.964, 2.752.", "Gray lines indicate significant pairs between W x Wrist vs. W x Elbow, W x Forearm vs. W x Elbow, SW x Wrist vs. SW x Elbow, SW x Forearm vs. SW x Elbow, S x Wrist vs. S x Elbow, S x Forearm vs. S x Elbow, and SE x Wrist vs. SE x Elbow."], "caption": "", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_016.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "The four subfigures present the main effects of the four independent variables.      The TP results are:      Subfigure a (Awareness Scheme): Known (2.963), Unknown (2.515), with significant difference;      Subfigure b (Longitude): N (2.782), NW (2.618), W (2.600), SW (2.673), S (2.711), SE (2.778), E (2.864), and NE (2.886), with significant differences between: N vs. NW, NW vs. E, NW vs. NE, W vs. E, W vs. NE, SW vs. E, SW vs. NE, and S vs. E;      Subfigure c (Latitude): Wrist (2.872), Forearm (2.802), and Elbow (2.543), with significant differences between all pairwise comparisons;      Subfigure d (Height): Close (2.859), Medium (2.790), and Far (2.568), with significant differences between all pairwise comparisons.", "levels": [[1], [2]], "corpus_id": 233987272, "sentences": ["The four subfigures present the main effects of the four independent variables.", "The TP results are:      Subfigure a (Awareness Scheme): Known (2.963), Unknown (2.515), with significant difference;      Subfigure b (Longitude): N (2.782), NW (2.618), W (2.600), SW (2.673), S (2.711), SE (2.778), E (2.864), and NE (2.886), with significant differences between: N vs. NW, NW vs. E, NW vs. NE, W vs. E, W vs. NE, SW vs. E, SW vs. NE, and S vs. E;      Subfigure c (Latitude): Wrist (2.872), Forearm (2.802), and Elbow (2.543), with significant differences between all pairwise comparisons;      Subfigure d (Height): Close (2.859), Medium (2.790), and Far (2.568), with significant differences between all pairwise comparisons."], "caption": "Figure 6: Mean throughput (TP) data segmented by the (a) Awareness Scheme, (b) Longitude, (c) Latitude, and (d) Height. Gray lines indicate signifcant pairwise comparisons (\u03b1 = .05, .0018, .017, and .017, respectively). Error bars show 95% CIs.", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_017.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "The 3D TP heatmap unwrapped at each height level.       As a result, there are 6 subfigures (3 height levels x 2 awareness schemes), and within each subfigure, there are 24 cells (3 latitude levels x 8 longitude levels).      The TP values are (rows: latitude levels 0, 1, 2; columns: longitude levels 2, 3, 4, ..., 7, 0, 1):      (top-left: Close x Unknown)      2.67, 2.90, 2.97, 3.03, 2.87, 2.88, 2.71, 2.57;      2.67, 2.79, 2.77, 2.84, 2.86, 2.86, 2.65, 2.45;      2.25, 2.37, 2.41, 2.51, 2.60, 2.52, 2.34, 2.36.      (top-center: Medium x Unknown)      2.68, 2.70, 2.84, 2.90, 2.85, 2.81, 2.67, 2.50; 2.48, 2.58, 2.51, 2.65, 2.80, 2.73, 2.51, 2.61;      2.26, 2.11, 2.03, 2.36, 2,48, 2.69, 2.48, 2.34.      (top-right: Far x Unknown)      2.42, 2.49, 2.47, 2.64, 2.64, 2.64, 2.55, 2.35;      2.27, 2.29, 2.26, 2.48, 2.60, 2.64, 2.40, 2.31;      1.97, 1.73, 1.87, 1.99, 2.26, 2.26, 2.15, 2.00.      (bottom-left: Close x Known)      3.05, 3.22, 3.20, 3.17, 3.07, 3.22, 3.19, 2.93;      2.92, 3.11, 3.15, 3.10, 3.12, 3.24, 3.18, 2.88;      2.71, 2.83, 2.95, 3.06, 3.08, 3.06, 3.08, 2.89.      (bottom-center: Medium x Known)      2.91, 3.12, 3.17, 3.08, 3.26, 3.09, 3.11, 2.98;      2.85, 3.05, 3.21, 3.01, 3.15, 3.20, 3.27, 3.03;      2.65, 2.81, 2.80, 2.84, 2.91, 3.03, 3.04, 2.76.      (bottom-right: Far x Known)      2.87, 2.85, 2.87, 2.97, 3.02, 3.02, 3.01, 2.73;      2.72, 2.78, 2.83, 2.77, 3.08, 3.12, 2.95, 2.76;      2.47, 2.40, 2.48, 2.60, 2.89, 2.94, 2.78, 2.66.", "levels": null, "corpus_id": 233987272, "sentences": ["The 3D TP heatmap unwrapped at each height level.", "As a result, there are 6 subfigures (3 height levels x 2 awareness schemes), and within each subfigure, there are 24 cells (3 latitude levels x 8 longitude levels).", "The TP values are (rows: latitude levels 0, 1, 2; columns: longitude levels 2, 3, 4, ..., 7, 0, 1):      (top-left: Close x Unknown)      2.67, 2.90, 2.97, 3.03, 2.87, 2.88, 2.71, 2.57;      2.67, 2.79, 2.77, 2.84, 2.86, 2.86, 2.65, 2.45;      2.25, 2.37, 2.41, 2.51, 2.60, 2.52, 2.34, 2.36.      (top-center: Medium x Unknown)      2.68, 2.70, 2.84, 2.90, 2.85, 2.81, 2.67, 2.50; 2.48, 2.58, 2.51, 2.65, 2.80, 2.73, 2.51, 2.61;      2.26, 2.11, 2.03, 2.36, 2,48, 2.69, 2.48, 2.34.      (top-right: Far x Unknown)      2.42, 2.49, 2.47, 2.64, 2.64, 2.64, 2.55, 2.35;      2.27, 2.29, 2.26, 2.48, 2.60, 2.64, 2.40, 2.31;      1.97, 1.73, 1.87, 1.99, 2.26, 2.26, 2.15, 2.00.      (bottom-left: Close x Known)      3.05, 3.22, 3.20, 3.17, 3.07, 3.22, 3.19, 2.93;      2.92, 3.11, 3.15, 3.10, 3.12, 3.24, 3.18, 2.88;      2.71, 2.83, 2.95, 3.06, 3.08, 3.06, 3.08, 2.89.      (bottom-center: Medium x Known)      2.91, 3.12, 3.17, 3.08, 3.26, 3.09, 3.11, 2.98;      2.85, 3.05, 3.21, 3.01, 3.15, 3.20, 3.27, 3.03;      2.65, 2.81, 2.80, 2.84, 2.91, 3.03, 3.04, 2.76.      (bottom-right: Far x Known)      2.87, 2.85, 2.87, 2.97, 3.02, 3.02, 3.01, 2.73;      2.72, 2.78, 2.83, 2.77, 3.08, 3.12, 2.95, 2.76;      2.47, 2.40, 2.48, 2.60, 2.89, 2.94, 2.78, 2.66."], "caption": "", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_077.jpg"], "annotated": false, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "Photos taken during the study, showing postures of P4, P5, and P12 during the study.       In subfigure a, P4 is rotating his left arm so that the medial side is facing up. He is tapping a virtual target close to his elbow.      In subfigure b, P5 is lifting her left arm a little bit so she could tap on a virtual target located above her left wrist.      In subfigure c, P12 is bending his left arm inside in front of his body, so he could approach the target on the left side of his forearm.", "levels": null, "corpus_id": 233987272, "sentences": ["Photos taken during the study, showing postures of P4, P5, and P12 during the study.", "In subfigure a, P4 is rotating his left arm so that the medial side is facing up.", "He is tapping a virtual target close to his elbow.", "In subfigure b, P5 is lifting her left arm a little bit so she could tap on a virtual target located above her left wrist.", "In subfigure c, P12 is bending his left arm inside in front of his body, so he could approach the target on the left side of his forearm."], "caption": "Figure 8: Photos from the user study, demonstrating how P4, P5, and P12 were trying to center the target in front of their body for convenience and better visibility by (a) rotat- ing their arm, (b) lifting their arm, and (c) bending their arm.", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_078.png"], "annotated": false, "compound": false}
{"title": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "pdf_hash": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "year": 2021, "venue": "CHI", "alt_text": "Examples of arranging 5 UI controls around the arm using the throughput heatmap data.       In subfigure a, based on the unknown dataset, the 5 UI controls are placed at: SE x wrist x close; S x wrist x close; SE x wrist x medium; SW x wrist x close; NE x wrist x close.       In subfigure b, based on the known dataset, the same 5 controls are placed at: N x forearm x medium; E x wrist x medium; NE x forearm x close; SW x wrist x close; NE x wrist x close.       In Subfigure c, based on the known dataset with preference of forearm, the 5 UI controls are placed at: NE x forearm x close; NE x forearm x medium; NE x forearm x far; N x forearm x close; N x forearm x medium.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987272, "sentences": ["Examples of arranging 5 UI controls around the arm using the throughput heatmap data.", "In subfigure a, based on the unknown dataset, the 5 UI controls are placed at: SE x wrist x close; S x wrist x close; SE x wrist x medium; SW x wrist x close; NE x wrist x close.", "In subfigure b, based on the known dataset, the same 5 controls are placed at: N x forearm x medium; E x wrist x medium; NE x forearm x close; SW x wrist x close; NE x wrist x close.", "In Subfigure c, based on the known dataset with preference of forearm, the 5 UI controls are placed at: NE x forearm x close; NE x forearm x medium; NE x forearm x far; N x forearm x close; N x forearm x medium."], "caption": "", "local_uri": ["a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_092.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\u201cIt Feels Like I am Talking into a Void\u201d: Understanding Interaction Gaps in Synchronous Online Classrooms", "pdf_hash": "538460460d734a137cf6f6a87d57cf86c1a69a52", "year": 2021, "venue": "CHI", "alt_text": "A snapshot of a discussion-based lecture consisting of 21 students and 1 instructor. Only 8 students decided to share videos. A student with no video is speaking in the shot which is highlighted by a yellow border. Faces are blurred, and profile images and names are pseudonymized.", "levels": null, "corpus_id": 233987070, "sentences": ["A snapshot of a discussion-based lecture consisting of 21 students and 1 instructor.", "Only 8 students decided to share videos.", "A student with no video is speaking in the shot which is highlighted by a yellow border.", "Faces are blurred, and profile images and names are pseudonymized."], "caption": "", "local_uri": ["538460460d734a137cf6f6a87d57cf86c1a69a52_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "\u201cIt Feels Like I am Talking into a Void\u201d: Understanding Interaction Gaps in Synchronous Online Classrooms", "pdf_hash": "538460460d734a137cf6f6a87d57cf86c1a69a52", "year": 2021, "venue": "CHI", "alt_text": "24 Likert-scale prompts and the distribution of students' responses from 'Strongly Disagree' to 'Strongly Agree'. 80 percent of students identified that they do not share videos during lecture, despite almost 70 percent who have access to webcams. 78 percent of students strongly favored the chat feature.", "levels": [[1], [2], [2]], "corpus_id": 233987070, "sentences": ["24 Likert-scale prompts and the distribution of students' responses from 'Strongly Disagree' to 'Strongly Agree'.", "80 percent of students identified that they do not share videos during lecture, despite almost 70 percent who have access to webcams.", "78 percent of students strongly favored the chat feature."], "caption": "", "local_uri": ["538460460d734a137cf6f6a87d57cf86c1a69a52_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "How Context and User Behavior Affect Indoor Navigation Assistance for Blind People", "pdf_hash": "d6d375c0624a564514699a6cbebfc7d6313b3ba7", "year": 2018, "venue": "W4A", "alt_text": "This figure shows two images. In the first one it shows one white-cane user reaching a turning point, where she should turn left. Immediately after that left turn, she should turn right. The second image shows her passing the second turning point, which results in a late turn.", "levels": null, "corpus_id": 4830194, "sentences": ["This figure shows two images.", "In the first one it shows one white-cane user reaching a turning point, where she should turn left.", "Immediately after that left turn, she should turn right.", "The second image shows her passing the second turning point, which results in a late turn."], "caption": "(b)Figure 1: An example of two consecutive turns: Af\u00ad ter completing the \ufb01rst turn (a), separate instruc\u00ad tions can cause a late turn (b). Users can lever\u00ad age the knowledge of the next action, particularly in cases of short spatial and temporal proximity.", "local_uri": ["d6d375c0624a564514699a6cbebfc7d6313b3ba7_Image_001.png", "d6d375c0624a564514699a6cbebfc7d6313b3ba7_Image_002.png"], "annotated": false, "compound": true}
{"title": "How Context and User Behavior Affect Indoor Navigation Assistance for Blind People", "pdf_hash": "d6d375c0624a564514699a6cbebfc7d6313b3ba7", "year": 2018, "venue": "W4A", "alt_text": "In the first image, it shows one participant slightly veering in the direction of tables and chairs, in a wide corridor. The second image, shows one participant trying to go through a door after performing a 90 degree turn, when only a 30 degree turn would be required.", "levels": null, "corpus_id": 4830194, "sentences": ["In the first image, it shows one participant slightly veering in the direction of tables and chairs, in a wide corridor.", "The second image, shows one participant trying to go through a door after performing a 90 degree turn, when only a 30 degree turn would be required."], "caption": "(b)Figure 2: Two example scenarios from the user study which  motivate  our thematic analysis. (a) The user is directed to go forward, but veering in the open space results in confusion near the chair area on the right. (b) The user is directed to make a slight right turn in order to align with the new position of the corridor, yet an over-turn leads her to a door that she thought to be part of the path.", "local_uri": ["d6d375c0624a564514699a6cbebfc7d6313b3ba7_Image_003.jpg", "d6d375c0624a564514699a6cbebfc7d6313b3ba7_Image_004.png"], "annotated": false, "compound": true}
{"title": "FileWeaver: Flexible File Management with Automatic Dependency Tracking", "pdf_hash": "fdec60f955e657275d94ec2c239da063bb0da50e", "year": 2020, "venue": "UIST", "alt_text": "The representation of polymorphic files in the Graph View. On the left, the expanded view shows three variants of the file (fig2.png, fig2.jpg, fig2.svg) connected together by two blue edges. The three variants are represented by their media type icon. Fig2.svg is connected to the file beamer.tex by a black directed edge. On the right the collapsed view shows a superposition of media type icons, with the name fig2.gifc. This node is connected to the file beamer.tex with a black directed edge. blue edge, showing that it is the version currently in use. The two nodes on the right are selected, as shown by a red outline. Below the nodes is a display of the differences between the two selected files. Green and yellow highlights show", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 221897046, "sentences": ["The representation of polymorphic files in the Graph View.", "On the left, the expanded view shows three variants of the file (fig2.png, fig2.jpg, fig2.svg) connected together by two blue edges.", "The three variants are represented by their media type icon.", "Fig2.svg is connected to the file beamer.tex by a black directed edge.", "On the right the collapsed view shows a superposition of media type icons, with the name fig2.gifc.", "This node is connected to the file beamer.tex with a black directed edge.", "blue edge, showing that it is the version currently in use.", "The two nodes on the right are selected, as shown by a red outline.", "Below the nodes is a display of the differences between the two selected files.", "Green and yellow highlights show"], "caption": "Figure 2. Graph view for the scenario (see text)", "local_uri": ["fdec60f955e657275d94ec2c239da063bb0da50e_Image_005.jpg", "fdec60f955e657275d94ec2c239da063bb0da50e_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "FileWeaver: Flexible File Management with Automatic Dependency Tracking", "pdf_hash": "fdec60f955e657275d94ec2c239da063bb0da50e", "year": 2020, "venue": "UIST", "alt_text": "The figure shows the History View, with three rectangular nodes representing three versions of the file main.tex, connected from left to right by black directed edges. The text inside each node displays a short summary of changes (e.g. \"changed fig\") and a time reference (e.g. \"2 minutes ago\"). A blue rectangle node labeled 'master' below the rightmost node is connected to it by a thinwhich lines are different.", "levels": null, "corpus_id": 221897046, "sentences": ["The figure shows the History View, with three rectangular nodes representing three versions of the file main.tex, connected from left to right by black directed edges.", "The text inside each node displays a short summary of changes (e.g. \"changed fig\") and a time reference (e.g. \"2 minutes ago\").", "A blue rectangle node labeled 'master' below the rightmost node is connected to it by a thinwhich lines are different."], "caption": "Figure 4. History view showing the differences between two versions.", "local_uri": ["fdec60f955e657275d94ec2c239da063bb0da50e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "A photo showing a user interacting with ATK. The user sits at a table, stretch out his hands above the Leap Motion sensor, while the screen shows the software of ATK.", "levels": null, "corpus_id": 9349793, "sentences": ["A photo showing a user interacting with ATK.", "The user sits at a table, stretch out his hands above the Leap Motion sensor, while the screen shows the software of ATK."], "caption": "Figure 1: Experiment setup", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "The software interface of ATK. It shows the control buttons, current task, statues panel and the keyboard panel.", "levels": null, "corpus_id": 9349793, "sentences": ["The software interface of ATK.", "It shows the control buttons, current task, statues panel and the keyboard panel."], "caption": "Figure 2: Experiment software", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_002.png"], "annotated": false, "compound": false}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "Illustration of the relative coordinate system and the leap motion coordinate system.", "levels": [[1]], "corpus_id": 9349793, "sentences": ["Illustration of the relative coordinate system and the leap motion coordinate system."], "caption": "Relative coordinate system           (b) Leap Motion coordinate\u200csystemFigure 3: Two types of coordinate systemsTo clarify the geometry, we established two types of coordinate systems: Relative coordinate system and absolute coordinate system. Relative coordinate system follows the movement of hands, and describes the movement of \ufb01ngers regardless of hand translation and rotation. For each hand, the origin of relative coordinate system is the center of thepalm. X axis is parallel to the palm plane, and is positive in the direction of the pinky \ufb01nger. Y axis is perpendicular to the palm plane, and is positive in the direction of the hand back. Z axis is parallel to the direction of the hand, and is positive in the direction of the wrist (Figure 3a). To account for varying hand sizes, the relative coordinate was normalized according to the length of users\u2019 middle \ufb01ngers (Mean = 81mm, SD = 3.2mm).Absolute coordinate system describes the movement of the palm and \ufb01ngers relative to the home position. For each hand, the origin of absolute coordinate system is its 3D position when it is registered; the axes are parallel to the axes of the Leap Motion coordinate system (Figure 3b). We did not normalize the absolute coordinate, as it is affected by not only hand size, but also hand translation and \ufb01nger movement.As we did not have a tap detection algorithm at this stage, we manually labeled the data. We checked the relative coordinate of \ufb01ngers to determine taps, and labeled them with the target characters. We collected 3093 taps from all participants in total.Results\u200cFingertip Kinematics During TappingFigure 4: Illustration of \ufb01ngertip kinematics within a typical tap, shows relative coordinate (lower) and velocity (upper) in Y axis aligned by timeFigure 4 illustrates the relative coordinate and the velocity of \ufb01ngertip during a typical tap in Y axis. We determined the start and the end points of a tap to be at the moment where velocity is 0. We de\ufb01ne duration as the time elapse between start and end points, and amplitude as the difference between the highest and the lowest points within a tap.The average duration of a tap was 496ms (SD = 170), with an average amplitude of 69mm (SD = 23). It typically comprised a \ufb02exion phase and a stretch phase. The amplitude of the \ufb02exion phase was greater than that of the stretch phase, with mean amplitudes being 64mm (SD = 24) and 43mm (SD = 26) respectively. The \ufb02exion phase was 30% shorter thanthe stretch phase, with average duration of each phase being 205ms (SD = 81) and 291ms (SD = 148) respectively.The velocity curve revealed an acceleration-deceleration process in both \ufb02exion and stretch phases. The maximum velocities of each phase were 623 mm/s (SD = 262) and 304 mm/s (SD = 136), and were reached at 99ms (SD = 59) and 327ms (SD = 99) respectively. This suggested that the \ufb02exion phase was faster than the stretch phase during tapping.We performed RM-ANOVA to test whether individual \ufb01ngers exhibited different moving patterns. We merged data from both hands for analyzing. No signi\ufb01cant effect of \ufb01nger (little, ring, middle, index) on tapping duration was found (F3,105 = 2.59, n.s.). However, \ufb01nger showed a signi\ufb01cant effect on tapping amplitude (F3,105 = 48.2, p < .0001). This suggested that a \ufb01xed size of time window with varying thresholds of movement amplitude for different \ufb01ngers might be necessary for tap detection. Table 1 shows the duration and amplitude of individual \ufb01ngers.FingerIndexMiddleRingPinkyDuration (ms)496 (174)505 (171)480 (154)509 (180) \u00a0Amplitude (mm) \u00a0\u00a0\u00a0 \u00a070 (23) \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a072 (24) \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a067 (23) \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a058 (23) \u00a0 \u00a0Table 1: Mean (SD) duration and amplitude of each \ufb01ngerCorrelated Movement among FingersCorrelated movement arises from the interdependence among multiple \ufb01ngers, which has been extensively studied in ergonomics [15]. Sridhar et al. [32] also studied the \ufb01nger individuation when users controlled one-dimensional movement of a cursor on the screen using \ufb01nger \ufb02exion. Correlated movement exists as a salient problem in mid-air typing. For example, when tapping \u2018k\u2019 with middle \ufb01nger, the ring \ufb01nger moves with it (see Figure 5). This problem may lead to false detection of taps.Figure 5: Correlated movement illustration: ring \ufb01nger moves with middle \ufb01nger during tappingTo analyze correlated movement among \ufb01ngers, we de\ufb01ned Amplitude Ratio (AR) for each active-passive \ufb01nger combination as:\u00d7AR =  Apassive 100%Aactivewhere Apassive and Aactive denote the tapping amplitude of the passive and the active \ufb01nger respectively. An AR of 100% indicates that the passive \ufb01nger taps as deep as the active \ufb01nger. As shown in Figure 6, we observed signi\ufb01cant correlated movement between adjacent \ufb01ngers,especially among middle, ring and little \ufb01nger (AR > 50%). The top 4 combinations with the highest AR were little-ring, middle-ring, ring-middle, and ring-little. The strongest interdependence occurs between the ring \ufb01nger and the adjacent ones. Compared to Sridhar et al.\u2019s research, our results showed even stronger correlation among \ufb01ngers. This suggested that ten-\ufb01nger typing in mid-air exhibited more complex \ufb01nger movement pattern than one-dimensional \ufb02exion tasks.Figure 6: AR of each active-passive \ufb01nger combination3D Endpoint DistributionWhen air typing, users specify target letters by controlling movements of both hands and \ufb01ngers. For example, a user\u2019s index \ufb01nger may \ufb02ex more when tapping \u2018m\u2019 than \u2018j\u2019; a user\u2019s right hand may move slightly forward when typing \u2018y\u2019 in \u201cmy\u201d, or move backward when typing \u2018n\u2019 in \u201con\u201d. Therefore, we analyzed the absolute coordinate of tapping endpoints, which re\ufb02ects both hand and \ufb01nger movement. Here, an endpoint in the context of mid-air typing is de\ufb01ned as the \ufb01nger position with minimum relative Y coordinate during a tap (see Figure 4). We removed outliers (92, 2.9% of the data) for each character that were > 3SD from the mean in X, Y or Z axis.Figure 7 shows the distribution of endpoints for each character. From Figure 7a and 7b, we can see that the distribution of endpoints roughly followed the QWERTY layout. We also examined the effect of Finger (index, middle, ring, little) and Row (top, middle, bottom) on endpoint coordinates. Finger revealed a main effect on X coordinate for both the left hand (F3,75 = 502, p < .0001) and the right hand (F3,12 = 174, p < .0001). Figure 7c and 7d show that endpoints of individual keys in different rows varied in both Y and Z dimensions. We conducted RM-ANOVA to con\ufb01rm this. Signi\ufb01cant effect of row on Y coordinate was observed for both hands (F2,34 = 11.6, p < .0001 and F2,36 = 50.9, p < .0001). Row also exhibited a main effect on Z coordinate for both hands (F2,34 = 281, p < .0001 and F2,36 = 428, p < .0001).To examine the ambiguity of air tapping endpoints, we ran a simple 1-NN classi\ufb01er on our data like Findlater [7]. Using Left hand                                  (b) Right hand\u200c\u200c", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_003.png", "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "Illustration of the fingertip velocity and relative coordinate within a tap. It shows a acceleration-deceleration process in both flexion and stretch phase.", "levels": null, "corpus_id": 9349793, "sentences": ["Illustration of the fingertip velocity and relative coordinate within a tap.", "It shows a acceleration-deceleration process in both flexion and stretch phase."], "caption": "Figure 4: Illustration of \ufb01ngertip kinematics within a typical tap, shows relative coordinate (lower) and velocity (upper) in Y axis aligned by time", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_005.png"], "annotated": false, "compound": false}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "Illustration of the correlated movement. When tapping middle finger, the ring finger moves with it.", "levels": null, "corpus_id": 9349793, "sentences": ["Illustration of the correlated movement.", "When tapping middle finger, the ring finger moves with it."], "caption": "Figure 5: Correlated movement illustration: ring \ufb01nger moves with middle \ufb01nger during tapping", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "AR of each active-passive finger combination. The correlated movement between adjacent fingers are the most significant.", "levels": null, "corpus_id": 9349793, "sentences": ["AR of each active-passive finger combination.", "The correlated movement between adjacent fingers are the most significant."], "caption": "Figure 6: AR of each active-passive \ufb01nger combination", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_007.png"], "annotated": false, "compound": false}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "3D distribution of endpoints for each character. The top view and the left view show that the endpoints roughly follow a QWERTY layout.", "levels": [[1], [3]], "corpus_id": 9349793, "sentences": ["3D distribution of endpoints for each character.", "The top view and the left view show that the endpoints roughly follow a QWERTY layout."], "caption": "(c) Left hand                                  (d) Right hand\u200c", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_008.jpg", "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_009.jpg", "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_010.jpg", "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data", "pdf_hash": "fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7", "year": 2015, "venue": "UIST", "alt_text": "F1 value with different values of alpha. An alpha of 0.45 yield the maximum F1 value.", "levels": null, "corpus_id": 9349793, "sentences": ["F1 value with different values of alpha.", "An alpha of 0.45 yield the maximum F1 value."], "caption": "Figure 9: F 1 value with \u03b1 \u2208 [0.1, 1.0], sampled every 0.01", "local_uri": ["fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7_Image_021.png"], "annotated": false, "compound": false}
{"title": "Exploring pinch and spread gestures on mobile devices", "pdf_hash": "c7583a584220d5d8724e951ba243e556f83e5d6e", "year": 2013, "venue": "MobileHCI '13", "alt_text": "Figure 7 has two pictures of the pinch and spread tasks and the corresponding red square and blue border. For the pinch task the red square is larger than the dashed blue border. For the spread task, the red square is smaller than a dashed blue border. Each of these images are marked to indicate the width of the blue dashed border and A, which is the distance from the edge of the res square to the center of the blue border.", "levels": null, "corpus_id": 11591659, "sentences": ["Figure 7 has two pictures of the pinch and spread tasks and the corresponding red square and blue border.", "For the pinch task the red square is larger than the dashed blue border.", "For the spread task, the red square is smaller than a dashed blue border.", "Each of these images are marked to indicate the width of the blue dashed border and A, which is the distance from the edge of the res square to the center of the blue border."], "caption": "Figure 7. Definition of W and A in pinch and spread tasks.", "local_uri": ["c7583a584220d5d8724e951ba243e556f83e5d6e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Working Paper Series Working Paper # 020 VotingVoice : An Accessible Voter \u2019 s Guide for People with Aphasia", "pdf_hash": "e2a95b071eae74a445996aceffae5f0cbdc90482", "year": 2014, "venue": "", "alt_text": "Four low-fidelity user interface sketches: a web page with pop up notes, a web page with a note sidebar, a printed information sheet, and an electronic slide show", "levels": [[-1]], "corpus_id": 53065924, "sentences": ["Four low-fidelity user interface sketches: a web page with pop up notes, a web page with a note sidebar, a printed information sheet, and an electronic slide show"], "caption": "", "local_uri": ["e2a95b071eae74a445996aceffae5f0cbdc90482_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Working Paper Series Working Paper # 020 VotingVoice : An Accessible Voter \u2019 s Guide for People with Aphasia", "pdf_hash": "e2a95b071eae74a445996aceffae5f0cbdc90482", "year": 2014, "venue": "", "alt_text": "Example of text simplification: Lleft, original web page. Right, extraneous page content has been removed, leaving only the text and main images.", "levels": null, "corpus_id": 53065924, "sentences": ["Example of text simplification: Lleft, original web page.", "Right, extraneous page content has been removed, leaving only the text and main images."], "caption": "", "local_uri": ["e2a95b071eae74a445996aceffae5f0cbdc90482_Image_002.png"], "annotated": false, "compound": false}
{"title": "Working Paper Series Working Paper # 020 VotingVoice : An Accessible Voter \u2019 s Guide for People with Aphasia", "pdf_hash": "e2a95b071eae74a445996aceffae5f0cbdc90482", "year": 2014, "venue": "", "alt_text": "The user can select a block of text and press \"speak\" from a pop-up menu to read the text aloud.", "levels": null, "corpus_id": 53065924, "sentences": ["The user can select a block of text and press \"speak\" from a pop-up menu to read the text aloud."], "caption": "", "local_uri": ["e2a95b071eae74a445996aceffae5f0cbdc90482_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Working Paper Series Working Paper # 020 VotingVoice : An Accessible Voter \u2019 s Guide for People with Aphasia", "pdf_hash": "e2a95b071eae74a445996aceffae5f0cbdc90482", "year": 2014, "venue": "", "alt_text": "The user can select annotated text and press \"speak note\" from a pop-up menu to read the note aloud.", "levels": null, "corpus_id": 53065924, "sentences": ["The user can select annotated text and press \"speak note\" from a pop-up menu to read the note aloud."], "caption": "", "local_uri": ["e2a95b071eae74a445996aceffae5f0cbdc90482_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Working Paper Series Working Paper # 020 VotingVoice : An Accessible Voter \u2019 s Guide for People with Aphasia", "pdf_hash": "e2a95b071eae74a445996aceffae5f0cbdc90482", "year": 2014, "venue": "", "alt_text": "The user can tag blocks of text with images, such as a smiley face, frowning face, check mark, or star.", "levels": null, "corpus_id": 53065924, "sentences": ["The user can tag blocks of text with images, such as a smiley face, frowning face, check mark, or star."], "caption": "", "local_uri": ["e2a95b071eae74a445996aceffae5f0cbdc90482_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "HapticPrint: Designing Feel Aesthetics for Digital Fabrication", "pdf_hash": "0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c", "year": 2015, "venue": "UIST", "alt_text": "Recognition accuracy continues to improve with data from additional sessions. Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions.", "levels": [[-1], [-1]], "corpus_id": 8393243, "sentences": ["Recognition accuracy continues to improve with data from additional sessions.", "Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions."], "caption": "", "local_uri": ["0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HapticPrint: Designing Feel Aesthetics for Digital Fabrication", "pdf_hash": "0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c", "year": 2015, "venue": "UIST", "alt_text": "Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations.", "levels": null, "corpus_id": 8393243, "sentences": ["Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations."], "caption": "A", "local_uri": ["0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c_Image_015.jpg", "0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c_Image_016.jpg", "0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c_Image_017.jpg"], "annotated": false, "compound": true}
{"title": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "pdf_hash": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "year": 2015, "venue": "W4A", "alt_text": "Description: image of a chart that has automatic adapation and no notification selected", "levels": [[1]], "corpus_id": 207225134, "sentences": ["Description: image of a chart that has automatic adapation and no notification selected"], "caption": "Table 4. Personas representing three common preferences towards notification and automatic adaptations.", "local_uri": ["3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "pdf_hash": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "year": 2015, "venue": "W4A", "alt_text": "Description: image of a chart that has manual adaptation and no notification selected", "levels": [[-1]], "corpus_id": 207225134, "sentences": ["Description: image of a chart that has manual adaptation and no notification selected"], "caption": "Participants: YA2, YA16, OA1, OA3, OA4", "local_uri": ["3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_009.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "pdf_hash": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "year": 2015, "venue": "W4A", "alt_text": "Description: image of a chart that has manual adaptation and notification selected", "levels": [[-1]], "corpus_id": 207225134, "sentences": ["Description: image of a chart that has manual adaptation and notification selected"], "caption": "", "local_uri": ["3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_010.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "pdf_hash": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "year": 2015, "venue": "W4A", "alt_text": "Description: image of a chart that has automatic adaptation and notification selected", "levels": [[-1]], "corpus_id": 207225134, "sentences": ["Description: image of a chart that has automatic adaptation and notification selected"], "caption": "Participants: YA4, YA7, YA8, YA11, YA28, YA30, OA6, OA8", "local_uri": ["3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_011.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Crowd of Your Own: Crowdsourcing for On-Demand Personalization", "pdf_hash": "e0daf74a08a4652a308620db1043a0e20b25efc0", "year": 2014, "venue": "HCOMP", "alt_text": "Three lines are shows, all decreasing (and thus increasing in performance) as more workers are added to the pool. The lines representing the food tasks improve in quality notably more than the salt shaker line.", "levels": null, "corpus_id": 15162086, "sentences": ["Three lines are shows, all decreasing (and thus increasing in performance) as more workers are added to the pool.", "The lines representing the food tasks improve in quality notably more than the salt shaker line."], "caption": "", "local_uri": ["e0daf74a08a4652a308620db1043a0e20b25efc0_Image_006.gif"], "annotated": false, "compound": false}
{"title": "Interacting with Literary Style through Computational Tools", "pdf_hash": "574211ab6e038ddee2e964502fb0b8a07930805a", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: Two photographs and two screenshots show examples of style analysis.  From left to right: a library bookshelf holds seven books with visualizations of style as small rectangular patches on their spines.  A close-up of a hand holding an e-reader, where the thumb rests on a textured bar along the side of the case. An Amazon search results page shows three science-fiction books, with style visualizations inserted below the covers and above the title and book metadata.  A screenshot shows several paragraphs of text, to the left of which a visualization shows a gradient of color, most of which is blue, but one chunk in the middle is magenta.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483032, "sentences": ["Figure 1: Two photographs and two screenshots show examples of style analysis.", "From left to right: a library bookshelf holds seven books with visualizations of style as small rectangular patches on their spines.", "A close-up of a hand holding an e-reader, where the thumb rests on a textured bar along the side of the case.", "An Amazon search results page shows three science-fiction books, with style visualizations inserted below the covers and above the title and book metadata.", "A screenshot shows several paragraphs of text, to the left of which a visualization shows a gradient of color, most of which is blue, but one chunk in the middle is magenta."], "caption": "", "local_uri": ["574211ab6e038ddee2e964502fb0b8a07930805a_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Interacting with Literary Style through Computational Tools", "pdf_hash": "574211ab6e038ddee2e964502fb0b8a07930805a", "year": 2020, "venue": "CHI", "alt_text": "Figure 5:  A scatterplot of points overlaid over a color space.  Clockwise from the top left, the colors are blue, purple, red, yellow, green.  In this plot, most of the points are scattered relatively evenly over the top two-thirds of the space, though the bottom third is sparse.  A denser than average line of points shows up along the top border of the image.", "levels": [[1], [1], [3], [3]], "corpus_id": 218483032, "sentences": ["Figure 5:  A scatterplot of points overlaid over a color space.", "Clockwise from the top left, the colors are blue, purple, red, yellow, green.", "In this plot, most of the points are scattered relatively evenly over the top two-thirds of the space, though the bottom third is sparse.", "A denser than average line of points shows up along the top border of the image."], "caption": "Figure 5. Exercises in Style by Ray- mond Queneau retells one scene in", "local_uri": ["574211ab6e038ddee2e964502fb0b8a07930805a_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Discovering Users for Technical Innovations through Systematic Matchmaking", "pdf_hash": "670c8e3f636c33a863fb1c42db0b1d9b11019fd9", "year": 2019, "venue": "CHI Extended Abstracts", "alt_text": "PaperID - Different forms of PaperID RFID tags are shown. From left to right: A stencil drawn antenna, a printed antenna, a sticker, and a handdrawn antenna.", "levels": null, "corpus_id": 144207663, "sentences": ["PaperID - Different forms of PaperID RFID tags are shown.", "From left to right: A stencil drawn antenna, a printed antenna, a sticker, and a handdrawn antenna."], "caption": "Figure 1. Different forms of PaperID [5]", "local_uri": ["670c8e3f636c33a863fb1c42db0b1d9b11019fd9_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Discovering Users for Technical Innovations through Systematic Matchmaking", "pdf_hash": "670c8e3f636c33a863fb1c42db0b1d9b11019fd9", "year": 2019, "venue": "CHI Extended Abstracts", "alt_text": "Viband - A schematic drawing of how the ViBand interactions work. On the left, Sensing User input is shown. On the right, the ability to Sense External Objects is illustrated.", "levels": null, "corpus_id": 144207663, "sentences": ["Viband - A schematic drawing of how the ViBand interactions work.", "On the left, Sensing User input is shown.", "On the right, the ability to Sense External Objects is illustrated."], "caption": "Figure 2. Schematic sketch of ViBand uses [4]", "local_uri": ["670c8e3f636c33a863fb1c42db0b1d9b11019fd9_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "pdf_hash": "9475d822749fd4754e46de81e21b28445e4f98a7", "year": 2020, "venue": "ASSETS", "alt_text": "It shows four screenshots of AIGuide. The top-left screenshot shows the Selection Interface, which contains a list with three items: Fruit Bar, Lipton Iced Tea and Lucky Charms. The top-right shows the guidance interface, which contains two labels. One indicates that the item was found. Other indicates that the item is two feet away, 15 degrees left and 5 inches below the camera view. Also, it has a guide, confirm , exit and restart button. The bottom-left shows the user settings, which contains a toggle switch for camera access, two toggle switches to control haptic and sound feedback, a scrolling bar for the speaking rate and a submenu for the measuring system. The bottom right shows the tutorial interface, which contains a page number at the top, the description of that explains how the localization phase works, a button to play a demo, and two buttons to switch page.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 225964798, "sentences": ["It shows four screenshots of AIGuide.", "The top-left screenshot shows the Selection Interface, which contains a list with three items: Fruit Bar, Lipton Iced Tea and Lucky Charms.", "The top-right shows the guidance interface, which contains two labels.", "One indicates that the item was found.", "Other indicates that the item is two feet away, 15 degrees left and 5 inches below the camera view.", "Also, it has a guide, confirm , exit and restart button.", "The bottom-left shows the user settings, which contains a toggle switch for camera access, two toggle switches to control haptic and sound feedback, a scrolling bar for the speaking rate and a submenu for the measuring system.", "The bottom right shows the tutorial interface, which contains a page number at the top, the description of that explains how the localization phase works, a button to play a demo, and two buttons to switch page."], "caption": "", "local_uri": ["9475d822749fd4754e46de81e21b28445e4f98a7_Image_002.jpg", "9475d822749fd4754e46de81e21b28445e4f98a7_Image_003.jpg", "9475d822749fd4754e46de81e21b28445e4f98a7_Image_004.jpg", "9475d822749fd4754e46de81e21b28445e4f98a7_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "pdf_hash": "9475d822749fd4754e46de81e21b28445e4f98a7", "year": 2020, "venue": "ASSETS", "alt_text": "A diagram showing the different steps the app takes to guide the user's hand. The entry point is selection and it goes through localization, guidance  and item confirmation. It explains that to transition the app needs to either locate the item or detect is close. Other source of transition are button clicks.", "levels": null, "corpus_id": 225964798, "sentences": ["A diagram showing the different steps the app takes to guide the user's hand.", "The entry point is selection and it goes through localization, guidance  and item confirmation.", "It explains that to transition the app needs to either locate the item or detect is close.", "Other source of transition are button clicks."], "caption": "Figure 2: Hand-to-Object Guidance Flow Diagram. Users go through 4 phases: (1) selection, (2) localization, (3) guidance and (4) confrmation. The transition between these phases are triggered by the systems (e.g. when localizing an object) or user actions (i.e. clicking a button or shaking the phone).", "local_uri": ["9475d822749fd4754e46de81e21b28445e4f98a7_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "pdf_hash": "9475d822749fd4754e46de81e21b28445e4f98a7", "year": 2020, "venue": "ASSETS", "alt_text": "Two screenshots of video conference showing two camera angles of user trial. In both images a person is standing in front of a table with products on top. Both people are holding their phone perpendicular to the products.", "levels": [[-1], [-1], [-1]], "corpus_id": 225964798, "sentences": ["Two screenshots of video conference showing two camera angles of user trial.", "In both images a person is standing in front of a table with products on top.", "Both people are holding their phone perpendicular to the products."], "caption": "", "local_uri": ["9475d822749fd4754e46de81e21b28445e4f98a7_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparison of Methods for Teaching Accessibility in University Computing Courses", "pdf_hash": "3243dde72340fe11b84bf83a9cc4e54a8eb6402d", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 1 describes our data collection intervals among the computing students whose degree required to take the HCI intervention course. An identical survey was administered three times: at beginning of the HCI course (Pre-survey), at the end of the HCI course (Post-survey), and 18-24 months after the HCI course (Senior-survey).", "levels": [[1], [1]], "corpus_id": 225957196, "sentences": ["Figure 1 describes our data collection intervals among the computing students whose degree required to take the HCI intervention course.", "An identical survey was administered three times: at beginning of the HCI course (Pre-survey), at the end of the HCI course (Post-survey), and 18-24 months after the HCI course (Senior-survey)."], "caption": "", "local_uri": ["3243dde72340fe11b84bf83a9cc4e54a8eb6402d_Image_003.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparison of Methods for Teaching Accessibility in University Computing Courses", "pdf_hash": "3243dde72340fe11b84bf83a9cc4e54a8eb6402d", "year": 2020, "venue": "ASSETS", "alt_text": "Figure2(a) presents the comparison of the short-term (Pre vs. Post) analysis for all the measures (Voting, Awareness, Knowledge) for all intervention conditions (Control, Lectures, Projects, Interaction, Team member). The results are described in section 5.2.1. Figure2(b) demonstrates the comparison of the long-term (Pre vs. Senior) analysis for all the measures (Voting, Awareness, Knowledge) for all intervention conditions (Control, Lectures, Projects, Interaction, Team member). The results are described in section 5.2.2.", "levels": null, "corpus_id": 225957196, "sentences": ["Figure2(a) presents the comparison of the short-term (Pre vs. Post) analysis for all the measures (Voting, Awareness, Knowledge) for all intervention conditions (Control, Lectures, Projects, Interaction, Team member).", "The results are described in section 5.2.1.", "Figure2(b) demonstrates the comparison of the long-term (Pre vs. Senior) analysis for all the measures (Voting, Awareness, Knowledge) for all intervention conditions (Control, Lectures, Projects, Interaction, Team member).", "The results are described in section 5.2.2."], "caption": "", "local_uri": ["3243dde72340fe11b84bf83a9cc4e54a8eb6402d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a VR Controller", "pdf_hash": "ea00ecb9dd8dc6edd5f33a4b6e081f4b8c76b451", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A user holding the haptic controller with their index finger resting on the haptic wheel.", "levels": null, "corpus_id": 5074671, "sentences": ["A user holding the haptic controller with their index finger resting on the haptic wheel."], "caption": "We present Haptic Revolver[5], a handheld virtual reality controller that renders fngertip haptics when interacting with virtual surfaces. Haptic Revolver\u2019s core haptic element is an actuated wheel that raises and lowers underneath the fnger to render contact with a virtual surface. As the user\u2019s fnger moves along the surface of an object, the controller spins the wheel to render shear forces and motion under the fngertip. The wheel is interchangeable and can contain physical textures, shapes, edges, or active elements to provide different sensations to the user. Because the controller is spatially tracked, these physical features can be spatially registered with the geometry of the virtual environment and rendered on-demand.", "local_uri": ["ea00ecb9dd8dc6edd5f33a4b6e081f4b8c76b451_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a VR Controller", "pdf_hash": "ea00ecb9dd8dc6edd5f33a4b6e081f4b8c76b451", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A simple use case of the device showing the finger in the virtual environment and the corresponding wheel configuration.", "levels": null, "corpus_id": 5074671, "sentences": ["A simple use case of the device showing the finger in the virtual environment and the corresponding wheel configuration."], "caption": "", "local_uri": ["ea00ecb9dd8dc6edd5f33a4b6e081f4b8c76b451_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "\"Holy Starches Batman!! We are Getting Walloped!\": Crowdsourcing Comic Book Transcriptions", "pdf_hash": "cd01711acce60a104735f06ca8de48ee184cbc11", "year": 2016, "venue": "ASSETS", "alt_text": "Two-page spread from a Batman comic, showing Batman and Robin chasing the Riddler in the Batmobile.", "levels": null, "corpus_id": 18268156, "sentences": ["Two-page spread from a Batman comic, showing Batman and Robin chasing the Riddler in the Batmobile."], "caption": "Figure 1. Comics panel that we showed to crowd workers [7]. Image", "local_uri": ["cd01711acce60a104735f06ca8de48ee184cbc11_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of a 3D computer aided design application, Blender, with an inset image of the cylindical Ollie robot held in the user's hand. The on-screen 3D cube model matches the orientation of the handheld robot.", "levels": null, "corpus_id": 19551494, "sentences": ["Screen shot of a 3D computer aided design application, Blender, with an inset image of the cylindical Ollie robot held in the user's hand.", "The on-screen 3D cube model matches the orientation of the handheld robot."], "caption": "Figure 1. Using an off-the-shelf robot (Ollie) as a tangible input/output device for a 3D modeling application (Blender). Our system dynamically pairs the robot with the on-screen model: manipulating the robot moves the model; changing the model on screen causes mirrored movements in the robot.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of the GMail web application, with inset image of the user grasping the Ollie robot.", "levels": null, "corpus_id": 19551494, "sentences": ["Screen shot of the GMail web application, with inset image of the user grasping the Ollie robot."], "caption": "Figure 3. Haptic Web Browser Control enables the user to scroll a web page by rotating or rolling the robot, and provides haptic feedback based on the page content.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of the Angry Birds game, with an inset image of the Ollie robot in the user's hand.", "levels": null, "corpus_id": 19551494, "sentences": ["Screen shot of the Angry Birds game, with an inset image of the Ollie robot in the user's hand."], "caption": "Figure 4. GUI Robot used as a tangible game controller. The user pulls the robot back to arm the slingshot. The robot launches itself forward and shakes after the slingshot is fired, adding expressive haptic output to the game experience.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of the GUI Input Manager shows an image of the Ollie robot. Dropdown menus surround the image and can be used to assign input events to motions of the robot.", "levels": null, "corpus_id": 19551494, "sentences": ["Screen shot of the GUI Input Manager shows an image of the Ollie robot.", "Dropdown menus surround the image and can be used to assign input events to motions of the robot."], "caption": "Figure 5. GUI Robot Input Manager allows end users to map interactions with the robot to keyboard and mouse commands.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Photograph of Ollie robot being rolled over the desk in the user's hand, next to a screenshot of a Max/MSP program controlled by the robot's movements.", "levels": [[-1]], "corpus_id": 19551494, "sentences": ["Photograph of Ollie robot being rolled over the desk in the user's hand, next to a screenshot of a Max/MSP program controlled by the robot's movements."], "caption": "Figure 6. Ollie Robot paired to Max/MSP using the GUI Robots framework. The robot moves and flashes its lights based on the music. Manipulating the robot affects music playback.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Screen shot of Notification Manager application. The application shows a list of notification mappings. Each notification mapping contains a text pattern and a list of robot movements.", "levels": null, "corpus_id": 19551494, "sentences": ["Screen shot of Notification Manager application.", "The application shows a list of notification mappings.", "Each notification mapping contains a text pattern and a list of robot movements."], "caption": "Figure 7. Notification Manager application enables a user to enter a notification pattern and specify a corresponding robot behavior. When a notification appears that matches the specified pattern, the robot performs the specified behaviors.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "GUI Robots: Using Off-the-Shelf Robots as Tangible Input and Output Devices for Unmodified GUI Applications", "pdf_hash": "1f53688548887500ff6736dc1abf007bdecb1033", "year": 2017, "venue": "Conference on Designing Interactive Systems", "alt_text": "Photograph of a study participant in the process of creating an application using the GUI Robots framework. The participant is holding an Ollie robot and looking at the device screen. On the desk is a cheat sheet of API commands used in the study.", "levels": [[-1], [-1], [-1]], "corpus_id": 19551494, "sentences": ["Photograph of a study participant in the process of creating an application using the GUI Robots framework.", "The participant is holding an Ollie robot and looking at the device screen.", "On the desk is a cheat sheet of API commands used in the study."], "caption": "Figure 8. Study participant testing haptic feedback with the Ollie robot.", "local_uri": ["1f53688548887500ff6736dc1abf007bdecb1033_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A General Methodology to Quantify Biases in Natural Language Data", "pdf_hash": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Warning!  Tagging errors. Text on Page 3 of this paper is not tagged. Figure 1: Workflow of the proposed general methodology to quantify and mitigate biases in natural language data.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218482844, "sentences": ["Warning!", "Tagging errors.", "Text on Page 3 of this paper is not tagged.", "Figure 1: Workflow of the proposed general methodology to quantify and mitigate biases in natural language data."], "caption": "Figure 1: We propose a general methodology to quantify and mitigate biases in natural language data. 1) Feature extraction: extract observational space and latent space features to capture explicit and implicit biases. 2) Quantify distribution discrepancy: based on extracted features, use Maximum Mean Discrepancy to measure the difference between the potentially biased data and the reference data. 3) Mitigate Biases: if biases are detected, reduce the level of biases leveraging con\ufb01dence ranking or Deep Generative Models.", "local_uri": ["ce4b730f4eaddd98be36ce4ba95548a95a6d69fb_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A General Methodology to Quantify Biases in Natural Language Data", "pdf_hash": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 4: Error bar of user ratings on how samples in different datasets state ideological stereotypes.", "levels": [[1]], "corpus_id": 218482844, "sentences": ["Figure 4: Error bar of user ratings on how samples in different datasets state ideological stereotypes."], "caption": "Figure 4: Error bar of user ratings on how samples in different datasets state ideological stereotypes. For each data sample, we ask the crowd workers if \u201cthe above sentence is from a ideologically neutral or non-neutral stance\u201d. If a sentence is not neutral, we ask them to rate how it states its stereotype on a 5-point scale, from \u201c1: very implicitly\u201d to \u201c5: very explicitly\u201d. We provide background for the crowd workers that explicit stereotypes are overt expressions of judgement with clearly hateful words or phrases; and that implicit stereotypes employ circumlocution, metaphor, or other compositional meanings. We randomize the data sources for all samples, and recruit at least 5 crowd workers for each sample.", "local_uri": ["ce4b730f4eaddd98be36ce4ba95548a95a6d69fb_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A General Methodology to Quantify Biases in Natural Language Data", "pdf_hash": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 6: Error bar of user ratings on how samples state ideological stereotypes.", "levels": [[1]], "corpus_id": 218482844, "sentences": ["Figure 6: Error bar of user ratings on how samples state ideological stereotypes."], "caption": "Figure 6: Error bar of user ratings on how samples state ideological stereotypes, among data detected as most-biased and after mitigation using observational and latent space features. 1: very implicitly; 5: very explicitly.", "local_uri": ["ce4b730f4eaddd98be36ce4ba95548a95a6d69fb_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Modeling Expertise in Assistive Navigation Interfaces for Blind People", "pdf_hash": "428c50a13844b247cc6bff879d3bd34128ea3273", "year": 2018, "venue": "IUI", "alt_text": "The figure shows the time it took to complete each trial, in route A and route B. Overall, route completion time are significantly reduced throughout the trials across all participants.  The figure also shows the error and recall rates of describing POIs and navigation instructions after performing each trial.", "levels": [[3, 1], [1]], "corpus_id": 3778488, "sentences": ["The figure shows the time it took to complete each trial, in route A and route B. Overall, route completion time are significantly reduced throughout the trials across all participants.", "The figure also shows the error and recall rates of describing POIs and navigation instructions after performing each trial."], "caption": "Figure 2. User performance metrics of completion time and route knowledge. Analysis is shown for each of the four trials and two routes in the study.", "local_uri": ["428c50a13844b247cc6bff879d3bd34128ea3273_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Deaf Individuals' Views on Speaking Behaviors of Hearing Peers when Using an Automatic Captioning App", "pdf_hash": "4523166717934b1ddb4919182da3004ccec5ad60", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "This is a screenshot of the Google Live Transcribe application. The screen has a dark background. In the top of the screen, the message \"Hi, this is an example. This is really good. Awesome\" has been transcribed from speech to text. In the bottom half of the screen a person has the virtual keyboard open and has just typed \"hi\".", "levels": null, "corpus_id": 218483196, "sentences": ["This is a screenshot of the Google Live Transcribe application.", "The screen has a dark background.", "In the top of the screen, the message \"Hi, this is an example.", "This is really good.", "Awesome\" has been transcribed from speech to text.", "In the bottom half of the screen a person has the virtual keyboard open and has just typed \"hi\"."], "caption": "", "local_uri": ["4523166717934b1ddb4919182da3004ccec5ad60_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This figure depicts 3 examples of use of our device, Flashpen. The first one, on the left, demonstrate how it can be used to annotate documents. The second one, on the middle, shows vertical interaction on a wall to guide animated objects. The last one, on the right, depicts a user using Flashpen as a magic lens to inspect and explore a multi-layered interface. On the three images, virtual content has been superimposed to demonstrate system usage.", "levels": null, "corpus_id": 233987528, "sentences": ["This figure depicts 3 examples of use of our device, Flashpen.", "The first one, on the left, demonstrate how it can be used to annotate documents.", "The second one, on the middle, shows vertical interaction on a wall to guide animated objects.", "The last one, on the right, depicts a user using Flashpen as a magic lens to inspect and explore a multi-layered interface.", "On the three images, virtual content has been superimposed to demonstrate system usage."], "caption": "", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_001.png"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This figure depicts the Flashpen prototype. On the left picture we can see the full pen device, while on the right picture, the focus is on the tip of the pen. The tip is composed of four elements, the markers for the tracking, the optical flow sensor, the sensor lens and the plastic hinge to accommodate the writing at various angles.", "levels": null, "corpus_id": 233987528, "sentences": ["This figure depicts the Flashpen prototype.", "On the left picture we can see the full pen device, while on the right picture, the focus is on the tip of the pen.", "The tip is composed of four elements, the markers for the tracking, the optical flow sensor, the sensor lens and the plastic hinge to accommodate the writing at various angles."], "caption": "", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This pictures shows Flashpen electronic components inside our prototype. It is composed of a PMW3360 breakout board, a MPU6500, and an ESP32 prototyping board.", "levels": null, "corpus_id": 233987528, "sentences": ["This pictures shows Flashpen electronic components inside our prototype.", "It is composed of a PMW3360 breakout board, a MPU6500, and an ESP32 prototyping board."], "caption": "", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_005.png"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This pictures depicts a user interacting on a two layered map, on a vertical surface, using Flashpen. While dragging the pen on the surface, the user use the pen as a magic lens to make appear the satellite layer view. The figure is composed of two views, the user and the environment on the left, and the VR view on the right.", "levels": null, "corpus_id": 233987528, "sentences": ["This pictures depicts a user interacting on a two layered map, on a vertical surface, using Flashpen.", "While dragging the pen on the surface, the user use the pen as a magic lens to make appear the satellite layer view.", "The figure is composed of two views, the user and the environment on the left, and the VR view on the right."], "caption": "", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This pictures depicts a user colorizing a shape using Flashpen. The figure is composed of two views, the user and its environment on the left, and the VR view on the right.", "levels": null, "corpus_id": 233987528, "sentences": ["This pictures depicts a user colorizing a shape using Flashpen.", "The figure is composed of two views, the user and its environment on the left, and the VR view on the right."], "caption": "", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This pictures depicts a user annotating a document using Flashpen. The figure is composed of two views, the user and its environment on the left, and the VR view on the right.", "levels": null, "corpus_id": 233987528, "sentences": ["This pictures depicts a user annotating a document using Flashpen.", "The figure is composed of two views, the user and its environment on the left, and the VR view on the right."], "caption": "Figure 6: The user annotates a document. Pages are turned with hand gestures (non-dominant hand).", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_008.png"], "annotated": false, "compound": false}
{"title": "Demonstration of Flashpen: A high-fidelity and high-precision multi-surface pen for Virtual Reality", "pdf_hash": "e073f86c99a485f7801934eb19c5611d22fe0372", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This pictures depicts a user tracing a path followed by a animated plane. The figure is composed of two views, the user and its environment on the left, and the VR view on the right.", "levels": null, "corpus_id": 233987528, "sentences": ["This pictures depicts a user tracing a path followed by a animated plane.", "The figure is composed of two views, the user and its environment on the left, and the VR view on the right."], "caption": "", "local_uri": ["e073f86c99a485f7801934eb19c5611d22fe0372_Image_009.png"], "annotated": false, "compound": false}
{"title": "The Participatory Design of an Adaptive Interface to Support Users with Changing Pointing Ability", "pdf_hash": "4c44d6396b1f55bfdd891c10eab62e25a8be3d89", "year": 2017, "venue": "ASSETS", "alt_text": "This image shows three webspages, each demonstrating one type of notification. The top one shows a red bar on top of the page, the middle a bar with text and buttons, and the botton a popup window with buttons.", "levels": [[-1], [-1]], "corpus_id": 11563849, "sentences": ["This image shows three webspages, each demonstrating one type of notification.", "The top one shows a red bar on top of the page, the middle a bar with text and buttons, and the botton a popup window with buttons."], "caption": "Figure 1. The Bar notification (A) consists of a horizontal bar. The Bar+ notification (B) includes text and interactive buttons. The Dialog box (C) is a popup alert with text and buttons.", "local_uri": ["4c44d6396b1f55bfdd891c10eab62e25a8be3d89_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Participatory Design of an Adaptive Interface to Support Users with Changing Pointing Ability", "pdf_hash": "4c44d6396b1f55bfdd891c10eab62e25a8be3d89", "year": 2017, "venue": "ASSETS", "alt_text": "The image shows a settings page where the user can access information through options such as \"Notifications Summary\" and \"My Stats\".", "levels": null, "corpus_id": 11563849, "sentences": ["The image shows a settings page where the user can access information through options such as \"Notifications Summary\" and \"My Stats\"."], "caption": "Figure 2. PINATA allows each user to review and specify their preferences.", "local_uri": ["4c44d6396b1f55bfdd891c10eab62e25a8be3d89_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Bonk: accessible programming for accessible audio games", "pdf_hash": "480759c084d52b54eaa26870f7b433a223e149fb", "year": 2018, "venue": "IDC", "alt_text": "Two blind high school students, both male, sit in front of laptops, wearing headphones. Two female teachers stand by the students. Each student is demonstrating his programming project to the teacher.", "levels": null, "corpus_id": 195352361, "sentences": ["Two blind high school students, both male, sit in front of laptops, wearing headphones.", "Two female teachers stand by the students.", "Each student is demonstrating his programming project to the teacher."], "caption": "Figure 1. Blind and visually impaired students design and program accessible video games in a programming workshop.", "local_uri": ["480759c084d52b54eaa26870f7b433a223e149fb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "pdf_hash": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "year": 2020, "venue": "CHI", "alt_text": "This figure shows a screenshot of the prototype, with examples of the \"User Initiative\" and \"Change Visibility\" markings on a certain phrase inside an article on a webpage.  There are three columns, the first shows an article on a webpage. In this article there is a phrase that is highlighted with a box. This phrase is \"unassuming as they are ubiquitous,\".  The second column shows this phrase with three different \"User Initiative\" markings: \"automatic\", where the word \"ubiquitous\" has been simplified to \"ever-present\" but the entire phrase appears as plain text; \"suggestions\", where the words \"unassuming\" and \"ubiquitous\" are highlighted in a gray color; and \"no suggestions\", where it appears as unchanged plain text.  The third column shows this phrase with three different \"Change Visibility\" markings: \"pop-up\", where the word \"ubiquitous\" is grayed out and the word \"ever-present\" appears in a pop up window directly above the word; \"trace\", where the word \"ubiquitous\" has been changed to \"ever-present\", and appears highlighted in yellow; and \"no trace\" where the word \"ubiquitous\" has been changed to \"ever-present\", but appears as plain text.", "levels": null, "corpus_id": 218483129, "sentences": ["This figure shows a screenshot of the prototype, with examples of the \"User Initiative\" and \"Change Visibility\" markings on a certain phrase inside an article on a webpage.", "There are three columns, the first shows an article on a webpage.", "In this article there is a phrase that is highlighted with a box.", "This phrase is \"unassuming as they are ubiquitous,\".", "The second column shows this phrase with three different \"User Initiative\" markings: \"automatic\", where the word \"ubiquitous\" has been simplified to \"ever-present\" but the entire phrase appears as plain text; \"suggestions\", where the words \"unassuming\" and \"ubiquitous\" are highlighted in a gray color; and \"no suggestions\", where it appears as unchanged plain text.", "The third column shows this phrase with three different \"Change Visibility\" markings: \"pop-up\", where the word \"ubiquitous\" is grayed out and the word \"ever-present\" appears in a pop up window directly above the word; \"trace\", where the word \"ubiquitous\" has been changed to \"ever-present\", and appears highlighted in yellow; and \"no trace\" where the word \"ubiquitous\" has been changed to \"ever-present\", but appears as plain text."], "caption": "Figure 1: An article used for the video demonstrations in our preliminary study, with a zoomed-in view of what some text looked like under different design variations for user initiative and change visibility, which were displayed to users in this study.", "local_uri": ["15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_001.png"], "annotated": false, "compound": false}
{"title": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "pdf_hash": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "year": 2020, "venue": "CHI", "alt_text": "This figure is divided into two parts: (a) and (b). \n\nPart (a) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of User Initiative. The title is \"User Initiative\", the y-axis has 5 categories; \"Toggle Suggestions\", \"No Suggestions\", \"Suggestions\", \"Replace All\", and \"Automatic\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The scale responses for 1-3 are on the left side, and 4-6 are on the right side. It can be seen that for \u201cToggle Suggestions\u201d, \u201cNo Suggestions\u201d, and \u201cSuggestions\u201d, the majority of responses are on the right side, while for the \u201cReplace All\u201d and \u201cAutomatic\u201d conditions, the majority of responses are on the left side.\n\nFor \"Toggle Suggestions\" 8.33% responded 1, 0% for 2, 0% for 3, 33.33% for 4, 33.33% for 5, and 25% for 6.\nFor \"No Suggestions\" 0% responded 1, 8.33% for 2, 8.33% for 3, 33.33% for 4, 16.67% for 5, and 33.33% for 6.\nFor \"Suggestions\" 0% responded 1, 8.33% for 2, 16.67% for 3, 16.67% for 4, 41.67% for 5, and 16.67% for 6.\nFor \"Replace All\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 0% for 6.\nFor \"Automatic\" 16.67% responded 1, 33.33% for 2, 33.33% for 3, 8.33% for 4, 8.33% for 5, and 0% for 6.\n\nPart (b) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of Change Visibility. The title is \"Change Visibility\", the y-axis has 4 categories; \"Trace\", \"Pop-up\", \"No Trace\", and \"Sidebar\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The scale responses for 1-3 are on the left side, and 4-6 are on the right side. For \u201cTrace\u201d and \u201cPop-up\u201d, the majority of responses are on the right side. For \u201cNo Trace\u201d and \u201cSidebar\u201d the majority of responses are on the left side.\n\nFor \"Trace\" 0% responded 1, 0% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6. \nFor \"Pop-up\" 8.33% responded 1, 25% for 2, 8.33% for 3, 8.33% for 4, 16.67% for 5, and 33.33% for 6. \nFor \"No Trace\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6. \nFor \"Sidebar\" 50% responded 1, 10% for 2, 10% for 3, 20% for 4, 10% for 5, and 0% for 6.", "levels": [[1], [1], [1], [1], [1], [3], [2], [2], [2], [2], [2], [1], [1], [1], [1], [3], [3], [2], [2], [2], [2]], "corpus_id": 218483129, "sentences": ["This figure is divided into two parts: (a) and (b).", "Part (a) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of User Initiative.", "The title is \"User Initiative\", the y-axis has 5 categories; \"Toggle Suggestions\", \"No Suggestions\", \"Suggestions\", \"Replace All\", and \"Automatic\".", "The x-axis shows percentage, centered at 0, and going up to 100 percent left and right.", "The scale responses for 1-3 are on the left side, and 4-6 are on the right side.", "It can be seen that for \u201cToggle Suggestions\u201d, \u201cNo Suggestions\u201d, and \u201cSuggestions\u201d, the majority of responses are on the right side, while for the \u201cReplace All\u201d and \u201cAutomatic\u201d conditions, the majority of responses are on the left side.", "For \"Toggle Suggestions\" 8.33% responded 1, 0% for 2, 0% for 3, 33.33% for 4, 33.33% for 5, and 25% for 6.", "For \"No Suggestions\" 0% responded 1, 8.33% for 2, 8.33% for 3, 33.33% for 4, 16.67% for 5, and 33.33% for 6.", "For \"Suggestions\" 0% responded 1, 8.33% for 2, 16.67% for 3, 16.67% for 4, 41.67% for 5, and 16.67% for 6.", "For \"Replace All\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 0% for 6.", "For \"Automatic\" 16.67% responded 1, 33.33% for 2, 33.33% for 3, 8.33% for 4, 8.33% for 5, and 0% for 6.", "Part (b) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of Change Visibility.", "The title is \"Change Visibility\", the y-axis has 4 categories; \"Trace\", \"Pop-up\", \"No Trace\", and \"Sidebar\".", "The x-axis shows percentage, centered at 0, and going up to 100 percent left and right.", "The scale responses for 1-3 are on the left side, and 4-6 are on the right side.", "For \u201cTrace\u201d and \u201cPop-up\u201d, the majority of responses are on the right side.", "For \u201cNo Trace\u201d and \u201cSidebar\u201d the majority of responses are on the left side.", "For \"Trace\" 0% responded 1, 0% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6.", "For \"Pop-up\" 8.33% responded 1, 25% for 2, 8.33% for 3, 8.33% for 4, 16.67% for 5, and 33.33% for 6.", "For \"No Trace\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6.", "For \"Sidebar\" 50% responded 1, 10% for 2, 10% for 3, 20% for 4, 10% for 5, and 0% for 6."], "caption": "(b)", "local_uri": ["15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_002.jpg", "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": true}
{"title": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "pdf_hash": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "year": 2020, "venue": "CHI", "alt_text": "This figure shows the four different prototype designs, with four different forms of visual decoration, of an English text phrase inside an article shown on a webpage. It shows a screenshot of the prototype with a phrase in the article highlighted with a box. This phrase is \"the quintessential birds\". To the right of this screenshot, there are four boxes in a 2x2 layout. The top left shows the \"original\" condition, in which the phrase is shown as unchanged plain text. The top right, \"automatic\", shows the phrase with the word \"typical\" instead of \"quintessential\", and appears as plain text. The bottom left, \"decoration\", shows the phrase with the word \"quintessential\" highlighted in gray, with a hand-shaped mouse pointer on it. Underneath this phrase, the same phrase appears with the word \"typical\" instead of \"quintessential\", and is highlighted in yellow. The bottom right, \"pop-up\", shows the word \"typical\" in a pop-up window that appears directly above the word \"quintessential\", which is grayed out and has a hand-shaped mouse pointer on it.", "levels": null, "corpus_id": 218483129, "sentences": ["This figure shows the four different prototype designs, with four different forms of visual decoration, of an English text phrase inside an article shown on a webpage.", "It shows a screenshot of the prototype with a phrase in the article highlighted with a box.", "This phrase is \"the quintessential birds\".", "To the right of this screenshot, there are four boxes in a 2x2 layout.", "The top left shows the \"original\" condition, in which the phrase is shown as unchanged plain text.", "The top right, \"automatic\", shows the phrase with the word \"typical\" instead of \"quintessential\", and appears as plain text.", "The bottom left, \"decoration\", shows the phrase with the word \"quintessential\" highlighted in gray, with a hand-shaped mouse pointer on it.", "Underneath this phrase, the same phrase appears with the word \"typical\" instead of \"quintessential\", and is highlighted in yellow.", "The bottom right, \"pop-up\", shows the word \"typical\" in a pop-up window that appears directly above the word \"quintessential\", which is grayed out and has a hand-shaped mouse pointer on it."], "caption": "", "local_uri": ["15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "pdf_hash": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "year": 2020, "venue": "CHI", "alt_text": "This Figure is divided into three parts: a, b, and c.\n\nPart (a) of this figure shows a stacked bar plot showing percentages of 5-point likert-scale responses. The title is \u201cThis text was easy to read\u201d, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The likert-scale responses for \u201cNeutral\u201d is centered, and \u201cDisagree\u201d and \u201cStrongly Disagree\u201d are on the left side, and \u201cAgree\u201d and \u201cStrongly Agree\u201d are on the right side. For all the conditions, the majority of responses are on the right side, and there is very little on the left side.\n\nFor \"Original\" 4% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 24% for \u201cNeutral\u201d, 56% for \u201cAgree\u201d, and 12% for \u201cStrongly Agree\u201d. \nFor \"Automatic\" 0% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 8% for \u201cNeutral\u201d, 60% for \u201cAgree\u201d, and 28% for \u201cStrongly Agree\u201d.\nFor \"Pop-up\" 0% responded \u201cStrongly Disagree\u201d, 8% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 48% for \u201cAgree\u201d, and 32% for \u201cStrongly Agree\u201d. \nFor \"Decoration\" 0% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 52% for \u201cAgree\u201d, and 32% for \u201cStrongly Agree\u201d.\n\n\nPart (b) of this figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses. The title is \u201cI was able to understand this text well.\u201d, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. There is a bracket indicating p<.05 significance between the \u201cOriginal\u201d and \u201cPop-up\u201d plots. There is also a bracket indicating p<.05 significance between the \u201cOriginal\u201d and \u201cDecoration\u201d plots. The Likert-scale responses for \u201cNeutral\u201d is centered, and \u201cDisagree\u201d and \u201cStrongly Disagree\u201d are on the left side, and \u201cAgree\u201d and \u201cStrongly Agree\u201d are on the right side. For all conditions, the majority of responses are on the right side.\n\nFor \"Original\" 0% responded \u201cStrongly Disagree\u201d, 12% for \u201cDisagree\u201d, 20% for \u201cNeutral\u201d, 60% for \u201cAgree\u201d, and 8% for \u201cStrongly Agree\u201d. \nFor \"Automatic\" 0% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 20% for \u201cNeutral\u201d, 40% for \u201cAgree\u201d, and 36% for \u201cStrongly Agree\u201d.\nFor \"Pop-up\" 0% responded \u201cStrongly Disagree\u201d, 0% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 48% for \u201cAgree\u201d, and 40% for \u201cStrongly Agree\u201d. \nFor \"Decoration\" 0% responded \u201cStrongly Disagree\u201d, 0% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 44% for \u201cAgree\u201d, and 44% for \u201cStrongly Agree\u201d.\n\n\nPart (c) of this figure shows a boxplot for Comprehension Scores for four different conditions, \u201cOriginal\u201d, \u201cAutomatic\u201d, \u201cPop-up\u201d, and \u201cDecoration\u201d, which are on the x-axis. The y-axis is the score, and is a percentage from 0 to 100. For \u201cOriginal\u201d and \u201cAutomatic\u201d the boxplots appear to be almost equal and most of the boxplot is between 30% and 70%. For \u201cPop-up\u201d, it is higher up, with the first quartile starting at around 70%. For \u201cDecoration\u201d there is a wider range, the first quartile is at around 30% and the third quartile is at 100%.\n\nThe \u201cOriginal\u201d boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 62.70%, 3rd quartile 66.67%, and a maximum of 100%.\nThe \u201cAutomatic\u201d boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.7%, mean 58.7%, 3rd quartile 66.67%, and a maximum of 100%.\nThe \u201cPop-up\u201d boxplot has a minimum of 33.33%, 1st quartile 66.67%, median 66.7%, mean 76%, 3rd quartile 100%, and a maximum of 100%.\nThe \u201cDecoration\u201d boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 60%, 3rd quartile 100%, and a maximum of 100%.", "levels": [[1], [1], [1], [1], [3], [2], [2], [2], [2], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483129, "sentences": ["This Figure is divided into three parts: a, b, and c.\n\nPart (a) of this figure shows a stacked bar plot showing percentages of 5-point likert-scale responses.", "The title is \u201cThis text was easy to read\u201d, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\".", "The x-axis shows percentage, centered at 0, and going up to 100 percent left and right.", "The likert-scale responses for \u201cNeutral\u201d is centered, and \u201cDisagree\u201d and \u201cStrongly Disagree\u201d are on the left side, and \u201cAgree\u201d and \u201cStrongly Agree\u201d are on the right side.", "For all the conditions, the majority of responses are on the right side, and there is very little on the left side.", "For \"Original\" 4% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 24% for \u201cNeutral\u201d, 56% for \u201cAgree\u201d, and 12% for \u201cStrongly Agree\u201d.", "For \"Automatic\" 0% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 8% for \u201cNeutral\u201d, 60% for \u201cAgree\u201d, and 28% for \u201cStrongly Agree\u201d.", "For \"Pop-up\" 0% responded \u201cStrongly Disagree\u201d, 8% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 48% for \u201cAgree\u201d, and 32% for \u201cStrongly Agree\u201d.", "For \"Decoration\" 0% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 52% for \u201cAgree\u201d, and 32% for \u201cStrongly Agree\u201d.", "Part (b) of this figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses.", "The title is \u201cI was able to understand this text well.\u201d,", "the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\".", "The x-axis shows percentage, centered at 0, and going up to 100 percent left and right.", "There is a bracket indicating p<.05 significance between the \u201cOriginal\u201d and \u201cPop-up\u201d plots.", "There is also a bracket indicating p<.05 significance between the \u201cOriginal\u201d and \u201cDecoration\u201d plots.", "The Likert-scale responses for \u201cNeutral\u201d is centered, and \u201cDisagree\u201d and \u201cStrongly Disagree\u201d are on the left side, and \u201cAgree\u201d and \u201cStrongly Agree\u201d are on the right side.", "For all conditions, the majority of responses are on the right side.", "For \"Original\" 0% responded \u201cStrongly Disagree\u201d, 12% for \u201cDisagree\u201d, 20% for \u201cNeutral\u201d, 60% for \u201cAgree\u201d, and 8% for \u201cStrongly Agree\u201d.", "For \"Automatic\" 0% responded \u201cStrongly Disagree\u201d, 4% for \u201cDisagree\u201d, 20% for \u201cNeutral\u201d, 40% for \u201cAgree\u201d, and 36% for \u201cStrongly Agree\u201d.", "For \"Pop-up\" 0% responded \u201cStrongly Disagree\u201d, 0% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 48% for \u201cAgree\u201d, and 40% for \u201cStrongly Agree\u201d.", "For \"Decoration\" 0% responded \u201cStrongly Disagree\u201d, 0% for \u201cDisagree\u201d, 12% for \u201cNeutral\u201d, 44% for \u201cAgree\u201d, and 44% for \u201cStrongly Agree\u201d.", "Part (c) of this figure shows a boxplot for Comprehension Scores for four different conditions, \u201cOriginal\u201d, \u201cAutomatic\u201d, \u201cPop-up\u201d, and \u201cDecoration\u201d, which are on the x-axis.", "The y-axis is the score, and is a percentage from 0 to 100.", "For \u201cOriginal\u201d and \u201cAutomatic\u201d the boxplots appear to be almost equal and most of the boxplot is between 30% and 70%.", "For \u201cPop-up\u201d, it is higher up, with the first quartile starting at around 70%.", "For \u201cDecoration\u201d there is a wider range, the first quartile is at around 30% and the third quartile is at 100%.", "The \u201cOriginal\u201d boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 62.70%, 3rd quartile 66.67%, and a maximum of 100%.", "The \u201cAutomatic\u201d boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.7%, mean 58.7%, 3rd quartile 66.67%, and a maximum of 100%.", "The \u201cPop-up\u201d boxplot has a minimum of 33.33%, 1st quartile 66.67%, median 66.7%, mean 76%, 3rd quartile 100%, and a maximum of 100%.", "The \u201cDecoration\u201d boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 60%, 3rd quartile 100%, and a maximum of 100%."], "caption": "(c)  Figure 4. Participants\u2019 responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "local_uri": ["15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_005.jpg", "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_006.jpg", "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": true}
{"title": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "pdf_hash": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "year": 2020, "venue": "CHI", "alt_text": "This figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses. The title is \u201cI would be likely to use:\u201d, and the y-axis has 3 categories; \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. There is a bracket indicating p<.01 significance between the \u201cAutomatic\u201d and \u201cPop-up\u201d plots. There is also a bracket indicating p<.001 significance between the \u201cAutomatic\u201d and \u201cDecoration\u201d plots. The Likert-scale responses for \u201cNeutral\u201d is centered, and \u201cDisagree\u201d and \u201cStrongly Disagree\u201d are on the left side, and \u201cAgree\u201d and \u201cStrongly Agree\u201d are on the right side. It can be seen that a majority of the responses for \u201cPop-up\u201d and \u201cDecoration\u201d are either \u201cAgree\u201d or \u201cStrongly Agree\u201d.\n\nFor \"Automatic\" 28% responded \u201cStrongly Disagree\u201d, 28% for \u201cDisagree\u201d, 24% for \u201cNeutral\u201d, 12% for \u201cAgree\u201d, and 8% for \u201cStrongly Agree\u201d.\nFor \"Pop-up\" 4% responded \u201cStrongly Disagree\u201d, 8% for \u201cDisagree\u201d, 8% for \u201cNeutral\u201d, 28% for \u201cAgree\u201d, and 52% for \u201cStrongly Agree\u201d. \nFor \"Decoration\" 4% responded \u201cStrongly Disagree\u201d, 8% for \u201cDisagree\u201d, 8% for \u201cNeutral\u201d, 36% for \u201cAgree\u201d, and 44% for \u201cStrongly Agree\u201d.", "levels": [[1], [1], [1], [2, 1], [2, 1], [1], [3], [2], [2], [2]], "corpus_id": 218483129, "sentences": ["This figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses.", "The title is \u201cI would be likely to use:\u201d, and the y-axis has 3 categories; \"Automatic\", \"Pop-up\", and \"Decoration\".", "The x-axis shows percentage, centered at 0, and going up to 100 percent left and right.", "There is a bracket indicating p<.01 significance between the \u201cAutomatic\u201d and \u201cPop-up\u201d plots.", "There is also a bracket indicating p<.001 significance between the \u201cAutomatic\u201d and \u201cDecoration\u201d plots.", "The Likert-scale responses for \u201cNeutral\u201d is centered, and \u201cDisagree\u201d and \u201cStrongly Disagree\u201d are on the left side, and \u201cAgree\u201d and \u201cStrongly Agree\u201d are on the right side.", "It can be seen that a majority of the responses for \u201cPop-up\u201d and \u201cDecoration\u201d are either \u201cAgree\u201d or \u201cStrongly Agree\u201d.", "For \"Automatic\" 28% responded \u201cStrongly Disagree\u201d, 28% for \u201cDisagree\u201d, 24% for \u201cNeutral\u201d, 12% for \u201cAgree\u201d, and 8% for \u201cStrongly Agree\u201d.", "For \"Pop-up\" 4% responded \u201cStrongly Disagree\u201d, 8% for \u201cDisagree\u201d, 8% for \u201cNeutral\u201d, 28% for \u201cAgree\u201d, and 52% for \u201cStrongly Agree\u201d.", "For \"Decoration\" 4% responded \u201cStrongly Disagree\u201d, 8% for \u201cDisagree\u201d, 8% for \u201cNeutral\u201d, 36% for \u201cAgree\u201d, and 44% for \u201cStrongly Agree\u201d."], "caption": "Figure 5. Participants\u2019 agreement to a Likert-scale question, presented at the end of the study, as to whether they would be likely to use each of the three simplification conditions, with asterisks marking significant pairwise differences (** p<0.01,", "local_uri": ["15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Visualizations for self-reflection on mouse pointer performance for older adults", "pdf_hash": "22e30452d31598d12867d84e4357cabe2a444e3e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 1 shows the Target Graph visualization of three different clicks. The image shows that the duration of the first click was too long, the second click was too short, and the third click was ok.", "levels": [[1], [3]], "corpus_id": 11786113, "sentences": ["Figure 1 shows the Target Graph visualization of three different clicks.", "The image shows that the duration of the first click was too long, the second click was too short, and the third click was ok."], "caption": "Figure 1. \u201cTarget Graph\u201d Visualizing the duration of individual mouse clicks with a bulls-eye visual metaphor.", "local_uri": ["22e30452d31598d12867d84e4357cabe2a444e3e_Image_001.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Visualizations for self-reflection on mouse pointer performance for older adults", "pdf_hash": "22e30452d31598d12867d84e4357cabe2a444e3e", "year": 2012, "venue": "ASSETS '12", "alt_text": "Figure 3 and Figure 4. Figure 3 shows the Bar Graph visualization. Figure 4 shows the Venn Graph visualization", "levels": [[0], [1], [1]], "corpus_id": 11786113, "sentences": ["Figure 3 and Figure 4.", "Figure 3 shows the Bar Graph visualization.", "Figure 4 shows the Venn Graph visualization"], "caption": "", "local_uri": ["22e30452d31598d12867d84e4357cabe2a444e3e_Image_002.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Cross-Strait Frenemies", "pdf_hash": "7dbf38bec181ecc368fe87742c0bb9405a2c4bb4", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A post calling for other Chinese netizens to change their profile pictures to the \u201cofficial\u201d one.", "levels": null, "corpus_id": 7886209, "sentences": ["A post calling for other Chinese netizens to change their profile pictures to the \u201cofficial\u201d one."], "caption": "", "local_uri": ["7dbf38bec181ecc368fe87742c0bb9405a2c4bb4_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Suhrid: A Collaborative Mobile Phone Interface for Low Literate People", "pdf_hash": "c4c4e184a796fab6f32be6a695292d1398c12c73", "year": 2015, "venue": "ACM DEV", "alt_text": "Figure 1. A  focus group discussion at the rickshaw garage. The garage owner (wearing a cap) participated with seven other rickshaw pullers in that session. (The picture is taken and shared with proper permission of the people in the picture. The faces are burred for anonymity)", "levels": null, "corpus_id": 11464495, "sentences": ["Figure 1.", "A  focus group discussion at the rickshaw garage.", "The garage owner (wearing a cap) participated with seven other rickshaw pullers in that session.", "(The picture is taken and shared with proper permission of the people in the picture.", "The faces are burred for anonymity)"], "caption": "Figure 1. A focus group discussion at the rickshaw garage. The garage owner (wearing a cap) participated with seven other rickshaw pullers in that session. (The picture is taken and shared with proper permission of the people in the picture.", "local_uri": ["c4c4e184a796fab6f32be6a695292d1398c12c73_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Suhrid: A Collaborative Mobile Phone Interface for Low Literate People", "pdf_hash": "c4c4e184a796fab6f32be6a695292d1398c12c73", "year": 2015, "venue": "ACM DEV", "alt_text": "Figure 3. Daily usage of Suhrid during field deployment. Use of Suhrid was higher in the first few weeks, then declined.", "levels": null, "corpus_id": 11464495, "sentences": ["Figure 3.", "Daily usage of Suhrid during field deployment.", "Use of Suhrid was higher in the first few weeks, then declined."], "caption": "Figure 4. Daily usage of Suhrid during field deployment. Use of Suhrid was higher in the first few weeks, then declined.", "local_uri": ["c4c4e184a796fab6f32be6a695292d1398c12c73_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "V-braille: haptic braille perception using a touch-screen and vibration on mobile phones", "pdf_hash": "ccb67c786a442568a74729adef450666280606fd", "year": 2010, "venue": "ASSETS '10", "alt_text": "Image of the letter p conveyed on the G1 phone. Shorter vibrations are shown by dotted lines and stronger ones by solid lines. The phone is divided into 6 sections like a Braille cell. The letter p uses dots 1, 2, 3, 4.", "levels": null, "corpus_id": 16563134, "sentences": ["Image of the letter p conveyed on the G1 phone.", "Shorter vibrations are shown by dotted lines and stronger ones by solid lines.", "The phone is divided into 6 sections like a Braille cell.", "The letter p uses dots 1, 2, 3, 4."], "caption": "Figure 1: V-Braille representation of the lowercase letter \u2018p\u2019 on a smartphone touchscreen, simulating localized vibration on Dots 1 through 4. Dots 2 would vibrate more strongly in this case, as would 5 if it was raised.", "local_uri": ["ccb67c786a442568a74729adef450666280606fd_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Blocks4All Demonstration: a Blocks-Based Programming Environment for Blind Children", "pdf_hash": "c6abbb0deece13d957d295e074c682c1c50cad4d", "year": 2017, "venue": "ASSETS", "alt_text": "Interface showing a hand moving a cat block across the screen with a speech bubble that says \"Place after drive forward block\", there are four blocks in a row on the bottom of the screen: (1) a horse block, (2) a repeat two times block, (3) a forward block and (4) a dog barking block. The block being moved is  directly above the move forward block. There is a menu with more blocks on the left side of the screen.", "levels": [[-1], [-1], [-1]], "corpus_id": 23353184, "sentences": ["Interface showing a hand moving a cat block across the screen with a speech bubble that says \"Place after drive forward block\", there are four blocks in a row on the bottom of the screen: (1) a horse block, (2) a repeat two times block, (3) a forward block and (4) a dog barking block.", "The block being moved is  directly above the move forward block.", "There is a menu with more blocks on the left side of the screen."], "caption": "Figure 1. Audio-guided Drag and Drop interface with audio cues for understanding code structure.", "local_uri": ["c6abbb0deece13d957d295e074c682c1c50cad4d_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Manipulation, Learning, and Recall with Tangible Pen-Like Input", "pdf_hash": "3268f9e46075231d0b32683cdd6feaa60a4e0468", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: Touchscreen laptop flat on table, secured to a board with velcro. A block is falling from the top of the screen, and a user's hand is display, using Conte, with a green outline around the block.", "levels": null, "corpus_id": 218483600, "sentences": ["Figure 7: Touchscreen laptop flat on table, secured to a board with velcro.", "A block is falling from the top of the screen, and a user's hand is display, using Conte, with a green outline around the block."], "caption": "Figure 7. Touchscreen laptop, external keyboard, and game.", "local_uri": ["3268f9e46075231d0b32683cdd6feaa60a4e0468_Image_029.jpg"], "annotated": false, "compound": false}
{"title": "Coming to grips: 3D printing for accessibility", "pdf_hash": "49c48c03bb0a9dbf3ecdc844f283567438b9e83c", "year": 2014, "venue": "ASSETS", "alt_text": "A hand holding a 3D-printed plastic grip. The grip is contoured to the shape of the thumb and first finger of a person's right hand. A cylindrical barrel protrudes from the grip approximately 2 inches and holds a basic stylus with a rubber tip for interacting with touch screens. This allows the user to hold the stylus with reduced muscle fatigue and extends the reach of their stylus to assist with limited range of movement.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 6549568, "sentences": ["A hand holding a 3D-printed plastic grip.", "The grip is contoured to the shape of the thumb and first finger of a person's right hand.", "A cylindrical barrel protrudes from the grip approximately 2 inches and holds a basic stylus with a rubber tip for interacting with touch screens.", "This allows the user to hold the stylus with reduced muscle fatigue and extends the reach of their stylus to assist with limited range of movement."], "caption": "Figure 1. A custom 3D-printed grip designed to hold a stylus.", "local_uri": ["49c48c03bb0a9dbf3ecdc844f283567438b9e83c_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Coming to grips: 3D printing for accessibility", "pdf_hash": "49c48c03bb0a9dbf3ecdc844f283567438b9e83c", "year": 2014, "venue": "ASSETS", "alt_text": "An input window from the GripFab software. There are textfields for the user to enter 1) the diameter object being inserted into the grip, 2) the depth the object will insert into the grip, 3) the length of the barrel extending away from the grip, and 4) the angle at which the barrel should protrude from the grip. There are drop down options to select the shape of the grip itself and a checkbox to indicate if the grip is for a left-handed user. Finally, a place to provide a name for the .stl file that will be rendered based on the inputted values.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 6549568, "sentences": ["An input window from the GripFab software.", "There are textfields for the user to enter 1) the diameter object being inserted into the grip, 2) the depth the object will insert into the grip, 3) the length of the barrel extending away from the grip, and 4) the angle at which the barrel should protrude from the grip.", "There are drop down options to select the shape of the grip itself and a checkbox to indicate if the grip is for a left-handed user.", "Finally, a place to provide a name for the .stl file that will be rendered based on the inputted values."], "caption": "Figure 2. Screenshot of a prototype interface for GripFab.", "local_uri": ["49c48c03bb0a9dbf3ecdc844f283567438b9e83c_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Increasing Native Speakers' Awareness of the Need to Slow Down in Multilingual Conversations Using a Real-Time Speech Speedometer", "pdf_hash": "23ed58b594d25a29795c70a43b268dd6558d6f39", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Videoconferencing interface with real-time evaluative Speech Speedometer. NS see their own speech rate shown on the bar in the center of their screen. It updates every time they make a pause longer than 1.2s. The height of the bar suggests the speed of the utterance prior to the pause. The actual speech rate is shown next to the top of the bar. The color provides evaluative feedback on whether the speech rate is too fast or acceptable. When the speech rate exceeds 140 wpm, the bars above the threshold become red. When the NS is not speaking, the bar stays at the speech rate of their last utterance and does not reset.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 207957597, "sentences": ["Videoconferencing interface with real-time evaluative Speech Speedometer.", "NS see their own speech rate shown on the bar in the center of their screen.", "It updates every time they make a pause longer than 1.2s.", "The height of the bar suggests the speed of the utterance prior to the pause.", "The actual speech rate is shown next to the top of the bar.", "The color provides evaluative feedback on whether the speech rate is too fast or acceptable.", "When the speech rate exceeds 140 wpm, the bars above the threshold become red.", "When the NS is not speaking, the bar stays at the speech rate of their last utterance and does not reset."], "caption": "", "local_uri": ["23ed58b594d25a29795c70a43b268dd6558d6f39_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Increasing Native Speakers' Awareness of the Need to Slow Down in Multilingual Conversations Using a Real-Time Speech Speedometer", "pdf_hash": "23ed58b594d25a29795c70a43b268dd6558d6f39", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 2: Study procedure. After general instruction, participants started with a preliminary survey. Then they worked on the task individually followed by a 15-minute discussion to come to a group decision. After the discussion, they completed a post-task survey. This task-discussion-survey repeated for 3 trials. In the 2nd trial, NS were given the Speech Speedometer interface or the equivalent control display. Upon completion of the last survey, participants were interviewed individually.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 207957597, "sentences": ["Figure 2: Study procedure.", "After general instruction, participants started with a preliminary survey.", "Then they worked on the task individually followed by a 15-minute discussion to come to a group decision.", "After the discussion, they completed a post-task survey.", "This task-discussion-survey repeated for 3 trials.", "In the 2nd trial, NS were given the Speech Speedometer interface or the equivalent control display.", "Upon completion of the last survey, participants were interviewed individually."], "caption": "", "local_uri": ["23ed58b594d25a29795c70a43b268dd6558d6f39_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Increasing Native Speakers' Awareness of the Need to Slow Down in Multilingual Conversations Using a Real-Time Speech Speedometer", "pdf_hash": "23ed58b594d25a29795c70a43b268dd6558d6f39", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Mean NS Motivation to Slow for NNS on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean)", "levels": [[1]], "corpus_id": 207957597, "sentences": ["Mean NS Motivation to Slow for NNS on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean)"], "caption": "", "local_uri": ["23ed58b594d25a29795c70a43b268dd6558d6f39_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Increasing Native Speakers' Awareness of the Need to Slow Down in Multilingual Conversations Using a Real-Time Speech Speedometer", "pdf_hash": "23ed58b594d25a29795c70a43b268dd6558d6f39", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 4: Mean NS Speech Rate, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard error of the mean).", "levels": [[1]], "corpus_id": 207957597, "sentences": ["Figure 4: Mean NS Speech Rate, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard error of the mean)."], "caption": "", "local_uri": ["23ed58b594d25a29795c70a43b268dd6558d6f39_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Increasing Native Speakers' Awareness of the Need to Slow Down in Multilingual Conversations Using a Real-Time Speech Speedometer", "pdf_hash": "23ed58b594d25a29795c70a43b268dd6558d6f39", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 7: Mean NNS Perceived NS Speech Clarity on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean).", "levels": [[1]], "corpus_id": 207957597, "sentences": ["Figure 7: Mean NNS Perceived NS Speech Clarity on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean)."], "caption": "Figure 7: Mean NNS Perceived NS Speech Clarity on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean).", "local_uri": ["23ed58b594d25a29795c70a43b268dd6558d6f39_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Increasing Native Speakers' Awareness of the Need to Slow Down in Multilingual Conversations Using a Real-Time Speech Speedometer", "pdf_hash": "23ed58b594d25a29795c70a43b268dd6558d6f39", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 8: Mean NNS perceived NS Accommodation on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean).", "levels": [[1]], "corpus_id": 207957597, "sentences": ["Figure 8: Mean NNS perceived NS Accommodation on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean)."], "caption": "Figure 8: Mean NNS perceived NS Accommodation on a scale of 1 to 7, by presence or absence of Speech Speedometer for Trial 2 and Trial 3 (error bars represent standard errors of the mean).", "local_uri": ["23ed58b594d25a29795c70a43b268dd6558d6f39_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Self-monitoring practices, attitudes, and needs of individuals with bipolar disorder: implications for the design of technologies to manage mental health", "pdf_hash": "906767cd0b41039c065c9404dc400682b8a15ebf", "year": 2016, "venue": "J. Am. Medical Informatics Assoc.", "alt_text": "Figure 2: Percentages of respondents (y axis) who report tracking specified indicators (x axis).", "levels": null, "corpus_id": 14578953, "sentences": ["Figure 2: Percentages of respondents (y axis) who report tracking specified indicators (x axis)."], "caption": "", "local_uri": ["906767cd0b41039c065c9404dc400682b8a15ebf_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Self-monitoring practices, attitudes, and needs of individuals with bipolar disorder: implications for the design of technologies to manage mental health", "pdf_hash": "906767cd0b41039c065c9404dc400682b8a15ebf", "year": 2016, "venue": "J. Am. Medical Informatics Assoc.", "alt_text": "Figure 3: Percentages (y axis) of various methods (key) used to track specified indicator (x axis).", "levels": null, "corpus_id": 14578953, "sentences": ["Figure 3: Percentages (y axis) of various methods (key) used to track specified indicator (x axis)."], "caption": "Figure 3: Percentages (y axis) of various methods (key) used to track specified indicator (x axis).", "local_uri": ["906767cd0b41039c065c9404dc400682b8a15ebf_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Decipher: An Interactive Visualization Tool for Interpreting Unstructured Design Feedback from Multiple Providers", "pdf_hash": "393a3e0760f10264cbe78c4d8ad9ec7ea3584a57", "year": 2020, "venue": "CHI", "alt_text": "Figure 1 shows a sample screenshot of the Decipher visualizations. Two images labeled (a) and (b) are included in Figure 1. Image (a) shows a flyer design created for a running event and received eight pieces of feedback. Image (b) shows how Decipher visually presents the feedback using its tabular layout. A column in Decipher represents feedback received from one provider, while a row represents all the feedback related to one topic across all providers. Each cell in the table contains a circle and the color of the circle represents the sentiment of a single provider toward a particular topic. Pink indicates negative opinions, green represents positive ones, and blue is for neutral statements or suggestions. The designer can identify the strengths and weaknesses of different aspects of the design through row-wise comparison and compare opinions between providers through column-wise comparisons without having to read the content itself.  The details about the visualization are described in the main text.", "levels": null, "corpus_id": 218482797, "sentences": ["Figure 1 shows a sample screenshot of the Decipher visualizations.", "Two images labeled (a) and (b) are included in Figure 1.", "Image (a) shows a flyer design created for a running event and received eight pieces of feedback.", "Image (b) shows how Decipher visually presents the feedback using its tabular layout.", "A column in Decipher represents feedback received from one provider, while a row represents all the feedback related to one topic across all providers.", "Each cell in the table contains a circle and the color of the circle represents the sentiment of a single provider toward a particular topic.", "Pink indicates negative opinions, green represents positive ones, and blue is for neutral statements or suggestions.", "The designer can identify the strengths and weaknesses of different aspects of the design through row-wise comparison and compare opinions between providers through column-wise comparisons without having to read the content itself.", "The details about the visualization are described in the main text."], "caption": "Figure 1: In (a), a designer has created a preliminary design and received feedback from multiple providers. In (b), the designer has imported the feedback into Decipher to visualize the topic and sentiment structure of the feedback. The designer can identify the strengths and weaknesses of different aspects of the design (row-wise comparison) and compare opinions between providers (column-wise comparison) without having to read the content itself. The designer can also interact with the content (e.g., to mark statements to incorporate in a revision or that need further clari\ufb01cation).", "local_uri": ["393a3e0760f10264cbe78c4d8ad9ec7ea3584a57_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Decipher: An Interactive Visualization Tool for Interpreting Unstructured Design Feedback from Multiple Providers", "pdf_hash": "393a3e0760f10264cbe78c4d8ad9ec7ea3584a57", "year": 2020, "venue": "CHI", "alt_text": "Figure 3 shows four screenshots of Decipher, each depicting one type of user interaction. The details are provided in the main text.", "levels": null, "corpus_id": 218482797, "sentences": ["Figure 3 shows four screenshots of Decipher, each depicting one type of user interaction.", "The details are provided in the main text."], "caption": "bottom of the window.", "local_uri": ["393a3e0760f10264cbe78c4d8ad9ec7ea3584a57_Image_007.png", "393a3e0760f10264cbe78c4d8ad9ec7ea3584a57_Image_008.png"], "annotated": false, "compound": true}
{"title": "Relational Aspects in Patient-provider Interactions: A Facial Paralysis Case Study", "pdf_hash": "c919044e3fb5449ee1e62e5091b0fe397a558239", "year": 2020, "venue": "CHI", "alt_text": "This is the typical timeframe and workflow stage breakdowns for hospital visits with HPs.   Basically, FP workflow consisted of five higher level activities: clinical preparation, informative collaboration, treatment, finalizing documentation, and patient self-care.", "levels": null, "corpus_id": 218482746, "sentences": ["This is the typical timeframe and workflow stage breakdowns for hospital visits with HPs.", "Basically, FP workflow consisted of five higher level activities: clinical preparation, informative collaboration, treatment, finalizing documentation, and patient self-care."], "caption": "Table 3. This is the typical timeframe and work\ufb02ow stage breakdowns for hospital visits with HPs. Healthcare participants are shown within stages where they are present, and tools used during stages are noted.", "local_uri": ["c919044e3fb5449ee1e62e5091b0fe397a558239_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Reading difficulty in adults with intellectual disabilities: analysis with a hierarchical latent trait model", "pdf_hash": "72589a05772d58a8e698dd32a82a542cbbe4c51b", "year": 2010, "venue": "ASSETS '10", "alt_text": "The probability that the indicator term Y superscript v subscript s a q equals 1 is equal to the inverse logit function of alpha subscript s minus theta subscript v a q.  Alpha subscript s is drawn from the normal distribution with first parameter mu subscript alpha and the second parameter sigma squared subscript alpha.  Theta subscript v a q is drawn from the normal distribution with first parameter mu subscript theta and second parameter sigma squared subscript theta.", "levels": [[-1], [-1]], "corpus_id": 15456851, "sentences": ["The probability that the indicator term Y superscript v subscript s a q equals 1 is equal to the inverse logit function of alpha subscript s minus theta subscript v a q.  Alpha subscript s is drawn from the normal distribution with first parameter mu subscript alpha and the second parameter sigma squared subscript alpha.", "Theta subscript v a q is drawn from the normal distribution with first parameter mu subscript theta and second parameter sigma squared subscript theta."], "caption": "This model only captures some of the hierarchical structure inherent in the experimental design: each participant s is given a", "local_uri": ["72589a05772d58a8e698dd32a82a542cbbe4c51b_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Reading difficulty in adults with intellectual disabilities: analysis with a hierarchical latent trait model", "pdf_hash": "72589a05772d58a8e698dd32a82a542cbbe4c51b", "year": 2010, "venue": "ASSETS '10", "alt_text": "This figure depicts the hierarchical structure of the model by showing arrows connecting the terms which have relationships.  The same information is expressed on the right side of the figure using equations, which are as follows:  Greek letter Tau is drawn from the gamma distribution with first parameter 0.001 and second parameter 0.001.  Greek letter eta subscript a is drawn from the normal distribution with first parameter 0 and second parameter 10 raised to the negative sixth power.  Greek letter delta subscript a is drawn from the normal distribution with first parameter 0 and second parameter 10 raised to the negative sixth power.  Greek letter theta superscript com subscript a q is drawn from the normal distribution with first parameter Greek letter eta subscript a and second parameter Greek letter Tau.  Greek letter theta superscript sim subscript a q is equal to the previously defined term (Greek letter theta superscript com subscript a q) minus Greek letter delta subscript a.  The indicator term y superscript v subscript s a q is drawn from the Bernoulli distribution with the parameter the inverse of function g of the difference of alpha subscript s minus theta superscript v subscript a q.  Finally, the term Greek letter alpha subscript s is drawn from the normal distribution with first parameter 0 and second parameter 1.  --", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 15456851, "sentences": ["This figure depicts the hierarchical structure of the model by showing arrows connecting the terms which have relationships.", "The same information is expressed on the right side of the figure using equations, which are as follows:  Greek letter Tau is drawn from the gamma distribution with first parameter 0.001 and second parameter 0.001.", "Greek letter eta subscript a is drawn from the normal distribution with first parameter 0 and second parameter 10 raised to the negative sixth power.", "Greek letter delta subscript a is drawn from the normal distribution with first parameter 0 and second parameter 10 raised to the negative sixth power.", "Greek letter theta superscript com subscript a q is drawn from the normal distribution with first parameter Greek letter eta subscript a and second parameter Greek letter Tau.", "Greek letter theta superscript sim subscript a q is equal to the previously defined term (Greek letter theta superscript com subscript a q) minus Greek letter delta subscript a.  The indicator term y superscript v subscript s a q is drawn from the Bernoulli distribution with the parameter the inverse of function g of the difference of alpha subscript s minus theta superscript v subscript a q.  Finally, the term Greek letter alpha subscript s is drawn from the normal distribution with first parameter 0 and second parameter 1.", "--"], "caption": "Figure 1: Hierarchical latent trait model.", "local_uri": ["72589a05772d58a8e698dd32a82a542cbbe4c51b_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BrailleBlocks: Computational Braille Toys for Collaborative Learning", "pdf_hash": "5f71ecbaf5ff03875a2e336f6b314a6e4c387615", "year": 2020, "venue": "CHI", "alt_text": "A close-up of a dialog box from the BrailleBlocks interface. The dialog box says \"'ELEPHANT'\". Below the dialog box is an enlarged visual representation of the word \"elephant\" in Grade 1 Braille.", "levels": null, "corpus_id": 215807734, "sentences": ["A close-up of a dialog box from the BrailleBlocks interface.", "The dialog box says \"'ELEPHANT'\".", "Below the dialog box is an enlarged visual representation of the word \"elephant\" in Grade 1 Braille."], "caption": "Figure 4. BrailleBlocks shows words as both Braille and text, allowing a sighted parent or teacher to participate in the activity even if they are unfamiliar with Braille.", "local_uri": ["5f71ecbaf5ff03875a2e336f6b314a6e4c387615_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "BrailleBlocks: Computational Braille Toys for Collaborative Learning", "pdf_hash": "5f71ecbaf5ff03875a2e336f6b314a6e4c387615", "year": 2020, "venue": "CHI", "alt_text": "Top:  An overhead view of the Braille blocks in the white cardboard frame. There are eight green blocks in the frame. They have red pegs in them. The blocks and pegs spell out \"elephant\" in Braille.  Middle: A cropped image of the BrailleBlocks in their frame. There are eight green blocks with red pegs in them. The blocks and pegs spell out \"elephant\" in Braille.  Bottom: A transformed image of the Braille blocks. The image is cropped and the colors have been inverted. Everything is black except for where the pegs in the blocks would be, which are bright green blobs. The blobs spells out \"elephant\" in braille.", "levels": null, "corpus_id": 215807734, "sentences": ["Top:  An overhead view of the Braille blocks in the white cardboard frame.", "There are eight green blocks in the frame.", "They have red pegs in them.", "The blocks and pegs spell out \"elephant\" in Braille.  Middle: A cropped image of the BrailleBlocks in their frame.", "There are eight green blocks with red pegs in them.", "The blocks and pegs spell out \"elephant\" in Braille.  Bottom: A transformed image of the Braille blocks.", "The image is cropped and the colors have been inverted.", "Everything is black except for where the pegs in the blocks would be, which are bright green blobs.", "The blobs spells out \"elephant\" in braille."], "caption": "", "local_uri": ["5f71ecbaf5ff03875a2e336f6b314a6e4c387615_Image_006.jpg", "5f71ecbaf5ff03875a2e336f6b314a6e4c387615_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Exploring Human-Robot Interaction with the Elderly: Results from a Ten-Week Case Study in a Care Home", "pdf_hash": "3dba4015e16137062e87b8748020b0b58013ff94", "year": 2020, "venue": "CHI", "alt_text": "Group setting with the Pepper Robot. The figure shows the room in the care home in that the study took place. A room is usually used as a cafeteria. Five people and one robot can be seen in the picture. The persons sit on chairs that are arranged in a semi-circle. In the middle of this circle is the robot Pepper. Pepper is a robot that looks humanoid, he is white and has a smooth plastic \"skin\". He is 1.2 meters high and has no legs, he has two arms and one head with two big eyes. In the picture, he is raising his arms and orientated towards the five people. These five are also raising their arms and imitating the movements the robot is doing.", "levels": null, "corpus_id": 218483496, "sentences": ["Group setting with the Pepper Robot.", "The figure shows the room in the care home in that the study took place.", "A room is usually used as a cafeteria.", "Five people and one robot can be seen in the picture.", "The persons sit on chairs that are arranged in a semi-circle.", "In the middle of this circle is the robot Pepper.", "Pepper is a robot that looks humanoid, he is white and has a smooth plastic \"skin\".", "He is 1.2 meters high and has no legs, he has two arms and one head with two big eyes.", "In the picture, he is raising his arms and orientated towards the five people.", "These five are also raising their arms and imitating the movements the robot is doing."], "caption": "Figure 1: Group setting with the Pepper Robot", "local_uri": ["3dba4015e16137062e87b8748020b0b58013ff94_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Exploring the Design of Audio-Kinetic Graphics for Education", "pdf_hash": "637c83a83bbb7ec214e7085036f5401d50ccd0fa", "year": 2018, "venue": "ICMI", "alt_text": "A pen plotter consists of two metal rails, with motors to move the plotter arm along the rails. A person lightly holds the plotter head with two hands to follow its motion.", "levels": [[-1], [-1]], "corpus_id": 52896906, "sentences": ["A pen plotter consists of two metal rails, with motors to move the plotter arm along the rails.", "A person lightly holds the plotter head with two hands to follow its motion."], "caption": "Figure 1. A pen plotter, typically used to enable computer- aided drawing, is repurposed to provide tactile representations of shapes using kinetic motion. A user grips the mechanical arm to experience kinetic content.", "local_uri": ["637c83a83bbb7ec214e7085036f5401d50ccd0fa_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploring the Design of Audio-Kinetic Graphics for Education", "pdf_hash": "637c83a83bbb7ec214e7085036f5401d50ccd0fa", "year": 2018, "venue": "ICMI", "alt_text": "Authoring interface: a screen shot of the authoring tool shows, on the right, a video screenshot of a curve and, on the left, a simplified representation of that curve.", "levels": null, "corpus_id": 52896906, "sentences": ["Authoring interface: a screen shot of the authoring tool shows, on the right, a video screenshot of a curve and, on the left, a simplified representation of that curve."], "caption": "Figure 2. WeeGee\u2019s authoring tool enables an author to watch an existing video (from Khan Academy, right) while sketching a tactile representation of that content (left).", "local_uri": ["637c83a83bbb7ec214e7085036f5401d50ccd0fa_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring the Design of Audio-Kinetic Graphics for Education", "pdf_hash": "637c83a83bbb7ec214e7085036f5401d50ccd0fa", "year": 2018, "venue": "ICMI", "alt_text": "Image of a pen plotter drawing a radius from the center of a circle to the edge.", "levels": [[-1]], "corpus_id": 52896906, "sentences": ["Image of a pen plotter drawing a radius from the center of a circle to the edge."], "caption": "\u201cWhat is that distance \u2026 that equal distance that everything is from the center? Right there.\u201d", "local_uri": ["637c83a83bbb7ec214e7085036f5401d50ccd0fa_Image_014.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploring the Design of Audio-Kinetic Graphics for Education", "pdf_hash": "637c83a83bbb7ec214e7085036f5401d50ccd0fa", "year": 2018, "venue": "ICMI", "alt_text": "Image of a pen plotter drawing two radii from the center of a circle to the edge.", "levels": [[-1]], "corpus_id": 52896906, "sentences": ["Image of a pen plotter drawing two radii from the center of a circle to the edge."], "caption": "\u201cIf that radius is 3 cm then this radius is", "local_uri": ["637c83a83bbb7ec214e7085036f5401d50ccd0fa_Image_015.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Neo-Noumena: Augmenting Emotion Communication", "pdf_hash": "7d4aa2d0577ebe304e9df7f2eb1e78c721903c87", "year": 2020, "venue": "CHI", "alt_text": "This image shows two people using the Neo-Noumena system. The pair appear to be engaged in conversation, wearing EEG caps and Microsoft HoloLens head mounted displays. Around them, each person has an aura of fractals, with the person of the left being closely followed by red, spikey asymmetrical fractals, and the person on the right being followed by green, smooth, symmetrical fractals.", "levels": null, "corpus_id": 212655729, "sentences": ["This image shows two people using the Neo-Noumena system.", "The pair appear to be engaged in conversation, wearing EEG caps and Microsoft HoloLens head mounted displays.", "Around them, each person has an aura of fractals, with the person of the left being closely followed by red, spikey asymmetrical fractals, and the person on the right being followed by green, smooth, symmetrical fractals."], "caption": "Figure 1. Dynamic fractal shapes representative of each user\u2019s affect state are generated from brain activity and projected into mixed reality. The users view their fractals and their partner\u2019s via the HMDs. Two wirelessly connected laptops acting as servers and signals processing units complete the system architecture.", "local_uri": ["7d4aa2d0577ebe304e9df7f2eb1e78c721903c87_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Neo-Noumena: Augmenting Emotion Communication", "pdf_hash": "7d4aa2d0577ebe304e9df7f2eb1e78c721903c87", "year": 2020, "venue": "CHI", "alt_text": "This image illustrates the system architecture of Neo-Noumea. The architecture is represented by a flow chart showing infromation going from the users EEG, to a laptop computing the signal with OpenBCI, to the support vector machine, to a HoloLens. There is then a circular loop between the signals going to and from the HoloLens and a server, which is being sent via OSC.", "levels": [[-1], [-1], [-1]], "corpus_id": 212655729, "sentences": ["This image illustrates the system architecture of Neo-Noumea.", "The architecture is represented by a flow chart showing infromation going from the users EEG, to a laptop computing the signal with OpenBCI, to the support vector machine, to a HoloLens.", "There is then a circular loop between the signals going to and from the HoloLens and a server, which is being sent via OSC."], "caption": "Figure 2. Neo-Noumena\u2019s System Architecture", "local_uri": ["7d4aa2d0577ebe304e9df7f2eb1e78c721903c87_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Neo-Noumena: Augmenting Emotion Communication", "pdf_hash": "7d4aa2d0577ebe304e9df7f2eb1e78c721903c87", "year": 2020, "venue": "CHI", "alt_text": "This image illustrates the four categories Neo-Noumena groups affect by.  This is represented by a cartesian plane drawn up into four quadrants. Clockwise and starting from top left these quadrants are High-Arousal-Low-Valance, High-Arousal-High-Valance, Low-Arousal-High-Valance, and Low-Arousal-Low-Valance. Within each quadrant is a sample of the fractal swarn the classification generates", "levels": null, "corpus_id": 212655729, "sentences": ["This image illustrates the four categories Neo-Noumena groups affect by.", "This is represented by a cartesian plane drawn up into four quadrants.", "Clockwise and starting from top left these quadrants are High-Arousal-Low-Valance, High-Arousal-High-Valance, Low-Arousal-High-Valance, and Low-Arousal-Low-Valance.", "Within each quadrant is a sample of the fractal swarn the classification generates"], "caption": "Figure 3. Four categories Neo-Noumena groups affect by", "local_uri": ["7d4aa2d0577ebe304e9df7f2eb1e78c721903c87_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Assessment of Semantic Taxonomies for Blind Indoor Navigation Based on a Shopping Center Use Case", "pdf_hash": "ca084360fd522adf8970c75d12cef70a4865d10f", "year": 2017, "venue": "W4A", "alt_text": "Bar chart showing mean ratings about usefulness of vocal messages, grouped by mobility aid used and visual condition of participants. Values are explained along subsection 3.3 entitled subjective ratings.", "levels": [[1], [0]], "corpus_id": 28257270, "sentences": ["Bar chart showing mean ratings about usefulness of vocal messages, grouped by mobility aid used and visual condition of participants.", "Values are explained along subsection 3.3 entitled subjective ratings."], "caption": "", "local_uri": ["ca084360fd522adf8970c75d12cef70a4865d10f_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Assessment of Semantic Taxonomies for Blind Indoor Navigation Based on a Shopping Center Use Case", "pdf_hash": "ca084360fd522adf8970c75d12cef70a4865d10f", "year": 2017, "venue": "W4A", "alt_text": "Bar chart showing mean ratings about usefulness of vocal messages, grouped by previous experience of participants with smartphones and voice navigation apps. Values are explained along subsection 3.3 entitled subjective ratings.", "levels": [[1], [0]], "corpus_id": 28257270, "sentences": ["Bar chart showing mean ratings about usefulness of vocal messages, grouped by previous experience of participants with smartphones and voice navigation apps.", "Values are explained along subsection 3.3 entitled subjective ratings."], "caption": "", "local_uri": ["ca084360fd522adf8970c75d12cef70a4865d10f_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Supporting Self-Regulation of Children with ADHD Using Wearables: Tensions and Design Challenges", "pdf_hash": "91924fd23f8c1fd86db68871d790f2be014be5df", "year": 2020, "venue": "CHI", "alt_text": "An image of smartwatch sketches for social, healthcare and physical activity goals", "levels": null, "corpus_id": 218483261, "sentences": ["An image of smartwatch sketches for social, healthcare and physical activity goals"], "caption": "", "local_uri": ["91924fd23f8c1fd86db68871d790f2be014be5df_Image_004.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_005.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Supporting Self-Regulation of Children with ADHD Using Wearables: Tensions and Design Challenges", "pdf_hash": "91924fd23f8c1fd86db68871d790f2be014be5df", "year": 2020, "venue": "CHI", "alt_text": "An image of smartwatch sketches showing examples of notification when children are not regulated", "levels": null, "corpus_id": 218483261, "sentences": ["An image of smartwatch sketches showing examples of notification when children are not regulated"], "caption": "", "local_uri": ["91924fd23f8c1fd86db68871d790f2be014be5df_Image_008.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_010.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_011.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_012.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_013.jpg"], "annotated": false, "compound": true}
{"title": "Supporting Self-Regulation of Children with ADHD Using Wearables: Tensions and Design Challenges", "pdf_hash": "91924fd23f8c1fd86db68871d790f2be014be5df", "year": 2020, "venue": "CHI", "alt_text": "An image with a sketch showing the relationship that children think have physical activity, heart rate and emotion in the left. In the right is a smartphone sketch showing the relationship between heart rate and emotion", "levels": null, "corpus_id": 218483261, "sentences": ["An image with a sketch showing the relationship that children think have physical activity, heart rate and emotion in the left.", "In the right is a smartphone sketch showing the relationship between heart rate and emotion"], "caption": "", "local_uri": ["91924fd23f8c1fd86db68871d790f2be014be5df_Image_014.jpg", "91924fd23f8c1fd86db68871d790f2be014be5df_Image_015.jpg"], "annotated": false, "compound": true}
{"title": "Arm-A-Dine: Towards Understanding the Design of Playful Embodied Eating Experiences", "pdf_hash": "413942edab8cf30aba9bc6d1e664b973da4f7085", "year": 2018, "venue": "CHI PLAY", "alt_text": "Arm-A-Dine features an on-body robotic arm (supporting the feeding action from plate to mouth) and an attached smartphone to capture facial expression of the eating partner.", "levels": null, "corpus_id": 53081679, "sentences": ["Arm-A-Dine features an on-body robotic arm (supporting the feeding action from plate to mouth) and an attached smartphone to capture facial expression of the eating partner."], "caption": "Figure 2: Arm-A-Dine features an on-body robotic arm (supporting the feeding action from plate to mouth) and an attached smartphone to capture facial expression of the eating partner.", "local_uri": ["413942edab8cf30aba9bc6d1e664b973da4f7085_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Arm-A-Dine: Towards Understanding the Design of Playful Embodied Eating Experiences", "pdf_hash": "413942edab8cf30aba9bc6d1e664b973da4f7085", "year": 2018, "venue": "CHI PLAY", "alt_text": "Any \u201cneutral\u201d facial expression makes the partner\u2019s third arm hesitate mid-air before finally deciding whom to feed at random", "levels": null, "corpus_id": 53081679, "sentences": ["Any \u201cneutral\u201d facial expression makes the partner\u2019s third arm hesitate mid-air before finally deciding whom to feed at random"], "caption": "Figure 4: Any \u201cneutral\u201d facial expression makes the partner\u2019s third arm hesitate mid-air before finally deciding whom to feed at random", "local_uri": ["413942edab8cf30aba9bc6d1e664b973da4f7085_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Fabrication Games: Using 3D Printers to Explore New Interactions for Tabletop Games", "pdf_hash": "a8e2888c5b9ee5e6ecccdb83a8c0954af695310f", "year": 2017, "venue": "Creativity & Cognition", "alt_text": "Photograph of a clay prototype of a dice-based game. Image shows 4 groups of clay dice in red, purple, blue, and green.", "levels": [[-1], [-1]], "corpus_id": 23235869, "sentences": ["Photograph of a clay prototype of a dice-based game.", "Image shows 4 groups of clay dice in red, purple, blue, and green."], "caption": "Figure 1. Prototype of Star Dice, a 3D-printed dice-rolling game created by a team of tabletop gamers. In Star Dice, players customize and print a set of personalized dice that matches their chosen gameplay strategy.", "local_uri": ["a8e2888c5b9ee5e6ecccdb83a8c0954af695310f_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Fabrication Games: Using 3D Printers to Explore New Interactions for Tabletop Games", "pdf_hash": "a8e2888c5b9ee5e6ecccdb83a8c0954af695310f", "year": 2017, "venue": "Creativity & Cognition", "alt_text": "Hand-drawn storyboard showing a potential interaction with a 3D printer-enhanced game. In this storyboard, players press a button on a laptop, which causes the 3D printer to generate a random die.", "levels": null, "corpus_id": 23235869, "sentences": ["Hand-drawn storyboard showing a potential interaction with a 3D printer-enhanced game.", "In this storyboard, players press a button on a laptop, which causes the 3D printer to generate a random die."], "caption": "", "local_uri": ["a8e2888c5b9ee5e6ecccdb83a8c0954af695310f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Fabrication Games: Using 3D Printers to Explore New Interactions for Tabletop Games", "pdf_hash": "a8e2888c5b9ee5e6ecccdb83a8c0954af695310f", "year": 2017, "venue": "Creativity & Cognition", "alt_text": "Sketch of a map for a strategy game, which shows a game grid and several player's regions on randomly generated terrain", "levels": null, "corpus_id": 23235869, "sentences": ["Sketch of a map for a strategy game, which shows a game grid and several player's regions on randomly generated terrain"], "caption": "Figure 5. Game sketches. Left: Mat for the game Zombie Twister can be customized with swappable faceplates. Right: Strip Mine board shows randomly generated terrain.", "local_uri": ["a8e2888c5b9ee5e6ecccdb83a8c0954af695310f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Deaf Users\u2019 Preferences Among Wake-Up Approaches during Sign-Language Interaction with Personal Assistant Devices", "pdf_hash": "f3390946ff54b660e1667ebb15d779e22bb23a2c", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figure includes multiple screenshots illustrating steps in interacting with a personal assistant device sitting on a small table. In addition, there are six detailed images of how the actor in the photo is interacting with the device to wake it up, including: (a) signing an ASL sign-name, (b) pressing the screen of a smartphone running an app, (c) fingerspelling the letters of the device's English name, (d) waving a hand in the air in the direction of the device, (e) clapping the hands, or (f) pressing a button on a remote control.", "levels": null, "corpus_id": 233987663, "sentences": ["The figure includes multiple screenshots illustrating steps in interacting with a personal assistant device sitting on a small table.", "In addition, there are six detailed images of how the actor in the photo is interacting with the device to wake it up, including: (a) signing an ASL sign-name, (b) pressing the screen of a smartphone running an app, (c) fingerspelling the letters of the device's English name, (d) waving a hand in the air in the direction of the device, (e) clapping the hands, or (f) pressing a button on a remote control."], "caption": "Figure 1: Video storyboard with the device-user interaction steps: (1) user uses the wake-up technique (2), device wakes-up,", "local_uri": ["f3390946ff54b660e1667ebb15d779e22bb23a2c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "I'm All Eyes and Ears: Exploring Effective Locators for Privacy Awareness in IoT Scenarios", "pdf_hash": "f77775acce870cf1038e91cda9eef889e5a9d7f9", "year": 2020, "venue": "CHI", "alt_text": "To address growing privacy concerns about being monitored by IoT devices while unaware, we designed, prototyped, and evaluated several designs for locators (visual, auditory, and contextual pictures) to help people physically locate IoT devices.", "levels": null, "corpus_id": 218483167, "sentences": ["To address growing privacy concerns about being monitored by IoT devices while unaware, we designed, prototyped, and evaluated several designs for locators (visual, auditory, and contextual pictures) to help people physically locate IoT devices."], "caption": "(a) Visual cues, e.g., LED light                                (b) Auditory cues, e.g., beep sound              (c) Contextualized pictures, e.g., on a smartphone app", "local_uri": ["f77775acce870cf1038e91cda9eef889e5a9d7f9_Image_001.jpg", "f77775acce870cf1038e91cda9eef889e5a9d7f9_Image_002.jpg", "f77775acce870cf1038e91cda9eef889e5a9d7f9_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "Gaze Guidance for Captioned Videos for DHH Users", "pdf_hash": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "year": 2020, "venue": "", "alt_text": "An example of a study stimuli. On the top left corner, there is a video lecture containing the lecturer and under that video, there is the fact list. In the top-center, there is the slide title. The center area present three bullet points. In the lower-center, there is ASR-generated subtitles text.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 229194491, "sentences": ["An example of a study stimuli.", "On the top left corner, there is a video lecture containing the lecturer and under that video, there is the fact list.", "In the top-center, there is the slide title.", "The center area present three bullet points.", "In the lower-center, there is ASR-generated subtitles text."], "caption": "", "local_uri": ["acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gaze Guidance for Captioned Videos for DHH Users", "pdf_hash": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "year": 2020, "venue": "", "alt_text": "Overview of experiments, which address research questions. Outputs of each box go as input for the next box. The input for the first box is \"Randomized\". The fist box contains information about videos, quizzes, and language. The second box contains the experiment 1 analysis and first research question. the third box, represent experiment 2 with DHH and presents with guidance and without guidance information. The fourth box, present the analysis for experiment 2.", "levels": null, "corpus_id": 229194491, "sentences": ["Overview of experiments, which address research questions.", "Outputs of each box go as input for the next box.", "The input for the first box is \"Randomized\".", "The fist box contains information about videos, quizzes, and language.", "The second box contains the experiment 1 analysis and first research question.", "the third box, represent experiment 2 with DHH and presents with guidance and without guidance information.", "The fourth box, present the analysis for experiment 2."], "caption": "", "local_uri": ["acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Gaze Guidance for Captioned Videos for DHH Users", "pdf_hash": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "year": 2020, "venue": "", "alt_text": "Box plot present percent fixation times of native and nonnative. mean fixation times of \"Nonnative\" is higher than \"Native\".", "levels": [[1], [3]], "corpus_id": 229194491, "sentences": ["Box plot present percent fixation times of native and nonnative.", "mean fixation times of \"Nonnative\" is higher than \"Native\"."], "caption": "", "local_uri": ["acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gaze Guidance for Captioned Videos for DHH Users", "pdf_hash": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "year": 2020, "venue": "", "alt_text": "Steps for the algorithm to select gaze guidance modulations. The most left part represents participants gaze points. Then, iMotions fixation clustering. Then the next step is mean shift clustering. finally, the right-most part is to show \u201cModulation\u201d selecting top thirty clusters with the most fixations.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 229194491, "sentences": ["Steps for the algorithm to select gaze guidance modulations.", "The most left part represents participants gaze points.", "Then, iMotions fixation clustering.", "Then the next step is mean shift clustering.", "finally, the right-most part is to show \u201cModulation\u201d selecting top thirty clusters with the most fixations."], "caption": "", "local_uri": ["acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gaze Guidance for Captioned Videos for DHH Users", "pdf_hash": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "year": 2020, "venue": "", "alt_text": "Box plot present percent fixation times for videos with vs. without. mean fixation times of \"Without\" is higher than \"With Video\".", "levels": [[1], [2]], "corpus_id": 229194491, "sentences": ["Box plot present percent fixation times for videos with vs. without.", "mean fixation times of \"Without\" is higher than \"With Video\"."], "caption": "", "local_uri": ["acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Gaze Guidance for Captioned Videos for DHH Users", "pdf_hash": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "year": 2020, "venue": "", "alt_text": "Box plot results of experiment 2. There are two boxes presents the comprehension scores for videos with guidance versus without guidance. Comprehension scores for videos without guidance is higher than comprehension scores for videos with guidance.", "levels": [[1], [1], [2]], "corpus_id": 229194491, "sentences": ["Box plot results of experiment 2.", "There are two boxes presents the comprehension scores for videos with guidance versus without guidance.", "Comprehension scores for videos without guidance is higher than comprehension scores for videos with guidance."], "caption": "", "local_uri": ["acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Optimizing Portrait Lighting at Capture-Time Using a 360 Camera as a Light Probe", "pdf_hash": "7ea75488547826945f768cd13897b7c559139a8d", "year": 2019, "venue": "UIST", "alt_text": "Figure 4 shows graphs of the image quality metric for two scenes. For each graph, it shows an image/PRT render pair that correspond to min, median, and max image quality metric scores.", "levels": [[-1], [-1]], "corpus_id": 201702154, "sentences": ["Figure 4 shows graphs of the image quality metric for two scenes.", "For each graph, it shows an image/PRT render pair that correspond to min, median, and max image quality metric scores."], "caption": "Figure 4. The image quality metric varies as we rotate the environment. Maximizing the metric produces a good match (both photo and PRT render) to the target facial appearance and the bright/dark areas in the environment map match those in the pre-integrated target weighting function Ft . At the median and minimum metric values the matches are not as good.", "local_uri": ["7ea75488547826945f768cd13897b7c559139a8d_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparison of Finite-Repertoire and Data-Driven Facial Expressions for Sign Language Avatars", "pdf_hash": "c798e119140274c1d642488b9681ceb8dae1906b", "year": 2015, "venue": "HCI", "alt_text": "An avatar performing the sentence: \"Do you like my shirt\"\nASL glosses: MY SHIRT YOU LIKE\nExpression: Topic (MY SHIRT) Yes/no Question (YOU LIKE)\nGaze: Head and eyes at viewer", "levels": [[-1]], "corpus_id": 122568, "sentences": ["An avatar performing the sentence: \"Do you like my shirt\"\nASL glosses: MY SHIRT YOU LIKE\nExpression: Topic (MY SHIRT) Yes/no Question (YOU LIKE)\nGaze: Head and eyes at viewer"], "caption": "Fig. 1. This graphic depicts a timeline of an ASL sentence consisting of four signs (shown in the \u201cGlosses\u201d row) with co-occurring facial expressions from the software\u2019s built-in repertoire as specified by the user (shown in the \u201cexpression\u201d row). The creator of this timeline has speci- fied that a \u201cTopic\u201d facial expression should occur during the first two words and a \u201cYes No Question\u201d facial expression during the final two.", "local_uri": ["c798e119140274c1d642488b9681ceb8dae1906b_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparison of Finite-Repertoire and Data-Driven Facial Expressions for Sign Language Avatars", "pdf_hash": "c798e119140274c1d642488b9681ceb8dae1906b", "year": 2015, "venue": "HCI", "alt_text": "Another avatar performing the sentence: \"Do you like my shirt\"\nASL glosses: MY SHIRT YOU LIKE\nFAP01, FAP02, ..., FAP67: curves of these features applitude changing in time", "levels": [[1]], "corpus_id": 122568, "sentences": ["Another avatar performing the sentence: \"Do you like my shirt\"\nASL glosses: MY SHIRT YOU LIKE\nFAP01, FAP02, ..., FAP67: curves of these features applitude changing in time"], "caption": "Fig. 2. A timeline is shown that specifies an ASL sentence with four words (shown in the \u201cGlosses\u201d row), with additional curves plotted above, each of which depicts the changing val- ues of a single MPEG-4 parameter that governs the movements of the face/head. For instance, one parameter may govern the height of the inner portion of the signer\u2019s left eyebrow.", "local_uri": ["c798e119140274c1d642488b9681ceb8dae1906b_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparison of Finite-Repertoire and Data-Driven Facial Expressions for Sign Language Avatars", "pdf_hash": "c798e119140274c1d642488b9681ceb8dae1906b", "year": 2015, "venue": "HCI", "alt_text": "Expr(OLD) Scores boxplot\nMin value: 1.0\nMax value: 10.0\nMedian: 2.5\nAverage: 3.3\n\nNon(OLD) Scores boxplot\nMin value: 1.0\nMax value: 7.0\nMedian: 1.0\nAverage: 2.17\n\nExpr(NEW) Scores boxplot\nMin value: 1.0\nMax value: 10.0\nMedian: 2.0\nAverage: 3.56\n\nNon(NEW) Scores boxplot\nMin value: 1.0\nMax value: 10.0\nMedian: 1.0\nAverage: 2.14", "levels": [[2, 1]], "corpus_id": 122568, "sentences": ["Expr(OLD) Scores boxplot\nMin value: 1.0\nMax value: 10.0\nMedian: 2.5\nAverage: 3.3\n\nNon(OLD) Scores boxplot\nMin value: 1.0\nMax value: 7.0\nMedian: 1.0\nAverage: 2.17\n\nExpr(NEW) Scores boxplot\nMin value: 1.0\nMax value: 10.0\nMedian: 2.0\nAverage: 3.56\n\nNon(NEW) Scores boxplot\nMin value: 1.0\nMax value: 10.0\nMedian: 1.0\nAverage: 2.14"], "caption": "Fig. 3. Notice Scores for OLD and NEW Animation Platform.", "local_uri": ["c798e119140274c1d642488b9681ceb8dae1906b_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Comparison of Finite-Repertoire and Data-Driven Facial Expressions for Sign Language Avatars", "pdf_hash": "c798e119140274c1d642488b9681ceb8dae1906b", "year": 2015, "venue": "HCI", "alt_text": "Expr(OLD) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.5\nAverage: 0.51\n\nNon(OLD) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.5\nAverage: 0.46\n\nExpr(NEW) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.83\nAverage: 0.63\n\nNon(NEW) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.33\nAverage: 0.4", "levels": [[2, 1]], "corpus_id": 122568, "sentences": ["Expr(OLD) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.5\nAverage: 0.51\n\nNon(OLD) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.5\nAverage: 0.46\n\nExpr(NEW) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.83\nAverage: 0.63\n\nNon(NEW) Scores boxplot\nMin value: 0.0\nMax value: 1.0\nMedian: 0.33\nAverage: 0.4"], "caption": "Fig. 4. Comprehension Scores for OLD and NEW Animation Platform.", "local_uri": ["c798e119140274c1d642488b9681ceb8dae1906b_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The Design of COVID Alert NY in Six Parables", "pdf_hash": "09369e64b181ccf5cf47f88e178414a22a60f052", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Cognitive Walkthrough goes from July 17th to July 30th, paper prototyping goes from July 30th to August 11th, Wizard of Oz goes from August 8th to August 11th, UserZoom unmoderated remote think-aloud tests goe from July 12th to August 31st, Columbia Field Trial goes from August 28th to September 15th, SUNY Field Trial goes from September 7th to September 30. Launch was October 1, 2020.", "levels": [[-1], [-1]], "corpus_id": 233987359, "sentences": ["Cognitive Walkthrough goes from July 17th to July 30th, paper prototyping goes from July 30th to August 11th, Wizard of Oz goes from August 8th to August 11th, UserZoom unmoderated remote think-aloud tests goe from July 12th to August 31st, Columbia Field Trial goes from August 28th to September 15th, SUNY Field Trial goes from September 7th to September 30.", "Launch was October 1, 2020."], "caption": "", "local_uri": ["09369e64b181ccf5cf47f88e178414a22a60f052_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Design of COVID Alert NY in Six Parables", "pdf_hash": "09369e64b181ccf5cf47f88e178414a22a60f052", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "COVID Tracker says \"You must be 16 or older to use this app\" and presents two buttons, \"I am 16 or older\" and \"I am under 16.\"   COVID Alert NY says \"I am over 18, or I am a parent or guardian of a minor confirming that they can use the app.\" with a button that says \"I Confirm.\"", "levels": null, "corpus_id": 233987359, "sentences": ["COVID Tracker says \"You must be 16 or older to use this app\" and presents two buttons, \"I am 16 or older\" and \"I am under 16.\"   COVID Alert NY says \"I am over 18, or I am a parent or guardian of a minor confirming that they can use the app.\" with a button that says \"I Confirm.\""], "caption": "", "local_uri": ["09369e64b181ccf5cf47f88e178414a22a60f052_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "The Design of COVID Alert NY in Six Parables", "pdf_hash": "09369e64b181ccf5cf47f88e178414a22a60f052", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "a) NYS Dept of Health: Open COVID Alert NY app and go to My COVID Alerts to enter the following code: 123456  (b) NYS Dept of Health: Open COVID Alert NY app. Go to My COVID Alerts tab. Tap \"What to do if you test positive for COVID-19\". Scroll to bottom to enter 123456  (c) COVID Alert NY: Open App. Tap \"My COVID Alerts\" then \"What to do if you test positive.\" Scroll to bottom and tap \"Share Your Close Contact Codes\". Enter 123456.", "levels": null, "corpus_id": 233987359, "sentences": ["a) NYS Dept of Health: Open COVID Alert NY app and go to My COVID Alerts to enter the following code: 123456  (b) NYS Dept of Health: Open COVID Alert NY app.", "Go to My COVID Alerts tab.", "Tap \"What to do if you test positive for COVID-19\".", "Scroll to bottom to enter 123456  (c) COVID Alert NY: Open App.", "Tap \"My COVID Alerts\" then \"What to do if you test positive.\" Scroll to bottom and tap \"Share Your Close Contact Codes\". Enter 123456."], "caption": "", "local_uri": ["09369e64b181ccf5cf47f88e178414a22a60f052_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "HCI and mHealth Wearable Tech: A Multidisciplinary Research Challenge", "pdf_hash": "7eec1dbe7a3772c555d61f6f24944da044d71ca6", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "A close up of an Oura smart ring. Its a round thick, silver ring with sensors around the inside lining of the ring.", "levels": null, "corpus_id": 218483146, "sentences": ["A close up of an Oura smart ring.", "Its a round thick, silver ring with sensors around the inside lining of the ring."], "caption": "", "local_uri": ["7eec1dbe7a3772c555d61f6f24944da044d71ca6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Design in the Public Square", "pdf_hash": "4309c83e6ec0f83482a20347903d53f518c813d2", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A picture of a blind paddler on the water in a one-person outrigger canoe with the final version of the CoOP system attached to the rear of the canoe.", "levels": null, "corpus_id": 207957556, "sentences": ["A picture of a blind paddler on the water in a one-person outrigger canoe with the final version of the CoOP system attached to the rear of the canoe."], "caption": "", "local_uri": ["4309c83e6ec0f83482a20347903d53f518c813d2_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Design in the Public Square", "pdf_hash": "4309c83e6ec0f83482a20347903d53f518c813d2", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A picture of version three of the CoOP system attached to an outrigger canoe. In the corner of the picture is a hand holding the CoOP transmitter used to control the rudder.", "levels": null, "corpus_id": 207957556, "sentences": ["A picture of version three of the CoOP system attached to an outrigger canoe.", "In the corner of the picture is a hand holding the CoOP transmitter used to control the rudder."], "caption": "", "local_uri": ["4309c83e6ec0f83482a20347903d53f518c813d2_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Design in the Public Square", "pdf_hash": "4309c83e6ec0f83482a20347903d53f518c813d2", "year": 2019, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A picture of the five iterations of the CoOP system. The first three pictures from left to right use two legs and four feet with straps to attach to the canoe. The final two pictures have replaced the legs and straps with a suction cup.", "levels": null, "corpus_id": 207957556, "sentences": ["A picture of the five iterations of the CoOP system.", "The first three pictures from left to right use two legs and four feet with straps to attach to the canoe.", "The final two pictures have replaced the legs and straps with a suction cup."], "caption": "", "local_uri": ["4309c83e6ec0f83482a20347903d53f518c813d2_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Everyday Experiences of Reminiscence for People with Blindness: Practices, Tensions and Probing New Design Possibilities", "pdf_hash": "807a038e7ecafe205bfbc672e5388ba87faa3f5d", "year": 2021, "venue": "CHI", "alt_text": "The left photo shows two action figures placed on a tissue box on the table. The action figure on the left is a male character in a black suit, holding a gun on his left hand. The figure on the right is wearing a green jacket and holding two short knives on each hand. Two figures are facing at each other as if they are about to start a fight.    The right photo shows two handmade artwork that look like male superheros in clay. The one on the left has an abstract form of male superhero and spiky structure intertwined with each other. The one of the right has a form of masculine male with spiky hair and a muscular torso.", "levels": null, "corpus_id": 233987329, "sentences": ["The left photo shows two action figures placed on a tissue box on the table.", "The action figure on the left is a male character in a black suit, holding a gun on his left hand.", "The figure on the right is wearing a green jacket and holding two short knives on each hand.", "Two figures are facing at each other as if they are about to start a fight.", "The right photo shows two handmade artwork that look like male superheros in clay.", "The one on the left has an abstract form of male superhero and spiky structure intertwined with each other.", "The one of the right has a form of masculine male with spiky hair and a muscular torso."], "caption": "", "local_uri": ["807a038e7ecafe205bfbc672e5388ba87faa3f5d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Everyday Experiences of Reminiscence for People with Blindness: Practices, Tensions and Probing New Design Possibilities", "pdf_hash": "807a038e7ecafe205bfbc672e5388ba87faa3f5d", "year": 2021, "venue": "CHI", "alt_text": "The left photo shows a male sitting on a comfortable chair with a black digital recorder placed on his left lap.    The right photo shows a black digital recorder placed on the table.", "levels": null, "corpus_id": 233987329, "sentences": ["The left photo shows a male sitting on a comfortable chair with a black digital recorder placed on his left lap.", "The right photo shows a black digital recorder placed on the table."], "caption": "", "local_uri": ["807a038e7ecafe205bfbc672e5388ba87faa3f5d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Everyday Experiences of Reminiscence for People with Blindness: Practices, Tensions and Probing New Design Possibilities", "pdf_hash": "807a038e7ecafe205bfbc672e5388ba87faa3f5d", "year": 2021, "venue": "CHI", "alt_text": "The image shows an elderly male touching photo frames on the wall and a young female holding a paper is watching him. There are more than ten photo frames of different sizes hanging on the wall.", "levels": null, "corpus_id": 233987329, "sentences": ["The image shows an elderly male touching photo frames on the wall and a young female holding a paper is watching him. There are more than ten photo frames of different sizes hanging on the wall."], "caption": "", "local_uri": ["807a038e7ecafe205bfbc672e5388ba87faa3f5d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Everyday Experiences of Reminiscence for People with Blindness: Practices, Tensions and Probing New Design Possibilities", "pdf_hash": "807a038e7ecafe205bfbc672e5388ba87faa3f5d", "year": 2021, "venue": "CHI", "alt_text": "The image shows an iPhone sitting on the table. The smartphone is showing a snapshot of mobile Facebook application, including a few search results on the top and three profile images on the bottom.", "levels": null, "corpus_id": 233987329, "sentences": ["The image shows an iPhone sitting on the table.", "The smartphone is showing a snapshot of mobile Facebook application, including a few search results on the top and three profile images on the bottom."], "caption": "", "local_uri": ["807a038e7ecafe205bfbc672e5388ba87faa3f5d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Everyday Experiences of Reminiscence for People with Blindness: Practices, Tensions and Probing New Design Possibilities", "pdf_hash": "807a038e7ecafe205bfbc672e5388ba87faa3f5d", "year": 2021, "venue": "CHI", "alt_text": "The left image is a close-up view of a stack of old photo albums on the far bottom-right corner of a shelf.     The right image shows a young female sitting on the table is lightly touching a photo album with her left hand. The photo album is placed on the table, right in front of her.", "levels": null, "corpus_id": 233987329, "sentences": ["The left image is a close-up view of a stack of old photo albums on the far bottom-right corner of a shelf.", "The right image shows a young female sitting on the table is lightly touching a photo album with her left hand.", "The photo album is placed on the table, right in front of her."], "caption": "", "local_uri": ["807a038e7ecafe205bfbc672e5388ba87faa3f5d_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "This image is taken in a classroom. There is a large red ball hanging near the ceiling. This ball has electrical sockets and red charging wires hanging from it. One of the wires is plugged into a Smart Brailler which is on a table.", "levels": null, "corpus_id": 233987449, "sentences": ["This image is taken in a classroom.", "There is a large red ball hanging near the ceiling.", "This ball has electrical sockets and red charging wires hanging from it.", "One of the wires is plugged into a Smart Brailler which is on a table."], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "(Left) A colorful tactile map of the United States. Parts of the map are raised to indicate that country's terrain. The longitude and latitude have Braille labels. There are two plastic dinosaur toys also on the map. (Right) A child sits at a table with food prep materials in front of her. She is trying to open a bottle of mayonnaise. In front of her is a bowl of cut up eggs, a slide of bread, relish, black pepper, salt, and mustard.", "levels": null, "corpus_id": 233987449, "sentences": ["(Left) A colorful tactile map of the United States.", "Parts of the map are raised to indicate that country's terrain.", "The longitude and latitude have Braille labels.", "There are two plastic dinosaur toys also on the map. (Right) A child sits at a table with food prep materials in front of her.", "She is trying to open a bottle of mayonnaise.", "In front of her is a bowl of cut up eggs, a slide of bread, relish, black pepper, salt, and mustard."], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_003.jpg", "58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "Letter magnets attached to a white sheet. Each magnet has a print letter and a Braille label on top.", "levels": null, "corpus_id": 233987449, "sentences": ["Letter magnets attached to a white sheet.", "Each magnet has a print letter and a Braille label on top."], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "There is a tub filled with a variety of blocks. Some of the blocks have numbers, letters, Braille, and pictures. Two students are partially in view. One student is stacking blocks in a tower. Another student is playing with a toy called \"bopit!\"", "levels": null, "corpus_id": 233987449, "sentences": ["There is a tub filled with a variety of blocks.", "Some of the blocks have numbers, letters, Braille, and pictures.", "Two students are partially in view.", "One student is stacking blocks in a tower.", "Another student is playing with a toy called \"bopit!\""], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "A student is sitting in front of a large keyboard and tinkering with buttons on it. The keyboard has Braille labels on it.", "levels": null, "corpus_id": 233987449, "sentences": ["A student is sitting in front of a large keyboard and tinkering with buttons on it.", "The keyboard has Braille labels on it."], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "Three posters against a wall in the classroom detailing what tasks should complete for their daily routine, classroom expectations, and levels of behavior.", "levels": null, "corpus_id": 233987449, "sentences": ["Three posters against a wall in the classroom detailing what tasks should complete for their daily routine, classroom expectations, and levels of behavior."], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "A student typing on a smart brailler. The screen of the brailler shows Braille letters and the print letters p and d.", "levels": null, "corpus_id": 233987449, "sentences": ["A student typing on a smart brailler.", "The screen of the brailler shows Braille letters and the print letters p and d."], "caption": "Figure 8: A student uses the Smart Brailler to practice letter contractions in Braille.", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Technology Design for Students with Vision Impairment in the Classroom and Remotely", "pdf_hash": "58c57c3ed4dd312f04151f8f46263212ae8385eb", "year": 2021, "venue": "CHI", "alt_text": "(Left) In this iPad user interface mockup, there is a time counting down at the top. Below the timer are the letters \"c\", \"a\", and \"t\" and a submit button. The image also shows sketches of the tangible blocks at the bottom of the iPad. (Middle) In this mockup of Story Maker, the screen says \"Chef's Tales\" which is the story prompt. Below that is a sentence with a highlighted blank and a prompt for what kind of word the student should fill in the blank. Below that is a submit button and sketches of the tangibles on the iPad screen. (Right) In this Grocery Games interface mock up, the screen displays a recipe and the ingredients needed for that recipe. Below the ingredients is a \"submit ingredient\" button. Below that are sketches of the blocks on the iPad.", "levels": null, "corpus_id": 233987449, "sentences": ["(Left) In this iPad user interface mockup, there is a time counting down at the top.", "Below the timer are the letters \"c\", \"a\", and \"t\" and a submit button.", "The image also shows sketches of the tangible blocks at the bottom of the iPad. (Middle) In this mockup of Story Maker, the screen says \"Chef's Tales\" which is the story prompt.", "Below that is a sentence with a highlighted blank and a prompt for what kind of word the student should fill in the blank.", "Below that is a submit button and sketches of the tangibles on the iPad screen. (Right) In this Grocery Games interface mock up, the screen displays a recipe and the ingredients needed for that recipe.", "Below the ingredients is a \"submit ingredient\" button.", "Below that are sketches of the blocks on the iPad."], "caption": "", "local_uri": ["58c57c3ed4dd312f04151f8f46263212ae8385eb_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection", "pdf_hash": "7940222e83f0bea7bea567509fbf9dd90c735230", "year": 2015, "venue": "UIST", "alt_text": "(a) A circle with dotted lines describing cross target angles. They are equally spaced at 22.5 degree increments. (b) (c) and (d) show different touch tasks,", "levels": null, "corpus_id": 1029718, "sentences": ["(a) A circle with dotted lines describing cross target angles.", "They are equally spaced at 22.5 degree increments.", "(b) (c) and (d) show different touch tasks,"], "caption": "Figure 2. Experiment task: (a) 16 crossing line target locations in a radial array 22.5\u00b0 apart; (b) 1PIN-Touch task requires a one finger touch on a circular target for the pin, then crossing a single target with another finger (for example, the 22.5\u00b0 target shown here); (c) 2PIN-Touch task requires a one finger touch on a circular target and a second finger touch in the grey area for a two fin\u00ad ger pin before crossing; (d) 1PIN-Drag requires a one finger pin, then a downward drag below a line before crossing; (e) 2PIN- Drag requires same two finger pin, then downward drag before crossing. See accompanying video for task demonstrations.", "local_uri": ["7940222e83f0bea7bea567509fbf9dd90c735230_Image_002.png"], "annotated": false, "compound": false}
{"title": "Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection", "pdf_hash": "7940222e83f0bea7bea567509fbf9dd90c735230", "year": 2015, "venue": "UIST", "alt_text": "Graphs of results for each task. Each graph has three lines: A middle line that shows time in milliseconds vs angle, and two lines that show the 95% confidence intervals.There are also dotted horizontal lines for preference and error rate.", "levels": [[1], [1], [1]], "corpus_id": 1029718, "sentences": ["Graphs of results for each task.", "Each graph has three lines: A middle line that shows time in milliseconds vs angle, and two lines that show the 95% confidence intervals.", "There are also dotted horizontal lines for preference and error rate."], "caption": "Figure 3. Time, error, and preference by crossing target Angle for each Task (lower values are better, 95% CI shown for time).", "local_uri": ["7940222e83f0bea7bea567509fbf9dd90c735230_Image_003.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection", "pdf_hash": "7940222e83f0bea7bea567509fbf9dd90c735230", "year": 2015, "venue": "UIST", "alt_text": "Hands interacting with various context menus and demonstrating different technigues.", "levels": null, "corpus_id": 1029718, "sentences": ["Hands interacting with various context menus and demonstrating different technigues."], "caption": "Figure 8. Demonstrations: (a) context menu showing rotate command; (b) combining with a marking menu; (c) objects at the edge are dragged to middle, pin-and-cross performed, then released, springing back to original location; (d) two-finger scrolling extend\u00ad ed to support paging and go to end; (c) changing transformation constraints like angle snapping mid-transformation; (d) changing drawing mode from curve, to line, to rectangle without lifting drawing finger. (see accompanying video for full demonstrations)", "local_uri": ["7940222e83f0bea7bea567509fbf9dd90c735230_Image_013.png"], "annotated": false, "compound": false}
{"title": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "pdf_hash": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "year": 2020, "venue": "", "alt_text": "The components of the BrailleBlocks system. On the left is the overhead webcam. In the middle are the green BrailleBlocks, red pegs, and white frame. On the right is a laptop displaying the home screen of the BrailleBlocks companion application.", "levels": null, "corpus_id": 219570509, "sentences": ["The components of the BrailleBlocks system.", "On the left is the overhead webcam. In the middle are the green BrailleBlocks, red pegs, and white frame.", "On the right is a laptop displaying the home screen of the BrailleBlocks companion application."], "caption": "Figure 1. The physical components of BrailleBlocks: An overhead webcam, blocks and pegs to create Braille letters, a white cardboard frame to place the blocks in, and a laptop with the BrailleBlocks application.", "local_uri": ["ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "pdf_hash": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "year": 2020, "venue": "", "alt_text": "A BrailleBlocks user playing with the tangibles. The user has placed five blocks in the frame. They are reaching for red pegs to place in the holes of the blocks.", "levels": null, "corpus_id": 219570509, "sentences": ["A BrailleBlocks user playing with the tangibles.", "The user has placed five blocks in the frame.", "They are reaching for red pegs to place in the holes of the blocks."], "caption": "Figure 2. A user is placing pegs into the blocks to create Braille letters, and placing blocks in the cardboard frame to create words.", "local_uri": ["ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "pdf_hash": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "year": 2020, "venue": "", "alt_text": "Two screens from the BrailleBlocks companion application. On the top is the selection chart from the Animal Name Game. The chart shows a cartoon dog, elephant, duck, and sheep. On the bottom is a level 1 of the Word Scramble Game. The screen shows instructions on how to play the game. It also shows a visual Braille representation of the letters \"t\", \"c\", and \"a\". Below the Braille are a list of words that the letters can unscramble to make.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 219570509, "sentences": ["Two screens from the BrailleBlocks companion application.", "On the top is the selection chart from the Animal Name Game.", "The chart shows a cartoon dog, elephant, duck, and sheep.", "On the bottom is a level 1 of the Word Scramble Game.", "The screen shows instructions on how to play the game.", "It also shows a visual Braille representation of the letters \"t\", \"c\", and \"a\".", "Below the Braille are a list of words that the letters can unscramble to make."], "caption": "Figure 3 (top). The Animal Name Game selection page. Figure 4 (bottom). Level 1 of the Word Scramble game.", "local_uri": ["ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_003.jpg", "ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "pdf_hash": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "year": 2020, "venue": "", "alt_text": "A participant family using BrailleBlocks. The parent has placed their hands over their childs hands and they are feeling the holes in the BrailleBlocks which are in the white frame.", "levels": null, "corpus_id": 219570509, "sentences": ["A participant family using BrailleBlocks.", "The parent has placed their hands over their childs hands and they are feeling the holes in the BrailleBlocks which are in the white frame."], "caption": "Figure 8. A participant family using BrailleBlocks. The parent is using a hand-over-hand guidance technique with their child to orient them with the blocks.", "local_uri": ["ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "pdf_hash": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "year": 2020, "venue": "", "alt_text": "A structure that one of the participants made while engaging in creative play. The blocks are stacked into two layers and all the holes are filled with pegs. The participant has also stacked the cups that the pegs come in. The participant's parent is sitting in front of the laptop.", "levels": null, "corpus_id": 219570509, "sentences": ["A structure that one of the participants made while engaging in creative play.", "The blocks are stacked into two layers and all the holes are filled with pegs.", "The participant has also stacked the cups that the pegs come in.", "The participant's parent is sitting in front of the laptop."], "caption": "Figure 9. A participant family using BrailleBlocks for creative play. The child participant stacked the blocks built a story around the blocks, pegs, frame, and webcam.", "local_uri": ["ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "DDF Seeks Same : Sexual Health-Related Language in Online Personal Ads For Men Who Have Sex With Men", "pdf_hash": "be00f1bcb7d9f911a3ca855e2e7b1bc926be47e8", "year": 2014, "venue": "", "alt_text": "Village Voice 1978: 0.00% of ads contained SHR language Village Voice 1982: 1.70% of ads contained SHR language Village Voice 1985: 14.01% of ads contained SHR language Village Voice 1988: 22.99% of ads contained SHR language Craigslist 2013: 53.50% of ads contained SHR language", "levels": null, "corpus_id": 202696423, "sentences": ["Village Voice 1978: 0.00% of ads contained SHR language Village Voice 1982: 1.70% of ads contained SHR language Village Voice 1985: 14.01% of ads contained SHR language Village Voice 1988: 22.99% of ads contained SHR language Craigslist 2013: 53.50% of ads contained SHR language"], "caption": "CategoryVillage Voice (1988)Craigslist, NYC only (2013)Craigslist, 95 locations (2013)Overall SHR22.99%53.50%53.96%DiseaseNA46.30%48.34%HIV (sub-category of Disease)NA17.03%14.76%SafetyNA17.87%12.85%ProtectionNA3.56%4.78%Risk (sub-category of Protection)NA2.35%3.38%HealthNA2.12%1.23%Davidson\u2019s health- related dictionary [14]22.99%34.35%32.73%Davidson\u2019s sexual exclusivity subcode [14]13.41%0.07%0.15%", "local_uri": ["be00f1bcb7d9f911a3ca855e2e7b1bc926be47e8_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "DDF Seeks Same : Sexual Health-Related Language in Online Personal Ads For Men Who Have Sex With Men", "pdf_hash": "be00f1bcb7d9f911a3ca855e2e7b1bc926be47e8", "year": 2014, "venue": "", "alt_text": "This figure is a scatterplot showing the relationship between HIV Estimated Prevalence Rate Per 100,000 Population (CDC, 2011) on the x-axis and Percentage of Ads Containing Sexual Health-Related Language on the y-axis in 95 locations. The relationship is estimated linearly by the equation SHR language = 46.360 + 0.009 * prevalence rate, p = 0.009. Each location is represented by a circle, with the size of the circle representing its population relative to the other locations. Outliers with a low HIV prevalence rate and a high % of SHR language in ads include SF Bay Area and Boise, ID. Outliers with a high HIV prevalence rate and a low % of SHR language in ads include Wichita, KS and Jackson, MS. New York City is close to the linear trend, with a high HIV prevalence rate and a high % of SHR language in ads. In general, locations with a higher population tend to have a greater percentage of ads containing SHR language.", "levels": [[1], [2], [1], [1], [1], [3], [3]], "corpus_id": 202696423, "sentences": ["This figure is a scatterplot showing the relationship between HIV Estimated Prevalence Rate Per 100,000 Population (CDC, 2011) on the x-axis and Percentage of Ads Containing Sexual Health-Related Language on the y-axis in 95 locations.", "The relationship is estimated linearly by the equation SHR language = 46.360 + 0.009 * prevalence rate, p = 0.009.", "Each location is represented by a circle, with the size of the circle representing its population relative to the other locations.", "Outliers with a low HIV prevalence rate and a high % of SHR language in ads include SF Bay Area and Boise, ID.", "Outliers with a high HIV prevalence rate and a low % of SHR language in ads include Wichita, KS and Jackson, MS.", "New York City is close to the linear trend, with a high HIV prevalence rate and a high % of SHR language in ads.", "In general, locations with a higher population tend to have a greater percentage of ads containing SHR language."], "caption": "SHR language = 46.360 + 0.009 * prevalence rate, p=0.009", "local_uri": ["be00f1bcb7d9f911a3ca855e2e7b1bc926be47e8_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "CoolCraig: A Smart Watch/Phone Application Supporting Co-Regulation of Children with ADHD", "pdf_hash": "db45ab95535835f78bca814a87bf1fd2657f0b2e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: Screenshots of the CoolCraig smartphone app where parents can set-up goals and rewards for the children.", "levels": null, "corpus_id": 218482553, "sentences": ["Figure 1: Screenshots of the CoolCraig smartphone app where parents can set-up goals and rewards for the children."], "caption": "", "local_uri": ["db45ab95535835f78bca814a87bf1fd2657f0b2e_Image_002.jpg", "db45ab95535835f78bca814a87bf1fd2657f0b2e_Image_003.jpg", "db45ab95535835f78bca814a87bf1fd2657f0b2e_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "CoolCraig: A Smart Watch/Phone Application Supporting Co-Regulation of Children with ADHD", "pdf_hash": "db45ab95535835f78bca814a87bf1fd2657f0b2e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3. Screenshot of the check-in survey to support self-regulation of emotions", "levels": null, "corpus_id": 218482553, "sentences": ["Figure 3.", "Screenshot of the check-in survey to support self-regulation of emotions"], "caption": "", "local_uri": ["db45ab95535835f78bca814a87bf1fd2657f0b2e_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Co-Designing with Orangutans: Enhancing the Design of Enrichment for Animals", "pdf_hash": "923f935dc61ac2fd18fe73d7f283c97c1a45b50c", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Zoo keeper in uniform holds an iPad with two hands at the gridded bars (approx 7cm square) of an orangutan enclosure. An orangutan looks at the iPad and passes fingers through the grid to touch the iPad with a finger tip. The iPad app shows small images on a green background.", "levels": [[-1], [-1], [-1]], "corpus_id": 220324286, "sentences": ["Zoo keeper in uniform holds an iPad with two hands at the gridded bars (approx 7cm square) of an orangutan enclosure.", "An orangutan looks at the iPad and passes fingers through the grid to touch the iPad with a finger tip.", "The iPad app shows small images on a green background."], "caption": "Figure 3: Orangutan Malu interacting with an iPad game.", "local_uri": ["923f935dc61ac2fd18fe73d7f283c97c1a45b50c_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Data-Driven, Player-Centric Approach to Evaluating Spatial Skill Training Games", "pdf_hash": "b0e8f277b1b96ad4abe82203b553ad78d1765713", "year": 2020, "venue": "CHI PLAY", "alt_text": "Three rows of isometric block drawings. First row has two with text \"is rotated to\" in-between. Second row as one in between the text \"as\" and \"is rotated to\". Third row has five, each labelled A, B, C, D, or E.", "levels": null, "corpus_id": 226238875, "sentences": ["Three rows of isometric block drawings.", "First row has two with text \"is rotated to\" in-between.", "Second row as one in between the text \"as\" and \"is rotated to\".", "Third row has five, each labelled A, B, C, D, or E."], "caption": "Figure 1: A sample question from the PSVT:R test of mental rotation. Participants are shown an exemplar fgure before and after being rotated a certain way and must decide which of the answer choices results from performing the same ro- tation on the second exemplar fgure.", "local_uri": ["b0e8f277b1b96ad4abe82203b553ad78d1765713_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "A Data-Driven, Player-Centric Approach to Evaluating Spatial Skill Training Games", "pdf_hash": "b0e8f277b1b96ad4abe82203b553ad78d1765713", "year": 2020, "venue": "CHI PLAY", "alt_text": "Labelled A: first-person view looking down a canyon with rocky outcroppings in the distance. Labelled B: a picture of two blocky 3D objects, each with one glowing face. One has three sets of arrows for rotation around it. Interface controls for selecting different objects, showing a picture of the finished object, and the rotations remaining are shown on the edges. Labelled C: A 3D cylindrical object next to four 2D shapes labelled A through D. Labelled D: Two drawings of 3D objects with buttons below them displaying the choices, Cut, Join, and Intersect. Labelled E: A large isometric drawing of a 3D block-figure with the letters Z and X encircled by counterclockwise circular arrows shown below, with three different isometric drawings to the left labelled A, B, and D.", "levels": null, "corpus_id": 226238875, "sentences": ["Labelled A: first-person view looking down a canyon with rocky outcroppings in the distance.", "Labelled B: a picture of two blocky 3D objects, each with one glowing face.", "One has three sets of arrows for rotation around it.", "Interface controls for selecting different objects, showing a picture of the finished object, and the rotations remaining are shown on the edges.", "Labelled C: A 3D cylindrical object next to four 2D shapes labelled A through D. Labelled D: Two drawings of 3D objects with buttons below them displaying the choices, Cut, Join, and Intersect.", "Labelled E: A large isometric drawing of a 3D block-figure with the letters Z and X encircled by counterclockwise circular arrows shown below, with three different isometric drawings to the left labelled A, B, and", "D."], "caption": "Figure 2: Sample screens from the two spatial training interventions. Lef: screenshots of Exploration Mode (A) and Construc- tion Mode (B) in Homeworld Bound: Redux. Right: Examples of three question types used in the Spatial Exercises intervention: Choosing the 2D object that produced the shown 3D solid of revolution (C), determining which operation has been performed on the two intersecting objects shown on the left to produce the object on the right (D), and selecting which of the answer views on the right is produced by performing the two indicated 90 degree axis rotations on the fgure on the left (E).", "local_uri": ["b0e8f277b1b96ad4abe82203b553ad78d1765713_Image_003.png"], "annotated": false, "compound": false}
{"title": "A Data-Driven, Player-Centric Approach to Evaluating Spatial Skill Training Games", "pdf_hash": "b0e8f277b1b96ad4abe82203b553ad78d1765713", "year": 2020, "venue": "CHI PLAY", "alt_text": "A boxplot with IMI Enjoyment Rating on the y-axis and Intervention on the x-axis. The box for the Python Graphics intervention is centered at 5 on the y-axis, the Spatial Exercises box is centered at about 3.6, and the Homeworld Bound box is centered at about 4.", "levels": [[1], [2]], "corpus_id": 226238875, "sentences": ["A boxplot with IMI Enjoyment Rating on the y-axis and Intervention on the x-axis.", "The box for the Python Graphics intervention is centered at 5 on the y-axis, the Spatial Exercises box is centered at about 3.6, and the Homeworld Bound box is centered at about 4."], "caption": "", "local_uri": ["b0e8f277b1b96ad4abe82203b553ad78d1765713_Image_004.gif"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "A Data-Driven, Player-Centric Approach to Evaluating Spatial Skill Training Games", "pdf_hash": "b0e8f277b1b96ad4abe82203b553ad78d1765713", "year": 2020, "venue": "CHI PLAY", "alt_text": "Four pictures, each containing a pair of blocky 3D objects with glowing attachment areas. Labelled B1: Objects with rectangular attachment areas. Labelled B2: Objects with six-sided asymmetrical attachment areas. Labelled B3: Objects with six-sided corner-shaped attachment areas. Labelled B4: Objects with triangular attachment areas.", "levels": null, "corpus_id": 226238875, "sentences": ["Four pictures, each containing a pair of blocky 3D objects with glowing attachment areas.", "Labelled B1: Objects with rectangular attachment areas.", "Labelled B2: Objects with six-sided asymmetrical attachment areas.", "Labelled B3: Objects with six-sided corner-shaped attachment areas.", "Labelled B4: Objects with triangular attachment areas."], "caption": "Figure 4: Examples of 2D attachment area shapes in lev- els B1-B4 in Homeworld Bound: Redux\u2019s Construction mode. Glowing green areas indicate the attachment areas the player is currently trying to align by rotating one of the parts. In levels B1 and B4, the 2D shapes are simpler and more symmetric, potentially reducing the need to employ mental rotation skills to complete the level.", "local_uri": ["b0e8f277b1b96ad4abe82203b553ad78d1765713_Image_005.png"], "annotated": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "TipText is a one-handed text entry system using thumb-tip tapping on the index finger. It can be used in many wearable applications, including interacting with a smartwatch, wearing google glass when carrying a  shopping bag (c) This is the TipText keyboard layout. It's a grid layout with 'Q' 'W' 'E' 'R' in the first key, 'T' 'Y' 'U' in the second key, 'I' 'O' 'P' in the third key, 'A' 'S' 'D' 'Z' 'X' in the fourth key, 'F' 'G' 'H' 'C' 'V' 'B' in the fifth key and 'J' 'K' 'L' 'N' 'M' in the sixth key.", "levels": null, "corpus_id": 203579571, "sentences": ["TipText is a one-handed text entry system using thumb-tip tapping on the index finger.", "It can be used in many wearable applications, including interacting with a smartwatch, wearing google glass when carrying a  shopping bag (c) This is the TipText keyboard layout.", "It's a grid layout with 'Q' 'W' 'E' 'R' in the first key, 'T' 'Y' 'U' in the second key, 'I' 'O' 'P' in the third key, 'A' 'S' 'D' 'Z' 'X' in the fourth key, 'F' 'G' 'H' 'C' 'V' 'B' in the fifth key and 'J' 'K' 'L' 'N' 'M' in the sixth key."], "caption": "Figure 1(a-b) One-handed text entry using thumb-tip tapping on the index finger in wearable applications; (c) TipText keyboard layout.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "This is the study setup. (a) the participant was typing in front of a monitor in eyes-free condition; (b) markers were attached on the fingers; (c) clay models of a participant\u2019s fingertips used for 3D scanning.", "levels": null, "corpus_id": 203579571, "sentences": ["This is the study setup. (a) the participant was typing in front of a monitor in eyes-free condition; (b) markers were attached on the fingers; (c) clay models of a participant\u2019s fingertips used for 3D scanning."], "caption": "Figure 2. Study setup (a) the participant was typing in front of a monitor surrounded by 5 Vicon cameras in eyes-free condition; (b) markers attached on the fingers; (c) clay models of a participant\u2019s fingertips used for 3D scanning.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "This is a demonstration that shows the 3D simulation of two intersected fingers. The green contour refers to the contact area of the index finger surface and the red dot refers to the input point.", "levels": null, "corpus_id": 203579571, "sentences": ["This is a demonstration that shows the 3D simulation of two intersected fingers.", "The green contour refers to the contact area of the index finger surface and the red dot refers to the input point."], "caption": "Figure 3. 3D touch simulation of two intersected fingers: green contour refers to contact area of the index finger surface and red dot refers to the input point.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "This is a demonstration that shows the distributions of all touch points over all keys in the study 1. Scatter plots with 95% confidence ellipses of touch points in a 26 key QWERTY keyboard.", "levels": [[-1], [-1]], "corpus_id": 203579571, "sentences": ["This is a demonstration that shows the distributions of all touch points over all keys in the study 1.", "Scatter plots with 95% confidence ellipses of touch points in a 26 key QWERTY keyboard."], "caption": "Figure 4. Scatter plots with 95% confidence ellipses of touch points in a 26 key QWERTY keyboard.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_015.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "This is a demonstration that shows the distributions of all touch points over all keys in three gird conditions. Scatter plots with 95% confidence ellipses of touch points in three grid layouts.", "levels": [[-1], [-1]], "corpus_id": 203579571, "sentences": ["This is a demonstration that shows the distributions of all touch points over all keys in three gird conditions.", "Scatter plots with 95% confidence ellipses of touch points in three grid layouts."], "caption": "Figure 6. Scatter plots with 95% confidence ellipses of touch points in three grid layouts.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_017.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "(a) This is the final keyboard layout (the same as Figure 1c); (b) This is the layout with the lowest score.", "levels": null, "corpus_id": 203579571, "sentences": ["(a) This is the final keyboard layout (the same as Figure 1c); (b) This is the layout with the lowest score."], "caption": "Figure 7. (a) The final keyboard layout; (b) The layout with lowest score.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "This is a demonstration of three prototypes we implemented. (a) first prototype with PET film; (b) second prototype with FPC; (c) third prototype on temporary tattoo paper.", "levels": null, "corpus_id": 203579571, "sentences": ["This is a demonstration of three prototypes we implemented. (a) first prototype with PET film; (b) second prototype with FPC; (c) third prototype on temporary tattoo paper."], "caption": "Figure 8. (a) first prototype with PET film; (b) second prototype with FPC; (c) third prototype on temporary tattoo paper.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_019.jpg"], "annotated": false, "compound": false}
{"title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard", "pdf_hash": "50b2e5adc0af45f9e22711267899383dc781c367", "year": 2019, "venue": "UIST", "alt_text": "This is a demonstration of text entry speed, mean UER and TER across 4 blocks in the evaluation.", "levels": [[1]], "corpus_id": 203579571, "sentences": ["This is a demonstration of text entry speed, mean UER and TER across 4 blocks in the evaluation."], "caption": "Figure 9. Text entry speed, mean UER and TER across 4 blocks.", "local_uri": ["50b2e5adc0af45f9e22711267899383dc781c367_Image_020.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"I Hear You, I Feel You\": Encouraging Deep Self-disclosure through a Chatbot", "pdf_hash": "dd99f5cb66b6243c24929ad6a5a6edde5d821b1e", "year": 2020, "venue": "CHI", "alt_text": "Figure 1. Illustration of the study design. Standard questions are given to users during two sessions, i.e., Journaling and Sensitive Questions, and the chatbot does not self-disclose and only gives general responses in these two sessions. During Small Talk session, the chatbot gives low self-disclosure to participants from group 2, and gives high self-disclosure to participants from group 3", "levels": null, "corpus_id": 218482970, "sentences": ["Figure 1.", "Illustration of the study design.", "Standard questions are given to users during two sessions, i.e., Journaling and Sensitive Questions, and the chatbot does not self-disclose and only gives general responses in these two sessions.", "During Small Talk session, the chatbot gives low self-disclosure to participants from group 2, and gives high self-disclosure to participants from group 3"], "caption": "Figure 1. Illustration of the study design. Standard questions are given to users during two sessions, i.e., Journaling and Sensitive Questions, and the chatbot does not self-disclose and only gives general responses in these two sessions. During Small Talk session, the chatbot gives low (high) self-disclosure to participants from group 2 (3).", "local_uri": ["dd99f5cb66b6243c24929ad6a5a6edde5d821b1e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\"I Hear You, I Feel You\": Encouraging Deep Self-disclosure through a Chatbot", "pdf_hash": "dd99f5cb66b6243c24929ad6a5a6edde5d821b1e", "year": 2020, "venue": "CHI", "alt_text": "Figure 4. The average self-disclosure level of different groups over time. They show the average levels of self-disclosure for Thoughts & Feelings across the 20 days. In the first week, the self-disclosure levels were similar among the three groups; the difference increased around day 9, with HD being the highest and ND being the lowest to disclose their thoughts.", "levels": [[0], [1], [1], [2]], "corpus_id": 218482970, "sentences": ["Figure 4.", "The average self-disclosure level of different groups over time.", "They show the average levels of self-disclosure for Thoughts & Feelings across the 20 days.", "In the first week, the self-disclosure levels were similar among the three groups; the difference increased around day 9, with HD being the highest and ND being the lowest to disclose their thoughts."], "caption": "", "local_uri": ["dd99f5cb66b6243c24929ad6a5a6edde5d821b1e_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Tactile graphics with a voice demonstration", "pdf_hash": "99046873563a36fe02f26eee252b994884eae7a1", "year": 2014, "venue": "ASSETS", "alt_text": "This is an image of the system in use. It shows a tactile graphics of a bar chart with QR code labels. There is a user scanning a QR code using finger pointing mode.", "levels": [[-1], [-1], [-1]], "corpus_id": 36450924, "sentences": ["This is an image of the system in use.", "It shows a tactile graphics of a bar chart with QR code labels.", "There is a user scanning a QR code using finger pointing mode."], "caption": "{cmbaker, milnel2, jeffsco, bennec3, ladner}@cs.washington.edu", "local_uri": ["99046873563a36fe02f26eee252b994884eae7a1_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Hands-Free Remote Collaboration Over Video: Exploring Viewer and Streamer Reactions", "pdf_hash": "5f1e6f31b47dc62d28c708c455137c2f08657eaa", "year": 2018, "venue": "ISS", "alt_text": "A picture containing wall, indoor, sewing machine, appliance  Description generated with very high confidence", "levels": null, "corpus_id": 53714254, "sentences": ["A picture containing wall, indoor, sewing machine, appliance  Description generated with very high confidence"], "caption": "", "local_uri": ["5f1e6f31b47dc62d28c708c455137c2f08657eaa_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Hands-Free Remote Collaboration Over Video: Exploring Viewer and Streamer Reactions", "pdf_hash": "5f1e6f31b47dc62d28c708c455137c2f08657eaa", "year": 2018, "venue": "ISS", "alt_text": "A person sitting at a table with a computer and smiling at the camera  Description generated with very high confidence", "levels": null, "corpus_id": 53714254, "sentences": ["A person sitting at a table with a computer and smiling at the camera  Description generated with very high confidence"], "caption": "Figure 4. User interfaces of manual hands-free condition.", "local_uri": ["5f1e6f31b47dc62d28c708c455137c2f08657eaa_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot", "pdf_hash": "bc2d3dfd2555c7590021db88d60d729dde30be82", "year": 2016, "venue": "2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "Figure shows final pictures of each of the four agents: AI, mechanical robot, humanoid robot, and human.", "levels": null, "corpus_id": 35269641, "sentences": ["Figure shows final pictures of each of the four agents: AI, mechanical robot, humanoid robot, and human."], "caption": "Fig. 2. Final depictions of agents in a moral dilemma (from left to right, AI, mechanical robot, humanoid robot, human). All drawings \u00a9Justin Finkenaur.", "local_uri": ["bc2d3dfd2555c7590021db88d60d729dde30be82_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot", "pdf_hash": "bc2d3dfd2555c7590021db88d60d729dde30be82", "year": 2016, "venue": "2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "Column graph shows results of Experiment 1: people's blame judgments for each of the four agents, for both action and inaction.", "levels": [[1]], "corpus_id": 35269641, "sentences": ["Column graph shows results of Experiment 1: people's blame judgments for each of the four agents, for both action and inaction."], "caption": "Expanding the analysis to the four-level Agent factor (under neutral phrasing) revealed that people\u2019s blame patterns for action vs. inaction for both the humanoid robot and the AI were similar to the blame pattern for the human agent (ps > .21)\u2014i.e., being blamed more for action than inaction\u2014whereas blame for the mechanical robot differed significantly from blame for the human agent, F(1, 310) = 6.08, p = .014 (see Figure 4). Particularly intriguing is the direct comparison of mechanical robot and humanoid robot, because their accompanying narratives and labels were identical (\u201cadvanced state-of-the-art repair robot\u201d). For the neutral phrasing, the mechanical robot received 7.4 points more blame for inaction than action, whereas the humanoid robot received 10.3 points fewer for inaction than action, F(1, 310) = 2.54, p = .11.", "local_uri": ["bc2d3dfd2555c7590021db88d60d729dde30be82_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot", "pdf_hash": "bc2d3dfd2555c7590021db88d60d729dde30be82", "year": 2016, "venue": "2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "Column graph shows results of Experiment 3: people's blame judgments for each of the four agents, for both action and inaction.", "levels": [[1]], "corpus_id": 35269641, "sentences": ["Column graph shows results of Experiment 3: people's blame judgments for each of the four agents, for both action and inaction."], "caption": "DISCUSSION", "local_uri": ["bc2d3dfd2555c7590021db88d60d729dde30be82_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot", "pdf_hash": "bc2d3dfd2555c7590021db88d60d729dde30be82", "year": 2016, "venue": "2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "Figure shows the combined results of Experiments 1 through 3: a difference score of people's blame judgments for  action minus inaction, for each of the four agents.", "levels": null, "corpus_id": 35269641, "sentences": ["Figure shows the combined results of Experiments 1 through 3: a difference score of people's blame judgments for  action minus inaction, for each of the four agents."], "caption": "Fig. 6. Difference scores of blame judgments for action vs. inaction across three experiments and four agent types.", "local_uri": ["bc2d3dfd2555c7590021db88d60d729dde30be82_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Overview of the ASSETS 2016 conference", "pdf_hash": "7426b2c946002085a8bf5f062737a9894a94a4f5", "year": 2017, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "Photograph of five people standing in a row, in front of a plain background, with the man in the middle (Richard Ladner) holding an award plaque.", "levels": [[-1]], "corpus_id": 28335971, "sentences": ["Photograph of five people standing in a row, in front of a plain background, with the man in the middle (Richard Ladner) holding an award plaque."], "caption": "Figure 1: Presentation of the Outstanding Contribution Award (left to right): SIGACCESS secretary/treasurer Jinjuan Heidi Feng, selection committee member Clayton Lewis, award recipient Richard Ladner, SIGACCESS vice-chair Matt Huenerfauth, and SIGACCESS chair Shari Trewin.", "local_uri": ["7426b2c946002085a8bf5f062737a9894a94a4f5_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Overview of the ASSETS 2016 conference", "pdf_hash": "7426b2c946002085a8bf5f062737a9894a94a4f5", "year": 2017, "venue": "ACM SIGACCESS Access. Comput.", "alt_text": "Photogtaph of a group of approximately 20 people on a tour of a museum, looking at a docent who is giving a tour.  On the right of the image, behind the docent, there is an old-fashioned automobile from the early 20th century.  The docent is speaking and a sign language interpreter is standing next to him.", "levels": null, "corpus_id": 28335971, "sentences": ["Photogtaph of a group of approximately 20 people on a tour of a museum, looking at a docent who is giving a tour.", "On the right of the image, behind the docent, there is an old-fashioned automobile from the early 20th century.", "The docent is speaking and a sign language interpreter is standing next to him."], "caption": "Figure 2: ASSETS attendees during a guided tour of the National Automobile Museum.", "local_uri": ["7426b2c946002085a8bf5f062737a9894a94a4f5_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Confronting People's Fears about Bats: Combining Multi-modal and Environmentally Sensed Data to Promote Curiosity and Discovery", "pdf_hash": "a935689af652fb94571a34338899d45eaca19ebd", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "A photo of a multi-modal device designed to spark curiosity and to confront people\u2019s preconceptions about bats.", "levels": null, "corpus_id": 47018478, "sentences": ["A photo of a multi-modal device designed to spark curiosity and to confront people\u2019s preconceptions about bats."], "caption": "Figure 1: PlayBat, a multi-modal device designed to spark curiosity and to confront people\u2019s preconceptions about bats.", "local_uri": ["a935689af652fb94571a34338899d45eaca19ebd_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Confronting People's Fears about Bats: Combining Multi-modal and Environmentally Sensed Data to Promote Curiosity and Discovery", "pdf_hash": "a935689af652fb94571a34338899d45eaca19ebd", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "A photo of bat activity data visual representation. Different colours represent the intensity of bat calls.", "levels": [[-1], [-1]], "corpus_id": 47018478, "sentences": ["A photo of bat activity data visual representation.", "Different colours represent the intensity of bat calls."], "caption": "Figure 3: Bat activity data visual representation. Different colours represent the intensity of bat calls.", "local_uri": ["a935689af652fb94571a34338899d45eaca19ebd_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Confronting People's Fears about Bats: Combining Multi-modal and Environmentally Sensed Data to Promote Curiosity and Discovery", "pdf_hash": "a935689af652fb94571a34338899d45eaca19ebd", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "An example answer to a quiz question displayed at the beginning of Story Unit 1.", "levels": null, "corpus_id": 47018478, "sentences": ["An example answer to a quiz question displayed at the beginning of Story Unit 1."], "caption": "Figure 4: An example answer to a quiz question displayed at the beginning of Story Unit 1.", "local_uri": ["a935689af652fb94571a34338899d45eaca19ebd_Image_004.gif"], "annotated": false, "compound": false}
{"title": "Confronting People's Fears about Bats: Combining Multi-modal and Environmentally Sensed Data to Promote Curiosity and Discovery", "pdf_hash": "a935689af652fb94571a34338899d45eaca19ebd", "year": 2018, "venue": "Conference on Designing Interactive Systems", "alt_text": "47% I love them, 21% I like them, 19% I don't know, 6% I dislike them, 7% I hate them.", "levels": null, "corpus_id": 47018478, "sentences": ["47% I love them, 21% I like them, 19% I don't know, 6% I dislike them, 7% I hate them."], "caption": "Figure 7. Embedded survey question results\u2014user opinions about bats before using PlayBat.", "local_uri": ["a935689af652fb94571a34338899d45eaca19ebd_Image_008.png"], "annotated": false, "compound": false}
{"title": "The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations", "pdf_hash": "280e9f6d4d794db1dd363bee52dd8190d3612892", "year": 2018, "venue": "ISS", "alt_text": "Two images of a tablet screen. On the left, the tablet shows a video with a small satellite crossing in front of a large star, and shows a visual chart of brightness. On the right, the same video is shown on the Haptic Video Player, with one robot on the screen representing the satellite, and another the approximate brightness level.", "levels": [[-1], [-1], [-1]], "corpus_id": 53717303, "sentences": ["Two images of a tablet screen.", "On the left, the tablet shows a video with a small satellite crossing in front of a large star, and shows a visual chart of brightness.", "On the right, the same video is shown on the Haptic Video Player, with one robot on the screen representing the satellite, and another the approximate brightness level."], "caption": "Figure 1. Side-by-side view of the original video of a small satellite crossing in front of a star and a graph of a temporarily decreasing brightness level (left), and a haptic representation of the same frame with robots representing the satellite (top robot), and the relative brightness level on the graph (bottom robot).", "local_uri": ["280e9f6d4d794db1dd363bee52dd8190d3612892_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations", "pdf_hash": "280e9f6d4d794db1dd363bee52dd8190d3612892", "year": 2018, "venue": "ISS", "alt_text": "Haptic Video Player: The two robots, each representing one ball in the animation, drive to the top of the screen.", "levels": null, "corpus_id": 53717303, "sentences": ["Haptic Video Player: The two robots, each representing one ball in the animation, drive to the top of the screen."], "caption": "", "local_uri": ["280e9f6d4d794db1dd363bee52dd8190d3612892_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations", "pdf_hash": "280e9f6d4d794db1dd363bee52dd8190d3612892", "year": 2018, "venue": "ISS", "alt_text": "Haptic Video Player: The two robots drive downward on the screen, matching the falling robots.", "levels": [[-1]], "corpus_id": 53717303, "sentences": ["Haptic Video Player: The two robots drive downward on the screen, matching the falling robots."], "caption": "Figure 3. Haptic video playback. The original video is shown above, while the haptic video representation is shown below. These snapshots represent various points in lifecycle of a haptic video annotation: (1) two robots sit idle waiting during unannotated portions of the video; (2) an annotation begins, the robots move to their initial positions, and the system provides a spoken description of the scene; (3) the robots move downward, reflecting the motion of the on-screen falling objects.", "local_uri": ["280e9f6d4d794db1dd363bee52dd8190d3612892_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Implementation and evaluation of animation controls sufficient for conveying ASL facial expressions", "pdf_hash": "d7ea0ebf716af351e8c2b23b1c0086048a811856", "year": 2014, "venue": "ASSETS", "alt_text": "Drawing of the fornt of a male face with enumerated dots allocated at characteristics points in the face e.g. three points in the left eyebrow with codes 4.1, 4.3, 4.5. \nThere are two categories of feature points: the ones directly affected by the MPEG-4 FAPs and the rest. E.g. The tip of the nose is directly affected by a control though the base of the nose under the tip is indireclty affected by both nose controls and upper lip controls.", "levels": [[-1], [-1], [-1]], "corpus_id": 18969309, "sentences": ["Drawing of the fornt of a male face with enumerated dots allocated at characteristics points in the face e.g. three points in the left eyebrow with codes 4.1, 4.3, 4.5.", "There are two categories of feature points: the ones directly affected by the MPEG-4 FAPs and the rest.", "E.g. The tip of the nose is directly affected by a control though the base of the nose under the tip is indireclty affected by both nose controls and upper lip controls."], "caption": "", "local_uri": ["d7ea0ebf716af351e8c2b23b1c0086048a811856_Image_001.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Implementation and evaluation of animation controls sufficient for conveying ASL facial expressions", "pdf_hash": "d7ea0ebf716af351e8c2b23b1c0086048a811856", "year": 2014, "venue": "ASSETS", "alt_text": "A screenshot of the front of a male animated face with a detailed wireframe model that indicated the vertices in the avatar's face. The avatar is more dense in vertices in areas such as between the eyebrows, around the eyes, nose and mouth to allow for more expressions. A few cross marks are visible that indicate the feature points upon the mesh.", "levels": [[-1], [-1], [-1]], "corpus_id": 18969309, "sentences": ["A screenshot of the front of a male animated face with a detailed wireframe model that indicated the vertices in the avatar's face.", "The avatar is more dense in vertices in areas such as between the eyebrows, around the eyes, nose and mouth to allow for more expressions.", "A few cross marks are visible that indicate the feature points upon the mesh."], "caption": "", "local_uri": ["d7ea0ebf716af351e8c2b23b1c0086048a811856_Image_002.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Implementation and evaluation of animation controls sufficient for conveying ASL facial expressions", "pdf_hash": "d7ea0ebf716af351e8c2b23b1c0086048a811856", "year": 2014, "venue": "ASSETS", "alt_text": "A screenshot from a human signer's face with a fitted 3D mask. The mask indicates where the eyes, eyebrows, nose, mouth, cheeks, forehead and other facial characteristics of the signer are located. The signer has a neutral face with his eyes wide open.", "levels": [[-1], [-1], [-1]], "corpus_id": 18969309, "sentences": ["A screenshot from a human signer's face with a fitted 3D mask.", "The mask indicates where the eyes, eyebrows, nose, mouth, cheeks, forehead and other facial characteristics of the signer are located.", "The signer has a neutral face with his eyes wide open."], "caption": "Figure 1: (a) Some MPEG-4 feature points, (b) wireframe and feature points in Max, (c) Visage tracker adaptive mask.", "local_uri": ["d7ea0ebf716af351e8c2b23b1c0086048a811856_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Implementation and evaluation of animation controls sufficient for conveying ASL facial expressions", "pdf_hash": "d7ea0ebf716af351e8c2b23b1c0086048a811856", "year": 2014, "venue": "ASSETS", "alt_text": "Two filmstrips of the animated character Max performing the same signs of an ASL sentence \"SHE LIVE WASHINGTON D.C. SHE\".\nThe first filmstip includes screenshots from an animation driven by a recording of a human using the Visage face tracker.  \nIn the second filmstrip the character has a neutral face and head position throughout the sentence. Comparing the first with the second they start differently (head tilted down vs. head slighly up) and they end differently (head tilted further down vs. head tilted slightly down).", "levels": null, "corpus_id": 18969309, "sentences": ["Two filmstrips of the animated character Max performing the same signs of an ASL sentence \"SHE LIVE WASHINGTON D.C. SHE\".", "The first filmstip includes screenshots from an animation driven by a recording of a human using the Visage face tracker.", "In the second filmstrip the character has a neutral face and head position throughout the sentence.", "Comparing the first with the second they start differently (head tilted down vs. head slighly up) and they end differently (head tilted further down vs. head tilted slightly down)."], "caption": "Figure 2: Screenshots from a human-recording-driven and neutral version of a Yes-No Question stimulus in the study.", "local_uri": ["d7ea0ebf716af351e8c2b23b1c0086048a811856_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Implementation and evaluation of animation controls sufficient for conveying ASL facial expressions", "pdf_hash": "d7ea0ebf716af351e8c2b23b1c0086048a811856", "year": 2014, "venue": "ASSETS", "alt_text": "Two graphs: \nLeft (Notice Scores for Driven and Neutral) and \nRight (Comprehension Scores for Driven and Neutral).\n\nNotice Scores:\nDriven Scores boxplot\nMin value: 1.0\nMax value: 10.0\nLower and Upper Quartile: (1.0, 6.0)\nMedian: 1.0\nAverage: 3.41\n\nNeutral Scores boxplot\nMin value: 1.0\nMax value: 10.0\nLower and Upper Quartile: (1.0, 1.0)\nMedian: 1.0\nAverage: 1.92\n\nComprehension Scores:\nDriven Scores boxplot\nMin value: 0.0\nMax value: 1.0\nLower and Upper Quartile: (0.0, 1.0)\nMedian: 0.67\nAverage: 0.56\n\nNeutral Scores boxplot\nMin value: 0.0\nMax value: 1.0\nLower and Upper Quartile: (0.0, 0.67)\nMedian: 0.0\nAverage: 0.31", "levels": [[1], [2]], "corpus_id": 18969309, "sentences": ["Two graphs: \nLeft (Notice Scores for Driven and Neutral) and \nRight (Comprehension Scores for Driven and Neutral).", "Notice Scores:\nDriven Scores boxplot\nMin value: 1.0\nMax value: 10.0\nLower and Upper Quartile: (1.0, 6.0)\nMedian: 1.0\nAverage: 3.41\n\nNeutral Scores boxplot\nMin value: 1.0\nMax value: 10.0\nLower and Upper Quartile: (1.0, 1.0)\nMedian: 1.0\nAverage: 1.92\n\nComprehension Scores:\nDriven Scores boxplot\nMin value: 0.0\nMax value: 1.0\nLower and Upper Quartile: (0.0, 1.0)\nMedian: 0.67\nAverage: 0.56\n\nNeutral Scores boxplot\nMin value: 0.0\nMax value: 1.0\nLower and Upper Quartile: (0.0, 0.67)\nMedian: 0.0\nAverage: 0.31"], "caption": "Figure 3: Notice and Comprehension scores for animations with facial expressions (Driven) and without (Neutral).", "local_uri": ["d7ea0ebf716af351e8c2b23b1c0086048a811856_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Neo-Noumena", "pdf_hash": "04080e7d9dd5bb8134505b5092da87b3b6468e16", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "This image shows two people using the Neo-Noumena system. The pair appear to be engaged in conversation, wearing EEG caps and Microsoft HoloLens head mounted displays. Around them, each person has an aura of fractals, with the person of the left being closely followed by red, spikey asymmetrical fractals, and the person on the right being followed by green, smooth, symmetrical fractals.A person wearing a hat  Description automatically generated", "levels": null, "corpus_id": 218483484, "sentences": ["This image shows two people using the Neo-Noumena system.", "The pair appear to be engaged in conversation, wearing EEG caps and Microsoft HoloLens head mounted displays.", "Around them, each person has an aura of fractals, with the person of the left being closely followed by red, spikey asymmetrical fractals, and the person on the right being followed by green, smooth, symmetrical fractals.", "A person wearing a hat  Description automatically generated"], "caption": "", "local_uri": ["04080e7d9dd5bb8134505b5092da87b3b6468e16_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Neo-Noumena", "pdf_hash": "04080e7d9dd5bb8134505b5092da87b3b6468e16", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "This image illustrates the system architecture of Neo-Noumea. The architecture is represented by a flow chart showing information going from the users EEG, to a laptop computing the signal with OpenBCI, to the support vector machine, to a HoloLens. There is then a circular loop between the signals going to and from the HoloLens and a server, which is being sent via OSC.", "levels": [[-1], [-1], [-1]], "corpus_id": 218483484, "sentences": ["This image illustrates the system architecture of Neo-Noumea.", "The architecture is represented by a flow chart showing information going from the users EEG, to a laptop computing the signal with OpenBCI, to the support vector machine, to a HoloLens.", "There is then a circular loop between the signals going to and from the HoloLens and a server, which is being sent via OSC."], "caption": "", "local_uri": ["04080e7d9dd5bb8134505b5092da87b3b6468e16_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Neo-Noumena", "pdf_hash": "04080e7d9dd5bb8134505b5092da87b3b6468e16", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "This image illustrates the four categories Neo-Noumena groups affect by. This is represented by a cartesian plane drawn up into four quadrants. Clockwise and starting from top left these quadrants are High-Arousal-Low-Valance, High-Arousal-High-Valance, Low-Arousal-High-Valance, and Low-Arousal-Low-Valance. Within each quadrant is a sample of the fractal swarm the classification generates.", "levels": null, "corpus_id": 218483484, "sentences": ["This image illustrates the four categories Neo-Noumena groups affect by.", "This is represented by a cartesian plane drawn up into four quadrants.", "Clockwise and starting from top left these quadrants are High-Arousal-Low-Valance, High-Arousal-High-Valance, Low-Arousal-High-Valance, and Low-Arousal-Low-Valance.", "Within each quadrant is a sample of the fractal swarm the classification generates."], "caption": "The use of fractals to represent emotion was informed by a body of cognitive psychology literature documenting aesthetic appraisal and emotional reactions to fractals [2,12]. Prior work using drones to communicate emotion and natural swarming behavior, has demonstrated how flight path and speed can be used to convey specific emotions [3,4]. Neo-Noumena takes these findings into consideration by modulating the movement speed of fractals, as well as the cohesion, avoidance and alignment of fractal swarms to reflect the emotional experience of the participant.", "local_uri": ["04080e7d9dd5bb8134505b5092da87b3b6468e16_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance", "pdf_hash": "673e92340023a4eb42572964ac9343eda8798a1e", "year": 2018, "venue": "W4A", "alt_text": "Five images A-E of showing examples of the notification and activation options presented to participants in the study.  Image A displays a browser window with a red bar under the URL bar.  Image B displays a browser window with a bar under the URL that states \"the webpage you are currently browsing seems to have small, hard to click targets!  Would you like the page to be zoomed in for a more enjoyable browsing experience?\".  The bar also includes in the right corner two buttons.  The first has the label \"Adapt my UI\" and the second button has the label \"Dismiss\".  Image C displays a webpage with a pop-up window in the center.  The pop-up window includes text that states  \"the webpage you are currently browsing seems to have small, hard to click targets!  Would you like the page to be zoomed in for a more enjoyable browsing experience?\".  The window also includes in the two buttons.  The first has the label \"Adapt my UI\" and the second button has the label \"Dismiss\".  Image D displays a Wikipedia webpage with text describing the University of Maryland, Baltimore County.  The webpage is displayed at 100% which is the default page viewing size.  Image E displays a Wikipedia webpage with text describing the University of Maryland, Baltimore County.  The webpage is displayed at 150% and has been zoomed in to increase the text size.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 49867125, "sentences": ["Five images A-E of showing examples of the notification and activation options presented to participants in the study.", "Image A displays a browser window with a red bar under the URL bar.", "Image B displays a browser window with a bar under the URL that states \"the webpage you are currently browsing seems to have small, hard to click targets!", "Would you like the page to be zoomed in for a more enjoyable browsing experience?\".", "The bar also includes in the right corner two buttons.", "The first has the label \"Adapt my UI\" and the second button has the label \"Dismiss\".", "Image C displays a webpage with a pop-up window in the center.", "The pop-up window includes text that states  \"the webpage you are currently browsing seems to have small, hard to click targets!", "Would you like the page to be zoomed in for a more enjoyable browsing experience?\".", "The window also includes in the two buttons.", "The first has the label \"Adapt my UI\" and the second button has the label \"Dismiss\".", "Image D displays a Wikipedia webpage with text describing the University of Maryland, Baltimore County.", "The webpage is displayed at 100% which is the default page viewing size.", "Image E displays a Wikipedia webpage with text describing the University of Maryland, Baltimore County.", "The webpage is displayed at 150% and has been zoomed in to increase the text size."], "caption": "Figure 1. Notification and Activation Options: (A) The Bar notification appears as a thin horizontal bar under the URL field. (B) The Bar+ notification appears on as a horizontal box under the URL field with text and interactive buttons. (C) The Dialog Box appears as a popup alert with text and interactive buttons. (D) Wikipedia page zoomed at 100% (default viewing size). (E) Wikipedia page at 150% viewing size after manually zooming in.", "local_uri": ["673e92340023a4eb42572964ac9343eda8798a1e_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance", "pdf_hash": "673e92340023a4eb42572964ac9343eda8798a1e", "year": 2018, "venue": "W4A", "alt_text": "An image of a framework of design considerations for personalized pointing in tabular layout.  The framework includes three categories which appear across the width of the top of the table.  These categories include HUTS (Help Users Trust the System), PUC(Put the User in Control), and KUI(Keep Users Informed).  Under each category there is a list of design considerations that relate to that category.   The Help Users Trust the System (HUTS) category includes the following design considerations: describe what occured and provide rationale for recommended assistance; in addition to accuracy and performance, consider users' individual preferences for task completion; understand and incorporate users' expectations; consider users expected level of automation and the desired level of involvement; and understand and incorporate users' expectations for autonomy in task completion.  The Put the User in Control (PUC) category includes: provide information to support informed decision-making about activation; provide a preview of how the automated task will be completed; provide alternative activation options but also provide a way for users to opt out of assistance; in addition to accuracy and performance, consider users' individual preferences for task completion; understand and incorporate users' expectations; and understand and incorporate users' expectation for autonomy in task completion. The Keep Users Informed (KUI)category includes: describe what occured and provide rationale for recommended assistance; provide information to support informed decision-making about activation; and understand and incorporate users' expectations for autonomy in task completion.", "levels": null, "corpus_id": 49867125, "sentences": ["An image of a framework of design considerations for personalized pointing in tabular layout.", "The framework includes three categories which appear across the width of the top of the table.", "These categories include HUTS (Help Users Trust the System), PUC(Put the User in Control), and KUI(Keep Users Informed).", "Under each category there is a list of design considerations that relate to that category.", "The Help Users Trust the System (HUTS) category includes the following design considerations: describe what occured and provide rationale for recommended assistance; in addition to accuracy and performance, consider users' individual preferences for task completion; understand and incorporate users' expectations; consider users expected level of automation and the desired level of involvement; and understand and incorporate users' expectations for autonomy in task completion.", "The Put the User in Control (PUC) category includes: provide information to support informed decision-making about activation; provide a preview of how the automated task will be completed; provide alternative activation options but also provide a way for users to opt out of assistance; in addition to accuracy and performance, consider users' individual preferences for task completion; understand and incorporate users' expectations; and understand and incorporate users' expectation for autonomy in task completion.", "The Keep Users Informed (KUI)category includes: describe what occured and provide rationale for recommended assistance; provide information to support informed decision-making about activation; and understand and incorporate users' expectations for autonomy in task completion."], "caption": "Figure 2. Recommended design considerations for personalized pointing system design.", "local_uri": ["673e92340023a4eb42572964ac9343eda8798a1e_Image_003.png"], "annotated": false, "compound": false}
{"title": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance", "pdf_hash": "673e92340023a4eb42572964ac9343eda8798a1e", "year": 2018, "venue": "W4A", "alt_text": "An image of two types of visualizations that are provided to the user. The Error Type Graph (top) shows the frequency of different kinds of error types over time. The Website Graph (bottom) shows the most common websites where pointing errors have occurred. When users hover over data points, additional information is shown.", "levels": [[1], [1], [1], [1]], "corpus_id": 49867125, "sentences": ["An image of two types of visualizations that are provided to the user.", "The Error Type Graph (top) shows the frequency of different kinds of error types over time.", "The Website Graph (bottom) shows the most common websites where pointing errors have occurred.", "When users hover over data points, additional information is shown."], "caption": "Figure 5. Pointing History Visualizations: Two types of visualizations are provided to the user. The Error Type Graph (top) shows the frequency of different kinds of error types over time. The Website Graph (bottom) shows the most common websites where pointing errors have occurred. When users hover over data points, additional information is shown.", "local_uri": ["673e92340023a4eb42572964ac9343eda8798a1e_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Eyetracking Metrics Related to Subjective Assessments of ASL Animations", "pdf_hash": "5fc3aef74377c8528d6ce115ea04499f30aa306f", "year": 2016, "venue": "", "alt_text": "R-squared = 28.2%, metrics are normalized to sum to 100%\u2028LMG Method, 95% bootstrap confidence intervals.\nNormFaceToFromHands (27.4%) \nFaceTotalFixTime (14.1%) \nNormLowerFaceToFromHands (9.6%)\nLowerFaceToUpperFace (8.8%)\nPercentUpperFaceFix (8.4%)\nBodyToUpperFace (7.5%)\nLowerFaceTotalFixTime (7.4%)\nTotalDetailedTrans (6.1%)\nPercentFaceFix (5.5%)\nNumBodyToOff (2.7%)\nBodyTotalFixTime (2.5%)", "levels": null, "corpus_id": 63285170, "sentences": ["R-squared = 28.2%, metrics are normalized to sum to 100%\u2028LMG Method, 95% bootstrap confidence intervals.", "NormFaceToFromHands (27.4%) \nFaceTotalFixTime (14.1%) \nNormLowerFaceToFromHands (9.6%)\nLowerFaceToUpperFace (8.8%)\nPercentUpperFaceFix (8.4%)\nBodyToUpperFace (7.5%)\nLowerFaceTotalFixTime (7.4%)\nTotalDetailedTrans (6.1%)\nPercentFaceFix (5.5%)\nNumBodyToOff (2.7%)\nBodyTotalFixTime (2.5%)"], "caption": "", "local_uri": ["5fc3aef74377c8528d6ce115ea04499f30aa306f_Image_006.png"], "annotated": false, "compound": false}
{"title": "Eyetracking Metrics Related to Subjective Assessments of ASL Animations", "pdf_hash": "5fc3aef74377c8528d6ce115ea04499f30aa306f", "year": 2016, "venue": "", "alt_text": "R-squared = 29.8%, metrics are normalized to sum to 100%\u2028LMG Method, 95% bootstrap confidence intervals\nPercentFaceFix (30.2%)\nLowerFaceTotalFixTime (14.4%)\nLowerFaceToUpperFace (12.8%)\nPercentLowerFaceFix (12.1%)\nBodyToUpperFace (10.3%)\nNormFaceToFromHands (5.8%)\nNormUpperFaceToFromHands (4.8%)\nBodyTotalFixTime (3.7%)\nLowerFaceToBody (3.1%)\nUpperFaceToBody (2.9%)", "levels": null, "corpus_id": 63285170, "sentences": ["R-squared = 29.8%, metrics are normalized to sum to 100%\u2028LMG Method, 95% bootstrap confidence intervals\nPercentFaceFix (30.2%)\nLowerFaceTotalFixTime (14.4%)\nLowerFaceToUpperFace (12.8%)\nPercentLowerFaceFix (12.1%)\nBodyToUpperFace (10.3%)\nNormFaceToFromHands (5.8%)\nNormUpperFaceToFromHands (4.8%)\nBodyTotalFixTime (3.7%)\nLowerFaceToBody (3.1%)\nUpperFaceToBody (2.9%)"], "caption": "", "local_uri": ["5fc3aef74377c8528d6ce115ea04499f30aa306f_Image_007.png"], "annotated": false, "compound": false}
{"title": "Eyetracking Metrics Related to Subjective Assessments of ASL Animations", "pdf_hash": "5fc3aef74377c8528d6ce115ea04499f30aa306f", "year": 2016, "venue": "", "alt_text": "R-squared = 39.7%, metrics are normalized to sum to 100%\nLMG Method, 95% bootstrap confidence intervals\nPercentFaceFix (19.9%)\nFaceTotalFixTime (19.2%)\nNormTrailDistance (17.1%)\nNumTotalTrans (11.1%)\nNormUpperFaceToFromLowerFace (8.7%)\nLowerFaceToUpperFace (8.3%)\nOffToUpperFace (6.2%)\nNormUpperFaceToFromHands (6.1%)\nNumBodyToOff (3.5%)", "levels": null, "corpus_id": 63285170, "sentences": ["R-squared = 39.7%, metrics are normalized to sum to 100%\nLMG Method, 95% bootstrap confidence intervals\nPercentFaceFix (19.9%)\nFaceTotalFixTime (19.2%)\nNormTrailDistance (17.1%)\nNumTotalTrans (11.1%)\nNormUpperFaceToFromLowerFace (8.7%)\nLowerFaceToUpperFace (8.3%)\nOffToUpperFace (6.2%)\nNormUpperFaceToFromHands (6.1%)\nNumBodyToOff (3.5%)"], "caption": "", "local_uri": ["5fc3aef74377c8528d6ce115ea04499f30aa306f_Image_008.png"], "annotated": false, "compound": false}
{"title": "Eyetracking Metrics Related to Subjective Assessments of ASL Animations", "pdf_hash": "5fc3aef74377c8528d6ce115ea04499f30aa306f", "year": 2016, "venue": "", "alt_text": "Three set of graphs: Grammar, Understand, and Natural.\n\nPercentage of Accounted Variance (Adj. R2) for Grammar:\n1. Best Performing Single-Metric Model for Each Dependent Variable: 9.95\n2. Multiple_Metric Model (corresponds to Figures 1, 2, and 3): 21.67\nSignificant deifference between 1 and 2: p < 0.01\nThe best single-metric model for Grammar uses the NormFaceToFromHands eyetracking metric\n\nPercentage of Accounted Variance (Adj. R2) for Understand:\n1. Best Performing Single-Metric Model for Each Dependent Variable: 18.32\n2. Multiple_Metric Model (corresponds to Figures 1, 2, and 3): 24.03\nSignificant deifference between 1 and 2: p < 0.05\nThe best single-metric model for Understand uses the FaceTotalFixTime eyetracking metric\n\nPercentage of Accounted Variance (Adj. R2) for Natural:\n1. Best Performing Single-Metric Model for Each Dependent Variable: 19.46\n2. Multiple_Metric Model (corresponds to Figures 1, 2, and 3): 35.24\nSignificant deifference between 1 and 2: p < 0.001\nThe best single-metric model for Natural uses the PercentFaceFix eyetracking metric", "levels": [[1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 63285170, "sentences": ["Three set of graphs: Grammar, Understand, and Natural.", "Percentage of Accounted Variance (Adj.", "R2) for Grammar:\n1.", "Best Performing Single-Metric Model for Each Dependent Variable: 9.95\n2.", "Multiple_Metric Model (corresponds to Figures 1, 2, and 3): 21.67\nSignificant deifference between 1 and 2: p < 0.01\nThe best single-metric model for Grammar uses the NormFaceToFromHands eyetracking metric\n\nPercentage of Accounted Variance (Adj.", "R2) for Understand:\n1.", "Best Performing Single-Metric Model for Each Dependent Variable: 18.32\n2.", "Multiple_Metric Model (corresponds to Figures 1, 2, and 3): 24.03\nSignificant deifference between 1 and 2: p < 0.05\nThe best single-metric model for Understand uses the FaceTotalFixTime eyetracking metric\n\nPercentage of Accounted Variance (Adj.", "R2) for Natural:\n1.", "Best Performing Single-Metric Model for Each Dependent Variable: 19.46\n2.", "Multiple_Metric Model (corresponds to Figures 1, 2, and 3): 35.24\nSignificant deifference between 1 and 2: p < 0.001\nThe best single-metric model for Natural uses the PercentFaceFix eyetracking metric"], "caption": "", "local_uri": ["5fc3aef74377c8528d6ce115ea04499f30aa306f_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Learn with Haptics: Improving Vocabulary Recall with Free-form Digital Annotation on Touchscreen Mobiles", "pdf_hash": "42309da40261476f2c86b8d21eecbb224e728caa", "year": 2020, "venue": "CHI", "alt_text": "Examples of verbal (top) and non-verbal (bottom) annotations  a learner produces for a sentence in Islenska", "levels": null, "corpus_id": 218482671, "sentences": ["Examples of verbal (top) and non-verbal (bottom) annotations  a learner produces for a sentence in Islenska"], "caption": "Figure 1. Examples of verbal (top) and non-verbal (bottom) annotations a learner produces for a sentence in Icelandic (\u00cdslenska).", "local_uri": ["42309da40261476f2c86b8d21eecbb224e728caa_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Learn with Haptics: Improving Vocabulary Recall with Free-form Digital Annotation on Touchscreen Mobiles", "pdf_hash": "42309da40261476f2c86b8d21eecbb224e728caa", "year": 2020, "venue": "CHI", "alt_text": "The different phases of the layout from 0 s to 8 s. Participants hear the audio at 0 s and once more between 1.5 - 5 s based on writing for Ww  and Ws, at 4 s for nW", "levels": null, "corpus_id": 218482671, "sentences": ["The different phases of the layout from 0 s to 8 s. Participants hear the audio at 0 s and once more between 1.5 - 5 s based on writing for Ww  and Ws, at 4 s for nW"], "caption": "", "local_uri": ["42309da40261476f2c86b8d21eecbb224e728caa_Image_005.png"], "annotated": false, "compound": false}
{"title": "Learn with Haptics: Improving Vocabulary Recall with Free-form Digital Annotation on Touchscreen Mobiles", "pdf_hash": "42309da40261476f2c86b8d21eecbb224e728caa", "year": 2020, "venue": "CHI", "alt_text": "The complete experimental set-up from day 1 to day 10. FR: Free recall test, CR: Cued recall test", "levels": null, "corpus_id": 218482671, "sentences": ["The complete experimental set-up from day 1 to day 10.", "FR: Free recall test, CR: Cued recall test"], "caption": "Figure 5. The complete experimental set-up from day 1 to day 10. FR: Free recall test, CR: Cued recall test.", "local_uri": ["42309da40261476f2c86b8d21eecbb224e728caa_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "\"This just felt to me like the right thing to do\": Decision-Making Experiences of Parents of Young Children", "pdf_hash": "086db04f80ca19b728f9335d22b48cfea2a820bb", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "This picture shows parents flipping thorugh the metaphor cards sitting at a table", "levels": null, "corpus_id": 220324270, "sentences": ["This picture shows parents flipping thorugh the metaphor cards sitting at a table"], "caption": "Figure 1. Parents selecting metaphor cards", "local_uri": ["086db04f80ca19b728f9335d22b48cfea2a820bb_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Social Technology Appropriation in Dementia: Investigating the Role of Caregivers in Engaging People with Dementia with a Videogame-based Training System", "pdf_hash": "b818fa2b583d5ef2b09bfe98d1167e25e063494f", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: In this Figure the final system  containing several technical components that  is illustrated. The components were organized around a TV. A mini-computer equipped with the system-software and game application that was developed with a Unity 3D Game Engine (C#) provided 20 videogames. The system was connected to an MS Kinect to detect the movements of the participant when interacting with the system. A set of more than thirty levels was incorporated into each videogame, providing an opportunity to match the resources and interests of the participants. After playing the basic levels, the degree of difficulty increased automatically and more dual and cognitive tasks appeared in every game. The system was connected to a backend information platform to generate user profiles, visualisation of results, initiating training schedules and unlock level progression. A PlayStation 3 Buzzer was used as a navigation tool and input device during the games (e.g. to choose an answer during a quiz). The game-system had three different core videogame elements: 1) movement games; 2) coordination and balance games; and 3) cognitive and creative activities. The system was designed to be played in single player mode, meaning that only one person was detected. However, other people could observe the different games and perform the exercises if they wished, though their movements were not detected by the system itself.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483579, "sentences": ["Figure 1: In this Figure the final system  containing several technical components that  is illustrated.", "The components were organized around a TV.", "A mini-computer equipped with the system-software and game application that was developed with a Unity 3D Game Engine (C#) provided 20 videogames.", "The system was connected to an MS Kinect to detect the movements of the participant when interacting with the system.", "A set of more than thirty levels was incorporated into each videogame, providing an opportunity to match the resources and interests of the participants.", "After playing the basic levels, the degree of difficulty increased automatically and more dual and cognitive tasks appeared in every game.", "The system was connected to a backend information platform to generate user profiles, visualisation of results, initiating training schedules and unlock level progression.", "A PlayStation 3 Buzzer was used as a navigation tool and input device during the games (e.g. to choose an answer during a quiz).", "The game-system had three different core videogame elements: 1) movement games; 2) coordination and balance games; and 3) cognitive and creative activities.", "The system was designed to be played in single player mode, meaning that only one person was detected.", "However, other people could observe the different games and perform the exercises if they wished, though their movements were not detected by the system itself."], "caption": "Figure 1 System Overview", "local_uri": ["b818fa2b583d5ef2b09bfe98d1167e25e063494f_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding, Detecting and Mitigating the Effects of Coactivations in Ten-Finger Mid-Air Typing in Virtual Reality", "pdf_hash": "fefc6dbbbd857453b3f7e84b8456dad9a8c8d875", "year": 2021, "venue": "CHI", "alt_text": "Figure 8 shows examples of the coactivation module operating, with and without the decoder. The first example shows the module correctly labelling coactivations in a text stream and the second example shows the decoder leveraging this information to remove coactivations before auto-correcting the remaining typed letters to the desired word.", "levels": null, "corpus_id": 233987505, "sentences": ["Figure 8 shows examples of the coactivation module operating, with and without the decoder.", "The first example shows the module correctly labelling coactivations in a text stream and the second example shows the decoder leveraging this information to remove coactivations before auto-correcting the remaining typed letters to the desired word."], "caption": "17000", "local_uri": ["fefc6dbbbd857453b3f7e84b8456dad9a8c8d875_Image_037.jpg"], "annotated": false, "compound": false}
{"title": "How far is up?: encouraging social interaction through children's book app design", "pdf_hash": "2552b0360100b69dea02e142ca7c8a416afe1f79", "year": 2014, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2: Scene 8, How Far is UP? This app combines space imagery and original illustration. Spacecraft window: Atlantis' Window on the World \u00a9 NASA 2009. Simulation of galaxies: Starry Night Tango \u00a9 NASA 2013. Illustration, writing, typography, digital design \u00a9 Betty Sargeant.", "levels": null, "corpus_id": 36198283, "sentences": ["Figure 2: Scene 8, How Far is UP? This app combines space imagery and original illustration.", "Spacecraft window: Atlantis' Window on the World \u00a9 NASA 2009.", "Simulation of galaxies: Starry Night Tango \u00a9 NASA 2013.", "Illustration, writing, typography, digital design \u00a9 Betty Sargeant."], "caption": "Figure 2: Scene 8, How Far is UP? This app combines space imagery and original illustration. Spacecraft window: Atlantis' Window on the World \u00a9 NASA 2009. Simulation of galaxies: Starry Night Tango \u00a9 NASA 2013. Illustration, writing, typography, digital design \u00a9 Betty Sargeant.", "local_uri": ["2552b0360100b69dea02e142ca7c8a416afe1f79_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "How far is up?: encouraging social interaction through children's book app design", "pdf_hash": "2552b0360100b69dea02e142ca7c8a416afe1f79", "year": 2014, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3: Scene 12, How Far is UP? The text in this scene primarily operates as dialogue. Earth \u00a9 NASA 2012. Illustration, writing, typography, digital design \u00a9 Betty Sargeant.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 36198283, "sentences": ["Figure 3: Scene 12, How Far is UP?", "The text in this scene primarily operates as dialogue.", "Earth \u00a9 NASA 2012.", "Illustration, writing, typography, digital design \u00a9 Betty Sargeant."], "caption": "Figure 3: Scene 12, How Far is UP? The text in this scene primarily operates as dialogue. Earth \u00a9 NASA 2012.", "local_uri": ["2552b0360100b69dea02e142ca7c8a416afe1f79_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How far is up?: encouraging social interaction through children's book app design", "pdf_hash": "2552b0360100b69dea02e142ca7c8a416afe1f79", "year": 2014, "venue": "CHI Extended Abstracts", "alt_text": "Figure 5: Scene 6, How Far is UP? Children may require adult explanations of the ideas presented in this animated scene. Moon \u00a9 NASA 2012. City Lights of the US \u00a9 NASA 2012. Illustration, writing, typography, digital design \u00a9 Betty Sargeant.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 36198283, "sentences": ["Figure 5: Scene 6, How Far is UP?", "Children may require adult explanations of the ideas presented in this animated scene.", "Moon \u00a9 NASA 2012.", "City Lights of the US \u00a9 NASA 2012.", "Illustration, writing, typography, digital design \u00a9 Betty Sargeant."], "caption": "", "local_uri": ["2552b0360100b69dea02e142ca7c8a416afe1f79_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "AccessCSforall: making computer science accessible to K-12 students in the United States", "pdf_hash": "6a6a3d87c572508907da1f777a9c079d5fb05af9", "year": 2017, "venue": "ASAC", "alt_text": "Several groups sitting at tables discussing accessible K-12 computer science education.", "levels": null, "corpus_id": 40508562, "sentences": ["Several groups sitting at tables discussing accessible K-12 computer science education."], "caption": "Figure 2. Discussions held at a Capacity Building Institute in 2016.", "local_uri": ["6a6a3d87c572508907da1f777a9c079d5fb05af9_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: Text stating \"I'd like to visualize 'Origin', 'Miles_per_Gallon', and 'Displacement'\". Two visualizations are shown. The first, a colored tick plot with 'Miles_per_Gallon' on x, 'Origin' on y, and 'Displacement' on color. Next, a colored scatterplot, with 'Miles_per_Gallon' on x, 'Displacement' on y, and 'Origin' on color. There is a question mark between them.", "levels": [[1], [1], [1], [1], [0]], "corpus_id": 210926937, "sentences": ["Figure 1: Text stating \"I'd like to visualize 'Origin', 'Miles_per_Gallon', and 'Displacement'\".", "Two visualizations are shown.", "The first, a colored tick plot with 'Miles_per_Gallon' on x, 'Origin' on y, and 'Displacement' on color.", "Next, a colored scatterplot, with 'Miles_per_Gallon' on x, 'Displacement' on y, and 'Origin' on color.", "There is a question mark between them."], "caption": "Figure 1. Which chart should a recommender suggest? Recommender systems are often forced to make decisions in the face of ambiguous user intent. Sometimes, these decisions will hamper exploration.", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_001.png", "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_002.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: A series of three visualizations. On the left, a histogram of 'life expectancy' on x and count on y. Next, text stating '+ country' followed by a binned circle plot of the two fields, 'country' on y, 'life expectancy on x, and count on size. Finally, text stating '+ fertility' and a colored tick plot, with 'life expectancy' on x, 'fertility' on y, and 'country' on color.", "levels": [[1], [1], [1]], "corpus_id": 210926937, "sentences": ["Figure 2: A series of three visualizations.", "On the left, a histogram of 'life expectancy' on x and count on y. Next, text stating '+ country' followed by a binned circle plot of the two fields, 'country' on y, 'life expectancy on x, and count on size.", "Finally, text stating '+ fertility' and a colored tick plot, with 'life expectancy' on x, 'fertility' on y, and 'country' on color."], "caption": "exploratory analysis tools (e.g., Voyager [27, 28], Tableau) mix the two authoring paradigms to marry agency and automa- tion [9] and provide a more ef\ufb01cient and amenable visualiza- tion authoring experience.", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "A bubble chart with \"IMDB Rating\" binned on x, \"Rotten Tomatoes Rating\" on y, and \"US Gross\" on size.", "levels": [[1]], "corpus_id": 210926937, "sentences": ["A bubble chart with \"IMDB Rating\" binned on x, \"Rotten Tomatoes Rating\" on y, and \"US Gross\" on size."], "caption": "Notice that this visualization, while effective if the analyst\u2019s intent is to gain an understanding of US_Gross as a function of the other variables, does not adhere to the encodings pro- duced by the imdb_by_us_gross query. If users intend to re\ufb01ne their chart (rather than ask a new question), they can anchor on a previous chart to signal their intent and guide Dziban towards a similar design. This can be done by either invoking the anchor function on a previous chart or invoking the anchor_on(...) function with an arbitrary chart.", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "A histogram with \"IMDB Rating\" binned on x, \"US Gross\" on y, and \"Mean of Rotten Tomatoes Rating\" encoded in the bars' color.", "levels": [[1]], "corpus_id": 210926937, "sentences": ["A histogram with \"IMDB Rating\" binned on x, \"US Gross\" on y, and \"Mean of Rotten Tomatoes Rating\" encoded in the bars' color."], "caption": "", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_006.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: Three visualizations. At the top, labeled (a), a bar chart of movie Genres on y and counts on x, along with the Dziban code to generate it (`genre = Chart(movies).field('Major_Genre'`) then new line, `genre`). To the left, labeled (b), a binned circle plot with Genre on x, MPAA Rating on y, and count on size, a cold recommendation (`genre.field('MPAA_Rating')`). To the right, labeled (c) a stacked bar chart with Genre on y, counts on x, and MPAA Rating on color---an anchored recommendation (`genre.anchor().field('MPAA_Rating')`).", "levels": [[1], [1], [1], [1]], "corpus_id": 210926937, "sentences": ["Figure 3: Three visualizations.", "At the top, labeled (a), a bar chart of movie Genres on y and counts on x, along with the Dziban code to generate it (`genre = Chart(movies).field('Major_Genre'`) then new line, `genre`).", "To the left, labeled (b), a binned circle plot with Genre on x, MPAA Rating on y, and count on size, a cold recommendation (`genre.field('MPAA_Rating')`).", "To the right, labeled (c) a stacked bar chart with Genre on y, counts on x, and MPAA Rating on color---an anchored recommendation (`genre.anchor().field('MPAA_Rating')`)."], "caption": "", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_012.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: An origin chart and follow-up authoring paths. The origin plot is a tick plot, displaying \"IMDB Rating\" on the x axis and \"Major Genre\" on the y axis (`genre_vs_rating = Chart(movies).field('IMDB_Rating', 'Major_Genre')` then a newline and `genre_vs_rating`). On the left, the cold path. The first query adds the \"IMDB Votes\" field (`plus_votes = genre_vs_ratings.field('IMDB_Votes')` then a newline and `plus_votes`), resulting in a binned bubble plot with \"IMDB Votes\" replacing \"IMDB Rating\" on the x axis, and \"IMDB Rating\" moving to the size channel. The second query requests \"IMDB Rating\" back on the x channel (`plus_votes2 = plus_votes.field('IMDB_Rating', channel='x')`, newline then `plus_votes2`). The result is a bubble plot with \"IMDB Rating\" and \"IMDB Votes\" switching channels. The third query requests \"IMDB Rating\" to be unbinned (`plus_votes3 = plus_votes2.field('IMDB_Rating', bin=False)`, newline then `plus_votes3`), resulting in a tick plot with color encoding \"IMDB Votes\" instead of size. The final query requests \"IMDB Votes\" to be placed on the size channel (`plus_votes3.field('IMDB_Votes', channel='size')`), resulting in a bubble plot (unbinned) with \"IMDB Rating\" on the x channel, \"Major Genre\" on y, and \"IMDB Votes\" on size. On the right, the anchored path. A single anchored query requesting an additional \"IMDB Votes\" field be added to the original visualization (`genre_vs_rating.anchor().field('IMDB_Votes')`. The result is a bubble plot identical to the final visualization in the cold query lineage.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 210926937, "sentences": ["Figure 7: An origin chart and follow-up authoring paths.", "The origin plot is a tick plot, displaying \"IMDB Rating\" on the x axis and \"Major Genre\" on the y axis (`genre_vs_rating = Chart(movies).field('IMDB_Rating', 'Major_Genre')` then a newline and `genre_vs_rating`).", "On the left, the cold path.", "The first query adds the \"IMDB Votes\" field (`plus_votes = genre_vs_ratings.field('IMDB_Votes')` then a newline and `plus_votes`), resulting in a binned bubble plot with \"IMDB Votes\" replacing \"IMDB Rating\" on the x axis, and \"IMDB Rating\" moving to the size channel.", "The second query requests \"IMDB Rating\" back on the x channel (`plus_votes2 = plus_votes.field('IMDB_Rating', channel='x')`, newline then `plus_votes2`).", "The result is a bubble plot with \"IMDB Rating\" and \"IMDB Votes\" switching channels.", "The third query requests \"IMDB Rating\" to be unbinned (`plus_votes3 = plus_votes2.field('IMDB_Rating', bin=False)`, newline then `plus_votes3`), resulting in a tick plot with color encoding \"IMDB Votes\" instead of size.", "The final query requests \"IMDB Votes\" to be placed on the size channel (`plus_votes3.field('IMDB_Votes', channel='size')`), resulting in a bubble plot (unbinned) with \"IMDB Rating\" on the x channel, \"Major Genre\" on y, and \"IMDB Votes\" on size.", "On the right, the anchored path.", "A single anchored query requesting an additional \"IMDB Votes\" field be added to the original visualization (`genre_vs_rating.anchor().field('IMDB_Votes')`.", "The result is a bubble plot identical to the final visualization in the cold query lineage."], "caption": "Figure 7. Trying to coerce cold recommendations towards a speci\ufb01c goal can result in a frustrating experience in which effectiveness is optimized at the expense of consistency. In this example, an anchored recommenda- tion maintains the unaggregated view of movies plotted by their IMDB Rating and Genre. On the other hand, a cold recommendation initially swaps channel assignments (removing the original visualized relation- ship) and aggregates \ufb01elds (removing sample awareness). Cold queries that attempt to correct these changes can result in further deviations (in this case, changing of mark and channels) that require further adjust- ments. Anchored recommendations ease this process by reducing the number of changes made to the prior.", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_031.jpg", "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_032.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "A heatmap with \"delay\" binned on x, \"time\" binned on y, and \"Count\" encoded in color for each rectangle. There are a low number of bins.", "levels": null, "corpus_id": 210926937, "sentences": ["A heatmap with \"delay\" binned on x, \"time\" binned on y, and \"Count\" encoded in color for each rectangle.", "There are a low number of bins."], "caption": "As it turns out, anchoring was not necessary, as a cold rec- ommendation results in the same visualization. However, we note that anchoring does no harm here. A common pattern with Dziban can be to anchor when in doubt, switching back to cold recommendations if it becomes clear that the intended visualization goal need not be constrained by prior context.", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_034.png"], "annotated": false, "compound": false}
{"title": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "pdf_hash": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "year": 2020, "venue": "CHI", "alt_text": "A heatmap with \"delay\" binned on x, \"time\" binned on y, and \"Count\" encoded in color for each rectangle. There are a high number of bins.", "levels": null, "corpus_id": 210926937, "sentences": ["A heatmap with \"delay\" binned on x, \"time\" binned on y, and \"Count\" encoded in color for each rectangle.", "There are a high number of bins."], "caption": "Here, we take advantage of field\u2019s multi-argument function- ality to modify maxbins for both \u201ctime\u201d and \u201cdelay\u201d at once.", "local_uri": ["2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_035.png"], "annotated": false, "compound": false}
{"title": "Gesture Knitter: A Hand Gesture Design Tool for Head-Mounted Mixed Reality Applications", "pdf_hash": "4cc90799668187b150c2647f1f8e41ff3319e49b", "year": 2021, "venue": "CHI", "alt_text": "Figure 2 shows the preference gallery interface for generating synthetic samples in which two synthetic samples are given to the designer. The designer is then to choose the one that provides significant enough variation without sacrificing the consistency with the original demonstration, which is depicted as an animation in the top frame.    Figure 3 shows the output of the discernability tool, which outputs a bar chart showing the negative log likelihood comparing to all other gestures of the same class. From this data, the designer can compare the negative log likelihood to the actual gesture's class to see if there are any possible recognition issues.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987161, "sentences": ["Figure 2 shows the preference gallery interface for generating synthetic samples in which two synthetic samples are given to the designer.", "The designer is then to choose the one that provides significant enough variation without sacrificing the consistency with the original demonstration, which is depicted as an animation in the top frame.", "Figure 3 shows the output of the discernability tool, which outputs a bar chart showing the negative log likelihood comparing to all other gestures of the same class.", "From this data, the designer can compare the negative log likelihood to the actual gesture's class to see if there are any possible recognition issues."], "caption": "", "local_uri": ["4cc90799668187b150c2647f1f8e41ff3319e49b_Image_026.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Engineering Gender-Inclusivity into Software: Tales from the Trenches", "pdf_hash": "1d1c5c4c1b37d15a0bc9af702c53293e3b45e458", "year": 2019, "venue": "ArXiv", "alt_text": "Abby persona: 4 photos of faces of various ages: 1 Caucasian woman, 1 African-American woman, 1 Asian man (Indian), and 1 Asian woman (not Indian).\n\nBackground text (28 years old, employed as an Accountant, lives in Cardiff, Wales)\n\nMotivations: Abi uses technologies to accomplish her tasks. She learns new technologies [only] if and when she needs to\u2026\nComputer Self-Efficacy: Abi has low confidence about doing unfamiliar computing tasks.  If problems arise \u2026 she often blames herself\u2026\nAttitude toward Risk: Abi\u2019s life is a little complicated and she rarely has spare time. So she is risk averse about using unfamiliar technologies that might need her to spend extra time \u2026\nInformation Processing Style: Abi tends towards a comprehensive information processing style \u2026 she gathers information comprehensively to try to form a complete understanding of the problem before trying to solve it. \u2026\nLearning: ... Abi leans toward process-oriented learning, e.g., tutorials, step-by-step processes, \u2026 She doesn't particularly like learning by tinkering with software \u2026, but when she does tinker, it has positive effects on her understanding of the software.", "levels": null, "corpus_id": 166227887, "sentences": ["Abby persona: 4 photos of faces of various ages: 1 Caucasian woman, 1 African-American woman, 1 Asian man (Indian), and 1 Asian woman (not Indian).", "Background text (28 years old, employed as an Accountant, lives in Cardiff, Wales)\n\nMotivations: Abi uses technologies to accomplish her tasks.", "She learns new technologies [only] if and when she needs to\u2026\nComputer Self-Efficacy: Abi has low confidence about doing unfamiliar computing tasks.", "If problems arise \u2026 she often blames herself\u2026\nAttitude toward Risk: Abi\u2019s life is a little complicated and she rarely has spare time.", "So she is risk averse about using unfamiliar technologies that might need her to spend extra time \u2026\nInformation Processing Style: Abi tends towards a comprehensive information processing style \u2026 she gathers information comprehensively to try to form a complete understanding of the problem before trying to solve it.", "\u2026\nLearning: ... Abi leans toward process-oriented learning, e.g., tutorials, step-by-step processes, \u2026 She doesn't particularly like learning by tinkering with software \u2026, but when she does tinker, it has positive effects on her understanding of the software."], "caption": "Figure 1: Key portions of the Abi persona. See Fig 2 & 3 in the Supplemental Document for complete personas.", "local_uri": ["1d1c5c4c1b37d15a0bc9af702c53293e3b45e458_Image_001.png"], "annotated": false, "compound": false}
{"title": "Engineering Gender-Inclusivity into Software: Tales from the Trenches", "pdf_hash": "1d1c5c4c1b37d15a0bc9af702c53293e3b45e458", "year": 2019, "venue": "ArXiv", "alt_text": "Subgoal Form: Scenario Name: Find Instructions to add TA to course site. (e.g. Boss just called Abby and told her to remove Kelly's access to the system) Subgoal #2 Subgoal Name: Get to Canvas (e.g. make Kelly not be able to log on) Q: Will <persona> have formed this sub-goal as a step to their overall goal? Yes, no or Maybe Maybe is circled. Q(a): Why? (Please explain.) Abby would be uncertain what to click on given the multiple canvas links and is disinclined to tinker. Maybe more interested in 'Learning Opportunities' instead, since it represents a process-oriented learning experience.", "levels": null, "corpus_id": 166227887, "sentences": ["Subgoal Form: Scenario Name: Find Instructions to add TA to course site.", "(e.g. Boss just called Abby and told her to remove Kelly's access to the system) Subgoal #2 Subgoal Name: Get to Canvas (e.g. make Kelly not be able to log on) Q: Will <persona> have formed this sub-goal as a step to their overall goal? Yes, no or Maybe Maybe is circled.", "Q(a): Why? (Please explain.) Abby would be uncertain what to click on given the multiple canvas links and is disinclined to tinker.", "Maybe more interested in 'Learning Opportunities' instead, since it represents a process-oriented learning experience."], "caption": "Figure 3: Team A circled MAYBE to demonstrate that their team members did not all agree on either YES or NO.", "local_uri": ["1d1c5c4c1b37d15a0bc9af702c53293e3b45e458_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Engineering Gender-Inclusivity into Software: Tales from the Trenches", "pdf_hash": "1d1c5c4c1b37d15a0bc9af702c53293e3b45e458", "year": 2019, "venue": "ArXiv", "alt_text": "Flow chart:  First Node: Find [feature] leads to second node: select academic year leads to third node select academic period Splits and leads to two separate nodes, 4a and 4b. 4a: Click on var that enables [feature] 4b: click on dropdown that contains [feature]. Both 4a and 4b come back together and point to node 5: view [feature].", "levels": [[-1], [-1], [-1]], "corpus_id": 166227887, "sentences": ["Flow chart:  First Node: Find [feature] leads to second node: select academic year leads to third node select academic period Splits and leads to two separate nodes, 4a and 4b.", "4a: Click on var that enables [feature] 4b: click on dropdown that contains [feature].", "Both 4a and 4b come back together and point to node 5: view [feature]."], "caption": "Figure 4: Team C evaluated both of the small paths that Abi could take to reach the same subgoal.", "local_uri": ["1d1c5c4c1b37d15a0bc9af702c53293e3b45e458_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Comparing Smartphone Speech Recognition and Touchscreen Typing for Composition and Transcription", "pdf_hash": "1d6ddc73eaa9d7bb263afce449290f07874c9017", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: Four examples of image triads. Semantically similar pairs are in columns, and triads are in different rows. For instance, one row shows an apple, a doctor, and a house, but in one column, the apple is red, and in the other, the apple is green (and so on).", "levels": null, "corpus_id": 218482596, "sentences": ["Figure 2: Four examples of image triads.", "Semantically similar pairs are in columns, and triads are in different rows.", "For instance, one row shows an apple, a doctor, and a house, but in one column, the apple is red, and in the other, the apple is green (and so on)."], "caption": "young amn, heart, soccer ball                    young amn, heart, soccer ball", "local_uri": ["1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_002.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_003.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_004.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_005.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_011.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_012.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_013.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_014.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_015.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_016.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_017.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_018.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_019.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_020.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_021.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_022.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_023.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_024.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_025.jpg", "1d6ddc73eaa9d7bb263afce449290f07874c9017_Image_026.jpg"], "annotated": false, "compound": true}
{"title": "What Makes Smartphone Use Meaningful or Meaningless?", "pdf_hash": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "year": 2018, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "This figure is a bar chart of the average level of  meaningfulness for each of 5 different types of smartphone uses and gratifications. Meaningfulness is shown on the y-axis on a 1-7 point scale. These values are: Productivity (3.6), Information (3.3), Communication (3.3), Entertainment (2.5), and Social Media (2.4).", "levels": [[1], [1], [2]], "corpus_id": 4351563, "sentences": ["This figure is a bar chart of the average level of  meaningfulness for each of 5 different types of smartphone uses and gratifications.", "Meaningfulness is shown on the y-axis on a 1-7 point scale.", "These values are: Productivity (3.6), Information (3.3), Communication (3.3), Entertainment (2.5), and Social Media (2.4)."], "caption": "", "local_uri": ["b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "What Makes Smartphone Use Meaningful or Meaningless?", "pdf_hash": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "year": 2018, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "This figure is a bar chart of the average level of  meaningfulness for 2 motivations of use. Meaningfulness is shown on the y-axis on a 1-7 point scale. These motivations are: Instrumental (3.5) and Habitual (2.5).", "levels": [[1], [1], [2, 1]], "corpus_id": 4351563, "sentences": ["This figure is a bar chart of the average level of  meaningfulness for 2 motivations of use.", "Meaningfulness is shown on the y-axis on a 1-7 point scale.", "These motivations are: Instrumental (3.5) and Habitual (2.5)."], "caption": "", "local_uri": ["b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "What Makes Smartphone Use Meaningful or Meaningless?", "pdf_hash": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "year": 2018, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "This bar graph shows desired frequency on the y-axis (1-5 scale). In order from highest desired frequency to lowest, the 5 U&G types are: productivity, information, communication, entertainment, and social media.", "levels": [[1], [2]], "corpus_id": 4351563, "sentences": ["This bar graph shows desired frequency on the y-axis (1-5 scale).", "In order from highest desired frequency to lowest, the 5 U&G types are: productivity, information, communication, entertainment, and social media."], "caption": "", "local_uri": ["b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "What Makes Smartphone Use Meaningful or Meaningless?", "pdf_hash": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "year": 2018, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "This figure is a stacked bar chart of the motivation behind use at the start, during, and end timing of app use samples. The percentage share of instrumental motivation declined from the start timing (65.6%) to the during timing (51.4%). There was a slight increase in instrumental use from the during timing to the end timing (55.7%).", "levels": [[1], [3, 2], [3, 2]], "corpus_id": 4351563, "sentences": ["This figure is a stacked bar chart of the motivation behind use at the start, during, and end timing of app use samples.", "The percentage share of instrumental motivation declined from the start timing (65.6%) to the during timing (51.4%).", "There was a slight increase in instrumental use from the during timing to the end timing (55.7%)."], "caption": "", "local_uri": ["b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "What Makes Smartphone Use Meaningful or Meaningless?", "pdf_hash": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "year": 2018, "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.", "alt_text": "This line graph shows the percentage share of instrumental motivation for 5 types of use as timing goes from start, during, to end. The absolute change in instrumental motivation from the start to the end timing was: 0.0% for productivity, -4.0% for information, -6.0% for communication, -7.9% for entertainment, and -9.8% for social media.", "levels": [[1], [2]], "corpus_id": 4351563, "sentences": ["This line graph shows the percentage share of instrumental motivation for 5 types of use as timing goes from start, during, to end.", "The absolute change in instrumental motivation from the start to the end timing was: 0.0% for productivity, -4.0% for information, -6.0% for communication, -7.9% for entertainment, and -9.8% for social media."], "caption": "", "local_uri": ["b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Game changer: accessible audio and tactile guidance for board and card games", "pdf_hash": "7fe4e4dd2312ca3ab889cf73013dc0adb0d99633", "year": 2020, "venue": "W4A", "alt_text": "Two players sitting at a table playing Sorry the board game with one player wearing headphones and using a number keypad connected to a laptop and webcam", "levels": null, "corpus_id": 209478893, "sentences": ["Two players sitting at a table playing Sorry the board game with one player wearing headphones and using a number keypad connected to a laptop and webcam"], "caption": "Figure 1. A player uses Game Changer\u2019s audio guidance and tactile landmarks to explore the board game SORRY!.", "local_uri": ["7fe4e4dd2312ca3ab889cf73013dc0adb0d99633_Image_001.png"], "annotated": false, "compound": false}
{"title": "Game changer: accessible audio and tactile guidance for board and card games", "pdf_hash": "7fe4e4dd2312ca3ab889cf73013dc0adb0d99633", "year": 2020, "venue": "W4A", "alt_text": "Game Changer setup with the board game Sorry sitting on a table with a number keypad and webcam that are connected to a laptop", "levels": null, "corpus_id": 209478893, "sentences": ["Game Changer setup with the board game Sorry sitting on a table with a number keypad and webcam that are connected to a laptop"], "caption": "Figure 2. System setup that shows the laptop, webcam, numeric keypad, and game board.", "local_uri": ["7fe4e4dd2312ca3ab889cf73013dc0adb0d99633_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Game changer: accessible audio and tactile guidance for board and card games", "pdf_hash": "7fe4e4dd2312ca3ab889cf73013dc0adb0d99633", "year": 2020, "venue": "W4A", "alt_text": "Tactile modifications added to the board labeled with each different modification denoted with a different shape.", "levels": null, "corpus_id": 209478893, "sentences": ["Tactile modifications added to the board labeled with each different modification denoted with a different shape."], "caption": "Figure 3. Tactile modifications added to the game board to mark the regions of the board.", "local_uri": ["7fe4e4dd2312ca3ab889cf73013dc0adb0d99633_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Game changer: accessible audio and tactile guidance for board and card games", "pdf_hash": "7fe4e4dd2312ca3ab889cf73013dc0adb0d99633", "year": 2020, "venue": "W4A", "alt_text": "4 examples of the game pieces created for the board game with Aruco Markers placed on them", "levels": null, "corpus_id": 209478893, "sentences": ["4 examples of the game pieces created for the board game with Aruco Markers placed on them"], "caption": "Figure 4. Wooden game pieces that have been laser cut into the shape of circles, clovers, squares, and pentagons with ArUco markers on them.", "local_uri": ["7fe4e4dd2312ca3ab889cf73013dc0adb0d99633_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Game changer: accessible audio and tactile guidance for board and card games", "pdf_hash": "7fe4e4dd2312ca3ab889cf73013dc0adb0d99633", "year": 2020, "venue": "W4A", "alt_text": "Image of the Sorry board taken from a webcam transformed so the board's corners line up with the image's corners", "levels": null, "corpus_id": 209478893, "sentences": ["Image of the Sorry board taken from a webcam transformed so the board's corners line up with the image's corners"], "caption": "", "local_uri": ["7fe4e4dd2312ca3ab889cf73013dc0adb0d99633_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Can Online Juries Make Consistent, Repeatable Decisions?", "pdf_hash": "d39617ff459ef7835fa64c16668b234b31a364b4", "year": 2021, "venue": "CHI", "alt_text": "A figure that illustrates how the experiment is structured. The title says, 'All participants experience 4 rounds.' Under this statement, there are four squares, each representing one of the four rounds. Two of the squares, colored green and labeled '2 individual rounds,' have six human figures, all wearing green hats, standing by themselves. The other two squares are colored purple and are labeled '2 groups rounds,' with six human figures standing in a circle and having a discussion. The human figures wear blue hats in the first purple square, and yellow hats in the second purple square. An arrow labeled 'randomize order' points right, towards a timeline. The timeline begins with a 15 minute calibration, and then represents 4 experimental rounds, each labeled 1 through 4. Two of the rounds are colored green (for individual) and two are colored purple (for group). The timeline ends with a final survey. The two green rounds are connected by an arrow, and the two purple rounds are connected by an arrow. Both arrows say, 'pairs of correlated cases.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 231751808, "sentences": ["A figure that illustrates how the experiment is structured.", "The title says, 'All participants experience 4 rounds.'", "Under this statement, there are four squares, each representing one of the four rounds.", "Two of the squares, colored green and labeled '2 individual rounds,' have six human figures, all wearing green hats, standing by themselves.", "The other two squares are colored purple and are labeled '2 groups rounds,' with six human figures standing in a circle and having a discussion.", "The human figures wear blue hats in the first purple square, and yellow hats in the second purple square.", "An arrow labeled 'randomize order' points right, towards a timeline.", "The timeline begins with a 15 minute calibration, and then represents 4 experimental rounds, each labeled 1 through 4.", "Two of the rounds are colored green (for individual) and two are colored purple (for group).", "The timeline ends with a final survey.", "The two green rounds are connected by an arrow, and the two purple rounds are connected by an arrow.", "Both arrows say, 'pairs of correlated cases."], "caption": "", "local_uri": ["d39617ff459ef7835fa64c16668b234b31a364b4_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Phoneme-based predictive text entry interface", "pdf_hash": "2206f67d914439c9f77cf7c42ede86124b3537e9", "year": 2014, "venue": "ASSETS", "alt_text": "Figure 1: Five stages of the pie menu in the process of creating the word \"excellent\"", "levels": null, "corpus_id": 13058190, "sentences": ["Figure 1: Five stages of the pie menu in the process of creating the word \"excellent\""], "caption": "Figure 1. Five stages of the pie menu in the process of creating the word \"excellent\"", "local_uri": ["2206f67d914439c9f77cf7c42ede86124b3537e9_Image_001.png"], "annotated": false, "compound": false}
{"title": "EvalMe: Exploring the Value of New Technologies for In Situ Evaluation of Learning Experiences", "pdf_hash": "efa458a7a213ed0fdcfe98b518b1ccf3634884a6", "year": 2021, "venue": "CHI", "alt_text": "Figure 1. Figure describing the research approach adopted in the paper. The figure demonstrates the four stages of the research process and the key findings from each stage. The first is our initial conceptualisation of EvalMe. The second is our co-design of EvalMe with partners. The third is our development of EvalMe and fourth is our evaluation in-the-wild.", "levels": null, "corpus_id": 233987375, "sentences": ["Figure 1.", "Figure describing the research approach adopted in the paper.", "The figure demonstrates the four stages of the research process and the key findings from each stage.", "The first is our initial conceptualisation of EvalMe.", "The second is our co-design of EvalMe with partners.", "The third is our development of EvalMe and fourth is our evaluation in-the-wild."], "caption": "", "local_uri": ["efa458a7a213ed0fdcfe98b518b1ccf3634884a6_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "EvalMe: Exploring the Value of New Technologies for In Situ Evaluation of Learning Experiences", "pdf_hash": "efa458a7a213ed0fdcfe98b518b1ccf3634884a6", "year": 2021, "venue": "CHI", "alt_text": "Figure 2. This figure demonstrates the shape and components included in the EvalMe Sender i.e. the tangible box that is passed around. The Sender is 13x19x10 cm in size. The top of the Sender which faces the ceiling when a person holds includes labels for questions and an LED ring indicating selected values. The side facing the individual holding the Sender has an RFID reader and an indicator LED for authenticated mode, as well as a button to wirelessly submit answers. Another side also includes an on/off switch and a USB charging port.", "levels": null, "corpus_id": 233987375, "sentences": ["Figure 2.", "This figure demonstrates the shape and components included in the EvalMe Sender i.e. the tangible box that is passed around.", "The Sender is 13x19x10 cm in size.", "The top of the Sender which faces the ceiling when a person holds includes labels for questions and an LED ring indicating selected values.", "The side facing the individual holding the Sender has an RFID reader and an indicator LED for authenticated mode, as well as a button to wirelessly submit answers.", "Another side also includes an on/off switch and a USB charging port."], "caption": "", "local_uri": ["efa458a7a213ed0fdcfe98b518b1ccf3634884a6_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Demonstrating TapID for Rapid Touch Interaction on Surfaces in Virtual Reality for Productivity Scenarios", "pdf_hash": "f218b5beb50cf7737ceca0394f806d7b290e80aa", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figure consists of three images. Image A shows a user wearing a VR headset and two TapID bands. TapID bands are made out of silicone and carry the electronics on the upper side of the wrist like a watch. Image B shows a person working in a virtual office setup. In image C there are two hands that enter text on a PowerPoint slide using a virtual keyboard.", "levels": null, "corpus_id": 233987674, "sentences": ["The figure consists of three images.", "Image A shows a user wearing a VR headset and two TapID bands.", "TapID bands are made out of silicone and carry the electronics on the upper side of the wrist like a watch.", "Image B shows a person working in a virtual office setup.", "In image C there are two hands that enter text on a PowerPoint slide using a virtual keyboard."], "caption": "Figure 1: We demonstrate multiple applications that bring rapid touch interaction to Virtual Reality, which is enabled by our integration of VR headset-based hand pose tracking and touch detection on a table. To detect touches, we demonstrate our prototype TapID, which is a wrist-worn sensor device that registers surface taps and identifes the tapping fnger. Using this recognition, TapID triggers the corresponding UI elements in VR. (A) A user is wearing a VR headset and two TapID bands to", "local_uri": ["f218b5beb50cf7737ceca0394f806d7b290e80aa_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Demonstrating TapID for Rapid Touch Interaction on Surfaces in Virtual Reality for Productivity Scenarios", "pdf_hash": "f218b5beb50cf7737ceca0394f806d7b290e80aa", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figure shows images of TapID's electronics from both sides. The main components of the PCB board are a BLE antenna, an SoC, a power switch, an LIS2DH and an ADXL355 accelerometer. It also connects to two LIS2DH accelerometers that are mounted on the ends of the flexible PCB.", "levels": null, "corpus_id": 233987674, "sentences": ["The figure shows images of TapID's electronics from both sides.", "The main components of the PCB board are a BLE antenna, an SoC, a power switch, an LIS2DH and an ADXL355 accelerometer.", "It also connects to two LIS2DH accelerometers that are mounted on the ends of the flexible PCB."], "caption": "Figure 2: The main PCB features a microcontroller that reads out the two low-power IMUs that are mounted on the ends of a fexible PCB. The fex PCB connects to the main PCB through an extension connector. For comparison in our evaluation, two additional IMUs are mounted on the main PCB.", "local_uri": ["f218b5beb50cf7737ceca0394f806d7b290e80aa_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Involving Syrian Refugees in Design Research: Lessons Learnt from the Field", "pdf_hash": "1f4784c8d34fa090db3fc622f3d89b040a27ee4a", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "In this image therer are 6 cards that are examples of the dialogue cards that were used in this study. The first card ha a snowflake on it to symbolise wonter, the second a cartoon depiction of NGO workers, the third a plate of food, the fourth a gas cannister used in cooking stoves in Lebanon, the fifth a pictorial representation of decreasing food quantitiy and the sixth is a picture of a small corner shop.", "levels": null, "corpus_id": 195259492, "sentences": ["In this image therer are 6 cards that are examples of the dialogue cards that were used in this study.", "The first card ha a snowflake on it to symbolise wonter, the second a cartoon depiction of NGO workers, the third a plate of food, the fourth a gas cannister used in cooking stoves in Lebanon, the fifth a pictorial representation of decreasing food quantitiy and the sixth is a picture of a small corner shop."], "caption": "", "local_uri": ["1f4784c8d34fa090db3fc622f3d89b040a27ee4a_Image_002.png"], "annotated": false, "compound": false}
{"title": "Involving Syrian Refugees in Design Research: Lessons Learnt from the Field", "pdf_hash": "1f4784c8d34fa090db3fc622f3d89b040a27ee4a", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "This picture shows how participants grouped the dialogue cards to co-construct narratives of their experiences of food insecurity.", "levels": null, "corpus_id": 195259492, "sentences": ["This picture shows how participants grouped the dialogue cards to co-construct narratives of their experiences of food insecurity."], "caption": "Figure 3. Dialogue cards used to co-construct narratives", "local_uri": ["1f4784c8d34fa090db3fc622f3d89b040a27ee4a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 1: A physiotherapist seeing two screens: one with the video stream of the patient, and another with the SoPhy visualisation.", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 1: A physiotherapist seeing two screens: one with the video stream of the patient, and another with the SoPhy visualisation."], "caption": "", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 3: The image includes four blocks each summarizing different phases of the project. The first block says Development of a working prototype in 2015, second block says laboratory evaluation in 2016, third block says Sophy design updated in 2017 and the last block mentions field evaluation with patients and physiotherapists in 2017", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 3: The image includes four blocks each summarizing different phases of the project.", "The first block says Development of a working prototype in 2015, second block says laboratory evaluation in 2016, third block says Sophy design updated in 2017 and the last block mentions field evaluation with patients and physiotherapists in 2017"], "caption": "", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 5: A picture containing three altered Arduino boards that were explored during prototyping", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 5: A picture containing three altered Arduino boards that were explored during prototyping"], "caption": "", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 6: A close up of the SoPhy socks showing the connections of electronic units using the mix of hardwire and conductive thread", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 6: A close up of the SoPhy socks showing the connections of electronic units using the mix of hardwire and conductive thread"], "caption": "Figure 6: Most of the electronic components like Arduino board and resistors were placed on an external cloth. This arrangement helped in developing both comfortable and stretchable sock prototype.", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 7: A picture showing three insertables that were used to develop stretchable socks", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 7: A picture showing three insertables that were used to develop stretchable socks"], "caption": "", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 8: A picture showing the pouch design of the socks with laces attached on different locations like toes and ankle. These laces can be adjusted based on the swelling of the patient's foot", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 8: A picture showing the pouch design of the socks with laces attached on different locations like toes and ankle.", "These laces can be adjusted based on the swelling of the patient's foot"], "caption": "", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 9: A picture containing a patient wearing the Sophy socks and the SoPhy visualisation", "levels": null, "corpus_id": 231929276, "sentences": ["Figure 9: A picture containing a patient wearing the Sophy socks and the SoPhy visualisation"], "caption": "Figure 9: Design of the SoPhy system used in the lab study.", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "pdf_hash": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "year": 2020, "venue": "OZCHI", "alt_text": "Figure 11: A collection of three images, with each image showing a patient performing specific movements with SoPhy socks on and the SoPhy visualisation showing the data accordingly.", "levels": [[-1]], "corpus_id": 231929276, "sentences": ["Figure 11: A collection of three images, with each image showing a patient performing specific movements with SoPhy socks on and the SoPhy visualisation showing the data accordingly."], "caption": "", "local_uri": ["28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_014.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Tangibles + Programming + Audio Stories = Fun", "pdf_hash": "7b80f93247b28029338a00d3fa5a2a4404efdd3e", "year": 2017, "venue": "ASSETS", "alt_text": "Image of several interlocking blocks that make up a program, presented alongside the output generated by this program. In this program, a mouse squeaks, a cat hisses, the mouse runs, and the cat eats the mouse.", "levels": null, "corpus_id": 10357913, "sentences": ["Image of several interlocking blocks that make up a program, presented alongside the output generated by this program.", "In this program, a mouse squeaks, a cat hisses, the mouse runs, and the cat eats the mouse."], "caption": "Figure 1. The Story Blocks programming language uses tangible blocks as the basis for programs. These blocks offer tactile features that allow them to be used by both blind and sighted users. The user assembles a program by connecting blocks, and captures an image of the program on a mobile device. The program is then executed on the phone in the form of an interactive audio story.", "local_uri": ["7b80f93247b28029338a00d3fa5a2a4404efdd3e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Designing for Bodily Play Experiences Based on Danish Linguistic Connotations of \"Playing a Game\"", "pdf_hash": "dbc010619f9fed8036ee67107b4b0466dedff630", "year": 2020, "venue": "CHI PLAY", "alt_text": "Et billede, der indeholder scene, fase, indend\u00f8rs, person\n\nAutomatisk genereret beskrivelse", "levels": null, "corpus_id": 226239265, "sentences": ["Et billede, der indeholder scene, fase, indend\u00f8rs, person\n\nAutomatisk genereret beskrivelse"], "caption": "Figure 3: \u201cInferno\u201d participants\u2019 upper bodies being", "local_uri": ["dbc010619f9fed8036ee67107b4b0466dedff630_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Designing for Bodily Play Experiences Based on Danish Linguistic Connotations of \"Playing a Game\"", "pdf_hash": "dbc010619f9fed8036ee67107b4b0466dedff630", "year": 2020, "venue": "CHI PLAY", "alt_text": "Et billede, der indeholder person, indend\u00f8rs, st\u00e5ende, afspiller\n\nAutomatisk genereret beskrivelse", "levels": null, "corpus_id": 226239265, "sentences": ["Et billede, der indeholder person, indend\u00f8rs, st\u00e5ende, afspiller\n\nAutomatisk genereret beskrivelse"], "caption": "Figure 5: The dislocated mice mimicking interaction with the vulva in The Undie Game. Picture by Simon Nielsen.", "local_uri": ["dbc010619f9fed8036ee67107b4b0466dedff630_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Figure 1. The figure shows a running-example of the interactions supported by EYEditor. The user sees the text on the smart glass screen, one sentence at a time. The sentence is vertically centered. If the user detects an error in the sentence, she can re-speak over the erroneous portion of the text to correct it. The user can navigate between sentences by swiping left or right on the hand-controller trackpad. Also, if the correction-by-respeaking fails, the user can enter a Select-to-Edit mode that gives the user a finer-grained, word-level control over the text. Here, a visual marker shows up on the smart glass screen. The user can select individual words or group of words using the hand-controller and speak over the selection in order to modify it. Once modified, the user can go back to the default re-speaking mode by going out of the Select-to-Edit mode.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483565, "sentences": ["Figure 1.", "The figure shows a running-example of the interactions supported by EYEditor.", "The user sees the text on the smart glass screen, one sentence at a time.", "The sentence is vertically centered.", "If the user detects an error in the sentence, she can re-speak over the erroneous portion of the text to correct it.", "The user can navigate between sentences by swiping left or right on the hand-controller trackpad.", "Also, if the correction-by-respeaking fails, the user can enter a Select-to-Edit mode that gives the user a finer-grained, word-level control over the text.", "Here, a visual marker shows up on the smart glass screen.", "The user can select individual words or group of words using the hand-controller and speak over the selection in order to modify it.", "Once modified, the user can go back to the default re-speaking mode by going out of the Select-to-Edit mode."], "caption": "Figure 1: EYEditor interactions: User sees the text on a smart glass, sentence-by-sentence. In the Re-speaking mode, correction is achieved by re-speaking over the text and a hand-controller is used to navigate between sentences. Users can enter the", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Figure 2. This figure shows the three interfaces that makes up the apparatus for Study 1 and 2. First is the visual interface that consists of the Vuzix Blade Smart Glass. Only one of the two glasses has a 480-by-480 pixels display embedded in it. The second interface is the audio interface which consists of a pair of Bose QC 35 Headphones (with microphone). The third interface is the Manual Input Interface that consists of a ring-mouse hand controller. The ring mouse has 4 buttons (left/right/top/down) and a centrally located trackpad which also supports a button press.", "levels": null, "corpus_id": 218483565, "sentences": ["Figure 2.", "This figure shows the three interfaces that makes up the apparatus for Study 1 and 2.", "First is the visual interface that consists of the Vuzix Blade Smart Glass. Only one of the two glasses has a 480-by-480 pixels display embedded in it.", "The second interface is the audio interface which consists of a pair of Bose QC 35 Headphones (with microphone).", "The third interface is the Manual Input Interface that consists of a ring-mouse hand controller.", "The ring mouse has 4 buttons (left/right/top/down) and a centrally located trackpad which also supports a button press."], "caption": "Figure 2: Apparatus showing all three interfaces.", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_002.png"], "annotated": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Figure 3. This figure shows screenshots from the smart glass display to show the 5 different output modes that resulted from combining two output modalities (audio and visual) with two presentation types (one that presents the text one screen at ta time, another that presents the text one sentence at a time, the sentence being centrally aligned both vertically and horizontally).", "levels": null, "corpus_id": 218483565, "sentences": ["Figure 3.", "This figure shows screenshots from the smart glass display to show the 5 different output modes that resulted from combining two output modalities (audio and visual) with two presentation types (one that presents the text one screen at ta time, another that presents the text one sentence at a time, the sentence being centrally aligned both vertically and horizontally)."], "caption": "", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_004.png"], "annotated": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Figure 4. Shows a comparative evaluation of the five output modes for two output measures: The task completion time and the stopping %. From both the sub-figures, it is evident that Visual presentation of the text, sentence-by-sentence allows the user to correct text fastest as well as with maximum path-awareness. Additionally, simultaneous audio with visual output results in slow task completions and the user stopping more while walking.", "levels": null, "corpus_id": 218483565, "sentences": ["Figure 4. Shows a comparative evaluation of the five output modes for two output measures: The task completion time and the stopping %.", "From both the sub-figures, it is evident that Visual presentation of the text, sentence-by-sentence allows the user to correct text fastest as well as with maximum path-awareness.", "Additionally, simultaneous audio with visual output results in slow task completions and the user stopping more while walking."], "caption": "Figure 4: Comparative evaluation of the output modes.", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_005.png"], "annotated": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Figure 5. Shows the functions mapped on to the different buttons of the ring mouse hand controller, for both the Re-speaking and Select-to-Edit modes.", "levels": null, "corpus_id": 218483565, "sentences": ["Figure 5. Shows the functions mapped on to the different buttons of the ring mouse hand controller, for both the Re-speaking and Select-to-Edit modes."], "caption": "", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_006.png"], "annotated": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Table 2. This table lists down the details of the independent and dependent variables used in Study 2.", "levels": null, "corpus_id": 218483565, "sentences": ["Table 2.", "This table lists down the details of the independent and dependent variables used in Study 2."], "caption": "Table 2: Study 2 Design Table. The rows under an independent variable column indicate its levels. All paths measure 50m from start (green pin) to \ufb01nish (red pin). For ObstPath, one round measures \u224828.5m, taking 1.75 rounds to complete the path.", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_007.png"], "annotated": false, "compound": false}
{"title": "EYEditor: Towards On-the-Go Heads-Up Text Editing Using Voice and Manual Input", "pdf_hash": "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0", "year": 2020, "venue": "CHI", "alt_text": "Figure 6. Sub-figures (a) and (b) compares between the smart glass and the phone for two measures, namely, corrections per second and PPWS for the correction-task and path combinations tested. Sub-figure (c) shows user preference of using the phone versus the glass for overall text editing as well as for individual task-path combinations. Results show that 75% of the participants prefer the glass over the phone if either the path or the task is difficult, however, if both the path and the task is simple, then more participants preferred phone than glass (7:5) and if both the task and path were difficult then the preference for either was almost the same. Overall, 75% of the participants preferred using the smart glass for editing text on the go.", "levels": null, "corpus_id": 218483565, "sentences": ["Figure 6.", "Sub-figures (a) and (b) compares between the smart glass and the phone for two measures, namely, corrections per second and PPWS for the correction-task and path combinations tested.", "Sub-figure (c) shows user preference of using the phone versus the glass for overall text editing as well as for individual task-path combinations.", "Results show that 75% of the participants prefer the glass over the phone if either the path or the task is difficult, however, if both the path and the task is simple, then more participants preferred phone than glass (7:5) and if both the task and path were difficult then the preference for either was almost the same.", "Overall, 75% of the participants preferred using the smart glass for editing text on the go."], "caption": "(b)                                           (c)", "local_uri": ["3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_008.png", "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_009.png", "3f4fc928ffa8321de4651b2637c25ea70fa9ebc0_Image_010.png"], "annotated": false, "compound": true}
{"title": "Mediating Relatedness for Adolescents with ME: Reducing Isolation through Minimal Interactions with a Robot Avatar", "pdf_hash": "a990c83331145b9c692f09865f8c6ff3db74323f", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "The robot in the classroom. The app when the robot is in use. AV1 units before assembly.", "levels": null, "corpus_id": 195259420, "sentences": ["The robot in the classroom.", "The app when the robot is in use.", "AV1 units before assembly."], "caption": "Figure 1. AV1 in the classroom, AV1 units before the assembly (\u00a9 Marius Vabo/No Isolation); the app to control the stream from AV1 by finger swipes (bottom left image). (Photo: Cul\u00e9n)", "local_uri": ["a990c83331145b9c692f09865f8c6ff3db74323f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Mediating Relatedness for Adolescents with ME: Reducing Isolation through Minimal Interactions with a Robot Avatar", "pdf_hash": "a990c83331145b9c692f09865f8c6ff3db74323f", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "The Social Map we used to break the ice and start conversations about relations with others", "levels": null, "corpus_id": 195259420, "sentences": ["The Social Map we used to break the ice and start conversations about relations with others"], "caption": "Figure 2. The Social Map we used to break the ice and start conversations about relations with others. (Photo: Cul\u00e9n)", "local_uri": ["a990c83331145b9c692f09865f8c6ff3db74323f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Mediating Relatedness for Adolescents with ME: Reducing Isolation through Minimal Interactions with a Robot Avatar", "pdf_hash": "a990c83331145b9c692f09865f8c6ff3db74323f", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "AV1 interacting with a dog. AV1 being part of family life with parents. AV1 used to enjoy nature.", "levels": null, "corpus_id": 195259420, "sentences": ["AV1 interacting with a dog.", "AV1 being part of family life with parents.", "AV1 used to enjoy nature."], "caption": "Figure 5. AV1 enabled prolonged interactions with pets, family, or enjoying nature with a friend. (Photos: B\u00f8rsting)", "local_uri": ["a990c83331145b9c692f09865f8c6ff3db74323f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Wanting To Live Here: Design After Anthropocentric Functionalism", "pdf_hash": "ed7839dc6a0fa9bc24dd3b256e261a9dff8a7eac", "year": 2021, "venue": "CHI", "alt_text": "A circular device of approximately 35 centimeters in diameter, with a trap door for capturing snails controlled by a motor on top.", "levels": null, "corpus_id": 233987557, "sentences": ["A circular device of approximately 35 centimeters in diameter, with a trap door for capturing snails controlled by a motor on top."], "caption": "", "local_uri": ["ed7839dc6a0fa9bc24dd3b256e261a9dff8a7eac_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Wanting To Live Here: Design After Anthropocentric Functionalism", "pdf_hash": "ed7839dc6a0fa9bc24dd3b256e261a9dff8a7eac", "year": 2021, "venue": "CHI", "alt_text": "Two individual bowls of food, each with a combination of fresh and pickled vegetables on top of a bed of rice.", "levels": null, "corpus_id": 233987557, "sentences": ["Two individual bowls of food, each with a combination of fresh and pickled vegetables on top of a bed of rice."], "caption": "Figure 2: Mei-Hong\u2019s home-style cooking", "local_uri": ["ed7839dc6a0fa9bc24dd3b256e261a9dff8a7eac_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "AN ABSTRACT OF THE THESIS OF William Jernigan for the degree of Master of Science in Computer Science presented on October 26, 2015. Title: Generalizing the Idea Garden: Principles and Contexts", "pdf_hash": "c673230f5bfbf4e25f1d77d719e20534972473a7", "year": 2015, "venue": "", "alt_text": "The Idea Garden Panel in Cloud9 and the Idea Garden icon in Cloud9's code editor", "levels": null, "corpus_id": 204535113, "sentences": ["The Idea Garden Panel in Cloud9 and the Idea Garden icon in Cloud9's code editor"], "caption": "Figure 7: (Main) The Idea Garden panel in the Cloud9 IDE as campers see it when they opened the panel for the first time. (Callout) An example of the Idea Garden decorating the code with an icon. Here, the icon links to the Iteration with For hint.", "local_uri": ["c673230f5bfbf4e25f1d77d719e20534972473a7_Image_044.jpg"], "annotated": false, "compound": false}
{"title": "Experimental Security Analyses of Non-Networked Compact Fluorescent Lamps: A Case Study of Home Automation Security", "pdf_hash": "da5aab6640de0c621ab5a53137722824ed805ebd", "year": 2013, "venue": "LASER", "alt_text": "7dLYlPu1xm3r-3ujXhUGLXQ79wb8hI2iMXJNQjlHXf-4BQ4tgfJ4m7OuF4TyIyyJinhjYrg0f1H_T62Npz7kyOh0jzJqLtvv_Y1vswFQb9dTXvLlh4IL0PtG", "levels": [[0]], "corpus_id": 14674022, "sentences": ["7dLYlPu1xm3r-3ujXhUGLXQ79wb8hI2iMXJNQjlHXf-4BQ4tgfJ4m7OuF4TyIyyJinhjYrg0f1H_T62Npz7kyOh0jzJqLtvv_Y1vswFQb9dTXvLlh4IL0PtG"], "caption": "Figure 6: Real-time Plotting Utility Reporting Current Consumption.", "local_uri": ["da5aab6640de0c621ab5a53137722824ed805ebd_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Experimental Security Analyses of Non-Networked Compact Fluorescent Lamps: A Case Study of Home Automation Security", "pdf_hash": "da5aab6640de0c621ab5a53137722824ed805ebd", "year": 2013, "venue": "LASER", "alt_text": "9WPdZLmtOhc3KNqBXU263nLN-IsMKjBGBxD6L0k2mqqeK8jeWGR8yiUFPTbYah2nrlh0TYy4OmQJQF0UUnAaHWA4AR_dgoq-UfiGY7wpW5_HRipnZ1_DSzYB", "levels": null, "corpus_id": 14674022, "sentences": ["9WPdZLmtOhc3KNqBXU263nLN-IsMKjBGBxD6L0k2mqqeK8jeWGR8yiUFPTbYah2nrlh0TYy4OmQJQF0UUnAaHWA4AR_dgoq-UfiGY7wpW5_HRipnZ1_DSzYB"], "caption": "Figure 15: Resultant spike from current surge in CFL.", "local_uri": ["da5aab6640de0c621ab5a53137722824ed805ebd_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Beyond iTunes for Papers: Redefining the Unit of Interaction in Literature Review Tools", "pdf_hash": "845af4d71b06c7d59ea8c6d6e948e714d3b3f355", "year": 2019, "venue": "CSCW Companion", "alt_text": "This figure shows how users interact with the Knowledge Compressor. The user uses mouse to quickly select a segment within the source document in the reading panel, and writes a re-description of the associated claim.", "levels": null, "corpus_id": 207959994, "sentences": ["This figure shows how users interact with the Knowledge Compressor.", "The user uses mouse to quickly select a segment within the source document in the reading panel, and writes a re-description of the associated claim."], "caption": "", "local_uri": ["845af4d71b06c7d59ea8c6d6e948e714d3b3f355_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "\u201dBeyond 3D printers\u201d: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth", "pdf_hash": "d8033cbdce16e405c7a22f78377ea02ddf25f1a6", "year": 2021, "venue": "CHI", "alt_text": "This is an horizontal image with 6 objects. On the top left corner, the metric conversion table has 8 physical columns and 4 rows. There are transparent braille labels on each columns, and slots in the bottom two rows for braille cubes. The cubes can be moved to change units (e.g., centimeters to meters). Below is a high contrast yellow square with black numbers. Next is a 3-colors and textures diagram of a circulatory system. Organs are very simplified and hold together with rounded puzzle connectors. Next is a model of a city block. Buildings are very colorful (green, blue, yellow, red, white). There are several types of simplified houses, of different sizes or with or without front gardens. Next is a bi-color tactile globe, blue for the ocean and orange for the continents. Oceans have a printed texture. The final image is a textile books with a Lilypad board which two hands are connecting.", "levels": null, "corpus_id": 233987454, "sentences": ["This is an horizontal image with 6 objects.", "On the top left corner, the metric conversion table has 8 physical columns and 4 rows.", "There are transparent braille labels on each columns, and slots in the bottom two rows for braille cubes.", "The cubes can be moved to change units (e.g., centimeters to meters).", "Below is a high contrast yellow square with black numbers.", "Next is a 3-colors and textures diagram of a circulatory system.", "Organs are very simplified and hold together with rounded puzzle connectors.", "Next is a model of a city block.", "Buildings are very colorful (green, blue, yellow, red, white).", "There are several types of simplified houses, of different sizes or with or without front gardens.", "Next is a bi-color tactile globe, blue for the ocean and orange for the continents.", "Oceans have a printed texture.", "The final image is a textile books with a Lilypad board which two hands are connecting."], "caption": "", "local_uri": ["d8033cbdce16e405c7a22f78377ea02ddf25f1a6_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "\u201dBeyond 3D printers\u201d: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth", "pdf_hash": "d8033cbdce16e405c7a22f78377ea02ddf25f1a6", "year": 2021, "venue": "CHI", "alt_text": "The figure is a series of diagram representing the three steps of analysis. Step 1: Representing projects as activity systems, identifying contradictions (here in red, between tools and rules of practice) and the other activity systems they interact with. The figure represents the activity system model triangle for a 3D printed globe. This activity system intersects with teaching and usual transcription practices. Their common goals: 1) Learning gains, 2) Improved learning experience. The new object emerging at their intersection: the design space. Step 2: Reconstructing how several projects contributed to a larger activity system (e.g., 3D printing for learning gains). This represents the gradual redefinition of \"3D printing for the education of blind youth\" as it encounters practices. The initial concept is \"Automated production of available high complexity 3D representations for replacing transcriptions. Becoming makers.\" The final concept is: \"Punctual hybrid (3D printing and laser cutting) creation of reused medium complexity manipulable representations. Being part of a making organisation.\" Step 3: Representing the entire expansive learning cycle. Phase 1: Questioning. Could digital fabrication ease or improve teaching and/or or automate transcriptions? Phase 2: Which existing objects could be replaced or made more efficiently? Which existing modeling tools used? Phase 3 and 4: Modeling the new solution; Examining new model. Broad range of experiments with laser cutting, 3D printing and prototyping electronic platforms, tested with professionals and students. Acquisition of a 3D printer. Active documentation. Phase 5: Implementing. Overall agreement between professionals over use and routinisation. Punctual conflicts on cost-effectiveness. Phase 6: Reflection. Formalisation through professional guidance for comments. Phase 7: Consolidating. Demands for professionalisation, increased financial support (ongoing).", "levels": null, "corpus_id": 233987454, "sentences": ["The figure is a series of diagram representing the three steps of analysis.", "Step 1: Representing projects as activity systems, identifying contradictions (here in red, between tools and rules of practice) and the other activity systems they interact with.", "The figure represents the activity system model triangle for a 3D printed globe.", "This activity system intersects with teaching and usual transcription practices.", "Their common goals: 1) Learning gains, 2) Improved learning experience.", "The new object emerging at their intersection: the design space.", "Step 2: Reconstructing how several projects contributed to a larger activity system (e.g., 3D printing for learning gains).", "This represents the gradual redefinition of \"3D printing for the education of blind youth\" as it encounters practices.", "The initial concept is \"Automated production of available high complexity 3D representations for replacing transcriptions.", "Becoming makers.\" The final concept is: \"Punctual hybrid (3D printing and laser cutting) creation of reused medium complexity manipulable representations.", "Being part of a making organisation.\" Step 3: Representing the entire expansive learning cycle.", "Phase 1: Questioning. Could digital fabrication ease or improve teaching and/or or automate transcriptions?", "Phase 2: Which existing objects could be replaced or made more efficiently? Which existing modeling tools used? Phase 3 and 4: Modeling the new solution; Examining new model.", "Broad range of experiments with laser cutting, 3D printing and prototyping electronic platforms, tested with professionals and students.", "Acquisition of a 3D printer.", "Active documentation.", "Phase 5: Implementing.", "Overall agreement between professionals over use and routinisation.", "Punctual conflicts on cost-effectiveness.", "Phase 6: Reflection.", "Formalisation through professional guidance for comments.", "Phase 7: Consolidating.", "Demands for professionalisation, increased financial support (ongoing)."], "caption": "", "local_uri": ["d8033cbdce16e405c7a22f78377ea02ddf25f1a6_Image_007.png"], "annotated": false, "compound": false}
{"title": "\u201dBeyond 3D printers\u201d: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth", "pdf_hash": "d8033cbdce16e405c7a22f78377ea02ddf25f1a6", "year": 2021, "venue": "CHI", "alt_text": "This figure is composed of six consecutive images. The first four are accessible world map (a black and white raised line drawing, a commercialised colorful plastic puzzle, a drawing for a laser cut puzzle of oceans with red outlines and a braille title, a blue and brown laser cut puzzle with engraved braille labels). The fifth is a detail of the circulatory system model. It's a rectangle with four horizontal sections, one for the heart, one for the liver, one for the intestine, one for the kidneys. The seventh is a realistic laser cut and engraved crocodile in plywood.", "levels": null, "corpus_id": 233987454, "sentences": ["This figure is composed of six consecutive images.", "The first four are accessible world map (a black and white raised line drawing, a commercialised colorful plastic puzzle, a drawing for a laser cut puzzle of oceans with red outlines and a braille title, a blue and brown laser cut puzzle with engraved braille labels).", "The fifth is a detail of the circulatory system model.", "It's a rectangle with four horizontal sections, one for the heart, one for the liver, one for the intestine, one for the kidneys.", "The seventh is a realistic laser cut and engraved crocodile in plywood."], "caption": "", "local_uri": ["d8033cbdce16e405c7a22f78377ea02ddf25f1a6_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "\u201dBeyond 3D printers\u201d: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth", "pdf_hash": "d8033cbdce16e405c7a22f78377ea02ddf25f1a6", "year": 2021, "venue": "CHI", "alt_text": "There are four pictures in this figure. The first is a clock with enlarged numbers painted in blue and red and needles painted in the same color. The second is a map of France with joyful colors and contrasting textile patterns such as dots, crossed etc. The third is a 2.5D maps on base of plastic. The floor is made of green and gray wood and felt, the buildings are blue. The fourth is a roman amphitheater with 3 sets of steps in grey cardboard, a wall with numerous doors and windows, and a brown scene.", "levels": null, "corpus_id": 233987454, "sentences": ["There are four pictures in this figure.", "The first is a clock with enlarged numbers painted in blue and red and needles painted in the same color.", "The second is a map of France with joyful colors and contrasting textile patterns such as dots, crossed etc.", "The third is a 2.5D maps on base of plastic.", "The floor is made of green and gray wood and felt, the buildings are blue.", "The fourth is a roman amphitheater with 3 sets of steps in grey cardboard, a wall with numerous doors and windows, and a brown scene."], "caption": "", "local_uri": ["d8033cbdce16e405c7a22f78377ea02ddf25f1a6_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "\u201dBeyond 3D printers\u201d: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth", "pdf_hash": "d8033cbdce16e405c7a22f78377ea02ddf25f1a6", "year": 2021, "venue": "CHI", "alt_text": "The first image is a completely white tactile globe held in a child's hand. The second is a simplified but complex bee: the head and torso have a 3D printed texture. It is white and blue, awaiting painting and collage. The third is a picture of a row of 3D printed representations of houses with a tree and a church, through which a primary school pupil represented what he thinks a small town is like: a long street. The fourth is a colourful set of 3D printed representations of gymnastics equipment on which a Playmobil toy stands.", "levels": null, "corpus_id": 233987454, "sentences": ["The first image is a completely white tactile globe held in a child's hand.", "The second is a simplified but complex bee: the head and torso have a 3D printed texture.", "It is white and blue, awaiting painting and collage.", "The third is a picture of a row of 3D printed representations of houses with a tree and a church, through which a primary school pupil represented what he thinks a small town is like: a long street.", "The fourth is a colourful set of 3D printed representations of gymnastics equipment on which a Playmobil toy stands."], "caption": "", "local_uri": ["d8033cbdce16e405c7a22f78377ea02ddf25f1a6_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "\u201dBeyond 3D printers\u201d: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth", "pdf_hash": "d8033cbdce16e405c7a22f78377ea02ddf25f1a6", "year": 2021, "venue": "CHI", "alt_text": "The figure is made of 3 images. The first is a computer mouse (in which a color captor was hidden) above a circle of all tertiary colors (blue to yellow with all intermediary greens, yellow to red with all intermediary oranges, red to blue with all intermediaries purple). On the edge of the circle, there are written notes. The second is a textile book, open. It depicts a street going to a park through a fence. On the left page, there is a text in braille and enlarged characters, as well as two toy characters and a button for sound. The right page is structure in two parts. The top half shows the sky, a textured park with a tree. The bottom half shows the end of the path, a door and a grid. The third picture is an horizontal box with 8 squares of aluminum paper acting as buttons and a white knob enabling to choose different level of description. There are two 3D printed blocks, one white, one red on top enabling to distinguish between periods of history.", "levels": null, "corpus_id": 233987454, "sentences": ["The figure is made of 3 images.", "The first is a computer mouse (in which a color captor was hidden) above a circle of all tertiary colors (blue to yellow with all intermediary greens, yellow to red with all intermediary oranges, red to blue with all intermediaries purple).", "On the edge of the circle, there are written notes.", "The second is a textile book, open.", "It depicts a street going to a park through a fence.", "On the left page, there is a text in braille and enlarged characters, as well as two toy characters and a button for sound.", "The right page is structure in two parts.", "The top half shows the sky, a textured park with a tree.", "The bottom half shows the end of the path, a door and a grid.", "The third picture is an horizontal box with 8 squares of aluminum paper acting as buttons and a white knob enabling to choose different level of description.", "There are two 3D printed blocks, one white, one red on top enabling to distinguish between periods of history."], "caption": "", "local_uri": ["d8033cbdce16e405c7a22f78377ea02ddf25f1a6_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Korero: Facilitating Complex Referencing of Visual Materials in Asynchronous Discussion Interface", "pdf_hash": "1885a9b16ab1afc22fa15f787472018b0ab8ea4c", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 1. (a) Overview of our Korero interface. The contextual activity window shows the materials (e.g., video or documents). Discussion window on the right shows all the discussion threads on the main material. (b) Multilinking popup. After selecting any texts (referential term) in the post, the popup would appear beneath. (c) Specifying referents in video material and (d) text material after the material is selected via the multilinking popup.", "levels": null, "corpus_id": 19449715, "sentences": ["Figure 1. (a) Overview of our Korero interface.", "The contextual activity window shows the materials (e.g., video or documents).", "Discussion window on the right shows all the discussion threads on the main material. (b) Multilinking popup.", "After selecting any texts (referential term) in the post, the popup would appear beneath. (c) Specifying referents in video material and (d) text material after the material is selected via the multilinking popup."], "caption": "Figure 1. (a) Overview of our Korero interface. The contextual activity window shows the materials (e.g., video or documents). Discussion window on the right shows all the discussion threads on the main material. (b) Multi-linking pop-up. After selecting any texts (referential term) in the post, the pop-up would appear beneath.", "local_uri": ["1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Korero: Facilitating Complex Referencing of Visual Materials in Asynchronous Discussion Interface", "pdf_hash": "1885a9b16ab1afc22fa15f787472018b0ab8ea4c", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 2. (a) On-demand widget. (b) To accommodate more thumbnails, we include a horizontal scrollbar to provide additional horizontal space.", "levels": [[-1], [-1], [-1]], "corpus_id": 19449715, "sentences": ["Figure 2. (", "a) On-demand widget. (", "b) To accommodate more thumbnails, we include a horizontal scrollbar to provide additional horizontal space."], "caption": "Figure 2. (a) On-demand widget. (b) To accommodate more thumbnails, we include a horizontal scrollbar to provide additional horizontal space.", "local_uri": ["1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Korero: Facilitating Complex Referencing of Visual Materials in Asynchronous Discussion Interface", "pdf_hash": "1885a9b16ab1afc22fa15f787472018b0ab8ea4c", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 3. Comprehending the referents of a referential term. In the widget, the user can hover on the different thumbnails side-by-side to get a quick glance of each referent in the activity window.", "levels": null, "corpus_id": 19449715, "sentences": ["Figure 3.", "Comprehending the referents of a referential term.", "In the widget, the user can hover on the different thumbnails side-by-side to get a quick glance of each referent in the activity window."], "caption": "Figure 3. Comprehending the referents of a referential term. In the widget, the user can hover on the different thumbnails side-by-side to get a quick glance of each referent in the activity window.", "local_uri": ["1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Korero: Facilitating Complex Referencing of Visual Materials in Asynchronous Discussion Interface", "pdf_hash": "1885a9b16ab1afc22fa15f787472018b0ab8ea4c", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 4. Completion time of each UI in the referencing tasks in Study 1. Data labels are mean values, and error bars represent standard deviation.", "levels": [[0], [1], [1]], "corpus_id": 19449715, "sentences": ["Figure 4.", "Completion time of each UI in the referencing tasks in Study 1.", "Data labels are mean values, and error bars represent standard deviation."], "caption": "Figure 4. Completion time of each UI in the referencing tasks in Study 1. Data labels are mean values, and error bars represent standard deviation.", "local_uri": ["1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Korero: Facilitating Complex Referencing of Visual Materials in Asynchronous Discussion Interface", "pdf_hash": "1885a9b16ab1afc22fa15f787472018b0ab8ea4c", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 5. (a) Cumbersomeness and (b) writing effort of each UI in the referencing tasks in Study 1. Data labels are mean values, and error bars represent standard deviation.", "levels": [[0], [1], [1]], "corpus_id": 19449715, "sentences": ["Figure 5. (", "a) Cumbersomeness and (b) writing effort of each UI in the referencing tasks in Study 1.", "Data labels are mean values, and error bars represent standard deviation."], "caption": "Figure 5. (a) Cumbersomeness and (b) writing effort of each UI in the referencing tasks in Study 1. Data labels are mean values, and error bars represent standard deviation.", "local_uri": ["1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Korero: Facilitating Complex Referencing of Visual Materials in Asynchronous Discussion Interface", "pdf_hash": "1885a9b16ab1afc22fa15f787472018b0ab8ea4c", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Participants\u2019 feedback on the design of our interface\u2019s main components were mostly positive. The contextual activity window and on-demand widget were deemed natural and not distracting. P9 in Study 1 felt that the display of different materials in the contextual activity window based on user\u2019s current action is intuitive, while P10 found the on-demand widget useful for checking what she has referred. While some found a slight learning curve with Korero, they all agreed that it was more convenient and efficient once they have learned it, to the extent that they can quickly adapt to the interface in the relatively short time of the user studies. In the post-study interviews, P3 and P12 from Study 1 voiced the need to support and facilitate elaborate references in certain scenarios, such as collaborative discussion in Google Docs and MOOCs.", "levels": [[3], [3], [3], [4, 3], [0]], "corpus_id": 19449715, "sentences": ["Participants\u2019 feedback on the design of our interface\u2019s main components were mostly positive.", "The contextual activity window and on-demand widget were deemed natural and not distracting.", "P9 in Study 1 felt that the display of different materials in the contextual activity window based on user\u2019s current action is intuitive, while P10 found the on-demand widget useful for checking what she has referred.", "While some found a slight learning curve with Korero, they all agreed that it was more convenient and efficient once they have learned it, to the extent that they can quickly adapt to the interface in the relatively short time of the user studies.", "In the post-study interviews, P3 and P12 from Study 1 voiced the need to support and facilitate elaborate references in certain scenarios, such as collaborative discussion in Google Docs and MOOCs."], "caption": "Figure 7. (a) Cumbersomeness and (b) mental effort of each UI in the referencing tasks in Study 2. Data labels are mean values, and error bars represent standard deviation.", "local_uri": ["1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_010.jpg", "1885a9b16ab1afc22fa15f787472018b0ab8ea4c_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 3, 4], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Addressing Cognitive and Emotional Barriers in Parent-Clinician Communication through Behavioral Visualization Webtools", "pdf_hash": "89f6ff9cc951bb633b2a1d6ceb6e78064a7efc94", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: Multiple sessions are displayed on this screen of Plexlines. Each horizontal line represents an RABC session and each circle represents a behavior. The modality of the behavior is represented through the color of the circle - red for vocalization, green for gesture, and blue for gaze.", "levels": null, "corpus_id": 218482663, "sentences": ["Figure 3: Multiple sessions are displayed on this screen of Plexlines.", "Each horizontal line represents an RABC session and each circle represents a behavior.", "The modality of the behavior is represented through the color of the circle - red for vocalization, green for gesture, and blue for gaze."], "caption": "Figure 3. Multiple sessions are displayed on this screen of Plexlines. Each horizontal line represents an RABC session and each circle represents a behavior. The modality of the behavior is represented through the color of the circle - red for vocalization, green for gesture, and blue for gaze.", "local_uri": ["89f6ff9cc951bb633b2a1d6ceb6e78064a7efc94_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "\u201cWait, Can You Move the Robot?\u201d: Examining Telepresence Robot Use in Collaborative Teams", "pdf_hash": "f6e3ba53b7adc9d5070de161b51eb186462acc83", "year": 2018, "venue": "2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "https://lh6.googleusercontent.com/R-w_MNKZFmihrbFAC2-rSc_LEJ12kZwVJEqrvO_DpcqfrSpvJabQnq1BTEoOn7UalYgmDcqSWBZjSsnSc9wavBT-zCWWlsCofASW3SpQNQRj8AmUtaNRId4HayT3FnduEkcwsSJv", "levels": null, "corpus_id": 28017212, "sentences": ["https://lh6.googleusercontent.com/R-w_MNKZFmihrbFAC2-rSc_LEJ12kZwVJEqrvO_DpcqfrSpvJabQnq1BTEoOn7UalYgmDcqSWBZjSsnSc9wavBT-zCWWlsCofASW3SpQNQRj8AmUtaNRId4HayT3FnduEkcwsSJv"], "caption": "", "local_uri": ["f6e3ba53b7adc9d5070de161b51eb186462acc83_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "\u201cWait, Can You Move the Robot?\u201d: Examining Telepresence Robot Use in Collaborative Teams", "pdf_hash": "f6e3ba53b7adc9d5070de161b51eb186462acc83", "year": 2018, "venue": "2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "https://lh4.googleusercontent.com/orfSTNiVPuZrbDdgkIe-H8kxrO_zg1OwHgSmwp47QIvhNU3WfX1WjKnzkKVlJyNsWs6T-rrkwDMi7c0gaRJw-3C8l4G6GOhZ8gezOxJm7Y_5YoJh2hmz-flu-SY9rYV1qlHAyDiA", "levels": null, "corpus_id": 28017212, "sentences": ["https://lh4.googleusercontent.com/orfSTNiVPuZrbDdgkIe-H8kxrO_zg1OwHgSmwp47QIvhNU3WfX1WjKnzkKVlJyNsWs6T-rrkwDMi7c0gaRJw-3C8l4G6GOhZ8gezOxJm7Y_5YoJh2hmz-flu-SY9rYV1qlHAyDiA"], "caption": "", "local_uri": ["f6e3ba53b7adc9d5070de161b51eb186462acc83_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "\u201cWait, Can You Move the Robot?\u201d: Examining Telepresence Robot Use in Collaborative Teams", "pdf_hash": "f6e3ba53b7adc9d5070de161b51eb186462acc83", "year": 2018, "venue": "2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "https://lh3.googleusercontent.com/5eCy5QLyeOjIq86jaBv64eE8t9VXx8syKhKKCg7si5lnuUH2eI4bnilmcy50Nn_f1ACCTTKBgBYRv2STrAa0uWTf_Ws04bnu8s8d7t3XhwikeOeuH9UrvRcUAutBrr_OCmmEdwRu", "levels": null, "corpus_id": 28017212, "sentences": ["https://lh3.googleusercontent.com/5eCy5QLyeOjIq86jaBv64eE8t9VXx8syKhKKCg7si5lnuUH2eI4bnilmcy50Nn_f1ACCTTKBgBYRv2STrAa0uWTf_Ws04bnu8s8d7t3XhwikeOeuH9UrvRcUAutBrr_OCmmEdwRu"], "caption": "", "local_uri": ["f6e3ba53b7adc9d5070de161b51eb186462acc83_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "CSCL and Eye-Tracking: Experiences, Opportunities and Challenges", "pdf_hash": "62c4d252ae95c74025f5134999aca5dabf9ffe11", "year": 2017, "venue": "CSCL", "alt_text": "B2ipOZdz4DD1xLvuE4aX8eVFei5HtX4DGgJZgW36G7pF0lG2opmammC7tA5RsOy28LBY-2mkOUDKcw2HaFQfJ5oCLTdr4GVtPKDcRKhpxotRnGfq_aNl5BdQiv7U4OYWRha4h4I", "levels": null, "corpus_id": 54139561, "sentences": ["B2ipOZdz4DD1xLvuE4aX8eVFei5HtX4DGgJZgW36G7pF0lG2opmammC7tA5RsOy28LBY-2mkOUDKcw2HaFQfJ5oCLTdr4GVtPKDcRKhpxotRnGfq_aNl5BdQiv7U4OYWRha4h4I"], "caption": "", "local_uri": ["62c4d252ae95c74025f5134999aca5dabf9ffe11_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "CSCL and Eye-Tracking: Experiences, Opportunities and Challenges", "pdf_hash": "62c4d252ae95c74025f5134999aca5dabf9ffe11", "year": 2017, "venue": "CSCL", "alt_text": "m7LtPABgeXVORQwpj4VkKoDme3s2tLJOY1Qq0bply5z4Jvj9t1wsGsfCHCUqNlMUrIowENxLCrA7TuhfBHhrafpYXK2QmHvuAzcLeUUp-YPlGnqEkwWEf6vU9wpBUa1r1RQDeVfU", "levels": [[-1]], "corpus_id": 54139561, "sentences": ["m7LtPABgeXVORQwpj4VkKoDme3s2tLJOY1Qq0bply5z4Jvj9t1wsGsfCHCUqNlMUrIowENxLCrA7TuhfBHhrafpYXK2QmHvuAzcLeUUp-YPlGnqEkwWEf6vU9wpBUa1r1RQDeVfU"], "caption": "Figure 2. On the left: remapping two s onto a ground truth using two synchronized mobile eye-trackers. The top left image is the perspective of the first student, the top right image is the perspective of the second student, and the bottom image is the ground truth. Red dots show joint visual attention, and line between the three perspectives show common points used to remap students\u2019 s onto the ground truth. On the right: traditional cross-recurrence graphs (top), augmented with spatial information (red, green, blue) and speech (bottom).", "local_uri": ["62c4d252ae95c74025f5134999aca5dabf9ffe11_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Watching Myself Watching Birds: Abjection, Ecological Thinking, and Posthuman Design", "pdf_hash": "fe5a150a491eada745aed9ed5f4c5c46abf281b1", "year": 2021, "venue": "CHI", "alt_text": "Minimum viable birdwatching kit used by first author, showing a Sibley Birds East field guide, a field journal and pair of binoculars.", "levels": null, "corpus_id": 233987475, "sentences": ["Minimum viable birdwatching kit used by first author, showing a Sibley Birds East field guide, a field journal and pair of binoculars."], "caption": "Figure 1: Minimum Viable Birdwatching Kit\u2014A Field Guide, feld notebook and binoculars.", "local_uri": ["fe5a150a491eada745aed9ed5f4c5c46abf281b1_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Watching Myself Watching Birds: Abjection, Ecological Thinking, and Posthuman Design", "pdf_hash": "fe5a150a491eada745aed9ed5f4c5c46abf281b1", "year": 2021, "venue": "CHI", "alt_text": "A series of still images from a film created by first author to think about cardinals. The stills show a cardinal on a tree branch with subtitles.", "levels": null, "corpus_id": 233987475, "sentences": ["A series of still images from a film created by first author to think about cardinals.", "The stills show a cardinal on a tree branch with subtitles."], "caption": "", "local_uri": ["fe5a150a491eada745aed9ed5f4c5c46abf281b1_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Watching Myself Watching Birds: Abjection, Ecological Thinking, and Posthuman Design", "pdf_hash": "fe5a150a491eada745aed9ed5f4c5c46abf281b1", "year": 2021, "venue": "CHI", "alt_text": "Two sketches drawn by first author, one is an idea for having a circular vortex of speakers you could stand inside to be part of a flock, and the second is a grid of performers all individually trying to be birds, effecitvely creating a flock when edited together.", "levels": null, "corpus_id": 233987475, "sentences": ["Two sketches drawn by first author, one is an idea for having a circular vortex of speakers you could stand inside to be part of a flock, and the second is a grid of performers all individually trying to be birds, effecitvely creating a flock when edited together."], "caption": "", "local_uri": ["fe5a150a491eada745aed9ed5f4c5c46abf281b1_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Crowdsourcing Design Guidance for Contextual Adaptation of Text Content in Augmented Reality", "pdf_hash": "81e34f94fbdcedf1439bfe7237ff7a0909724f24", "year": 2021, "venue": "CHI", "alt_text": "An illustration of the effect of sub-block size showing increasing levels of pixelation on a sample image.", "levels": null, "corpus_id": 233987316, "sentences": ["An illustration of the effect of sub-block size showing increasing levels of pixelation on a sample image."], "caption": "s = 20", "local_uri": ["81e34f94fbdcedf1439bfe7237ff7a0909724f24_Image_007.jpg", "81e34f94fbdcedf1439bfe7237ff7a0909724f24_Image_008.jpg", "81e34f94fbdcedf1439bfe7237ff7a0909724f24_Image_009.jpg", "81e34f94fbdcedf1439bfe7237ff7a0909724f24_Image_010.jpg"], "annotated": false, "compound": true}
{"title": "SenseBox: A DIY Prototyping Platform to Create Audio Interfaces for Therapy", "pdf_hash": "0b715ced4acbe572bc693c511dbe8e4df6137c54", "year": 2019, "venue": "Tangible and Embedded Interaction", "alt_text": "This image shows a green module next to a series of RFID tags. On the right, there are a series of CD cases and 3D printed figures that are augmented with tags.", "levels": null, "corpus_id": 83458519, "sentences": ["This image shows a green module next to a series of RFID tags.", "On the right, there are a series of CD cases and 3D printed figures that are augmented with tags."], "caption": "Figure 1. SenseBox consists of an audio playback module, housed in a 3D printed case (Left, top) and a series of RFID tags (Left, bottom) that can be embedded into physical objects (Right).", "local_uri": ["0b715ced4acbe572bc693c511dbe8e4df6137c54_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "SenseBox: A DIY Prototyping Platform to Create Audio Interfaces for Therapy", "pdf_hash": "0b715ced4acbe572bc693c511dbe8e4df6137c54", "year": 2019, "venue": "Tangible and Embedded Interaction", "alt_text": "This figure shows three images. The two images on the left show two different working SenseBox prototypes which consist of electronics contained in a 3D printed enclosures. The image on the right shows 3 different 3D printed objects in the form of a sphere, a box and a semi-sphere.", "levels": null, "corpus_id": 83458519, "sentences": ["This figure shows three images.", "The two images on the left show two different working SenseBox prototypes which consist of electronics contained in a 3D printed enclosures.", "The image on the right shows 3 different 3D printed objects in the form of a sphere, a box and a semi-sphere."], "caption": "Figure 2. Two functional SenseBox prototypes (Left), and three 3D printed mockups (Right) used during the design process.", "local_uri": ["0b715ced4acbe572bc693c511dbe8e4df6137c54_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SenseBox: A DIY Prototyping Platform to Create Audio Interfaces for Therapy", "pdf_hash": "0b715ced4acbe572bc693c511dbe8e4df6137c54", "year": 2019, "venue": "Tangible and Embedded Interaction", "alt_text": "This image shows a schematic of SenseBox which consists of a Raspberry Pi microcomputer, an RFID sensor, a speaker, a battery and a series of tags.", "levels": null, "corpus_id": 83458519, "sentences": ["This image shows a schematic of SenseBox which consists of a Raspberry Pi microcomputer, an RFID sensor, a speaker, a battery and a series of tags."], "caption": "", "local_uri": ["0b715ced4acbe572bc693c511dbe8e4df6137c54_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Understanding How Audio Mediates Experiences of Reminiscence for People Living with Blindness", "pdf_hash": "2507efe96b0a2cb4286ae4a02c0ba4df1a1acb02", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Figure 2-1: Field study interviews with participants. Most interviews took place at or around each respective participant\u2019s home.", "levels": null, "corpus_id": 220324184, "sentences": ["Figure 2-1: Field study interviews with participants.", "Most interviews took place at or around each respective participant\u2019s home."], "caption": "", "local_uri": ["2507efe96b0a2cb4286ae4a02c0ba4df1a1acb02_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Understanding How Audio Mediates Experiences of Reminiscence for People Living with Blindness", "pdf_hash": "2507efe96b0a2cb4286ae4a02c0ba4df1a1acb02", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Figure 2-2: Field study interviews with participants. Most interviews took place at or around each respective participant\u2019s home.", "levels": null, "corpus_id": 220324184, "sentences": ["Figure 2-2: Field study interviews with participants.", "Most interviews took place at or around each respective participant\u2019s home."], "caption": "", "local_uri": ["2507efe96b0a2cb4286ae4a02c0ba4df1a1acb02_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Understanding How Audio Mediates Experiences of Reminiscence for People Living with Blindness", "pdf_hash": "2507efe96b0a2cb4286ae4a02c0ba4df1a1acb02", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Figure 2-3: Field study interviews with participants. Most interviews took place at or around each respective participant\u2019s home.", "levels": null, "corpus_id": 220324184, "sentences": ["Figure 2-3: Field study interviews with participants.", "Most interviews took place at or around each respective participant\u2019s home."], "caption": "", "local_uri": ["2507efe96b0a2cb4286ae4a02c0ba4df1a1acb02_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Considering Wake Gestures for Smart Assistant Use", "pdf_hash": "ccbb556035dcb992b4b966f03ce8a59b20c064aa", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Illustrations of 5 most frequently suggested gestures, in a column: 1. Clapping hands - single clap sound marked 2. Hand fingersnapping - pre and post snap image. 3. Fingers clenched in a fist, pointing and middle fingers straightened. Vertical swipe with the finger to the right 4. Hand straightened, wrist horizontal waving movement. 5. Clapping hands - double clap sound marked", "levels": null, "corpus_id": 218482585, "sentences": ["Illustrations of 5 most frequently suggested gestures, in a column: 1.", "Clapping hands - single clap sound marked 2.", "Hand fingersnapping - pre and post snap image.", "3.", "Fingers clenched in a fist, pointing and middle fingers straightened.", "Vertical swipe with the finger to the right 4.", "Hand straightened, wrist horizontal waving movement.", "5. Clapping hands - double clap sound marked"], "caption": "Figure 1: Top 5 most frequently suggested gestures with respective number of suggestions received.", "local_uri": ["ccbb556035dcb992b4b966f03ce8a59b20c064aa_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Considering Wake Gestures for Smart Assistant Use", "pdf_hash": "ccbb556035dcb992b4b966f03ce8a59b20c064aa", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Bar chart of RSME results for respective gestures. From highest to lowest: Clap, Double Clap, Snap, Swipe and Wave. Error Bars showing standard error.", "levels": [[1], [2], [1]], "corpus_id": 218482585, "sentences": ["Bar chart of RSME results for respective gestures.", "From highest to lowest: Clap, Double Clap, Snap, Swipe and Wave.", "Error Bars showing standard error."], "caption": "Figure 2: RSME measurements for the respective gestures. Error bars show standard error.", "local_uri": ["ccbb556035dcb992b4b966f03ce8a59b20c064aa_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Considering Wake Gestures for Smart Assistant Use", "pdf_hash": "ccbb556035dcb992b4b966f03ce8a59b20c064aa", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Acceptability scores for respective gestures, collated scores of acceptance at home, while being alone, with friends and in presence of strangers.", "levels": null, "corpus_id": 218482585, "sentences": ["Acceptability scores for respective gestures, collated scores of acceptance at home, while being alone, with friends and in presence of strangers."], "caption": "", "local_uri": ["ccbb556035dcb992b4b966f03ce8a59b20c064aa_Image_005.jpg", "ccbb556035dcb992b4b966f03ce8a59b20c064aa_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Beyond Information Content", "pdf_hash": "b71239a846b2cce6923a950eb32a97b40e568a78", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A Screen shot of the IM interface used in this study. Participants typed in and read messages in left pane. The right pane showed either the task material or the pop-up survey, depending on the phrase of the task.", "levels": null, "corpus_id": 20314945, "sentences": ["A Screen shot of the IM interface used in this study.", "Participants typed in and read messages in left pane.", "The right pane showed either the task material or the pop-up survey, depending on the phrase of the task."], "caption": "Fig. 2 A Screen shot of the IM interface used in this study. Participants typed in and read messages in left pane. The right pane showed either", "local_uri": ["b71239a846b2cce6923a950eb32a97b40e568a78_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "View-Dependent Video Textures for 360\u00b0 Video", "pdf_hash": "472204d1e42f9b24e044dea662edaeff3d5116fe", "year": 2019, "venue": "UIST", "alt_text": "Figure 2 shows a screenshot of our prototype desktop video-editing interface. The upper left shows a preview of what is shown in the VR headset. The upper right has the full equirectangular frame of the video, which is a scene from \"Invasion!\" and shows a rabbit and two aliens. The current headset FOV is marked on the equirect frame with a pink rectangle. On the bottom, there is video timeline with two clips shown as dark blue rectangles (the clips have timecodes 107\u2026114 and 114\u2026133). The first clip is a gated clip with a red vertical line marking the gate timecode and many red backward arrows on top the clip indicating the loops. The region of interest (ROI) is shown on the equirectangular frame with a green box. Below each clip, there is a settings box with parameters that the author can tweak. For the gated clip, the parameters are: 1) Gated Clip? (toggle), 2) Mute Audio? (toggle), 3) Select ROI (button), 4) Lookat Gate? (toggle), 5) Min Loop Length (text field), 6) Perceptual Threshold (text field), 7) Load Arc Costs (button), 8) Apply Gate (button), 9) Enable Forward Jumps? (toggle), 10) Jump Perceptual Threshold (text field), 11) Cross-fade audio? (toggle)", "levels": null, "corpus_id": 201708898, "sentences": ["Figure 2 shows a screenshot of our prototype desktop video-editing interface.", "The upper left shows a preview of what is shown in the VR headset.", "The upper right has the full equirectangular frame of the video, which is a scene from \"Invasion!\" and shows a rabbit and two aliens.", "The current headset FOV is marked on the equirect frame with a pink rectangle.", "On the bottom, there is video timeline with two clips shown as dark blue rectangles (the clips have timecodes 107\u2026114 and 114\u2026133).", "The first clip is a gated clip with a red vertical line marking the gate timecode and many red backward arrows on top the clip indicating the loops.", "The region of interest (ROI) is shown on the equirectangular frame with a green box.", "Below each clip, there is a settings box with parameters that the author can tweak.", "For the gated clip, the parameters are: 1) Gated Clip? (toggle), 2) Mute Audio? (toggle), 3) Select ROI (button), 4) Lookat Gate? (toggle), 5) Min Loop Length (text field), 6) Perceptual Threshold (text field), 7) Load Arc Costs (button), 8) Apply Gate (button), 9) Enable Forward Jumps? (toggle), 10) Jump Perceptual Threshold (text field), 11) Cross-fade audio? (toggle)"], "caption": "Figure 2. Our prototype desktop video editing inter- face with a gated clip. The upper left pane shows a preview of the headset FOV, which is output live to the Oculus Rift. The upper right pane shows the full equirectangular view and marks the current headset FOV (pink border). The video timeline on the bottom acts as a conventional video editor, with each dark blue rectangle representing a clip. The \ufb01rst clip is a gated clip (white border). Filmmakers specify a gate timecode (red vertical line on timeline), a ROI (green box) on the equirectangular frame, as well as other parameters shown in the settings box below the clip. View-dependent arcs are shown as backward arcs (red arrows) on top of the gated clip. In this ex- ample, the ROI is the aliens, and the gate condition is a lookat gate, i.e., playback may not advance past the gate timecode unless the viewer is looking at the ROI. To avoid static loops and reduce arcs with visual arti- facts, the \ufb01lmmaker can set thresholds on the length of arcs and on the perceptual difference of arc transi- tions. After setting all thresholds, the \ufb01lmmaker can generate the view-dependent arcs by loading in (pre- processed) arc costs and applying the gate. To have viewers jump to the gate timecode as soon as they see the ROI, the \ufb01lmmaker can also enable forward arcs that are under a perceptual threshold. Finally, the \ufb01lmmaker can choose to cross-fade the audio during loop transitions or choose to mute the audio entirely.", "local_uri": ["472204d1e42f9b24e044dea662edaeff3d5116fe_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design", "pdf_hash": "529025645c70a935221bd434484faee695ad0f25", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"Mapping the human-AI interaction design challenges in the literature onto a user-centered design process (Double Diamond).\"", "levels": null, "corpus_id": 218483124, "sentences": ["Figure 1: \"Mapping the human-AI interaction design challenges in the literature onto a user-centered design process (Double Diamond).\""], "caption": "", "local_uri": ["529025645c70a935221bd434484faee695ad0f25_Image_001.png"], "annotated": false, "compound": false}
{"title": "Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design", "pdf_hash": "529025645c70a935221bd434484faee695ad0f25", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"Mapping UX design challenges of AI in prior research on a technology-driven design innovation process.\"", "levels": null, "corpus_id": 218483124, "sentences": ["Figure 2: \"Mapping UX design challenges of AI in prior research on a technology-driven design innovation process.\""], "caption": "", "local_uri": ["529025645c70a935221bd434484faee695ad0f25_Image_002.png"], "annotated": false, "compound": false}
{"title": "Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design", "pdf_hash": "529025645c70a935221bd434484faee695ad0f25", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"The conceptual pathway translating between AI's capabilities and thoughtful designs of human-AI interaction.\"", "levels": null, "corpus_id": 218483124, "sentences": ["Figure 3: \"The conceptual pathway translating between AI's capabilities and thoughtful designs of human-AI interaction.\""], "caption": "", "local_uri": ["529025645c70a935221bd434484faee695ad0f25_Image_003.png"], "annotated": false, "compound": false}
{"title": "Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design", "pdf_hash": "529025645c70a935221bd434484faee695ad0f25", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: \"The AI design complexity map. The two dimensions of this map -- capability uncertainty and output complexity -- outline whether and why a particular AI system is difficult to design.\"", "levels": null, "corpus_id": 218483124, "sentences": ["Figure 4: \"The AI design complexity map.", "The two dimensions of this map -- capability uncertainty and output complexity -- outline whether and why a particular AI system is difficult to design.\""], "caption": "", "local_uri": ["529025645c70a935221bd434484faee695ad0f25_Image_004.png"], "annotated": false, "compound": false}
{"title": "Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design", "pdf_hash": "529025645c70a935221bd434484faee695ad0f25", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: \"An example of the framework in use. Using the framework, researchers can easily outline the problem space of a human-AI interaction issue of their interest, for example, the issue of AI fairness.\"", "levels": null, "corpus_id": 218483124, "sentences": ["Figure 6: \"An example of the framework in use.", "Using the framework, researchers can easily outline the problem space of a human-AI interaction issue of their interest, for example, the issue of AI fairness.\""], "caption": "", "local_uri": ["529025645c70a935221bd434484faee695ad0f25_Image_006.png"], "annotated": false, "compound": false}
{"title": "Enhancing the Composition Task in Text Entry Studies: Eliciting Difficult Text and Improving Error Rate Calculation", "pdf_hash": "b539dbc3c0f2d19d7891b87312b99bb013b2c348", "year": 2021, "venue": "CHI", "alt_text": "The left image reads, \"1/10, Invent a message. Choose a message that would likely have no autocorrect errors if typed on a mobile device. You can tap and hold to lock in characters.\" The middle image reads, \"1/10, Invent a message. Choose a message that would likely have one or more autocorrect errors if typed on a mobile device. You can tap and hold to lock in characters.\" The right image reads, \"Input: please forward me the attachment. Entry rate: 31.8 wpm. Please type the message you actually intended on the laptop.\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987500, "sentences": ["The left image reads, \"1/10, Invent a message.", "Choose a message that would likely have no autocorrect errors if typed on a mobile device.", "You can tap and hold to lock in characters.\"", "The middle image reads, \"1/10, Invent a message.", "Choose a message that would likely have one or more autocorrect errors if typed on a mobile device.", "You can tap and hold to lock in characters.\"", "The right image reads, \"Input: please forward me the attachment.", "Entry rate: 31.8 wpm.", "Please type the message you actually intended on the laptop.\""], "caption": "", "local_uri": ["b539dbc3c0f2d19d7891b87312b99bb013b2c348_Image_002.jpg", "b539dbc3c0f2d19d7891b87312b99bb013b2c348_Image_003.jpg", "b539dbc3c0f2d19d7891b87312b99bb013b2c348_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Enhancing the Composition Task in Text Entry Studies: Eliciting Difficult Text and Improving Error Rate Calculation", "pdf_hash": "b539dbc3c0f2d19d7891b87312b99bb013b2c348", "year": 2021, "venue": "CHI", "alt_text": "Both images show the keyboard interface occupying the lower portion of the screen. It contains the letters a-z in a Qwerty layout. The upper portion of the screen is the text area containing the text \"ready to meet\". The user's finger is pressing the letter 'N', which is alpha-channeled over the top of the text area. In the left image, the 'N' is white. In the right image, it is orange, indicating the letter has been locked.", "levels": null, "corpus_id": 233987500, "sentences": ["Both images show the keyboard interface occupying the lower portion of the screen.", "It contains the letters a-z in a Qwerty layout.", "The upper portion of the screen is the text area containing the text \"ready to meet\".", "The user's finger is pressing the letter 'N', which is alpha-channeled over the top of the text area.", "In the left image, the 'N' is white.", "In the right image, it is orange, indicating the letter has been locked."], "caption": "Figure 2: Study 1 smartwatch interface. Users type each let- ter in a word before swiping right to obtain the most likely recognition. The interface shows the nearest key label to a user\u2019s touch. After 500 ms, the label changes color to signify the letter is locked and no longer subject to auto-correct.", "local_uri": ["b539dbc3c0f2d19d7891b87312b99bb013b2c348_Image_005.jpg", "b539dbc3c0f2d19d7891b87312b99bb013b2c348_Image_006.jpg"], "annotated": false, "compound": true}
{"title": "Enhancing the Composition Task in Text Entry Studies: Eliciting Difficult Text and Improving Error Rate Calculation", "pdf_hash": "b539dbc3c0f2d19d7891b87312b99bb013b2c348", "year": 2021, "venue": "CHI", "alt_text": "This figure depicts the smartwatch interface used. The lower portion of the screen contains the keyboard, with letters a-z in a Qwerty arrangement and apostrophe to the right of 'M' in the bottom row. The upper portion contains the text area with the text \"hello h_\". The \"h_\" is highlighted in black to show it is the current word in progress. There is a left arrow to the left of the text area that functions as a backspace key. There are two word suggestions each above and below the text area as well. The literal slot is the upper left slot, containing \"h\" in quotation marks. The top right slot contains the word \"have\", the bottom left the word \"has\", and the bottom right the word \"his\".", "levels": null, "corpus_id": 233987500, "sentences": ["This figure depicts the smartwatch interface used.", "The lower portion of the screen contains the keyboard, with letters a-z in a Qwerty arrangement and apostrophe to the right of 'M' in the bottom row.", "The upper portion contains the text area with the text \"hello h_\".", "The \"h_\" is highlighted in black to show it is the current word in progress.", "There is a left arrow to the left of the text area that functions as a backspace key.", "There are two word suggestions each above and below the text area as well.", "The literal slot is the upper left slot, containing \"h\" in quotation marks.", "The top right slot contains the word \"have\", the bottom left the word \"has\", and the bottom right the word \"his\"."], "caption": "Figure 4: Study 2 smartwatch interface. The center high- lighted text is the best word assuming typing is complete for this word. The top left slot shows the literal characters typed. The other slots show word completions. The left arrow is a backspace key. Options are selected either by tapping or by a swipe gesture (e.g. up-and-left for the literal slot).", "local_uri": ["b539dbc3c0f2d19d7891b87312b99bb013b2c348_Image_016.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "pdf_hash": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "year": 2021, "venue": "W4A", "alt_text": "The picture shows a screenshot of a phone with Google Live Transcribe installed. The screenshot of the app consists of two parts, the top part has a dark black background with white text and contains the word \"Hello\". The bottom part consists of a dark gray background with white text and contains the words \"How are you?\".", "levels": null, "corpus_id": 234794797, "sentences": ["The picture shows a screenshot of a phone with Google Live Transcribe installed.", "The screenshot of the app consists of two parts, the top part has a dark black background with white text and contains the word \"Hello\".", "The bottom part consists of a dark gray background with white text and contains the words \"How are you?\"."], "caption": "", "local_uri": ["377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "pdf_hash": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "year": 2021, "venue": "W4A", "alt_text": "This photo shows one DHH participant conversing with one hearing actor during the in-person study. They are in a meeting room with tan-colored walls. The DHH participant and the hearing actor are both sitting near a table corner, with the DHH participant sitting to the left while holding a phone, showing it's contents to the hearing actor who is sitting to the right. The face of the DHH participant is covered with a black circle.", "levels": null, "corpus_id": 234794797, "sentences": ["This photo shows one DHH participant conversing with one hearing actor during the in-person study.", "They are in a meeting room with tan-colored walls.", "The DHH participant and the hearing actor are both sitting near a table corner, with the DHH participant sitting to the left while holding a phone, showing it's contents to the hearing actor who is sitting to the right.", "The face of the DHH participant is covered with a black circle."], "caption": "Figure 1: (a) Screenshot of Google Live Transcribe [12] app used in in-person study. (b) Photo of a DHH participant (on the right) holding a smartphone with Google Live Transcribe [12] installed, communicating with a hearing actor (left). The researcher is sitting outside the picture to the left of the actor.", "local_uri": ["377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "pdf_hash": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "year": 2021, "venue": "W4A", "alt_text": "This photo shows three people connected via an videoconferencing application called Zoom. The screen is in Gallery mode, so there are three users within three boxes of equal size. The boxes are located in the top right, top left, and center bottom positions. The top left box contains the DHH participant, the top right box contains the researcher of the study, and the center bottom box contains the hearing actor. All three participants are located in a room with white walls and a door in the background. The face of the participant is covered with a black circle.", "levels": null, "corpus_id": 234794797, "sentences": ["This photo shows three people connected via an videoconferencing application called Zoom.", "The screen is in Gallery mode, so there are three users within three boxes of equal size.", "The boxes are located in the top right, top left, and center bottom positions.", "The top left box contains the DHH participant, the top right box contains the researcher of the study, and the center bottom box contains the hearing actor.", "All three participants are located in a room with white walls and a door in the background.", "The face of the participant is covered with a black circle."], "caption": "Figure 2: A photo of one DHH participant (pictured top left), the researcher conducting the interview (pictured top right), and the hearing actor (pictured center bottom) connected via Zoom [46]. Gallery mode is enabled to ensure that all three faces can be made large enough and seen equally during the experiment.", "local_uri": ["377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "pdf_hash": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "year": 2021, "venue": "W4A", "alt_text": "A grouped column graph showing results for means of satisfaction scores in the in-person study for each behavior. Each behavior is listed on the x-axis, with satisfaction scores on the y-axis. The following lists each behavior groups with three means, one for high, medium, and low levels of that behavior. Speech rate had means of 7.1, 7.95, and 6.95 for high, medium, and low, respectively. Voice intensity had means of 6.9, 7.25, and 5.9. Enunciation had means of 6.9, 8.3, and 5.9. Intonation had means of 7.85, 7.95, and 6.4. Eye contact had means of 7.95, 7.8, and 7. Gesturing had means of 8.65, 7.95, and 7.60. Intermittent pausing had means of 6.3, 7.3, and 7.45. A double asterisk is shown next to the behaviors Enunciation and Intonation as they had significant omnibus Friedman test results. A double asterisk, denoting pairwise significant differences are shown between the pairs medium and low enunciation as well as high and low intonation.", "levels": [[1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [3, 1], [3, 1]], "corpus_id": 234794797, "sentences": ["A grouped column graph showing results for means of satisfaction scores in the in-person study for each behavior.", "Each behavior is listed on the x-axis, with satisfaction scores on the y-axis.", "The following lists each behavior groups with three means, one for high, medium, and low levels of that behavior.", "Speech rate had means of 7.1, 7.95, and 6.95 for high, medium, and low, respectively.", "Voice intensity had means of 6.9, 7.25, and 5.9.", "Enunciation had means of 6.9, 8.3, and 5.9.", "Intonation had means of 7.85, 7.95, and 6.4.", "Eye contact had means of 7.95, 7.8, and 7.", "Gesturing had means of 8.65, 7.95, and 7.60.", "Intermittent pausing had means of 6.3, 7.3, and 7.45.", "A double asterisk is shown next to the behaviors Enunciation and Intonation as they had significant omnibus Friedman test results.", "A double asterisk, denoting pairwise significant differences are shown between the pairs medium and low enunciation as well as high and low intonation."], "caption": "", "local_uri": ["377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "pdf_hash": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "year": 2021, "venue": "W4A", "alt_text": "A grouped column graph showing results for means of satisfaction scores in the remote study for each behavior. Each behavior is listed on the x-axis, with satisfaction scores on the y-axis. The following lists each behavior groups with three means, one for high, medium, and low levels of that behavior. Speech rate had means of 3.61, 5.74, and 3.75 for high, medium, and low, respectively. Voice intensity had means of 6.91, 6.57, and 4.61. Enunciation had means of 5.96, 6.48, and 4.04. Intonation had means of 7, 6.48, and 5.74. Eye contact had means of 6.43, 5.83, and 3.65. Gesturing had means of 7, 6.74, and 5.65. Intermittent pausing had means of 5.52, 6.48, 5.39. A double asterisk is shown next to the behaviors Speech Rate, Voice Intensity, Enunciation, Intonation, and Eye Contact as they had significant omnibus Friedman test results. A double asterisk, denoting pairwise significant differences are shown between the pairs High and Medium speech rate, medium and low speech rate, high and low intensity, medium and low intensity, medium and low enunciation, high and low eye contact, and medium and low eye contact.", "levels": [[1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2, 1]], "corpus_id": 234794797, "sentences": ["A grouped column graph showing results for means of satisfaction scores in the remote study for each behavior.", "Each behavior is listed on the x-axis, with satisfaction scores on the y-axis.", "The following lists each behavior groups with three means, one for high, medium, and low levels of that behavior.", "Speech rate had means of 3.61, 5.74, and 3.75 for high, medium, and low, respectively.", "Voice intensity had means of 6.91, 6.57, and 4.61.", "Enunciation had means of 5.96, 6.48, and 4.04.", "Intonation had means of 7, 6.48, and 5.74.", "Eye contact had means of 6.43, 5.83, and 3.65.", "Gesturing had means of 7, 6.74, and 5.65.", "Intermittent pausing had means of 5.52, 6.48, 5.39.", "A double asterisk is shown next to the behaviors Speech Rate, Voice Intensity, Enunciation, Intonation, and Eye Contact as they had significant omnibus Friedman test results.", "A double asterisk, denoting pairwise significant differences are shown between the pairs High and Medium speech rate, medium and low speech rate, high and low intensity, medium and low intensity, medium and low enunciation, high and low eye contact, and medium and low eye contact."], "caption": "(a)", "local_uri": ["377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 1 -\u00ad Screenshot of Tinkercad website. File menus along the top, object and tool selection on the right, and main panel in center is the canvas/build space.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 1 -\u00ad Screenshot of Tinkercad website.", "File menus along the top, object and tool selection on the right, and main panel in center is the canvas/build space."], "caption": "Figure 1. Typical Tinkercad workspace.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_002.png"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 2 \u00ad- Screenshot of Thingiverse website. A long, vertical, dropdown menu of filters obscures a grid for browsing 3D models.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 2 \u00ad- Screenshot of Thingiverse website.", "A long, vertical, dropdown menu of filters obscures a grid for browsing 3D models."], "caption": "Figure 2. Sorting filters on Thingiverse\u2019s \u201cExplore\u201d page.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 3 -\u00ad Screenshot of the myUMBC homepage. There is a menu with words and icons along the top. A long list of alternative links on the right side. A center pane shows information on the featured selection.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 3 -\u00ad Screenshot of the myUMBC homepage.", "There is a menu with words and icons along the top.", "A long list of alternative links on the right side.", "A center pane shows information on the featured selection."], "caption": "Figure 3. Home page of our university's online portal.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 4 -\u00ad Screenshot of Google Drive landing page. Hierarchical menu structure on the left, main frame with current content focus in center/right.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 4 -\u00ad Screenshot of Google Drive landing page.", "Hierarchical menu structure on the left, main frame with current content focus in center/right."], "caption": "Figure 4. The home page of Google Drive.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 5 -\u00ad Screenshot of a menu on Thingiverse. The menu includes three separate ways to save or bookmark the currently viewed design.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 5 -\u00ad Screenshot of a menu on Thingiverse.", "The menu includes three separate ways to save or bookmark the currently viewed design."], "caption": "Figure 5. Models can be saved in three locations on Thingiverse: \u201cMy Things,\u201d \u201cMy Collections,\u201d or \u201cThings I\u2019ve Liked.\u201d", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 6 -\u00ad Icons from myUMBC. A bell, an envelope, a chalkboard, and a grid are all depicted with icons approximately 25x25 pixels.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 6 -\u00ad Icons from myUMBC.", "A bell, an envelope, a chalkboard, and a grid are all depicted with icons approximately 25x25 pixels."], "caption": "Figure 6. The Blackboard icon (black chalkboard) on our university\u2019s online portal.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 8 \u00ad Close up screenshot of the Google Drive navigation showing the \u201cShared with me\u201d option.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 8 \u00ad Close up screenshot of the Google Drive navigation showing the \u201cShared with me\u201d option."], "caption": "Figure 8. \u201cShared with me\u201d option on Google Drive.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 9 \u00ad- The OS X logo for the file management application called, \u201cFinder.\u201d The icon is a rectangular face with a line down the middle. The line creates the illusion of a single smiling face or two smiling faces in profile.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 9 \u00ad- The OS X logo for the file management application called, \u201cFinder.\u201d The icon is a rectangular face with a line down the middle.", "The line creates the illusion of a single smiling face or two smiling faces in profile."], "caption": "Figure 9. Apple\u2019s Finder Icon. Students with ID may have difficulty associating this image with file management.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Accessibility barriers to online education for young adults with intellectual disabilities", "pdf_hash": "635e8e0255787e6162746a360da53e82a92f15c6", "year": 2016, "venue": "W4A", "alt_text": "Figure 10 \u00ad Screenshot of the special desktop wallpaper imaged used in the 3D printing class to help students with file management. The wallpaper has squares and labels to convey the appropriate drag\u00adand\u00addrop interactions and locations for certain file types.", "levels": null, "corpus_id": 17300128, "sentences": ["Figure 10 \u00ad Screenshot of the special desktop wallpaper imaged used in the 3D printing class to help students with file management.", "The wallpaper has squares and labels to convey the appropriate drag\u00adand\u00addrop interactions and locations for certain file types."], "caption": "Figure 10. The desktop background used in 3D printing class includes visual cues for file locations and instructions.", "local_uri": ["635e8e0255787e6162746a360da53e82a92f15c6_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Developing an interface to support procedural memory training using a participatory-based approach", "pdf_hash": "1c56797c011e66b8653cf98f9ac0979de36acffb", "year": 2012, "venue": "BCS HCI", "alt_text": "Example of a drawing from the second design session displaying colour options for navigation.", "levels": null, "corpus_id": 2747153, "sentences": ["Example of a drawing from the second design session displaying colour options for navigation."], "caption": "", "local_uri": ["1c56797c011e66b8653cf98f9ac0979de36acffb_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "pdf_hash": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "year": 2020, "venue": "ASSETS", "alt_text": "The figure shows two screenshots of the video demonstrations showed to participants in the studies.  On the left, the screenshot shows a paragraph highlighted in yellow, with a black square surrounding labeled as \"Simpler Version.\"  On the right, the screenshot shows the original version of the same paragraph in the left, in this case showing certain words highlighted in yellow, including \"event,\" \"glowing,\" and \"bellies.\" The words \"split-second,\" \"precise,\" and \"remained\" are highlighted in gray.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 225954078, "sentences": ["The figure shows two screenshots of the video demonstrations showed to participants in the studies.", "On the left, the screenshot shows a paragraph highlighted in yellow, with a black square surrounding labeled as \"Simpler Version.\"", "On the right, the screenshot shows the original version of the same paragraph in the left, in this case showing certain words highlighted in yellow, including \"event,\" \"glowing,\" and \"bellies.\"", "The words \"split-second,\" \"precise,\" and \"remained\" are highlighted in gray."], "caption": "", "local_uri": ["b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "pdf_hash": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "year": 2020, "venue": "ASSETS", "alt_text": "This figure shows a stacked bar plot of percentages of a time-spent scale with 6 points. The y-axis shows the percentage of participants. The x-axis has X categories \"On-screen\" and \"Paper\".  The following scale responses are shown from bottom to top: \"> 60 minutes\", \"30-60 minutes\", \"15-29 minutes\", \"10-15 minutes\", \"5-9 minutes\", \"1-4 minutes\", \"None at all\".  For \"On-screen\",  47% responded \"> 60 minutes\", 31 % responded \"30-60 minutes\",  6% responded \"15-29 minutes\",  6% responded \"10-15 minutes\",  6% responded \"5-9 minutes\", 3% responded \"1-4 minutes\".  For \"Paper\",  31% responded \"> 60 minutes\",  16% responded \"30-60 minutes\",  12.5% responded \"15-29 minutes\",  31% responded \"10-15 minutes\",  12.5% responded \"5-9 minutes\",  3% responded \"1-4 minutes\",  22% responded \"None at all\".", "levels": [[1], [1], [1], [1], [2], [2]], "corpus_id": 225954078, "sentences": ["This figure shows a stacked bar plot of percentages of a time-spent scale with 6 points.", "The y-axis shows the percentage of participants.", "The x-axis has X categories \"On-screen\" and \"Paper\".", "The following scale responses are shown from bottom to top: \"> 60 minutes\", \"30-60 minutes\", \"15-29 minutes\", \"10-15 minutes\", \"5-9 minutes\", \"1-4 minutes\", \"None at all\".", "For \"On-screen\",  47% responded \"> 60 minutes\", 31 % responded \"30-60 minutes\",  6% responded \"15-29 minutes\",  6% responded \"10-15 minutes\",  6% responded \"5-9 minutes\", 3% responded \"1-4 minutes\".", "For \"Paper\",  31% responded \"> 60 minutes\",  16% responded \"30-60 minutes\",  12.5% responded \"15-29 minutes\",  31% responded \"10-15 minutes\",  12.5% responded \"5-9 minutes\",  3% responded \"1-4 minutes\",  22% responded \"None at all\"."], "caption": "", "local_uri": ["b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "pdf_hash": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "year": 2020, "venue": "ASSETS", "alt_text": "This figure shows a histogram of the frequency of participants' responses. The y-axis shows the number of participants. The x-axis has 10 categories: \"Work\", \"Academic\", \"Personal comm.\", \"Visual media\", \"News\", \"Recreation\", \"Personal reading\", \"Medical\", \"Legal\" and \"Other\".  29 responded \"Work\", 26 responded  \"Academic\", 25 responded \"Personal comm.\", 24 responded \"Visual media\", 21 responded \"News\", 20 responded \"Recreation\", 19 responded \"Personal Reading\", 8 responded \"Medical\", 8 responded \"Legal\", and 3 responded \"Other\".", "levels": null, "corpus_id": 225954078, "sentences": ["This figure shows a histogram of the frequency of participants' responses.", "The y-axis shows the number of participants.", "The x-axis has 10 categories: \"Work\", \"Academic\", \"Personal comm.\", \"Visual media\", \"News\", \"Recreation\", \"Personal reading\", \"Medical\", \"Legal\" and \"Other\".", "29 responded \"Work\", 26 responded  \"Academic\", 25 responded \"Personal comm.\", 24 responded \"Visual media\", 21 responded \"News\", 20 responded \"Recreation\", 19 responded \"Personal Reading\", 8 responded \"Medical\", 8 responded \"Legal\", and 3 responded \"Other\"."], "caption": "", "local_uri": ["b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "pdf_hash": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "year": 2020, "venue": "ASSETS", "alt_text": "This figure shows a histogram of the frequency of participants' responses. The y-axis shows the number of participants. The x-axis has 6 categories: \"Dictionary\", \"Find other website\", \"Ask coworkers\", \"Other, \"Find ASL translation\", \"Ask supervisor\".  25 responded \"Dictionary\", 21 responded  \"Find other website\", 11 responded \"Ask coworkers.\", 8 responded \"Other\", 4 responded \"Find ASL translation\", and 3 responded \"Ask supervisor\".", "levels": null, "corpus_id": 225954078, "sentences": ["This figure shows a histogram of the frequency of participants' responses.", "The y-axis shows the number of participants.", "The x-axis has 6 categories: \"Dictionary\", \"Find other website\", \"Ask coworkers\", \"Other, \"Find ASL translation\", \"Ask supervisor\".", "25 responded \"Dictionary\", 21 responded  \"Find other website\", 11 responded \"Ask coworkers.\", 8 responded \"Other\", 4 responded \"Find ASL translation\", and 3 responded \"Ask supervisor\"."], "caption": "", "local_uri": ["b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "pdf_hash": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "year": 2020, "venue": "ASSETS", "alt_text": "This figure shows a stacked bar plot of percentages of 5-point frequency scale responses. The y-axis shows the percentage of participants. The x-axis has X categories \"Reading\" and \"Video\".  The following scale responses are shown from bottom to top: \"Daily\", \"Often\", \"Weekly\", \"Monthly\",  and \"Rarely\"  For \"Reading\",  28% responded \"Daily\", 28% responded \"Often\", 15.5% responded \"Weekly\", 15.5% responded \"Monthly, and 12.5% responded \"Rarely.\"  For \"Video\",  10% responded \"Daily\", 29% responded \"Often\", 19% responded \"Weekly\", 29% responded \"Monthly, and 13% responded \"Rarely.\"", "levels": [[1], [1], [1], [2, 1], [2]], "corpus_id": 225954078, "sentences": ["This figure shows a stacked bar plot of percentages of 5-point frequency scale responses.", "The y-axis shows the percentage of participants.", "The x-axis has X categories \"Reading\" and \"Video\".", "The following scale responses are shown from bottom to top: \"Daily\", \"Often\", \"Weekly\", \"Monthly\",  and \"Rarely\"  For \"Reading\",  28% responded \"Daily\", 28% responded \"Often\", 15.5% responded \"Weekly\", 15.5% responded \"Monthly, and 12.5% responded \"Rarely.\"", "For \"Video\",  10% responded \"Daily\", 29% responded \"Often\", 19% responded \"Weekly\", 29% responded \"Monthly, and 13% responded \"Rarely.\""], "caption": "Figure 4: Participants\u2019 responses to the questions: \"How of- ten do you read to learn about technical topics for your work (e.g. information about new technologies, new software, pro- gramming information, etc.)?\" and \"How often do you watch videos to learn about technical topics for your work (e.g. in- formation about new technologies, new software, program- ming information, etc.)?\" The full text in the options was: \"rarely (less than once a month),\" \"monthly (one to three times a month),\" \"weekly (once a week),\" \"often (two to four times a week),\" and \"daily (fve or more times a week).\" There was a signifcant diference between the two (p < 0.05).", "local_uri": ["b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "pdf_hash": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 7 shows a stacked bar plot of percentages of 5-point frequency scale responses. The y-axis shows the percentage of participants. The x-axis, labeled as \"Purpose,\" has 9 categories: \"Academic\", \"Medical\", \"Legal\",  \"Work\", \"Personal Reading\", \"News\", \"Visual media\", \"Personal com.\",  \"Recreation\". These scale responses are shown from bottom to top: \"Daily\", \"Often\", \"Weekly\", \"Monthly\",  and \"Rarely\". These scalre responses are shown from bottom to top:  \"Extremely Interested\", \"Very Interested\", \"Somewhat Interested\", \"Slightly Interested\", \"Not Interested\". For \"Academic\", 37.5% selected \"Extremely Interested\", 22% \"Very Interested\", 21%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 6%  \"Not Interested\". For \"Medical\", 37.5% selected \"Extremely Interested\", 19%  \"Very Interested\", 25%  \"Somewhat Interested\", 6%  \"Slightly Interested\", 12.5%  \"Not Interested\". For \"Legal\", 47% selected \"Extremely Interested\", 6%  \"Very Interested\", 28%  \"Somewhat Interested\", 9%  \"Slightly Interested\", 9%  \"Not Interested\". For \"Work\", 31% selected \"Extremely Interested\", 19%  \"Very Interested\", 25%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 12.5%  \"Not Interested\". For \"Personal reading\", 22% selected \"Extremely Interested\", 25%  \"Very Interested\", 6%  \"Somewhat Interested\", 25%  \"Slightly Interested\", 22%  \"Not Interested\". For \"News\", 9% selected \"Extremely Interested\", 3%  \"Very Interested\", 16%  \"Somewhat Interested\", 22%  \"Slightly Interondedested\", 22%  \"Not Interested\". For \"Visual media\", 9% selected \"Extremely Interested\", 25%  \"Very Interested\", 19%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 34.5%  \"Not Interested\". For \"Personal comm.\", 19% selected \"Extremely Interested\", 12.5%  \"Very Interested\", 15.5%  \"Somewhat Interested\", 25%  \"Slightly Interested\", 28%  \"Not Interested\". For \"Recreation\", 3% selected \"Extremely Interested\", 12.5%  \"Very Interested\", 25%  \"Somewhat Interested\", 22%  \"Slightly Interested\", 37.5% \"Not interested\"", "levels": [[1], [1], [1], [1], [1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 225954078, "sentences": ["Figure 7 shows a stacked bar plot of percentages of 5-point frequency scale responses.", "The y-axis shows the percentage of participants.", "The x-axis, labeled as \"Purpose,\" has 9 categories: \"Academic\", \"Medical\", \"Legal\",  \"Work\", \"Personal Reading\", \"News\", \"Visual media\", \"Personal com.\",", "\"Recreation\".", "These scale responses are shown from bottom to top: \"Daily\", \"Often\", \"Weekly\", \"Monthly\",  and \"Rarely\".", "These scalre responses are shown from bottom to top:  \"Extremely Interested\", \"Very Interested\", \"Somewhat Interested\", \"Slightly Interested\", \"Not Interested\".", "For \"Academic\", 37.5% selected \"Extremely Interested\", 22% \"Very Interested\", 21%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 6%  \"Not Interested\".", "For \"Medical\", 37.5% selected \"Extremely Interested\", 19%  \"Very Interested\", 25%  \"Somewhat Interested\", 6%  \"Slightly Interested\", 12.5%  \"Not Interested\".", "For \"Legal\", 47% selected \"Extremely Interested\", 6%  \"Very Interested\", 28%  \"Somewhat Interested\", 9%  \"Slightly Interested\", 9%  \"Not Interested\".", "For \"Work\", 31% selected \"Extremely Interested\", 19%  \"Very Interested\", 25%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 12.5%  \"Not Interested\".", "For \"Personal reading\", 22% selected \"Extremely Interested\", 25%  \"Very Interested\", 6%  \"Somewhat Interested\", 25%  \"Slightly Interested\", 22%  \"Not Interested\".", "For \"News\", 9% selected \"Extremely Interested\", 3%  \"Very Interested\", 16%  \"Somewhat Interested\", 22%  \"Slightly Interondedested\", 22%  \"Not Interested\".", "For \"Visual media\", 9% selected \"Extremely Interested\", 25%  \"Very Interested\", 19%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 34.5%  \"Not Interested\".", "For \"Personal comm.\",", "19% selected \"Extremely Interested\", 12.5%  \"Very Interested\", 15.5%  \"Somewhat Interested\", 25%  \"Slightly Interested\", 28%  \"Not Interested\".", "For \"Recreation\", 3% selected \"Extremely Interested\", 12.5%  \"Very Interested\", 25%  \"Somewhat Interested\", 22%  \"Slightly Interested\", 37.5% \"Not interested\""], "caption": "Professionals                                                                                         ASSETS \u201920, October 26\u201328, 2020, Virtual Event, Greece", "local_uri": ["b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Social Boundaries for Personal Agents in the Interpersonal Space of the Home", "pdf_hash": "590da2ce7e48a1d7cb813df0cacd0227ed4a62fb", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"The figure presents three example storyboards that were used in the study. Storyboard 1 presents a situation in which an agent senses children's misbehavior and needs to decide whether to report it to their parents. Storyboard 2 shows a situation of conflict between siblings in which the agent is involved. Storyboard 3 details a situation in which partners use the agent to resolve a disagreement between them\".", "levels": null, "corpus_id": 218483114, "sentences": ["Figure 1: \"The figure presents three example storyboards that were used in the study.", "Storyboard 1 presents a situation in which an agent senses children's misbehavior and needs to decide whether to report it to their parents.", "Storyboard 2 shows a situation of conflict between siblings in which the agent is involved.", "Storyboard 3 details a situation in which partners use the agent to resolve a disagreement between them\"."], "caption": "(a)", "local_uri": ["590da2ce7e48a1d7cb813df0cacd0227ed4a62fb_Image_001.jpg", "590da2ce7e48a1d7cb813df0cacd0227ed4a62fb_Image_002.jpg", "590da2ce7e48a1d7cb813df0cacd0227ed4a62fb_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "Goby: A Wearable Swimming Aid for Blind Athletes", "pdf_hash": "d9c4ccb9c2d2eddae55f297c9927fde0c0b71ce1", "year": 2017, "venue": "ASSETS", "alt_text": "Two images. Left: a man swimming in a pool lane. Right: an image of the bottom of a pool captured by a wearable camera. The pool marker is detected by the Goby system and is outlined with a green rectangle.", "levels": null, "corpus_id": 6423486, "sentences": ["Two images.", "Left: a man swimming in a pool lane.", "Right: an image of the bottom of a pool captured by a wearable camera.", "The pool marker is detected by the Goby system and is outlined with a green rectangle."], "caption": "Figure 1: Goby is a wearable device that provides audio feedback for swimmers. Left: A swimmer wears the Goby prototype in the pool. Right: Goby\u2019s wearable camera identifies and tracks a lane marker at the bottom of the pool.", "local_uri": ["d9c4ccb9c2d2eddae55f297c9927fde0c0b71ce1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "A system overview diagram showing three stages of an experiment design: generating an idea, creating a design, and improving the design.", "levels": [[-1]], "corpus_id": 233987838, "sentences": ["A system overview diagram showing three stages of an experiment design: generating an idea, creating a design, and improving the design."], "caption": "Figure 1: Galileo enables people to design and run experiments to test their intuitions. Experiment creators can invite others to review and participate in the experiment. Participants from around the world join experiments, follow instructions, and provide data in response to automated data collection reminders.", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "Six steps of experimental design workflow shown using screenshots. Step 1 shows an intuition turning into a hypothesis using examples. Step 2 shows different ways to measure the cause in a hypothesis. Step 3 shows how to set up data collection messages using templated. Step 4 and 5 show different experimental conditions and instructions for participants. Step 6 shows inclusion/exclusion criteria.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987838, "sentences": ["Six steps of experimental design workflow shown using screenshots.", "Step 1 shows an intuition turning into a hypothesis using examples.", "Step 2 shows different ways to measure the cause in a hypothesis.", "Step 3 shows how to set up data collection messages using templated.", "Step 4 and 5 show different experimental conditions and instructions for participants.", "Step 6 shows inclusion/exclusion criteria."], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "A screenshot of a review instance showing structural, pragmatic and experience questions", "levels": null, "corpus_id": 233987838, "sentences": ["A screenshot of a review instance showing structural, pragmatic and experience questions"], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "Four steps of an experiment join workflow where people click on experiment to join it, answer some questions to see if they meet the criteria, provide consent, and receive instructions and provide data.", "levels": [[-1]], "corpus_id": 233987838, "sentences": ["Four steps of an experiment join workflow where people click on experiment to join it, answer some questions to see if they meet the criteria, provide consent, and receive instructions and provide data."], "caption": "participant ID rather than real name or username. When an ex- periment ends, Galileo sends a summary of results to participants. Participants can anonymously discuss experiments at the end, so the experimenter and other users on the platform can learn from their feedback. The experimenter\u2019s dashboard provides a summary of their experiment\u2019s progress and supports lightweight tasks to improve the quality of data collected. The dashboard lists tasks:", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "A screenshot of an experiment designers dashboard showing participant data details and instructions on how to remind participants", "levels": [[-1]], "corpus_id": 233987838, "sentences": ["A screenshot of an experiment designers dashboard showing participant data details and instructions on how to remind participants"], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "A distribution of experiment design scores showing that 33 experiments scores more than 11 on a scale of 13, 20 scored between 8.8 and 10.9, 7 scored between6.7 and 8.8, and 3 scored between 4.6 and 6.7, and 3 between 2.5 and 4.6. Another distribution shows topics across all experiments that include 61% experiments about diet, 11% about technology use, 9% about alternate treatments, and 20% others", "levels": [[2, 1], [2, 1]], "corpus_id": 233987838, "sentences": ["A distribution of experiment design scores showing that 33 experiments scores more than 11 on a scale of 13, 20 scored between 8.8 and 10.9, 7 scored between6.7 and 8.8, and 3 scored between 4.6 and 6.7, and 3 between 2.5 and 4.6.", "Another distribution shows topics across all experiments that include 61% experiments about diet, 11% about technology use, 9% about alternate treatments, and 20% others"], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "A distribution shows how review comments were distributed across different sections: 12% Overall, 9% Criteria, 21% Conditions, 35% Measures, and 23% Hypothesis. Comment distribution highlights 75 comments between 0 and 80 chars length and few beyond character length 311 chars. A long descriptive comment on an experiment.", "levels": [[2, 1], [2], [1]], "corpus_id": 233987838, "sentences": ["A distribution shows how review comments were distributed across different sections: 12% Overall, 9% Criteria, 21% Conditions, 35% Measures, and 23% Hypothesis.", "Comment distribution highlights 75 comments between 0 and 80 chars length and few beyond character length 311 chars.", "A long descriptive comment on an experiment."], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "Experiment results from 3 experiments. A boxplot comparison of people's stool consistency when they drank kombucha vs those who did not. No kombucha's stool consistency mean is 4.2 while yes kombucha's is 3.9. Two more graphs showing regression line and bar plots.", "levels": [[1], [1], [2], [1]], "corpus_id": 233987838, "sentences": ["Experiment results from 3 experiments.", "A boxplot comparison of people's stool consistency when they drank kombucha vs those who did not.", "No kombucha's stool consistency mean is 4.2 while yes kombucha's is 3.9.", "Two more graphs showing regression line and bar plots."], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Galileo: Citizen-led Experimentation Using a Social Computing System", "pdf_hash": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "year": 2021, "venue": "CHI", "alt_text": "Three bar plots showing how three communities -- kombucha, open humans, beer -- signed up, participated, and adhered to the instructions", "levels": [[1]], "corpus_id": 233987838, "sentences": ["Three bar plots showing how three communities -- kombucha, open humans, beer -- signed up, participated, and adhered to the instructions"], "caption": "", "local_uri": ["8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Maximizing children's opportunities with inclusive play: considerations for interactive technology design", "pdf_hash": "b591ba0e8dbf8e6f02016361aafd64b500c6463f", "year": 2015, "venue": "IDC", "alt_text": "Figure 1. Map of the technology design space for play among children with varying abilities. Shaded area is underexplored.  From top to bottom in the top left quadrant (More unstructured to less unstructured & all fully specialized): References [15], [11], [14], [18]. From bottom to top in the bottom left quadrant (less structured to more structured & all fully specialized): [27], [30], [2]. The only reference in the bottom right quadrant (Fully Inclusive, Structured) is [6].", "levels": null, "corpus_id": 647825, "sentences": ["Figure 1.", "Map of the technology design space for play among children with varying abilities.", "Shaded area is underexplored.", "From top to bottom in the top left quadrant (More unstructured to less unstructured & all fully specialized): References [15], [11], [14], [18].", "From bottom to top in the bottom left quadrant (less structured to more structured & all fully specialized): [27], [30], [2].", "The only reference in the bottom right quadrant (Fully Inclusive, Structured) is [6]."], "caption": "Figure 1. Map of the technology design space for play among children with varying abilities. Shaded area is underexplored.", "local_uri": ["b591ba0e8dbf8e6f02016361aafd64b500c6463f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Maximizing children's opportunities with inclusive play: considerations for interactive technology design", "pdf_hash": "b591ba0e8dbf8e6f02016361aafd64b500c6463f", "year": 2015, "venue": "IDC", "alt_text": "Figure 2. Children building aliens and spaceships together during the second design workshop.   Four children sit on a carpet on the floor with LEGO blocks and other arts and crafts materials.", "levels": null, "corpus_id": 647825, "sentences": ["Figure 2.", "Children building aliens and spaceships together during the second design workshop.", "Four children sit on a carpet on the floor with LEGO blocks and other arts and crafts materials."], "caption": "Figure 2. Children building aliens and spaceships together during the second design workshop.", "local_uri": ["b591ba0e8dbf8e6f02016361aafd64b500c6463f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Best practices for conducting evaluations of sign language animation", "pdf_hash": "c65a49e151b43249ad65aa091b05af25a0161921", "year": 2015, "venue": "", "alt_text": "Figure 2. Example of three types of stimuli in a user study: i) animation without facial expressions as lower baseline, ii) animation with facial expressions to be evaluated, and iii) video of human signer as upper baseline.", "levels": null, "corpus_id": 110639430, "sentences": ["Figure 2.", "Example of three types of stimuli in a user study: i) animation without facial expressions as lower baseline, ii) animation with facial expressions to be evaluated, and iii) video of human signer as upper baseline."], "caption": "", "local_uri": ["c65a49e151b43249ad65aa091b05af25a0161921_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Virtually-Extended Proprioception: Providing Spatial Reference in VR through an Appended Virtual Limb", "pdf_hash": "eec67f6d85f017c4b5c349b0ce85398b85c3919b", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: The virtually-extended proprioception concept. The person on the left is a user wearing a VR headset in the real world. The person on the right with a third arm appended to his shoulder on the left side is the avatar corresponding to the user in VR. The scene in the cloud above the two persons is the egocentric scene the user sees in VR. The user can see the index fingertip of the appended left limb point on a screw hole. The appended limb guides his real right limb to put a screw into the screw hole.", "levels": null, "corpus_id": 218482616, "sentences": ["Figure 1: The virtually-extended proprioception concept.", "The person on the left is a user wearing a VR headset in the real world.", "The person on the right with a third arm appended to his shoulder on the left side is the avatar corresponding to the user in VR.", "The scene in the cloud above the two persons is the egocentric scene the user sees in VR.", "The user can see the index fingertip of the appended left limb point on a screw hole.", "The appended limb guides his real right limb to put a screw into the screw hole."], "caption": "Figure 1. Virtually-extended proprioception concept. We append virtual body parts mimicking real ones to the user\u2019s embodied avatar to provide spatial reference to objects or locations in VR. In the example scenario (a virtual assembly) depicted in this \ufb01gure, we make an appended non- dominant upper limb point on the position of a screw hole. The user lifts his dominant embodied upper limb to insert a screw to the screw hole with the spatial reference provided by the appended upper limb.", "local_uri": ["eec67f6d85f017c4b5c349b0ce85398b85c3919b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Virtually-Extended Proprioception: Providing Spatial Reference in VR through an Appended Virtual Limb", "pdf_hash": "eec67f6d85f017c4b5c349b0ce85398b85c3919b", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: This figure illustrates study 1. Sub-figure (a) shows the random target balls used in this study. Sub-figure (b) shows a scene in the real world where a participant wearing a VR headset is lifting his right arm and using the index fingertip to point at a position in mid-air. He rests his left arm on the left thigh and uses the left hand to grab a VR joystick. Sub-figure (c) and (d) show the egocentric scenes that the participant sees in VR under the conditions of no reference and appended upper limb, respectively.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218482616, "sentences": ["Figure 2: This figure illustrates study 1.", "Sub-figure (a) shows the random target balls used in this study.", "Sub-figure (b) shows a scene in the real world where a participant wearing a VR headset is lifting his right arm and using the index fingertip to point at a position in mid-air.", "He rests his left arm on the left thigh and uses the left hand to grab a VR joystick.", "Sub-figure (c) and (d) show the egocentric scenes that the participant sees in VR under the conditions of no reference and appended upper limb, respectively."], "caption": "Figure 2. Study 1: reaching out to target balls in the user\u2019s egocentric view. (a) Target balls have random positions and sizes. (b) In the experiment, the participant uses the index \ufb01nger of his dominant hand to reach out to a target ball in VR, and holds a joystick on the non-dominant hand to record the starting time of each trial; note that the participant never lifts their non-dominant limb. (c) Participant\u2019s egocentric view in VR, when reaching out to a target ball in the no reference condition. (d) Participant\u2019s egocentric view in VR, when reaching out to a target ball in the appended limb condition.", "local_uri": ["eec67f6d85f017c4b5c349b0ce85398b85c3919b_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Virtually-Extended Proprioception: Providing Spatial Reference in VR through an Appended Virtual Limb", "pdf_hash": "eec67f6d85f017c4b5c349b0ce85398b85c3919b", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: This figure shows the results of Study 1. Sub-figure (a) shows that the average time to select targets under the condition of appended upper limb is significantly shorter than that under the condition of no reference. Sub-figure (b) shows that the average rating on target position estimation under the condition of appended upper limb is significantly better than that under the condition of no reference.", "levels": [[1], [2, 1], [2, 1]], "corpus_id": 218482616, "sentences": ["Figure 3: This figure shows the results of Study 1.", "Sub-figure (a) shows that the average time to select targets under the condition of appended upper limb is significantly shorter than that under the condition of no reference.", "Sub-figure (b) shows that the average rating on target position estimation under the condition of appended upper limb is significantly better than that under the condition of no reference."], "caption": "Figure 3. Study 1 results. AUL denotes Appended Upper Limb (See Figure 2 (d)), whereas NR denotes No Reference (See Figure 2 (c)). The error bars in the plots indicate the standard deviation (SD).", "local_uri": ["eec67f6d85f017c4b5c349b0ce85398b85c3919b_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Virtually-Extended Proprioception: Providing Spatial Reference in VR through an Appended Virtual Limb", "pdf_hash": "eec67f6d85f017c4b5c349b0ce85398b85c3919b", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: Sub-figure (a) shows the fifty icon positions distributed inside or outside the open hand area. Sub-figures (b), (c), and (d) show the egocentric scenes a participant can see in VR under the conditions of no reference, familiar size object, and appended upper limb, respectively.", "levels": null, "corpus_id": 218482616, "sentences": ["Figure 4: Sub-figure (a) shows the fifty icon positions distributed inside or outside the open hand area.", "Sub-figures (b), (c), and (d) show the egocentric scenes a participant can see in VR under the conditions of no reference, familiar size object, and appended upper limb, respectively."], "caption": "Figure 4. Study 2: selecting icons on planar UI. (a) We sample 50 icon positions roughly evenly on a 2D plane extended from the hand palm of the appended upper limb: twenty-eight icons are within the range of the hand, and the rest (twenty-two) are around the hand. (b)-(d) The participant selects a target icon (b) without spatial reference, (c) with a familiar size object (book) as reference, and (c) with appended limb as reference.", "local_uri": ["eec67f6d85f017c4b5c349b0ce85398b85c3919b_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Virtually-Extended Proprioception: Providing Spatial Reference in VR through an Appended Virtual Limb", "pdf_hash": "eec67f6d85f017c4b5c349b0ce85398b85c3919b", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: This figure shows the different appearances of the appended upper limb when we manipulate its transparency and granularity. The top row shows the appearances of an appended hand with an arm under four levels of transparency. The bottom row shows the appearances of only an appended hand under four levels of transparency.", "levels": null, "corpus_id": 218482616, "sentences": ["Figure 6: This figure shows the different appearances of the appended upper limb when we manipulate its transparency and granularity.", "The top row shows the appearances of an appended hand with an arm under four levels of transparency.", "The bottom row shows the appearances of only an appended hand under four levels of transparency."], "caption": "Figure 6. Study 3: exploring the appearance of the appended upper limb. Top row: appended upper limb with both hand and arm. Bottom row: we show only the hand part. (a) & (e): rendering in opaque; (b) & (f): 67% transparency; (c) & (g): 33% transparency; and (d) & (h): outline only.", "local_uri": ["eec67f6d85f017c4b5c349b0ce85398b85c3919b_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability", "pdf_hash": "3a9f9aa747a26ca3055ff567a7a646c592a5e8d8", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \u201cThree images. Left one shows the user interface of our rapid tapping task. Right one shows a person holding a handgrip dynamometer, specialized hardware to measure motor fatigue and fatigability. Middle image shows a plot of the comparable trend of performance using both methods. The plot shows that over the first 30 seconds of a trial using our approach is\nstrongly correlated with a standard hand dynamometer for patients (\u03c1 = 0.78)\nand the control group (\u03c1 = 0.84).\u201d", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 210705039, "sentences": ["Figure 1: \u201cThree images.", "Left one shows the user interface of our rapid tapping task.", "Right one shows a person holding a handgrip dynamometer, specialized hardware to measure motor fatigue and fatigability.", "Middle image shows a plot of the comparable trend of performance using both methods.", "The plot shows that over the first 30 seconds of a trial using our approach is\nstrongly correlated with a standard hand dynamometer for patients (\u03c1 = 0.78)\nand the control group (\u03c1 = 0.84).\u201d"], "caption": "1 - Norm. touch duration", "local_uri": ["3a9f9aa747a26ca3055ff567a7a646c592a5e8d8_Image_003.jpg", "3a9f9aa747a26ca3055ff567a7a646c592a5e8d8_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Surrogate-Aloud: A Human Surrogate Method for Remote Usability Evaluation and Ideation in Virtual Reality", "pdf_hash": "da1a352f538a061daf646ee59a9fc9669816216f", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Image (a) shows a participant's mouse clicking into the VR environment, creating a red geospatial pin attached to a location on the floor.  The VR Surrogate can be seen preparing to the teleport to that location.", "levels": null, "corpus_id": 233987498, "sentences": ["Image (a) shows a participant's mouse clicking into the VR environment, creating a red geospatial pin attached to a location on the floor.", "The VR Surrogate can be seen preparing to the teleport to that location."], "caption": "", "local_uri": ["da1a352f538a061daf646ee59a9fc9669816216f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Surrogate-Aloud: A Human Surrogate Method for Remote Usability Evaluation and Ideation in Virtual Reality", "pdf_hash": "da1a352f538a061daf646ee59a9fc9669816216f", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Image (b) shows the VR surrogate standing over the red geospatial pin, showing a thumbs up gesture.", "levels": null, "corpus_id": 233987498, "sentences": ["Image (b) shows the VR surrogate standing over the red geospatial pin, showing a thumbs up gesture."], "caption": "(b)Figure 2: (a) Participants can direct the surrogate to locations of interest using deictic phrases or aid in conversational com- mands using the point and click interface, transmitting a screen position through video conferencing, and presenting a geospa- tial pointer for the VR surrogate. (b) The Surrogate can respond through gestures to convey successful completion of requested commands.complex actions are abstracted away from potential system break- ing scenarios. For usability evaluation, due to the sharing of the frst-person perspective, the participant still retains the opportunity to provide valuable feedback throughout the interaction.", "local_uri": ["da1a352f538a061daf646ee59a9fc9669816216f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Shouldercam: evaluating the user experience of a depth camera system to measure shoulder range of motion", "pdf_hash": "1e7672d17103be22a238cadd435f9385b70e86c7", "year": 2016, "venue": "PervasiveHealth", "alt_text": "Goniometer, a manual tool used in clinical settings to measure shoulder range of motion in degrees.", "levels": null, "corpus_id": 45561230, "sentences": ["Goniometer, a manual tool used in clinical settings to measure shoulder range of motion in degrees."], "caption": "ABSTRACT", "local_uri": ["1e7672d17103be22a238cadd435f9385b70e86c7_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Shouldercam: evaluating the user experience of a depth camera system to measure shoulder range of motion", "pdf_hash": "1e7672d17103be22a238cadd435f9385b70e86c7", "year": 2016, "venue": "PervasiveHealth", "alt_text": "Picture of ShoulderCam situated in the corner of a medical exam room, with enough room for a patient to stand in front.", "levels": null, "corpus_id": 45561230, "sentences": ["Picture of ShoulderCam situated in the corner of a medical exam room, with enough room for a patient to stand in front."], "caption": "Figure 3. ShoulderCam in a medical exam room. The height", "local_uri": ["1e7672d17103be22a238cadd435f9385b70e86c7_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Shouldercam: evaluating the user experience of a depth camera system to measure shoulder range of motion", "pdf_hash": "1e7672d17103be22a238cadd435f9385b70e86c7", "year": 2016, "venue": "PervasiveHealth", "alt_text": "Example Stick Figure representation and corresponding measurement:\nExternal Rotation: 86", "levels": null, "corpus_id": 45561230, "sentences": ["Example Stick Figure representation and corresponding measurement:\nExternal Rotation: 86"], "caption": "John DoeU1234567SundayFebruary 222015Sore ShoulderTime7:58:35 PMRightAbductionFlexionExternal RotationExterInternal Rotational Rotation 86 degreesCross Body15715586-734.98LeftAbductionFlexionExternal RotationInternal RotationCross Body15912754-60-0.1", "local_uri": ["1e7672d17103be22a238cadd435f9385b70e86c7_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Shouldercam: evaluating the user experience of a depth camera system to measure shoulder range of motion", "pdf_hash": "1e7672d17103be22a238cadd435f9385b70e86c7", "year": 2016, "venue": "PervasiveHealth", "alt_text": "Bar chart displaying that ShoulderCam measurements took less time than (in seconds) than goniomter measurements. P4 and P8 were omitted due to being measured by a member of the research team. The data below:\n\n Gon ShoulderCam\nP1 252 69\nP2 342 95\nP3 330 127\nP5 478 124\nP6 158 77\nP7 124 65\nP9 395 269\nP10 208 117\nP11 218 106", "levels": [[3, 2, 1], [1], [2]], "corpus_id": 45561230, "sentences": ["Bar chart displaying that ShoulderCam measurements took less time than (in seconds) than goniomter measurements.", "P4 and P8 were omitted due to being measured by a member of the research team.", "The data below:\n\n Gon ShoulderCam\nP1 252 69\nP2 342 95\nP3 330 127\nP5 478 124\nP6 158 77\nP7 124 65\nP9 395 269\nP10 208 117\nP11 218 106"], "caption": "Figure 6. Average time for goniometer (blue) and Shoulder- Cam (red) in seconds. We omitted P4 and P8 since their measurements were taken by a member of the research team.", "local_uri": ["1e7672d17103be22a238cadd435f9385b70e86c7_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "HapticPrint: Designing Feel Aesthetics for 3D Printing", "pdf_hash": "69bdabd866882dd8e64fa072bcbe5dcbb3a732d0", "year": 2015, "venue": "", "alt_text": "Recognition accuracy continues to improve with data from additional sessions. Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions.", "levels": [[-1], [-1]], "corpus_id": 18185109, "sentences": ["Recognition accuracy continues to improve with data from additional sessions.", "Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions."], "caption": "", "local_uri": ["69bdabd866882dd8e64fa072bcbe5dcbb3a732d0_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HapticPrint: Designing Feel Aesthetics for 3D Printing", "pdf_hash": "69bdabd866882dd8e64fa072bcbe5dcbb3a732d0", "year": 2015, "venue": "", "alt_text": "Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations.", "levels": null, "corpus_id": 18185109, "sentences": ["Randomly sampling from all 3 training sessions, recognition accuracy begins to plateau after training with 40 demonstrations."], "caption": "A", "local_uri": ["69bdabd866882dd8e64fa072bcbe5dcbb3a732d0_Image_016.jpg", "69bdabd866882dd8e64fa072bcbe5dcbb3a732d0_Image_017.jpg", "69bdabd866882dd8e64fa072bcbe5dcbb3a732d0_Image_018.jpg"], "annotated": false, "compound": true}
{"title": "How Relevant is Hick's Law for HCI?", "pdf_hash": "cd857ed14546749328da5ddbc173e34a31ddb499", "year": 2020, "venue": "CHI", "alt_text": "Time segmentation in the classic stimulus-response paradigm of psychology. t_0 denotes stimulus onset, and t_1 and t_2 denote the response onset and termination, respectively. These three time markers make it possible to distinguish the response latency (RL), the response duration (RD), and the task completion time (TCT = RL + RD). While most reaction-time experiments are designed so that RD occupies a negligible portion of TCT, in more realistic settings (see Table 1) more often than not it is the RL that is relatively short.", "levels": null, "corpus_id": 218482607, "sentences": ["Time segmentation in the classic stimulus-response paradigm of psychology.", "t_0 denotes stimulus onset, and t_1 and t_2 denote the response onset and termination, respectively.", "These three time markers make it possible to distinguish the response latency (RL), the response duration (RD), and the task completion time (TCT = RL + RD).", "While most reaction-time experiments are designed so that RD occupies a negligible portion of TCT, in more realistic settings (see Table 1) more often than not it is the RL that is relatively short."], "caption": "", "local_uri": ["cd857ed14546749328da5ddbc173e34a31ddb499_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "How Relevant is Hick's Law for HCI?", "pdf_hash": "cd857ed14546749328da5ddbc173e34a31ddb499", "year": 2020, "venue": "CHI", "alt_text": "Reanalysis of data from Roy et al. and Liu et al.: reaction time as a function of stimulus uncertainty.", "levels": [[1], [1], [1]], "corpus_id": 218482607, "sentences": ["Reanalysis of data from Roy et al.", "and Liu et al.:", "reaction time as a function of stimulus uncertainty."], "caption": "", "local_uri": ["cd857ed14546749328da5ddbc173e34a31ddb499_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How Relevant is Hick's Law for HCI?", "pdf_hash": "cd857ed14546749328da5ddbc173e34a31ddb499", "year": 2020, "venue": "CHI", "alt_text": "A designer (a) displays all 32 items on the same page; (b) splits 32 items randomly on 4 pages and (c) splits 32 items on 4 pages according to their colors.", "levels": null, "corpus_id": 218482607, "sentences": ["A designer (a) displays all 32 items on the same page; (b) splits 32 items randomly on 4 pages and (c) splits 32 items on 4 pages according to their colors."], "caption": "", "local_uri": ["cd857ed14546749328da5ddbc173e34a31ddb499_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "The image shows a cropped photo of a conference room table covered in small pieces of paper. White pieces with words are organized into columns with colored post-it notes as headers. Each level of the hierarchy is represented by its own color.", "levels": null, "corpus_id": 52045721, "sentences": ["The image shows a cropped photo of a conference room table covered in small pieces of paper.", "White pieces with words are organized into columns with colored post-it notes as headers.", "Each level of the hierarchy is represented by its own color."], "caption": "Figure 1. Affinity diagram with hierarchical clusters.", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "This image shows a cropped photo with a child on the left and a parent sitting opposite on the right. Puzzle pieces are spread on the floor between them.", "levels": null, "corpus_id": 52045721, "sentences": ["This image shows a cropped photo with a child on the left and a parent sitting opposite on the right.", "Puzzle pieces are spread on the floor between them."], "caption": "", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "This image shows a cropped photo with a child on the left and a parent sitting opposite on the right. Lego bricks are spread on the floor between them.", "levels": null, "corpus_id": 52045721, "sentences": ["This image shows a cropped photo with a child on the left and a parent sitting opposite on the right.", "Lego bricks are spread on the floor between them."], "caption": "", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "The image shows a cropped photo with a child sitting on the floor hunched over a tablet on his lap. Parent sits nearby and looks on.", "levels": null, "corpus_id": 52045721, "sentences": ["The image shows a cropped photo with a child sitting on the floor hunched over a tablet on his lap.", "Parent sits nearby and looks on."], "caption": "", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_004.jpg", "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "The image shows a cropped photo with a child sitting on the floor hunched over a tablet on her lap. Parent sits nearby and looks on.", "levels": null, "corpus_id": 52045721, "sentences": ["The image shows a cropped photo with a child sitting on the floor hunched over a tablet on her lap.", "Parent sits nearby and looks on."], "caption": "", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_005.jpg", "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "This image shows a cropped photo with a child on the right and a parent sitting opposite on the left. Pieces of paper with drawings lie on the floor between them.", "levels": null, "corpus_id": 52045721, "sentences": ["This image shows a cropped photo with a child on the right and a parent sitting opposite on the left.", "Pieces of paper with drawings lie on the floor between them."], "caption": "", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "This image shows a cropped photo with an aerial view of a child on one side and a parent sitting opposite. A board game board is open on the floor between them.", "levels": null, "corpus_id": 52045721, "sentences": ["This image shows a cropped photo with an aerial view of a child on one side and a parent sitting opposite.", "A board game board is open on the floor between them."], "caption": "", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Let\u2019s Play! Digital and Analog Play Patterns between Preschoolers and Parents", "pdf_hash": "be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19", "year": 2018, "venue": "", "alt_text": "The image shows a cropped photo with a parent on the left and a child on the right holding a tablet on her lap. The child has ownership of the tablet, and wraps her left hand around one side while tapping the screen with her right hand. The parent sits opposite and reaches across the top of the screen to tap it from an upside-down orientation.", "levels": null, "corpus_id": 52045721, "sentences": ["The image shows a cropped photo with a parent on the left and a child on the right holding a tablet on her lap.", "The child has ownership of the tablet, and wraps her left hand around one side while tapping the screen with her right hand.", "The parent sits opposite and reaches across the top of the screen to tap it from an upside-down orientation."], "caption": "Figure 4. Mom and child each participate in an exploratory game in which multiple items on screen can be moved at once, such that they can each engage with the screen simultaneously. However, mom struggles to participate from an upside-down orientation and eventually stops playing.", "local_uri": ["be33ff4e2e0a7cb94fe9ef6789fbc1e45f8b6c19_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "#CHIversity: Implications for Equality, Diversity, and Inclusion Campaigns", "pdf_hash": "ec7960a28f0e7757c5871735a185c8b1e597f48a", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A zine page showing annotations to a twitter screenshot. Text reads: \"No specific details from the conference, hard for users to find (signs put up late). But glad they were included!!\"", "levels": null, "corpus_id": 5077547, "sentences": ["A zine page showing annotations to a twitter screenshot.", "Text reads: \"No specific details from the conference, hard for users to find (signs put up late).", "But glad they were included!!\""], "caption": "", "local_uri": ["ec7960a28f0e7757c5871735a185c8b1e597f48a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "#CHIversity: Implications for Equality, Diversity, and Inclusion Campaigns", "pdf_hash": "ec7960a28f0e7757c5871735a185c8b1e597f48a", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A screenshot from twitter: text reads \"Cross-stitching during break at #chi2017. So necessary. Support #CHIversity\"", "levels": null, "corpus_id": 5077547, "sentences": ["A screenshot from twitter: text reads \"Cross-stitching during break at #chi2017.", "So necessary.", "Support #CHIversity\""], "caption": "", "local_uri": ["ec7960a28f0e7757c5871735a185c8b1e597f48a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "#CHIversity: Implications for Equality, Diversity, and Inclusion Campaigns", "pdf_hash": "ec7960a28f0e7757c5871735a185c8b1e597f48a", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "The rear cover of the CHIversity 2017 Zine, including call-outs such as \"MORE POLITICS\" and \"GET MADDER\".", "levels": null, "corpus_id": 5077547, "sentences": ["The rear cover of the CHIversity 2017 Zine, including call-outs such as \"MORE POLITICS\" and \"GET MADDER\"."], "caption": "", "local_uri": ["ec7960a28f0e7757c5871735a185c8b1e597f48a_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "EyeDescribe: Combining Eye Gaze and Speech to Automatically Create Accessible Touch Screen Artwork", "pdf_hash": "b8dd2d2e7c270a449fa6abdcef57dcd4903846a5", "year": 2019, "venue": "ISS", "alt_text": "A picture containing farm animals. EyeDescribe created labels for each animal as follows (from left to right): goat, horse, shep, chicken, dog, and cow.", "levels": [[-1], [-1]], "corpus_id": 207959514, "sentences": ["A picture containing farm animals.", "EyeDescribe created labels for each animal as follows (from left to right): goat, horse, shep, chicken, dog, and cow."], "caption": "Figure 1. Using eye gaze and speech data, EyeDescribe identifies regions of interest in an image, which can then be presented interactively to blind and visually impaired end users via a talking touch screen application.", "local_uri": ["b8dd2d2e7c270a449fa6abdcef57dcd4903846a5_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EyeDescribe: Combining Eye Gaze and Speech to Automatically Create Accessible Touch Screen Artwork", "pdf_hash": "b8dd2d2e7c270a449fa6abdcef57dcd4903846a5", "year": 2019, "venue": "ISS", "alt_text": "This is a still life picture containing oranges, a jug, a glass of wine, a bowl of strawberries, and some grapes. Opaque regions created by image annotators act as bounding boxes for each object.", "levels": [[-1], [-1]], "corpus_id": 207959514, "sentences": ["This is a still life picture containing oranges, a jug, a glass of wine, a bowl of strawberries, and some grapes.", "Opaque regions created by image annotators act as bounding boxes for each object."], "caption": "Figure 2. To collect ground truth data, two annotators drew bounding boxes for each object in an image. The canonical bounding box was the intersection of the two annotators\u2019 bounding boxes. Image Copyright Phillip Gerrard.", "local_uri": ["b8dd2d2e7c270a449fa6abdcef57dcd4903846a5_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "EyeDescribe: Combining Eye Gaze and Speech to Automatically Create Accessible Touch Screen Artwork", "pdf_hash": "b8dd2d2e7c270a449fa6abdcef57dcd4903846a5", "year": 2019, "venue": "ISS", "alt_text": "The surface tablet is displaying an image from EyeDescribe and the participant is interacting with the touch screen by placing their right index finger on it. Their finger is on a dog in the image.", "levels": null, "corpus_id": 207959514, "sentences": ["The surface tablet is displaying an image from EyeDescribe and the participant is interacting with the touch screen by placing their right index finger on it.", "Their finger is on a dog in the image."], "caption": "Figure 4. Participant interacts with the EyeDescribe prototype.", "local_uri": ["b8dd2d2e7c270a449fa6abdcef57dcd4903846a5_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "RDTCheck: A Smartphone App for Monitoring Rapid Diagnostic Test Administration", "pdf_hash": "e0f24ef4f22725a4243145d7808b12528f2264ec", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Left-hand side image shows rapid diagnostic kit components. The kit consists of a nasal swab, dipstick, transparent test tube, and buffer solution dropper. The right hand side image shows a 3D-printed box that shold smartphone and test tube.", "levels": null, "corpus_id": 233987400, "sentences": ["Left-hand side image shows rapid diagnostic kit components.", "The kit consists of a nasal swab, dipstick, transparent test tube, and buffer solution dropper.", "The right hand side image shows a 3D-printed box that shold smartphone and test tube."], "caption": "Figure 1: (left) The components in Quidel\u2019s QuickVue In- fuenza A+B RDT kit: a dipstick, a nasal swab, a dropper of bufer solution, and a test tube. (right) A CAD rendering of the 3D-printed box we created to house a Google Pixel smart- phone and the RDT kit components.", "local_uri": ["e0f24ef4f22725a4243145d7808b12528f2264ec_Image_003.jpg", "e0f24ef4f22725a4243145d7808b12528f2264ec_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "Understanding Audio Production Practices of People with Vision Impairments", "pdf_hash": "50de45ce060cc645afa3db54d812faf4e52d6de7", "year": 2020, "venue": "ASSETS", "alt_text": "REAPER's user interface on the left image and a participant using a control surface on the right.", "levels": null, "corpus_id": 225953903, "sentences": ["REAPER's user interface on the left image and a participant using a control surface on the right."], "caption": "Figure 1: Lef: A screenshot of REAPER\u2019s user interface showing 5 audio tracks above and the mixer console below ; Right: A participant uses his control surface to perform mixing on Pro Tools.", "local_uri": ["50de45ce060cc645afa3db54d812faf4e52d6de7_Image_021.jpg", "50de45ce060cc645afa3db54d812faf4e52d6de7_Image_022.jpg"], "annotated": false, "compound": true}
{"title": "Experiencing Real-time 3D Interaction with Depth Maps for Mobile Augmented Reality in DepthLab", "pdf_hash": "676d496293be044ad1fdc0855d89d9ae4da51c4c", "year": 2020, "venue": "UIST", "alt_text": "Figure1: \"Figure 1A shows a deployed demo of physics-based collisions; virtual balls bounced on the physical stairs and fell down to the ground. Figure 1B shows a deployed demo of avatar path planning, where a virtual avatar navigated itself from ground up to sofa, leaving virtual shadow rendered on the physical sofa. Figure 1C shows a deployed demo of snow particles where white virtual snowflakes fell down on the soil. Figure 1D shows a deployed demo of geometry-aware flooding effects where virtual water fills the entire room. Figure 1E shows a deployed demo of ray-marching-based scene relighting effects, where virtual points lights lit up the real chair. Figure 1F shows a deployed demo of occlusion effects. A hand is holding a mobile phone, which depicts a virtual cat partially hidden behind a real bed.\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 221670609, "sentences": ["Figure1: \"Figure 1A shows a deployed demo of physics-based collisions; virtual balls bounced on the physical stairs and fell down to the ground.", "Figure 1B shows a deployed demo of avatar path planning, where a virtual avatar navigated itself from ground up to sofa, leaving virtual shadow rendered on the physical sofa.", "Figure 1C shows a deployed demo of snow particles where white virtual snowflakes fell down on the soil.", "Figure 1D shows a deployed demo of geometry-aware flooding effects where virtual water fills the entire room.", "Figure 1E shows a deployed demo of ray-marching-based scene relighting effects, where virtual points lights lit up the real chair.", "Figure 1F shows a deployed demo of occlusion effects.", "A hand is holding a mobile phone, which depicts a virtual cat partially hidden behind a real bed.\""], "caption": "(a) physics-based collisions                               (b) avatar path planning                                       (c) snow particles", "local_uri": ["676d496293be044ad1fdc0855d89d9ae4da51c4c_Image_001.png", "676d496293be044ad1fdc0855d89d9ae4da51c4c_Image_002.png", "676d496293be044ad1fdc0855d89d9ae4da51c4c_Image_003.png", "676d496293be044ad1fdc0855d89d9ae4da51c4c_Image_004.png", "676d496293be044ad1fdc0855d89d9ae4da51c4c_Image_005.png", "676d496293be044ad1fdc0855d89d9ae4da51c4c_Image_006.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 1: \"Teardrop glasses: an eyeglasses-style wearable device that aims to induce emotional contagion with pseudo tears. A drop of water flows onto the face near the inner corners of the user's eyes, and the water flows down his/her cheeks.\"", "levels": null, "corpus_id": 233987050, "sentences": ["Figure 1: \"Teardrop glasses: an eyeglasses-style wearable device that aims to induce emotional contagion with pseudo tears.", "A drop of water flows onto the face near the inner corners of the user's eyes, and the water flows down his/her cheeks.\""], "caption": "hirose@cyber.t.u-tokyo.ac.jp", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 2: \"External view of Teardrop glasses. (a) An infrared sensor is positioned at the top of the left arm for communication between the control circuit and the communication module attatched onto the display. The control circuit and a AAA battery are inside the stems of the glasses. (b) Two tubes are attached around the areas of the inner corners of eyes. (c) A tube before splitting into two, a filter, and a syringe are connected to an electromagnetic valve.\"", "levels": null, "corpus_id": 233987050, "sentences": ["Figure 2: \"External view of Teardrop glasses.", "(a) An infrared sensor is positioned at the top of the left arm for communication between the control circuit and the communication module attatched onto the display.", "The control circuit and a AAA battery are inside the stems of the glasses.", "(b) Two tubes are attached around the areas of the inner corners of eyes. (c) A tube before splitting into two, a filter, and a syringe are connected to an electromagnetic valve.\""], "caption": "", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 3: \"Mechanism of water flow. Two springs are always pushing the syringe. When the valve is closed, water stops flowing (left). When the valve is open, water flows (right).\"", "levels": null, "corpus_id": 233987050, "sentences": ["Figure 3: \"Mechanism of water flow.", "Two springs are always pushing the syringe.", "When the valve is closed, water stops flowing (left).", "When the valve is open, water flows (right).\""], "caption": "", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 4: \"A drop of water flow flows onto the region near the inner corners of the wearer's eyes.\"", "levels": null, "corpus_id": 233987050, "sentences": ["Figure 4: \"A drop of water flow flows onto the region near the inner corners of the wearer's eyes.\""], "caption": "", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_005.png"], "annotated": false, "compound": false}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 5: \"The control module, which is attached to the monitor, reads the brightness behind the module and control the flow of the water. (left) Water flow stops when the display brightness turns black. (right) Water flows when the display brightness turns black.\"", "levels": null, "corpus_id": 233987050, "sentences": ["Figure 5: \"The control module, which is attached to the monitor, reads the brightness behind the module and control the flow of the water. (left) Water flow stops when the display brightness turns black. (right) Water flows when the display brightness turns black.\""], "caption": "", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 6: \"Boxplots showing the sadness (left) and happiness (right) changes of wearers and non-wearers. The changes are calculated by subtracting the ratings in the control condition from the ratings in other conditions (emotion ratings in the glasses-without/with-tears condition - emotion ratings in the control condition).\"", "levels": [[1], [1]], "corpus_id": 233987050, "sentences": ["Figure 6: \"Boxplots showing the sadness (left) and happiness (right) changes of wearers and non-wearers.", "The changes are calculated by subtracting the ratings in the control condition from the ratings in other conditions (emotion ratings in the glasses-without/with-tears condition - emotion ratings in the control condition).\""], "caption": "", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_007.jpg", "32ae01c420064cbc5dffe553fc458c8604567f8b_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Teardrop Glasses: Pseudo Tears Induce Sadness in You and Those Around You", "pdf_hash": "32ae01c420064cbc5dffe553fc458c8604567f8b", "year": 2021, "venue": "CHI", "alt_text": "Figure 7: \"Average positive-negative scores estimated from the participans' texts written after each condition. Error bars denote standard errors.\"", "levels": [[1], [1]], "corpus_id": 233987050, "sentences": ["Figure 7: \"Average positive-negative scores estimated from the participans' texts written after each condition.", "Error bars denote standard errors.\""], "caption": "", "local_uri": ["32ae01c420064cbc5dffe553fc458c8604567f8b_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Virtual Reality Couch Configurator Leveraging Passive Haptic Feedback", "pdf_hash": "51c78566f61731ec21fe6fa0701aff60ccbc073f", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: \"A photograph of the system setup inside the furniture store. It shows a red couch and a user sitting on it while wearing a head-mounted display. On the right, a table can be seen at which the sales expert is sitting and controlling the experience with a standard desktop PC. Additionally, the virtual view of the immersed user is displayed through a projection on the wall in the background and on an additional monitor mounted on the table on the right.\"", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218483012, "sentences": ["Figure 1: \"A photograph of the system setup inside the furniture store.", "It shows a red couch and a user sitting on it while wearing a head-mounted display.", "On the right, a table can be seen at which the sales expert is sitting and controlling the experience with a standard desktop PC.", "Additionally, the virtual view of the immersed user is displayed through a projection on the wall in the background and on an additional monitor mounted on the table on the right.\""], "caption": "", "local_uri": ["51c78566f61731ec21fe6fa0701aff60ccbc073f_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Virtual Reality Couch Configurator Leveraging Passive Haptic Feedback", "pdf_hash": "51c78566f61731ec21fe6fa0701aff60ccbc073f", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3: \"Photograph of a physical couch with 3 seats inside the furniture store.\"", "levels": [[-1]], "corpus_id": 218483012, "sentences": ["Figure 3: \"Photograph of a physical couch with 3 seats inside the furniture store.\""], "caption": "", "local_uri": ["51c78566f61731ec21fe6fa0701aff60ccbc073f_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "A Virtual Reality Couch Configurator Leveraging Passive Haptic Feedback", "pdf_hash": "51c78566f61731ec21fe6fa0701aff60ccbc073f", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 6: \"Screenshot of a web interface. In the center, a 2D couch layout is shown and the side panels allow to choose from different couch modules, materials, and configurations. A dropdown menu allows switching between the different virtual environments.\"", "levels": null, "corpus_id": 218483012, "sentences": ["Figure 6: \"Screenshot of a web interface.", "In the center, a 2D couch layout is shown and the side panels allow to choose from different couch modules, materials, and configurations.", "A dropdown menu allows switching between the different virtual environments.\""], "caption": "", "local_uri": ["51c78566f61731ec21fe6fa0701aff60ccbc073f_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Independent Word Discovery for People with Aphasia", "pdf_hash": "833341108f82291da25f86cecc0ba6ebdfdd1a26", "year": 2017, "venue": "ASSETS", "alt_text": "PhotoSearch's first screen allows the user to select an image from already captured images or to capture a new image to be captioned.", "levels": null, "corpus_id": 12196172, "sentences": ["PhotoSearch's first screen allows the user to select an image from already captured images or to capture a new image to be captioned."], "caption": "", "local_uri": ["833341108f82291da25f86cecc0ba6ebdfdd1a26_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Independent Word Discovery for People with Aphasia", "pdf_hash": "833341108f82291da25f86cecc0ba6ebdfdd1a26", "year": 2017, "venue": "ASSETS", "alt_text": "When an image is captioned. The words in the caption are broken up and presented to the user as a list of word-picture pairs.", "levels": null, "corpus_id": 12196172, "sentences": ["When an image is captioned.", "The words in the caption are broken up and presented to the user as a list of word-picture pairs."], "caption": "", "local_uri": ["833341108f82291da25f86cecc0ba6ebdfdd1a26_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Independent Word Discovery for People with Aphasia", "pdf_hash": "833341108f82291da25f86cecc0ba6ebdfdd1a26", "year": 2017, "venue": "ASSETS", "alt_text": "The user can click on any word to reveal the word explanation screen that contains more images describing the word.", "levels": null, "corpus_id": 12196172, "sentences": ["The user can click on any word to reveal the word explanation screen that contains more images describing the word."], "caption": "Figure 1: Left: The user can either select an existing image from the gallery or take a new photo. Center: The captured photo is captioned. In this case, it is captioned as \u201cDonut topped with peanuts\u201d. Each word is listed below and associated with a descriptive image. Right: When the word \u201cPeanut\u201d is clicked more images that describe the word \u201cPeanut\u201d are retrieved. The images are ordered according to a prediction confidence.", "local_uri": ["833341108f82291da25f86cecc0ba6ebdfdd1a26_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Insights from people with ID on a transport application", "pdf_hash": "1cc8c16c4e3e353cd24045e3d2b74e07461ab025", "year": 2018, "venue": "OZCHI", "alt_text": "Figure 2: Initial digital prototype. (a) Home Screen (b) Prompt to start journey. (c) Choice of modality for directions. (d) In-bus instructions.", "levels": null, "corpus_id": 59337295, "sentences": ["Figure 2: Initial digital prototype. (a) Home Screen (b) Prompt to start journey. (c) Choice of modality for directions.", "(d) In-bus instructions."], "caption": "(b) (c)               (d)Figure 2: Initial digital prototype. (a) Home Screen (b) Prompt to start journey. (c) Choice of modality for directions. (d) In-bus instructions.mobile devices and to communicate in an interview setting. Their medical diagnoses were not available to us, nor did we or the partner organization believe they were relevant. Fifty percent of people with intellectual disabilities do not have a specific diagnosis and even within any given diagnostic category abilities vary widely. As a result, research to support people with intellectual disabilities is best served by taking an ability based approach, identifying abilities, teaching skills, and building technologies to suit abilities of people with intellectual disability [1][6].Our participants were young  adults who were regular or occasional users of local public transport, but would not be confident to attempt new routes or would be distressed if there was a change on their regular route, if they use one. They were familiar with mobile technology, physically and cognitively able to operate smart phones, able to read words, and were familiar with mobile applications, though only one of them owned a smartphone. The participants understood how to use and access our proposed application, and demonstrated a level of self-insight and ability to communicate verbally. With each participant, a carer (a teacher or a support worker) was also invited to actively participate.", "local_uri": ["1cc8c16c4e3e353cd24045e3d2b74e07461ab025_Image_003.jpg", "1cc8c16c4e3e353cd24045e3d2b74e07461ab025_Image_004.png"], "annotated": false, "compound": true}
{"title": "Personal Space in Play: Physical and Digital Boundaries in Large-Display Cooperative and Competitive Games", "pdf_hash": "a402b43d934bad20577e0c56fc5e5373bb97db15", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: ``The image is a composite of finger positions used to evoke either direct or long-range interaction for each tool. The direct interaction techniques use one hand, while the extended interactions techniques use two hands.''", "levels": null, "corpus_id": 218482675, "sentences": ["Figure 2: ``The image is a composite of finger positions used to evoke either direct or long-range interaction for each tool.", "The direct interaction techniques use one hand, while the extended interactions techniques use two hands.''"], "caption": "Figure 2. Interaction Techniques: direct: a single handed direct touch technique b) long-range: two handed extended touch technique using one hand to anchor to a spot on the screen (three or more \ufb01ngers), the sin- gle touch outside the exclusion zone (grey circle formed by the anchored hand) spawns a telepointer which can access distant display locations.", "local_uri": ["a402b43d934bad20577e0c56fc5e5373bb97db15_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Personal Space in Play: Physical and Digital Boundaries in Large-Display Cooperative and Competitive Games", "pdf_hash": "a402b43d934bad20577e0c56fc5e5373bb97db15", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: ``The image shows the outline of the screens that are combined into one large display. Additionally, it shows the placement of a grid on the floor that researchers used to understand the participant's physical position. Three cameras are denoted on the image showing the angles that participants were recorded from. Moreover, measurements of the room written in the paper are reiterated as an overlay on the diagram.''", "levels": null, "corpus_id": 218482675, "sentences": ["Figure 3: ``The image shows the outline of the screens that are combined into one large display.", "Additionally, it shows the placement of a grid on the floor that researchers used to understand the participant's physical position.", "Three cameras are denoted on the image showing the angles that participants were recorded from.", "Moreover, measurements of the room written in the paper are reiterated as an overlay on the diagram.''"], "caption": "Figure 3. The setup of the experiment equipment. The x-axis represents the width and the y-axis the height of the display (cm). Major grid lines denote the boarders between two consecutive displays combined to make the overall large display.", "local_uri": ["a402b43d934bad20577e0c56fc5e5373bb97db15_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Personal Space in Play: Physical and Digital Boundaries in Large-Display Cooperative and Competitive Games", "pdf_hash": "a402b43d934bad20577e0c56fc5e5373bb97db15", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: ``Heat maps presented and referred to in the paper are assembled as part of an overall comparison. Along the x-axis, its notes the different conditions (all, competitive, and cooperative) and the y-axis denotes fixed and floating (as alternating rows), and the nature of the recorded data (digital space enemies targeted, physical space finger touch points, and physical floor space).''", "levels": [[1], [1]], "corpus_id": 218482675, "sentences": ["Figure 4: ``Heat maps presented and referred to in the paper are assembled as part of an overall comparison.", "Along the x-axis, its notes the different conditions (all, competitive, and cooperative) and the y-axis denotes fixed and floating (as alternating rows), and the nature of the recorded data (digital space enemies targeted, physical space finger touch points, and physical floor space).''"], "caption": "Figure 4. The visualizations of the participants position on the \ufb02oor (bot- tom), \ufb01nger touch point position (middle), and \ufb01nally their digital posi- tion in the form of where they killed their enemies (top).", "local_uri": ["a402b43d934bad20577e0c56fc5e5373bb97db15_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery", "pdf_hash": "3db37ace99a8714b85ea5773fefd1e29ddc51da3", "year": 2021, "venue": "CHI", "alt_text": "The CoNotate Environment: including (a) Default Chrome Search Interface on the left, augmented with (b) Suggestions Bar with six query suggestions at the top- of the Search Interface and (c) the Note Taking Interface to the right of the search interface.  The system supports additional interactions including (d) scrolling query suggestions using arrows, (e) resizing note-taking interface using another arrow, and (f) the ability to highlight, drag and drop web page content into notes.", "levels": [[-1], [-1]], "corpus_id": 233987053, "sentences": ["The CoNotate Environment: including (a) Default Chrome Search Interface on the left, augmented with (b) Suggestions Bar with six query suggestions at the top- of the Search Interface and (c) the Note Taking Interface to the right of the search interface.", "The system supports additional interactions including (d) scrolling query suggestions using arrows, (e) resizing note-taking interface using another arrow, and (f) the ability to highlight, drag and drop web page content into notes."], "caption": "", "local_uri": ["3db37ace99a8714b85ea5773fefd1e29ddc51da3_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery", "pdf_hash": "3db37ace99a8714b85ea5773fefd1e29ddc51da3", "year": 2021, "venue": "CHI", "alt_text": "A flowchart showing the Architecture of the CoNotate system. There's a screenshot of the CoNotate system which has an arrow pointing to google to indicate that a query is issued by the user. An arrow points back from Google to the CoNotate system screenshot of the Search Interface with the label SERPs & Autocomplete suggestions indicating that's what Google returns. Then there is an arrow from the CoNotate search interface to the Flask Server sending Notes, SERPs, Query and Auto-complete Suggestions. Within the Flask server this goes through noun-phrase extraction, Word2Vec and a KMeansClusterer (all labelled) and then the NotesOverview and NotesGap suggestions are sent back to the CoNotate interface. Finally, there's an arrow pointing out of the interface that sends Study Data to the Firebase database.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987053, "sentences": ["A flowchart showing the Architecture of the CoNotate system.", "There's a screenshot of the CoNotate system which has an arrow pointing to google to indicate that a query is issued by the user.", "An arrow points back from Google to the CoNotate system screenshot of the Search Interface with the label SERPs & Autocomplete suggestions indicating that's what Google returns.", "Then there is an arrow from the CoNotate search interface to the Flask Server sending Notes, SERPs, Query and Auto-complete Suggestions.", "Within the Flask server this goes through noun-phrase extraction, Word2Vec and a KMeansClusterer (all labelled) and then the NotesOverview and NotesGap suggestions are sent back to the CoNotate interface.", "Finally, there's an arrow pointing out of the interface that sends Study Data to the Firebase database."], "caption": "", "local_uri": ["3db37ace99a8714b85ea5773fefd1e29ddc51da3_Image_003.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery", "pdf_hash": "3db37ace99a8714b85ea5773fefd1e29ddc51da3", "year": 2021, "venue": "CHI", "alt_text": "Two sets of phrases from SERPs and notes are extracted, then compared to get gap_phrases. Then further word embedding, clustering and labeling, six noun phrases, highlighted in green and yellow respectively, are added to the original query as NotesOverview suggestions and NotesGap suggestions and presented to the user in random order. Screenshot of CoNotate extension's Suggestion bar is included.", "levels": [[-1], [-1], [-1]], "corpus_id": 233987053, "sentences": ["Two sets of phrases from SERPs and notes are extracted, then compared to get gap_phrases.", "Then further word embedding, clustering and labeling, six noun phrases, highlighted in green and yellow respectively, are added to the original query as NotesOverview suggestions and NotesGap suggestions and presented to the user in random order.", "Screenshot of CoNotate extension's Suggestion bar is included."], "caption": "", "local_uri": ["3db37ace99a8714b85ea5773fefd1e29ddc51da3_Image_004.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery", "pdf_hash": "3db37ace99a8714b85ea5773fefd1e29ddc51da3", "year": 2021, "venue": "CHI", "alt_text": "The Baseline Environment showing the Default Chrome Search Interface, augmented with Suggestions Bar at the top of the search interface with query autocompletion suggestions (a). The standard query assistance features remain on the Search Engine Results Page: People Also ask (b), People Also Ask (c), Related Searches (d). Picture cropped to remove search results", "levels": [[-1], [-1], [0]], "corpus_id": 233987053, "sentences": ["The Baseline Environment showing the Default Chrome Search Interface, augmented with Suggestions Bar at the top of the search interface with query autocompletion suggestions (a).", "The standard query assistance features remain on the Search Engine Results Page: People Also ask (b), People Also Ask (c), Related Searches (d).", "Picture cropped to remove search results"], "caption": "", "local_uri": ["3db37ace99a8714b85ea5773fefd1e29ddc51da3_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Bike, Bus, and Beyond: Extending Cyclopath to Enable Multi-Modal Routing", "pdf_hash": "d3e11a1db4df9fe44543c78afdf058a6323a253e", "year": 2013, "venue": "", "alt_text": "Figure 2: Step-by-step instructions  Screenshot of a cue sheet showing columns for time, directions, duration and distance.", "levels": null, "corpus_id": 112818243, "sentences": ["Figure 2: Step-by-step instructions  Screenshot of a cue sheet showing columns for time, directions, duration and distance."], "caption": "", "local_uri": ["d3e11a1db4df9fe44543c78afdf058a6323a253e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Temporal Segmentation of Creative Live Streams", "pdf_hash": "8faf8db60e05a356f94fff2cf55954ccfd896f05", "year": 2020, "venue": "CHI", "alt_text": "A video viewing interface. On the left is a video player showing a creative live stream with segments rendered over the video timeline. One segment is selected and reads '21:22-34:57 Applying Filters'. On the right is a table of contents where each section includes a timestamp, thumbnail image, and description. The fourth section is expanded, showing tools, transcript sentences, and chat messages.", "levels": null, "corpus_id": 211531389, "sentences": ["A video viewing interface.", "On the left is a video player showing a creative live stream with segments rendered over the video timeline.", "One segment is selected and reads '21:22-34:57 Applying Filters'.", "On the right is a table of contents where each section includes a timestamp, thumbnail image, and description.", "The fourth section is expanded, showing tools, transcript sentences, and chat messages."], "caption": "Figure 1. We present a streamer-in-the-loop approach for creating a table of contents for creative live stream videos. We pair automatic segmentation via command logs and audio transcripts with streamer labeling. We built a prototype interface to evaluate the approach with streamers, shown above.", "local_uri": ["8faf8db60e05a356f94fff2cf55954ccfd896f05_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Temporal Segmentation of Creative Live Streams", "pdf_hash": "8faf8db60e05a356f94fff2cf55954ccfd896f05", "year": 2020, "venue": "CHI", "alt_text": "3 timelines stacked on top of each other. The first shows segments based on transcript sentences, the second shows navigational and editing command events, and the third shows all candidate boundary points created by combining the above two timelines together.", "levels": [[1], [1]], "corpus_id": 211531389, "sentences": ["3 timelines stacked on top of each other.", "The first shows segments based on transcript sentences, the second shows navigational and editing command events, and the third shows all candidate boundary points created by combining the above two timelines together."], "caption": "Figure 4. Pre-processing step of our segmentation algorithm. First, we identify candidate section boundaries by taking the union of the sentence boundaries and command timestamps, and removing mid- sentence boundaries. Then, we identify intro and outro sections by \ufb01nd- ing the \ufb01rst and last editing commands.", "local_uri": ["8faf8db60e05a356f94fff2cf55954ccfd896f05_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Temporal Segmentation of Creative Live Streams", "pdf_hash": "8faf8db60e05a356f94fff2cf55954ccfd896f05", "year": 2020, "venue": "CHI", "alt_text": "Two sets of two timelines stacked on top of each other. The title above the first pair is \"Streamer 1, Video 1\", and the title above the second pair is \"Streamer 2, Video 1\". The top timeline in each pair is labeled \"algorithm\" and the bottom is labeled \"streamer\". There are lines on all the timelines showing where section boundaries occurred. Some lines are in the same place for both timelines in a pair, others are different.", "levels": null, "corpus_id": 211531389, "sentences": ["Two sets of two timelines stacked on top of each other.", "The title above the first pair is \"Streamer 1, Video 1\", and the title above the second pair is \"Streamer 2, Video 1\".", "The top timeline in each pair is labeled \"algorithm\" and the bottom is labeled \"streamer\".", "There are lines on all the timelines showing where section boundaries occurred.", "Some lines are in the same place for both timelines in a pair, others are different."], "caption": "Figure 5. A comparison of our algorithm\u2019s segmentations to the stream- ers\u2019 \ufb01nal segmentations for the \ufb01rst video each streamer was given.", "local_uri": ["8faf8db60e05a356f94fff2cf55954ccfd896f05_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Exploring the Quality, Efficiency, and Representative Nature of Responses Across Multiple Survey Panels", "pdf_hash": "c34bb7e20d633c0f49746c367090a6f0b8172e0e", "year": 2020, "venue": "CHI", "alt_text": "Figure 1. Distribution of deviations for each of the panels for the 80 responses that contained a ground truth. Panels from User 1 to the right all had statistically significant higher deviations from ground truth than the Omnibus panel.", "levels": [[0], [1], [2]], "corpus_id": 218482718, "sentences": ["Figure 1.", "Distribution of deviations for each of the panels for the 80 responses that contained a ground truth.", "Panels from User 1 to the right all had statistically significant higher deviations from ground truth than the Omnibus panel."], "caption": "", "local_uri": ["c34bb7e20d633c0f49746c367090a6f0b8172e0e_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Sharing Perspectives on the Design of Shape-Changing Interfaces", "pdf_hash": "6d036277b30731e935d513e07560bb80d5e2fd63", "year": 2016, "venue": "CHI Extended Abstracts", "alt_text": "https://lh4.googleusercontent.com/u01lqQOBjo66B7Yl4vSO6Ekwiz2dqKBGq_kpiGrlNaZtW4K4dCaYDKpWRBWZf2YW_kCuJkD__UYtcZCndvSt-_wEDb3TkA8cA9FNOQnfyASZ3k9N9iNnWD15pfAgELId8SJdph7r", "levels": null, "corpus_id": 36790608, "sentences": ["https://lh4.googleusercontent.com/u01lqQOBjo66B7Yl4vSO6Ekwiz2dqKBGq_kpiGrlNaZtW4K4dCaYDKpWRBWZf2YW_kCuJkD__UYtcZCndvSt-_wEDb3TkA8cA9FNOQnfyASZ3k9N9iNnWD15pfAgELId8SJdph7r"], "caption": "", "local_uri": ["6d036277b30731e935d513e07560bb80d5e2fd63_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Theory is in the Eye of the Beholder: Exploring Difficulties with Validating Intervention Mechanisms: Theory is in the Eye of the Beholder", "pdf_hash": "23e02c0a571aa25d8382b50782ee0d687c64c861", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This figure contains four images. \"A\" is a screenshot of a survey question of a \"Build your Skills\" prototype. First the prototype is described as, \"This intervention suggests different ways for patients to build skills and confidence in speaking up about their concerns. Instructions: Select the construct(s) from the Integrated Behavioral Model through which you think the intervention operates. Check all construct(s) that apply.\" The prototype is headed, \"Considering bringing up some concerns about your care?\" and contains three modules: \"Reflect on your skills,\" \"Imagine how the conversation might go,\" and \"See some examples of how people can speak up about their concerns.\" Then there is a list of all constructs in the IBM for respondents to choose from. \"B\" shows a prototype headed, \"What do clinicians think patients should do when it comes to speaking up?\" and shows a pie chart showing 95% of clinicians think that patients shouldspeak up when they have concerns about their care. \"C\" shows a prototype headed, \"How often do patients speak up?\" and showing a bar graph of how many patients at the hospital have spoken up to clinicians about their concerns in the last few days. \"D\" is headed \"Your participation makes you safer\" and shows a bar graph of how many times patients have helped the hospital prevent medical errors by speaking up about their needs and concerns to clinicians.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987601, "sentences": ["This figure contains four images. \"", "A\" is a screenshot of a survey question of a \"Build your Skills\" prototype.", "First the prototype is described as, \"This intervention suggests different ways for patients to build skills and confidence in speaking up about their concerns.", "Instructions: Select the construct(s) from the Integrated Behavioral Model through which you think the intervention operates.", "Check all construct(s) that apply.\"", "The prototype is headed, \"Considering bringing up some concerns about your care?\"", "and contains three modules: \"Reflect on your skills,\" \"Imagine how the conversation might go,\" and \"See some examples of how people can speak up about their concerns.\"", "Then there is a list of all constructs in the IBM for respondents to choose from. \"", "B\" shows a prototype headed, \"What do clinicians think patients should do when it comes to speaking up?\"", "and shows a pie chart showing 95% of clinicians think that patients shouldspeak up when they have concerns about their care. \"", "C\" shows a prototype headed, \"How often do patients speak up?\"", "and showing a bar graph of how many patients at the hospital have spoken up to clinicians about their concerns in the last few days. \"", "D\" is headed \"Your participation makes you safer\" and shows a bar graph of how many times patients have helped the hospital prevent medical errors by speaking up about their needs and concerns to clinicians."], "caption": "", "local_uri": ["23e02c0a571aa25d8382b50782ee0d687c64c861_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Evaluation of real-time captioning by machine recognition with human support", "pdf_hash": "2222cbb735f1ad3e420972c853ccaff53ae12498", "year": 2015, "venue": "W4A", "alt_text": "Architecture of captionng system, which shows the relationship between Captioning Server. Speech tokenizer, Automatic Speech Recognition Engine and other components.", "levels": null, "corpus_id": 12053877, "sentences": ["Architecture of captionng system, which shows the relationship between Captioning Server.", "Speech tokenizer, Automatic Speech Recognition Engine and other components."], "caption": "", "local_uri": ["2222cbb735f1ad3e420972c853ccaff53ae12498_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Evaluation of real-time captioning by machine recognition with human support", "pdf_hash": "2222cbb735f1ad3e420972c853ccaff53ae12498", "year": 2015, "venue": "W4A", "alt_text": "Screen capture of caption editor. It has two modes, (a) User mode and (b) Captioner mode", "levels": null, "corpus_id": 12053877, "sentences": ["Screen capture of caption editor.", "It has two modes, (a) User mode and (b) Captioner mode"], "caption": "Figure 1. System Component Diagram", "local_uri": ["2222cbb735f1ad3e420972c853ccaff53ae12498_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Spatial Labeling: Leveraging Spatial Layout for Improving Label Quality in Non-Expert Image Annotation", "pdf_hash": "b7414f4acffe01df49928dc1da43cee75f3d419a", "year": 2021, "venue": "CHI", "alt_text": "Figure 4: User interaction for the spatial layout labeling in- terface. (a) Spatially laying out images and labels (b) Assigning labels to images (c) Indicating confidence states (d) Modifying the assigned labels", "levels": null, "corpus_id": 233987775, "sentences": ["Figure 4: User interaction for the spatial layout labeling in- terface. (a) Spatially laying out images and labels (b) Assigning labels to images (c) Indicating confidence states (d) Modifying the assigned labels"], "caption": "", "local_uri": ["b7414f4acffe01df49928dc1da43cee75f3d419a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Spatial Labeling: Leveraging Spatial Layout for Improving Label Quality in Non-Expert Image Annotation", "pdf_hash": "b7414f4acffe01df49928dc1da43cee75f3d419a", "year": 2021, "venue": "CHI", "alt_text": "Figure 6: Photograph of user study. A participant is doing our user study via a desktop computer.", "levels": [[-1], [-1]], "corpus_id": 233987775, "sentences": ["Figure 6: Photograph of user study.", "A participant is doing our user study via a desktop computer."], "caption": "", "local_uri": ["b7414f4acffe01df49928dc1da43cee75f3d419a_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Spatial Labeling: Leveraging Spatial Layout for Improving Label Quality in Non-Expert Image Annotation", "pdf_hash": "b7414f4acffe01df49928dc1da43cee75f3d419a", "year": 2021, "venue": "CHI", "alt_text": "Figure 7: Twenty dog breeds (labels) used in Dataset A and Dataset B, with the three difficulty levels.", "levels": [[-1]], "corpus_id": 233987775, "sentences": ["Figure 7: Twenty dog breeds (labels) used in Dataset A and Dataset B, with the three difficulty levels."], "caption": "", "local_uri": ["b7414f4acffe01df49928dc1da43cee75f3d419a_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "Figure 1. Four sample gesture using ProxiMic. The four devices are headphone, smart ring, smart watch, and smart phone. In each subfigure, the user holds the device in his right hand and puts the microphone on the device about 2 cm in front of his mouth.", "levels": null, "corpus_id": 233987870, "sentences": ["Figure 1.", "Four sample gesture using ProxiMic.", "The four devices are headphone, smart ring, smart watch, and smart phone.", "In each subfigure, the user holds the device in his right hand and puts the microphone on the device about 2 cm in front of his mouth."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "Figure 2. The amplitude waveform of close-to-mic speech. Contains the amplitude waveform of six audio segments. For each audio segment, the distance between the microphone and the mouth is 2 cm, 5 cm, 10 cm, 20 cm, 30 cm, and 50 cm respectively. The corresponding amplitudes are approximately 0.9, 0.7, 0.7, 0.35, 0.2, 0.15.", "levels": null, "corpus_id": 233987870, "sentences": ["Figure 2.", "The amplitude waveform of close-to-mic speech.", "Contains the amplitude waveform of six audio segments.", "For each audio segment, the distance between the microphone and the mouth is 2 cm, 5 cm, 10 cm, 20 cm, 30 cm, and 50 cm respectively.", "The corresponding amplitudes are approximately 0.9, 0.7, 0.7, 0.35, 0.2, 0.15."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "Figure 3. Histogram of 14 different types of audio amplitude. They are: 2 cm speech (0.61), 5 cm speech (0.42), 10 cm speech (0.35), crossroads (0.29), 20cm speech (0.26), airplane (0.22), 30 cm speech (0.18), 50 cm speech (0.13), plaza (0.12), subway (0.12), car (0.09), hospital hall (0.08), restaurant (0.04), office (0.03).", "levels": null, "corpus_id": 233987870, "sentences": ["Figure 3.", "Histogram of 14 different types of audio amplitude.", "They are: 2 cm speech (0.61), 5 cm speech (0.42), 10 cm speech (0.35), crossroads (0.29), 20cm speech (0.26), airplane (0.22), 30 cm speech (0.18), 50 cm speech (0.13), plaza (0.12), subway (0.12), car (0.09), hospital hall (0.08), restaurant (0.04), office (0.03)."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "Figure 4. Fully described in the text. Spectrogram of three speech audio segments. At the beginning of each word, there is a 50ms wide highlight area from 0 Hz to 2000 Hz, which indicates the pop noise.", "levels": null, "corpus_id": 233987870, "sentences": ["Figure 4. Fully described in the text.", "Spectrogram of three speech audio segments.", "At the beginning of each word, there is a 50ms wide highlight area from 0 Hz to 2000 Hz, which indicates the pop noise."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "In this figure, a 300-second audio waveform and a threshold curve are recorded. Most of the time, the waveform amplitude is below the threshold.", "levels": null, "corpus_id": 233987870, "sentences": ["In this figure, a 300-second audio waveform and a threshold curve are recorded.", "Most of the time, the waveform amplitude is below the threshold."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "Figure 6. Schematic diagram of the CNN model. Fully described in the Section 4.2.", "levels": null, "corpus_id": 233987870, "sentences": ["Figure 6.", "Schematic diagram of the CNN model.", "Fully described in the Section 4.2."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone", "pdf_hash": "ee386a6a05df1e41c83416974d0e0134e9e8ae4f", "year": 2021, "venue": "CHI", "alt_text": "Figure 7. Four signal diagrams about explainable analysis of the CNN. The first one is the waveform of the test audio contains \"How is the weather today\". The second one is the spectrogram of the test audio. The third one is the saliency map of the spectrogram. We can see that the high energy area is concentrated in the pop noise part. The fourth one is the occlusion sensitivity map of the spectrogram. Except for covering the pop noise area, the activation probability of almost all positions is greater than 90%.", "levels": null, "corpus_id": 233987870, "sentences": ["Figure 7.", "Four signal diagrams about explainable analysis of the CNN.", "The first one is the waveform of the test audio contains \"How is the weather today\".", "The second one is the spectrogram of the test audio.", "The third one is the saliency map of the spectrogram.", "We can see that the high energy area is concentrated in the pop noise part.", "The fourth one is the occlusion sensitivity map of the spectrogram.", "Except for covering the pop noise area, the activation probability of almost all positions is greater than 90%."], "caption": "", "local_uri": ["ee386a6a05df1e41c83416974d0e0134e9e8ae4f_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Rethinking Consumer Email: The Research Process for Yahoo Mail 6", "pdf_hash": "0178069e51f3c81f3ebe0e48fa77f373482a0bbb", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: Timeline. In 2016 research begins, followed by the CardMail prototype in May and clipping coupons launch in November. In 2017 we had ongoing design explorations and launched the Deals view in November. In 2018 we launched the Purchases View in March and planned for the major redesign in December. Yahoo Mail 6 launched in September 2019. Throughout this process we ran weekly research every week and ran 7 user nights with 75 people each and ran 11 surveys.", "levels": null, "corpus_id": 218483388, "sentences": ["Figure 1: Timeline.", "In 2016 research begins, followed by the CardMail prototype in May and clipping coupons launch in November.", "In 2017 we had ongoing design explorations and launched the Deals view in November.", "In 2018 we launched the Purchases View in March and planned for the major redesign in December.", "Yahoo Mail 6 launched in September 2019.", "Throughout this process we ran weekly research every week and ran 7 user nights with 75 people each and ran 11 surveys."], "caption": "", "local_uri": ["0178069e51f3c81f3ebe0e48fa77f373482a0bbb_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Rethinking Consumer Email: The Research Process for Yahoo Mail 6", "pdf_hash": "0178069e51f3c81f3ebe0e48fa77f373482a0bbb", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Table of 12 rows and 9 columns. Rows are different uses of email, and columns are countries.  Cells show percentages of uses in USA, UK, India Indonesia, Canada, Australia, Brazil and France.", "levels": null, "corpus_id": 218483388, "sentences": ["Table of 12 rows and 9 columns.", "Rows are different uses of email, and columns are countries.", "Cells show percentages of uses in USA, UK, India Indonesia, Canada, Australia, Brazil and France."], "caption": "Table 1: Uses of email across countries.", "local_uri": ["0178069e51f3c81f3ebe0e48fa77f373482a0bbb_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "The Care Work of Access", "pdf_hash": "07a1b6a5d7f3d652404b1553c365c2d2ac94b3ff", "year": 2020, "venue": "CHI", "alt_text": "William and Jason passing the ball.  Sequence of 5 images, labelled F-1 to F-5, show William and Jason passing the ball to one another using gestures and body movement and orientation.", "levels": null, "corpus_id": 218482481, "sentences": ["William and Jason passing the ball.", "Sequence of 5 images, labelled F-1 to F-5, show William and Jason passing the ball to one another using gestures and body movement and orientation."], "caption": "F-1", "local_uri": ["07a1b6a5d7f3d652404b1553c365c2d2ac94b3ff_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Ecology Meets Computer Science: Designing Tools to Reconcile People, Data, and Practices", "pdf_hash": "a125ac448f8d1121fbbdbb7b319b54d0b24cd519", "year": 2020, "venue": "CHI", "alt_text": "False colour spectrogram. Vertical axis is frequency, horizontal axis is time (hours). Red, green and blue are mapped to three acoustic indices. Image shows streaks of yellow, blue, red and magenta against a black background, over a 24 hour period.", "levels": null, "corpus_id": 218482982, "sentences": ["False colour spectrogram.", "Vertical axis is frequency, horizontal axis is time (hours).", "Red, green and blue are mapped to three acoustic indices.", "Image shows streaks of yellow, blue, red and magenta against a black background, over a 24 hour period."], "caption": "Figure 1. Example false-colour spectrogram depicting a 24- hour acoustic recording of Bhutanese wilderness", "local_uri": ["a125ac448f8d1121fbbdbb7b319b54d0b24cd519_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Ecology Meets Computer Science: Designing Tools to Reconcile People, Data, and Practices", "pdf_hash": "a125ac448f8d1121fbbdbb7b319b54d0b24cd519", "year": 2020, "venue": "CHI", "alt_text": "Greyscale spectrogram shows darker and lighter streaks. Vertical axis is frequency, horizontal axis is time (hours). The darker parts of the image depict lorikeet calls. These are boxed in various ways using a lime green lines. The size and shape of the box varies greatly over the four images in terms of scale, number of calls boxed, and use of the vertical axis (frequency).", "levels": null, "corpus_id": 218482982, "sentences": ["Greyscale spectrogram shows darker and lighter streaks.", "Vertical axis is frequency, horizontal axis is time (hours).", "The darker parts of the image depict lorikeet calls.", "These are boxed in various ways using a lime green lines.", "The size and shape of the box varies greatly over the four images in terms of scale, number of calls boxed, and use of the vertical axis (frequency)."], "caption": "Figure 2. Four example annotations of rainbow lorikeet calls using greyscale spectrograms", "local_uri": ["a125ac448f8d1121fbbdbb7b319b54d0b24cd519_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Diagramming Working Field Theories for Design in the HCI Classroom", "pdf_hash": "680984459a2ab9dcaed587a6e5775a25ce140804", "year": 2021, "venue": "CHI", "alt_text": "Figure 1: A diagram of a Rehabilitation Field Theory. It focuses on the motivation for patients to do self-directed physical therapy. It identifies four themes of struggle. Unclear instructions, boredom, disregard for benefits, and memory. It identifies four touch points where technology can contribute. Show what to do, make it fun, show progress and remind to do exercise.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987822, "sentences": ["Figure 1: A diagram of a Rehabilitation Field Theory.", "It focuses on the motivation for patients to do self-directed physical therapy.", "It identifies four themes of struggle.", "Unclear instructions, boredom, disregard for benefits, and memory.", "It identifies four touch points where technology can contribute.", "Show what to do, make it fun, show progress and remind to do exercise."], "caption": "", "local_uri": ["680984459a2ab9dcaed587a6e5775a25ce140804_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagramming Working Field Theories for Design in the HCI Classroom", "pdf_hash": "680984459a2ab9dcaed587a6e5775a25ce140804", "year": 2021, "venue": "CHI", "alt_text": "Figure 2: Diagram that represents the concepts that surround and inspire Field Theories. Field Theory is the central concept. Field theories synthesize field research that is inspired in technology examples, and that is analyzed using sensitizing concepts. Field theories are iterative: field theories are used to ideate prototype designs, the designs are tested in field trials, and data from field trials is used to refine the field theory.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987822, "sentences": ["Figure 2: Diagram that represents the concepts that surround and inspire Field Theories.", "Field Theory is the central concept.", "Field theories synthesize field research that is inspired in technology examples, and that is analyzed using sensitizing concepts.", "Field theories are iterative: field theories are used to ideate prototype designs, the designs are tested in field trials, and data from field trials is used to refine the field theory."], "caption": "", "local_uri": ["680984459a2ab9dcaed587a6e5775a25ce140804_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagramming Working Field Theories for Design in the HCI Classroom", "pdf_hash": "680984459a2ab9dcaed587a6e5775a25ce140804", "year": 2021, "venue": "CHI", "alt_text": "Figure 3: A diagram of a Field Theory of Managing Diabetes. It is represented as a two-dimension graph that shows glucose levels in the y-axis, and temporal milestones (hospital, diagnosis, treatment, tools, education, practice) in the x-axis. The glucose curve ascends and descends regularly, also representing the ups and downs of the journey of self-managing diabetes, leading to a more stable curve after the use of tools, education and practice. The curve leads to identifying opportunities for design using Internet of Things.", "levels": [[1], [1], [3], [1]], "corpus_id": 233987822, "sentences": ["Figure 3: A diagram of a Field Theory of Managing Diabetes.", "It is represented as a two-dimension graph that shows glucose levels in the y-axis, and temporal milestones (hospital, diagnosis, treatment, tools, education, practice) in the x-axis.", "The glucose curve ascends and descends regularly, also representing the ups and downs of the journey of self-managing diabetes, leading to a more stable curve after the use of tools, education and practice.", "The curve leads to identifying opportunities for design using Internet of Things."], "caption": "", "local_uri": ["680984459a2ab9dcaed587a6e5775a25ce140804_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagramming Working Field Theories for Design in the HCI Classroom", "pdf_hash": "680984459a2ab9dcaed587a6e5775a25ce140804", "year": 2021, "venue": "CHI", "alt_text": "Figure 4: The figure shows two photos. The left hand side shows the Growbot prototype displaying a sad face because the plant does not feel happy in the current location. The right hand side shows the device displaying the level of water that the plant currently has.", "levels": null, "corpus_id": 233987822, "sentences": ["Figure 4: The figure shows two photos.", "The left hand side shows the Growbot prototype displaying a sad face because the plant does not feel happy in the current location.", "The right hand side shows the device displaying the level of water that the plant currently has."], "caption": "", "local_uri": ["680984459a2ab9dcaed587a6e5775a25ce140804_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Diagramming Working Field Theories for Design in the HCI Classroom", "pdf_hash": "680984459a2ab9dcaed587a6e5775a25ce140804", "year": 2021, "venue": "CHI", "alt_text": "Figure 5: The figure shows two images. The left hand side shows a photo of a Recipe Globe device. A jar contains the ingredients of the recipe, and the lid shows buttons and components that allow interaction with the device, including audio playback controls, microphone and speaker. The figure on the right hand side shows a flow chart of cooking: decision making, preparing ingredients, follow recipe or cook by memory, finished meal.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987822, "sentences": ["Figure 5: The figure shows two images.", "The left hand side shows a photo of a Recipe Globe device.", "A jar contains the ingredients of the recipe, and the lid shows buttons and components that allow interaction with the device, including audio playback controls, microphone and speaker.", "The figure on the right hand side shows a flow chart of cooking: decision making, preparing ingredients, follow recipe or cook by memory, finished meal."], "caption": "", "local_uri": ["680984459a2ab9dcaed587a6e5775a25ce140804_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Diagramming Working Field Theories for Design in the HCI Classroom", "pdf_hash": "680984459a2ab9dcaed587a6e5775a25ce140804", "year": 2021, "venue": "CHI", "alt_text": "Figure 6: A diagram of a social cooking field theory. The diagram describes and illustrates the experiences of cooking from memory, finding a new recipe, and sharing recipes.", "levels": null, "corpus_id": 233987822, "sentences": ["Figure 6: A diagram of a social cooking field theory.", "The diagram describes and illustrates the experiences of cooking from memory, finding a new recipe, and sharing recipes."], "caption": "", "local_uri": ["680984459a2ab9dcaed587a6e5775a25ce140804_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs", "pdf_hash": "3fb3ada9e73195ffe690a724353cbde4b067528b", "year": 2020, "venue": "CHI", "alt_text": "A screenshot of interaction with Torii, shown in two stages. Stage 1: an author hovers over a snippet, and a toggle button appears that lets them switch between viewing just the snippet, or viewing the program snapshot that includes the snippet and all previous snippets in the tutorial. The author clicks the toggle to view the program snapshot. Stage 2: the view has changed to show the program snapshot. The snapshot contains all code from this snippet and previous snippets in the tutorial, appended in source order.", "levels": null, "corpus_id": 211019677, "sentences": ["A screenshot of interaction with Torii, shown in two stages.", "Stage 1: an author hovers over a snippet, and a toggle button appears that lets them switch between viewing just the snippet, or viewing the program snapshot that includes the snippet and all previous snippets in the tutorial.", "The author clicks the toggle to view the program snapshot.", "Stage 2: the view has changed to show the program snapshot.", "The snapshot contains all code from this snippet and previous snippets in the tutorial, appended in source order."], "caption": "Rhia can click on the \u201cProgram Snapshot\u201d tab in any snippet to see what Torii would execute to produce an output at that point in the tutorial. Most practically, this snapshot provides Rhia a view of the code the reader will have at this point in the tutorial, if they assemble the snippets in the order they appeared in Rhia\u2019s reference implementation.", "local_uri": ["3fb3ada9e73195ffe690a724353cbde4b067528b_Image_041.jpg"], "annotated": false, "compound": false}
{"title": "Quality of and Attention to Instructions in Telementoring", "pdf_hash": "c799523f117dc5823995859e9ba5e8bce7621c97", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 2. shows the task models used in the study: Shows the starting status of the models that were used for the subtasks. Different parts of the model including the blood vessel and the duct are demonstrated. The duct and the blood vessel are stick to each other by the peritoneum at the beginning of the task. The trainee is asked to dissect the duct and the blood vessel first. Next, they are asked to apply clips on both blood vessel and the duct. Finally, they cut them.", "levels": null, "corpus_id": 224805912, "sentences": ["Figure 2. shows the task models used in the study: Shows the starting status of the models that were used for the subtasks.", "Different parts of the model including the blood vessel and the duct are demonstrated.", "The duct and the blood vessel are stick to each other by the peritoneum at the beginning of the task.", "The trainee is asked to dissect the duct and the blood vessel first.", "Next, they are asked to apply clips on both blood vessel and the duct.", "Finally, they cut them."], "caption": "Figure 2. Task models used in the study: (a) Used for subtask 1 - mobilizing the cystic duct and artery; (b - without the staples across the structures) Used for subtask 2 - clipping the cystic duct; and subtask 3 - clipping the cystic artery; (b - as shown) Used for subtask 4 - cutting the cystic artery and duct.", "local_uri": ["c799523f117dc5823995859e9ba5e8bce7621c97_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Quality of and Attention to Instructions in Telementoring", "pdf_hash": "c799523f117dc5823995859e9ba5e8bce7621c97", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 3. Shows the set up of the two conditions: (a) distributed condition- a divider between the trainee and the trainer to simulate the distributed mentoring; (b) co-located condition - the trainer and the trainee standing next to each other.", "levels": null, "corpus_id": 224805912, "sentences": ["Figure 3. Shows the set up of the two conditions: (a) distributed condition- a divider between the trainee and the trainer to simulate the distributed mentoring; (b) co-located condition - the trainer and the trainee standing next to each other."], "caption": "Figure 3. The two conditions setups: (a) distributed condition- a divider between the trainee and the trainer to simulate the distributed mentoring; (b) collocated condition - the trainer and the trainee standing next to each other.", "local_uri": ["c799523f117dc5823995859e9ba5e8bce7621c97_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Context-Based Interface Prototyping and Evaluation for (Shared) Autonomous Vehicles Using a Lightweight Immersive Video-Based Simulator", "pdf_hash": "0677defc03e335e85f83f6027d0b3bad8d802ae4", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "AV simulator setup in an office room. Two people sit in the simulator setup facing the front screen of the simulator.", "levels": null, "corpus_id": 220324287, "sentences": ["AV simulator setup in an office room.", "Two people sit in the simulator setup facing the front screen of the simulator."], "caption": "Figure 1. Immersive video-based autonomous vehicle simulator with a 26-inch passenger information display.", "local_uri": ["0677defc03e335e85f83f6027d0b3bad8d802ae4_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Context-Based Interface Prototyping and Evaluation for (Shared) Autonomous Vehicles Using a Lightweight Immersive Video-Based Simulator", "pdf_hash": "0677defc03e335e85f83f6027d0b3bad8d802ae4", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Schematically illustrated AV simulator setup in an office room. The illustration shows an arrangement of six seats, three video projectors, stereo speakers, and an operator area. The front projection is 96.5 inch. Both side projections are 72.8 inch.", "levels": null, "corpus_id": 220324287, "sentences": ["Schematically illustrated AV simulator setup in an office room.", "The illustration shows an arrangement of six seats, three video projectors, stereo speakers, and an operator area.", "The front projection is 96.5 inch.", "Both side projections are 72.8 inch."], "caption": "Figure 2. Schematic illustration of the AV simulator setup consisting of three 1080p video projectors, stereo speakers, and a 3\u00d72 seating group.", "local_uri": ["0677defc03e335e85f83f6027d0b3bad8d802ae4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Context-Based Interface Prototyping and Evaluation for (Shared) Autonomous Vehicles Using a Lightweight Immersive Video-Based Simulator", "pdf_hash": "0677defc03e335e85f83f6027d0b3bad8d802ae4", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Sequence of 4 events during the two simulator rides. At the starting point the participant gets on the Shared Autonomous Vehicle. Another passenger gets on at the first stop and gets off at the second stop. At the \"end\" stop the participant gets off as well.", "levels": null, "corpus_id": 220324287, "sentences": ["Sequence of 4 events during the two simulator rides.", "At the starting point the participant gets on the Shared Autonomous Vehicle.", "Another passenger gets on at the first stop and gets off at the second stop.", "At the \"end\" stop the participant gets off as well."], "caption": "Figure 4. Schematic illustration of the ride sequence.", "local_uri": ["0677defc03e335e85f83f6027d0b3bad8d802ae4_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Context-Based Interface Prototyping and Evaluation for (Shared) Autonomous Vehicles Using a Lightweight Immersive Video-Based Simulator", "pdf_hash": "0677defc03e335e85f83f6027d0b3bad8d802ae4", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Radar plot displaying means and standarddeviations of the four IPQ subscales for both conditions of the user study.", "levels": [[1]], "corpus_id": 220324287, "sentences": ["Radar plot displaying means and standarddeviations of the four IPQ subscales for both conditions of the user study."], "caption": "Figure 6. Means (SD) of the IPQ [38, 39] subscales [from 0 = low to", "local_uri": ["0677defc03e335e85f83f6027d0b3bad8d802ae4_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "DataQuilt: Extracting Visual Elements from Images to Craft Pictorial Visualizations", "pdf_hash": "bfcb79f18dffe2777d49dd5bca6afcb3ee7244b5", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: \"Visual-to-data mapping can be added by dragging a visual variable icon from the menu, and dropping on a column header.\"", "levels": [[-1]], "corpus_id": 218482872, "sentences": ["Figure 5: \"Visual-to-data mapping can be added by dragging a visual variable icon from the menu, and dropping on a column header.\""], "caption": "", "local_uri": ["bfcb79f18dffe2777d49dd5bca6afcb3ee7244b5_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Remotely Shaping the View in Surgical Telementoring", "pdf_hash": "b909e354da633289e57ae53e69493aacf95640a2", "year": 2020, "venue": "CHI", "alt_text": "On the left is a picture of a rolling stand with a monitor suspended at the top and a video camera lens above that. On the right is a screenshot of the UI made of four quandrants - top eft is the view from the stand's video camera, top right is the view from the secondary imaging source (in this case the laparoscopic camera) and the bottom left has a small frame with the video of the remote surgeon's face. The right quandrant is made of various menus and setting's boxes.", "levels": null, "corpus_id": 218483582, "sentences": ["On the left is a picture of a rolling stand with a monitor suspended at the top and a video camera lens above that.", "On the right is a screenshot of the UI made of four quandrants - top eft is the view from the stand's video camera, top right is the view from the secondary imaging source (in this case the laparoscopic camera) and the bottom left has a small frame with the video of the remote surgeon's face.", "The right quandrant is made of various menus and setting's boxes."], "caption": "Figure 1. The Storz VISITOR1 System used in the telementoring cases. a. the cart unit that can be driven by remote surgeons to navigate the operating rooms; b. the remote surgeons\u2019 interface with external view on the left and endoscopic view on the right.", "local_uri": ["b909e354da633289e57ae53e69493aacf95640a2_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Remotely Shaping the View in Surgical Telementoring", "pdf_hash": "b909e354da633289e57ae53e69493aacf95640a2", "year": 2020, "venue": "CHI", "alt_text": "Second view - external camera showing the scope has been adjusted to point in a different location.", "levels": null, "corpus_id": 218483582, "sentences": ["Second view - external camera showing the scope has been adjusted to point in a different location."], "caption": "", "local_uri": ["b909e354da633289e57ae53e69493aacf95640a2_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Remotely Shaping the View in Surgical Telementoring", "pdf_hash": "b909e354da633289e57ae53e69493aacf95640a2", "year": 2020, "venue": "CHI", "alt_text": "First view - shows the external camera where the local surgeon is pointing and the virtual pointer is drawing.", "levels": null, "corpus_id": 218483582, "sentences": ["First view - shows the external camera where the local surgeon is pointing and the virtual pointer is drawing."], "caption": "10RS3: Do you know where my circle is?", "local_uri": ["b909e354da633289e57ae53e69493aacf95640a2_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Remotely Shaping the View in Surgical Telementoring", "pdf_hash": "b909e354da633289e57ae53e69493aacf95640a2", "year": 2020, "venue": "CHI", "alt_text": "Second view - shows that the local surgeon now shared the internal camera view.", "levels": null, "corpus_id": 218483582, "sentences": ["Second view - shows that the local surgeon now shared the internal camera view."], "caption": "14RS3: Take the marking, as long as you are above- ((Switches to the laparoscopic view as his primary view.))", "local_uri": ["b909e354da633289e57ae53e69493aacf95640a2_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Remotely Shaping the View in Surgical Telementoring", "pdf_hash": "b909e354da633289e57ae53e69493aacf95640a2", "year": 2020, "venue": "CHI", "alt_text": "Third view - remote surgeon has drawn at the bottom of the view with the pointer", "levels": null, "corpus_id": 218483582, "sentences": ["Third view - remote surgeon has drawn at the bottom of the view with the pointer"], "caption": "Annotation over the anatomy on bottom right.", "local_uri": ["b909e354da633289e57ae53e69493aacf95640a2_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Remotely Shaping the View in Surgical Telementoring", "pdf_hash": "b909e354da633289e57ae53e69493aacf95640a2", "year": 2020, "venue": "CHI", "alt_text": "Second view - same view as in first view but the bottom right has been drawn on by remote surgeon.", "levels": null, "corpus_id": 218483582, "sentences": ["Second view - same view as in first view but the bottom right has been drawn on by remote surgeon."], "caption": "Annotation over the anatomy on bottom right.", "local_uri": ["b909e354da633289e57ae53e69493aacf95640a2_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization", "pdf_hash": "b58b35da221a853d5b99bbfaa3ea40252d70016b", "year": 2020, "venue": "CHI", "alt_text": "The figure consists of 4 pictures next to each other.  Picture a (far left) - A scientist with gloves extract Pyrrole from a small bottle using a syringe (left of the picture). A scientist with gloves grinds Iron(III) Chloride using a little spoon in a small bowl.  Picture b (center left) - Several polymerized samples laid out on a table. The samples consist of various fabric and textiles that were polymerized using various traditional manufacturing processes. They are all black and white due to the polymerization process.  Picture c (center right) - A designer draws conductive patterns using the Batik traditional manufacturing process on a grey glove attached to a table using adhesive tape. The designer is holding a tool that spreads wax on the glove to create insulated areas.  Picture d (far right) - A person is wearing a polymerized leggings while standing. The front and back of the leggings are covered in black and white patterns.", "levels": null, "corpus_id": 211124001, "sentences": ["The figure consists of 4 pictures next to each other.", "Picture a (far left) - A scientist with gloves extract Pyrrole from a small bottle using a syringe (left of the picture).", "A scientist with gloves grinds Iron(III) Chloride using a little spoon in a small bowl.", "Picture b (center left) - Several polymerized samples laid out on a table.", "The samples consist of various fabric and textiles that were polymerized using various traditional manufacturing processes.", "They are all black and white due to the polymerization process.", "Picture c (center right) - A designer draws conductive patterns using the Batik traditional manufacturing process on a grey glove attached to a table using adhesive tape.", "The designer is holding a tool that spreads wax on the glove to create insulated areas.", "Picture d (far right) - A person is wearing a polymerized leggings while standing.", "The front and back of the leggings are covered in black and white patterns."], "caption": "Figure 1. a) Pyrrole (left) and Iron(III) Chloride (right) used for Polymerization. b) Various polymerized samples, of different materials showing tests of batik (left) and tie-dyed traces (middle). c) Traditional manufacturing processes (here Batik) can be use to draw conductive patterns. d) Leggings augmented with polymerization to sense movement.", "local_uri": ["b58b35da221a853d5b99bbfaa3ea40252d70016b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization", "pdf_hash": "b58b35da221a853d5b99bbfaa3ea40252d70016b", "year": 2020, "venue": "CHI", "alt_text": "The figure consists of two pictures next to each other.  Picture a (left) - A sketch demonstrating the piezo-resistive effect depicts a  piezoresistive material sandwiched between two conductive pads. The piezo-resistive material consists of multiple circles, indicating molecules. The electrical current goes from one electrode to the other through the piezo-resistive material.  Picture b (right) - The top electrode is deformed by an external force. As a result, the molecules composing the piezoresistive material move closer together, allowing more current to pass. The bond between the electrodes and the piezo-resistive material is also improved. This also provides better conductivity.", "levels": null, "corpus_id": 211124001, "sentences": ["The figure consists of two pictures next to each other.", "Picture a (left) - A sketch demonstrating the piezo-resistive effect depicts a  piezoresistive material sandwiched between two conductive pads.", "The piezo-resistive material consists of multiple circles, indicating molecules.", "The electrical current goes from one electrode to the other through the piezo-resistive material.", "Picture b (right) - The top electrode is deformed by an external force.", "As a result, the molecules composing the piezoresistive material move closer together, allowing more current to pass.", "The bond between the electrodes and the piezo-resistive material is also improved.", "This also provides better conductivity."], "caption": "Figure 2. Illustration of the piezo-resistive effect. When pressure is ap- plied (right), the spacing between the conductive molecules is reduced and the surface contact is improved. Both effects create a measurable change in resistance.", "local_uri": ["b58b35da221a853d5b99bbfaa3ea40252d70016b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization", "pdf_hash": "b58b35da221a853d5b99bbfaa3ea40252d70016b", "year": 2020, "venue": "CHI", "alt_text": "The figure consists of two columns of 3 pictures each.  Picture a (top left) - The two electrodes of a multimeter are connected to each side of a stretch-sensitive yarn. A ruler is situated next to the yarn to show its length which seems to be approximately 10 centimeters. The user does not seem to stretch the yarn and the multimeter shows a value of 0.12MOhm.  Picture b (center left) - It is the same setup as Picture a, but this time the yarn is stretched to reach a length of approximately 20 centimeters. The multimeter shows a value of 1.18MOhm.  Picture c (bottom left) - A plot with two axes shows the evolution of the yarn resistance depending on its extension. It shows linear progression from approximately 0 to 5MOhm (y-axis) going from an extension of 0% to 100% by steps of 20% (x-axis).  Picture d (top right) - This is a close-up on a pressure sensing yarn held within fingertips. The yarn consists of a copper conductive core and a polymerized outer layer.  Picture e (center right) - The two electrodes of a multimeter are connected to two independent pressure-sensitive yarns that stack up on each other. The user is pressing the yarns at their intersection. The value on the multimeter shows a value of 1.93MOhm.  Picture f (bottom right) - A plot shows how the resistances of the yarns evolve depending on the weight applied on top of them. It shows a linear progression from 6MOhm to 2MOhm (y-axis) for weights going from 0.1g to 1000g increasing the weight ten times at each step (x-axis, logarithmic scale).", "levels": [[1], [-1], [-1], [-1], [-1], [0], [1], [2, 1], [-1], [-1], [-1], [-1], [-1], [1], [2, 1]], "corpus_id": 211124001, "sentences": ["The figure consists of two columns of 3 pictures each.", "Picture a (top left) - The two electrodes of a multimeter are connected to each side of a stretch-sensitive yarn.", "A ruler is situated next to the yarn to show its length which seems to be approximately 10 centimeters.", "The user does not seem to stretch the yarn and the multimeter shows a value of 0.12MOhm.", "Picture b (center left) - It is the same setup as Picture a, but this time the yarn is stretched to reach a length of approximately 20 centimeters.", "The multimeter shows a value of 1.18MOhm.", "Picture c (bottom left) - A plot with two axes shows the evolution of the yarn resistance depending on its extension.", "It shows linear progression from approximately 0 to 5MOhm (y-axis) going from an extension of 0% to 100% by steps of 20% (x-axis).", "Picture d (top right) - This is a close-up on a pressure sensing yarn held within fingertips.", "The yarn consists of a copper conductive core and a polymerized outer layer.", "Picture e (center right) - The two electrodes of a multimeter are connected to two independent pressure-sensitive yarns that stack up on each other.", "The user is pressing the yarns at their intersection.", "The value on the multimeter shows a value of 1.93MOhm.", "Picture f (bottom right) - A plot shows how the resistances of the yarns evolve depending on the weight applied on top of them.", "It shows a linear progression from 6MOhm to 2MOhm (y-axis) for weights going from 0.1g to 1000g increasing the weight ten times at each step (x-axis, logarithmic scale)."], "caption": "", "local_uri": ["b58b35da221a853d5b99bbfaa3ea40252d70016b_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization", "pdf_hash": "b58b35da221a853d5b99bbfaa3ea40252d70016b", "year": 2020, "venue": "CHI", "alt_text": "Several applications are depicted with pairs of images on three lines.  Picture a - The user is wearing a polymerized glove that is connect to a laptop on which a digital hand is displayed. The user has her index and middle fingers straight, as does the digital hand.  Picture b - Now the user has her index and middle fingers bent,  the digital hand displayed on the laptop screen also has its fingers bent.  Picture c - The electrodes of a multimeter are connected on each side of a zipper. The zipper is closed and the multimeter shows a value of 25.25kOhm.  Picture d - The zipper is now open and the value on the multimeter is 60.20kOhm.  Picture e - The user is wearing polymerized fabric connected to a laptop on the wrist and forearm. As the user bends the wrist, changes of resistance are shown on the laptop screen.  Picture f - Another piece of polymerized fabric lays on the user forearm. The user is touching the fabric with the index finger from the other hand.", "levels": null, "corpus_id": 211124001, "sentences": ["Several applications are depicted with pairs of images on three lines.", "Picture a - The user is wearing a polymerized glove that is connect to a laptop on which a digital hand is displayed.", "The user has her index and middle fingers straight, as does the digital hand.", "Picture b - Now the user has her index and middle fingers bent,  the digital hand displayed on the laptop screen also has its fingers bent.", "Picture c - The electrodes of a multimeter are connected on each side of a zipper.", "The zipper is closed and the multimeter shows a value of 25.25kOhm.", "Picture d - The zipper is now open and the value on the multimeter is 60.20kOhm.", "Picture e - The user is wearing polymerized fabric connected to a laptop on the wrist and forearm.", "As the user bends the wrist, changes of resistance are shown on the laptop screen.", "Picture f - Another piece of polymerized fabric lays on the user forearm.", "The user is touching the fabric with the index finger from the other hand."], "caption": "Figure 8. Application examples of polymerization. Cotton gloves cap- ture hand movements (a and b), a zipper is turned into a linear poten- tiometer (c and d), and kinesio tape becomes a tool for rapidly prototyp- ing on-skin UI\u2019s.", "local_uri": ["b58b35da221a853d5b99bbfaa3ea40252d70016b_Image_038.jpg"], "annotated": false, "compound": false}
{"title": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization", "pdf_hash": "b58b35da221a853d5b99bbfaa3ea40252d70016b", "year": 2020, "venue": "CHI", "alt_text": "This depicts various textiles with 1000x magnification. Some fibers of the textiles are polymerized (appear black), while others remain white. At the bottom, the same textile is displayed twice, once relaxed and once stretched. The stretched textile has a different structure, than the relaxed textile.", "levels": null, "corpus_id": 211124001, "sentences": ["This depicts various textiles with 1000x magnification.", "Some fibers of the textiles are polymerized (appear black), while others remain white.", "At the bottom, the same textile is displayed twice, once relaxed and once stretched.", "The stretched textile has a different structure, than the relaxed textile."], "caption": "", "local_uri": ["b58b35da221a853d5b99bbfaa3ea40252d70016b_Image_041.jpg"], "annotated": false, "compound": false}
{"title": "\u201cOops...\u201d: Mobile Message Deletion in Conversation Error and Regret Remediation", "pdf_hash": "fa7c681ffd0ea94a482755623a107fce48740b0d", "year": 2021, "venue": "CHI", "alt_text": "Two screenshots. The left (a) is a screenshot from WhatsApp showing deleted messages. The right (b) is a screenshot from Facebook messenger showing deleted messages.", "levels": null, "corpus_id": 233987213, "sentences": ["Two screenshots.", "The left (a) is a screenshot from WhatsApp showing deleted messages.", "The right (b) is a screenshot from Facebook messenger showing deleted messages."], "caption": "", "local_uri": ["fa7c681ffd0ea94a482755623a107fce48740b0d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "\u201cOops...\u201d: Mobile Message Deletion in Conversation Error and Regret Remediation", "pdf_hash": "fa7c681ffd0ea94a482755623a107fce48740b0d", "year": 2021, "venue": "CHI", "alt_text": "Stacked bar chart showing how likely reasons for deleting messages. This graph is detailed in section 4.1. Sent by mistake, and sent to wrong recipient are the most likely reasons reported for deletion, while increase storage capacity is the least likely reason reported.", "levels": [[1], [-1], [2]], "corpus_id": 233987213, "sentences": ["Stacked bar chart showing how likely reasons for deleting messages.", "This graph is detailed in section 4.1.", "Sent by mistake, and sent to wrong recipient are the most likely reasons reported for deletion, while increase storage capacity is the least likely reason reported."], "caption": "", "local_uri": ["fa7c681ffd0ea94a482755623a107fce48740b0d_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "\u201cOops...\u201d: Mobile Message Deletion in Conversation Error and Regret Remediation", "pdf_hash": "fa7c681ffd0ea94a482755623a107fce48740b0d", "year": 2021, "venue": "CHI", "alt_text": "The left bar chart is showing reasons participants reported for why delete indicators might stop them from using the delete function. The chart is detailed in section 4.3 and shows most reported reason was to avoid negative assumptions, whilst fewest reported the reason was to avoid showing vulnerability.  The right bar chart is showing reasons participants reported for why delete indicators would not stop them from using the delete function. The chart is detailed in section 4.3.2 and shows most reported reason was the cost of it being read.", "levels": [[1], [2], [1], [2]], "corpus_id": 233987213, "sentences": ["The left bar chart is showing reasons participants reported for why delete indicators might stop them from using the delete function.", "The chart is detailed in section 4.3 and shows most reported reason was to avoid negative assumptions, whilst fewest reported the reason was to avoid showing vulnerability.", "The right bar chart is showing reasons participants reported for why delete indicators would not stop them from using the delete function.", "The chart is detailed in section 4.3.2 and shows most reported reason was the cost of it being read."], "caption": "Figure 6: Reasons participants reported for why delete in- dicators would not stop them from using the deletion func- tion.", "local_uri": ["fa7c681ffd0ea94a482755623a107fce48740b0d_Image_006.jpg", "fa7c681ffd0ea94a482755623a107fce48740b0d_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "\u201cOops...\u201d: Mobile Message Deletion in Conversation Error and Regret Remediation", "pdf_hash": "fa7c681ffd0ea94a482755623a107fce48740b0d", "year": 2021, "venue": "CHI", "alt_text": "Stacked bar chart showing participants assumed reasons for the message being deleted, separated by group and pairwise chats. The chart is detailed in section 5.1.3 and shows most reported assumption was that it was sent to the wrong recipient.", "levels": [[1], [2]], "corpus_id": 233987213, "sentences": ["Stacked bar chart showing participants assumed reasons for the message being deleted, separated by group and pairwise chats.", "The chart is detailed in section 5.1.3 and shows most reported assumption was that it was sent to the wrong recipient."], "caption": "", "local_uri": ["fa7c681ffd0ea94a482755623a107fce48740b0d_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "\u201cOops...\u201d: Mobile Message Deletion in Conversation Error and Regret Remediation", "pdf_hash": "fa7c681ffd0ea94a482755623a107fce48740b0d", "year": 2021, "venue": "CHI", "alt_text": "Stacked bar chart showing participants (senders) reasons for the message being deleted, separated by group and pairwise chats. The chart is detailed in section 5.1 and shows most reported reason was that the content was incorrect.", "levels": [[1], [2]], "corpus_id": 233987213, "sentences": ["Stacked bar chart showing participants (senders) reasons for the message being deleted, separated by group and pairwise chats.", "The chart is detailed in section 5.1 and shows most reported reason was that the content was incorrect."], "caption": "", "local_uri": ["fa7c681ffd0ea94a482755623a107fce48740b0d_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Towards Progress Assessment for Adaptive Hints in Educational Virtual Reality Games", "pdf_hash": "a2f752663771522fe7cb8908c1f8b579cf272e8a", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "A penetration tester stands inside an open-space office. No employees are around. He grabs inside a paper bin to retrieve confidential documents. This attack is called dumpster diving. The picture is a collage of a virtual in-game office and a photo of the penetration tester wearing the HMD and grabbing with the controller for the documents in the paper bin.", "levels": null, "corpus_id": 214815951, "sentences": ["A penetration tester stands inside an open-space office.", "No employees are around.", "He grabs inside a paper bin to retrieve confidential documents.", "This attack is called dumpster diving.", "The picture is a collage of a virtual in-game office and a photo of the penetration tester wearing the HMD and grabbing with the controller for the documents in the paper bin."], "caption": "Figure 1: In the role of a PT, a player applies dumpster diving on an offce paper bin to retrieve confdential documents.", "local_uri": ["a2f752663771522fe7cb8908c1f8b579cf272e8a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Genie in the Bottle: Anthropomorphized Perceptions of Conversational Agents", "pdf_hash": "a777263b988fe4cb0f1c0235b13e38d85bc3f958", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: Cumulative visualizations. The figure includes three portrait images: (left to right) Alexa, Google, Siri. Alexa has most prominently long wavy brownish hair, white-ish glasses, open smile, and business-casual blazer. Google has most prominently long straight brown-ish hair, glasses in black frame (post noticeable than in other agents), open smile, and hoody. Siri looks more male, has most prominently short hair, dark, mixed clothes, slightly visible round glasses, and an open smile.", "levels": null, "corpus_id": 218482586, "sentences": ["Figure 1: Cumulative visualizations.", "The figure includes three portrait images: (left to right) Alexa, Google, Siri.", "Alexa has most prominently long wavy brownish hair, white-ish glasses, open smile, and business-casual blazer.", "Google has most prominently long straight brown-ish hair, glasses in black frame (post noticeable than in other agents), open smile, and hoody.", "Siri looks more male, has most prominently short hair, dark, mixed clothes, slightly visible round glasses, and an open smile."], "caption": "Figure 1. Cumulative visualizations. Left to right: Alexa, Google, Siri", "local_uri": ["a777263b988fe4cb0f1c0235b13e38d85bc3f958_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Genie in the Bottle: Anthropomorphized Perceptions of Conversational Agents", "pdf_hash": "a777263b988fe4cb0f1c0235b13e38d85bc3f958", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: Three collages of portrait format visualizations (5 by 4 grid each), organized horizontally: (left to right) Alexa, Google, Siri.", "levels": null, "corpus_id": 218482586, "sentences": ["Figure 3: Three collages of portrait format visualizations (5 by 4 grid each), organized horizontally: (left to right) Alexa, Google, Siri."], "caption": "", "local_uri": ["a777263b988fe4cb0f1c0235b13e38d85bc3f958_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Pilot Deployment of HTML5 Video Descriptions", "pdf_hash": "7e81c854865f97e4bfd0ff755071f9b4e422d541", "year": 2012, "venue": "", "alt_text": "This chart shows that speech synthesis can reduce cost in the transcribing and describing tasks.", "levels": [[-1]], "corpus_id": 59779650, "sentences": ["This chart shows that speech synthesis can reduce cost in the transcribing and describing tasks."], "caption": "Speech synthesis reduce the cost", "local_uri": ["7e81c854865f97e4bfd0ff755071f9b4e422d541_Image_011.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Pilot Deployment of HTML5 Video Descriptions", "pdf_hash": "7e81c854865f97e4bfd0ff755071f9b4e422d541", "year": 2012, "venue": "", "alt_text": "Screen shot of the Hiroshima movie channel with about six movies availablie to select.", "levels": null, "corpus_id": 59779650, "sentences": ["Screen shot of the Hiroshima movie channel with about six movies availablie to select."], "caption": "", "local_uri": ["7e81c854865f97e4bfd0ff755071f9b4e422d541_Image_024.jpg"], "annotated": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The four pictures labeled (a), (b), (c), and (d) show in-game pictures of VRSketchIn. Picture (a) shows a scene created with VRSketchIn. It consists of three tropical islands and a pirate ship sailing in between them. Picture (b) shows how a sail of the pirate ship is sketched with unconstrained 3D mid-air sketching. Picture (c) shows how constrained drawing surfaces can be used to sketch the deck of the ship. The mapping of the surface floating in mid-air to the tablet is visualized with dotted lines. Picture (d) shows a World In Miniature floating above the tablet in the scene. In the background is the true scale pirate ship that is visualized in miniature in the World In Miniature in the foreground.", "levels": null, "corpus_id": 215196847, "sentences": ["The four pictures labeled (a), (b), (c), and (d) show in-game pictures of VRSketchIn.", "Picture (a) shows a scene created with VRSketchIn.", "It consists of three tropical islands and a pirate ship sailing in between them.", "Picture (b) shows how a sail of the pirate ship is sketched with unconstrained 3D mid-air sketching.", "Picture (c) shows how constrained drawing surfaces can be used to sketch the deck of the ship.", "The mapping of the surface floating in mid-air to the tablet is visualized with dotted lines.", "Picture (d) shows a World In Miniature floating above the tablet in the scene.", "In the background is the true scale pirate ship that is visualized in miniature in the World In Miniature in the foreground."], "caption": "Figure 1. VRSketchIn is an (a) immersive sketching application combining (b) unconstrained 3D mid-air sketching with a pen and (c) constrained surface-based sketching with pen on tablet. We created a design space and describe multiple interaction metaphors such as drawing surfaces (c) or World In Miniature (d) to enable a combination of 2D and 3D mid-air sketching.", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The three pictures (a), (b), and (c) show the used hardware. Picture (a) shows the tablet held in the left hand and the pen held in the right hand. The OptiTrack markers (four each) are attached to the pen and the tablet mounted on short sticks. Picture (b) shows the disassembled pen, which consists of the Wacom pen, a Logitech Presenter board, and a 3D printed case. Picture (c) shows a person wearing the HMD with six attached OptiTrack markers mounted on sticks and holding the pen and the tablet in the hands.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 215196847, "sentences": ["The three pictures (a), (b), and (c) show the used hardware.", "Picture (a) shows the tablet held in the left hand and the pen held in the right hand.", "The OptiTrack markers (four each) are attached to the pen and the tablet mounted on short sticks.", "Picture (b) shows the disassembled pen, which consists of the Wacom pen, a Logitech Presenter board, and a 3D printed case.", "Picture (c) shows a person wearing the HMD with six attached OptiTrack markers mounted on sticks and holding the pen and the tablet in the hands."], "caption": "Figure 4. Our setup (c) consisted of an HMD, a tablet, and a pen (a). We 3D printed a pen case and combined the graphical pen with wireless buttons (b). All devices were motion tracked using OptiTrack [42].", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn. Picture (a) shows sketching the flag of the pirate ship in 3D mid-air. Picture (b) shows object selection with the pen in 3D mid-air by intersecting a drawn stroke. The intersected stroke is highlighted with a yellow grid shader.", "levels": null, "corpus_id": 215196847, "sentences": ["The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn.", "Picture (a) shows sketching the flag of the pirate ship in 3D mid-air.", "Picture (b) shows object selection with the pen in 3D mid-air by intersecting a drawn stroke.", "The intersected stroke is highlighted with a yellow grid shader."], "caption": "Figure 5. 3D Mid-Air Sketching. a) Pen-based mid-air drawing in the immersive environment. b) Pen-based mid-air object selection.", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn. Picture (a) shows an active translation gizmo for the selected stroke rendered with a red grid shader. Picture (b) shows an active rotation gizmo for the selected stroke rendered with a red grid shader.", "levels": null, "corpus_id": 215196847, "sentences": ["The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn.", "Picture (a) shows an active translation gizmo for the selected stroke rendered with a red grid shader.", "Picture (b) shows an active rotation gizmo for the selected stroke rendered with a red grid shader."], "caption": "", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn. Picture (a) shows the drawing surface that is created at the moment and that slices the pirate ship. The mapping of the surface floating in mid-air to the tablet is visualized with dotted lines. The pen is moved on the tablet to scale and position the surface. Picture (b) shows how constrained drawing surfaces can be used to sketch the deck of the ship. The mapping of the surface floating in mid-air to the tablet is visualized with dotted lines. The activated gridlines are visualized on the tablet.", "levels": null, "corpus_id": 215196847, "sentences": ["The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn.", "Picture (a) shows the drawing surface that is created at the moment and that slices the pirate ship.", "The mapping of the surface floating in mid-air to the tablet is visualized with dotted lines.", "The pen is moved on the tablet to scale and position the surface.", "Picture (b) shows how constrained drawing surfaces can be used to sketch the deck of the ship.", "The mapping of the surface floating in mid-air to the tablet is visualized with dotted lines.", "The activated gridlines are visualized on the tablet."], "caption": "Figure 7. Drawing Surfaces. a) New drawing surfaces can be created by placing them in space with the tablet position and orientation. The scale and the distance are set by sliding the pen on the tablet. A preview of the sli\u00ad ced objects is provided on the tablet. b) Sketching on the tablet creates sto\u00ad kes in mid-air that lie on the de\ufb01ned surface. Gridlines can be activated.", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn. Picture (a) shows a World In Miniature floating above the tablet in the scene. In the background is the true scale pirate ship that is visualized in miniature in the World In Miniature in the foreground. Picture (b) shows a detached World In Miniature with an active rotation gizmo inside it. It is used to rotate a sail of the ship.", "levels": null, "corpus_id": 215196847, "sentences": ["The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn.", "Picture (a) shows a World In Miniature floating above the tablet in the scene.", "In the background is the true scale pirate ship that is visualized in miniature in the World In Miniature in the foreground.", "Picture (b) shows a detached World In Miniature with an active rotation gizmo inside it.", "It is used to rotate a sail of the ship."], "caption": "", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality", "pdf_hash": "227cbfc0b67e46f2c696982537c8add62e7618ce", "year": 2020, "venue": "CHI", "alt_text": "The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn. Picture (a) shows a portal that is placed in the space to create a permanent camera looking to the pirate ship. This camera is visualized on the tablet. Picture (b) shows how a sail of the ship is extruded from a stroke by sliding the pen on the tablet.", "levels": null, "corpus_id": 215196847, "sentences": ["The two pictures labeled (a) and (b) show in-game pictures of VRSketchIn.", "Picture (a) shows a portal that is placed in the space to create a permanent camera looking to the pirate ship.", "This camera is visualized on the tablet.", "Picture (b) shows how a sail of the ship is extruded from a stroke by sliding the pen on the tablet."], "caption": "Figure 9. a) Portals can be places in space and provide a virtual camera that is visible on the tablet. b) Primitives can be extruded to create solid objects.", "local_uri": ["227cbfc0b67e46f2c696982537c8add62e7618ce_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Culture in Action: Unpacking Capacities to Inform Assets-Based Design", "pdf_hash": "b5abb3646d53f9ed6e090dae39a659524b984c0e", "year": 2020, "venue": "CHI", "alt_text": "Figure 5:  \"Battery usage as a function of the message length while using MissIt vs. when is not being used (Idle).\"", "levels": null, "corpus_id": 218482478, "sentences": ["Figure 5:  \"Battery usage as a function of the message length while using MissIt vs. when is not being used (Idle).\""], "caption": "", "local_uri": ["b5abb3646d53f9ed6e090dae39a659524b984c0e_Image_004.png"], "annotated": false, "compound": false}
{"title": "Culture in Action: Unpacking Capacities to Inform Assets-Based Design", "pdf_hash": "b5abb3646d53f9ed6e090dae39a659524b984c0e", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: \"Bit error rate, E(D), as a function of inter-symbol difference, D, on different cellular networks.\"", "levels": [[-1]], "corpus_id": 218482478, "sentences": ["Figure 4: \"Bit error rate, E(D), as a function of inter-symbol difference, D, on different cellular networks.\""], "caption": "Figure 4: Participants\u2019 design process imagining futures for parent-education technologies: a) Reading \u201cDon Ram\u00f3n\u201d\u2019s letter (box of objects in the back- ground); b) and c) Presenting their solutions.", "local_uri": ["b5abb3646d53f9ed6e090dae39a659524b984c0e_Image_005.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People", "pdf_hash": "660a2244efa8af0b77fd314a1c75dcc01aa677fe", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1 is an illustration of an AI-powered chatbot user interface. It is one of many similar systems that are widely adopted by healthcare end-customers. User can have diagnosis advices through natural language question and answering processes. As illustrated in the chart, by the end of the conversation flow between the user and the chatbot, the chatbot predict the patient has Acute Upper Respiratory Infection and recommend the patient to see a doctor immediately.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218483040, "sentences": ["Figure 1 is an illustration of an AI-powered chatbot user interface.", "It is one of many similar systems that are widely adopted by healthcare end-customers.", "User can have diagnosis advices through natural language question and answering processes.", "As illustrated in the chart, by the end of the conversation flow between the user and the chatbot, the chatbot predict the patient has Acute Upper Respiratory Infection and recommend the patient to see a doctor immediately."], "caption": "", "local_uri": ["660a2244efa8af0b77fd314a1c75dcc01aa677fe_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People", "pdf_hash": "660a2244efa8af0b77fd314a1c75dcc01aa677fe", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2 is an illustration of the design for the dialog tree of a chatbot. It represents how today's chatbot systems often work. For example, as illustrated in this chart, the chatbot first classifies a user's input message as one out of the four predefined intents. This step is called clustering. Then, the chatbot can trigger a corresponding response generation algorithm to generate a response.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483040, "sentences": ["Figure 2 is an illustration of the design for the dialog tree of a chatbot.", "It represents how today's chatbot systems often work.", "For example, as illustrated in this chart, the chatbot first classifies a user's input message as one out of the four predefined intents.", "This step is called clustering.", "Then, the chatbot can trigger a corresponding response generation algorithm to generate a response."], "caption": "Figure 2: Chatbot", "local_uri": ["660a2244efa8af0b77fd314a1c75dcc01aa677fe_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HoliBraille: multipoint vibrotactile feedback on mobile devices", "pdf_hash": "544916d82b610b1721d7b9c271f9ef9016937635", "year": 2015, "venue": "W4A", "alt_text": "This figure has 3 images. The first illustrates the Braille code of character 'f'. The second is an illustration of fingers 1, 2, and 3 vibrating. The third is a picture of a user holding the prototype.", "levels": null, "corpus_id": 3244562, "sentences": ["This figure has 3 images.", "The first illustrates the Braille code of character 'f'.", "The second is an illustration of fingers 1, 2, and 3 vibrating.", "The third is a picture of a user holding the prototype."], "caption": "Figure 1. HoliBraille, a multipoint vibrotactile output system for touchscreen mobile devices. (a) Representation of \u2018f\u2019 using the Braille code: dots 1, 2, and 4. (b) The system outputs character \u2018f\u2019 through direct and localized feedback on the user\u2019s fingers. (c) The system consists of six vibrotactile motors attached to springs and a 3D-printed case. The springs mould to users\u2019 hands and dampen vibrations through the device allowing better stimuli discrimination", "local_uri": ["544916d82b610b1721d7b9c271f9ef9016937635_Image_001.png"], "annotated": false, "compound": false}
{"title": "HoliBraille: multipoint vibrotactile feedback on mobile devices", "pdf_hash": "544916d82b610b1721d7b9c271f9ef9016937635", "year": 2015, "venue": "W4A", "alt_text": "This figure illustrates 3 dampening materials (cork, sorbothane, and spring) and their vibration properties. Also, there is a picture of the prototype's components: Arduino nano, FET shield, battery, charger, and micro USB connection.", "levels": [[1], [1]], "corpus_id": 3244562, "sentences": ["This figure illustrates 3 dampening materials (cork, sorbothane, and spring) and their vibration properties.", "Also, there is a picture of the prototype's components: Arduino nano, FET shield, battery, charger, and micro USB connection."], "caption": "Figure 2. Dampening materials: (a) cork, (b) sorbothane, (c) spring. (d) Spectrum plot with peak magnitude labeled. Y-axis scale is 10 times lower for spring plot and its peak magnitude is significantly lower than other materials. (e) HoliBraille\u2019s components.", "local_uri": ["544916d82b610b1721d7b9c271f9ef9016937635_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SmartTokens: Embedding Motion and Grip Sensing in Small Tangible Objects", "pdf_hash": "7dca0f56046ece7e8b7c7ae2f371891567ac0b8b", "year": 2015, "venue": "UIST", "alt_text": "A photo of SmartTokens being manipulated. SmartTokens are small-sized tokens supporting touch and motion sensing, that communicates wirelessly. The figure shows multiple SmartTokens being manipulated simultaneously. A 2 euros coin placed alongside allows to compare the size, very close in diameter.", "levels": null, "corpus_id": 16073939, "sentences": ["A photo of SmartTokens being manipulated.", "SmartTokens are small-sized tokens supporting touch and motion sensing, that communicates wirelessly.", "The figure shows multiple SmartTokens being manipulated simultaneously.", "A 2 euros coin placed alongside allows to compare the size, very close in diameter."], "caption": "Figure 1. SmartTokens are small-sized tokens supporting touch and motion sensing, and wireless communication with a coordinator.", "local_uri": ["7dca0f56046ece7e8b7c7ae2f371891567ac0b8b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "SmartTokens: Embedding Motion and Grip Sensing in Small Tangible Objects", "pdf_hash": "7dca0f56046ece7e8b7c7ae2f371891567ac0b8b", "year": 2015, "venue": "UIST", "alt_text": "Four different grips with a SmartToken. This figure consists of four pictures illustrating four different possible grips SmartTokens are able to distinguish. For example, a touch, a pinch, or a tight hold are easily recognised.", "levels": null, "corpus_id": 16073939, "sentences": ["Four different grips with a SmartToken.", "This figure consists of four pictures illustrating four different possible grips SmartTokens are able to distinguish.", "For example, a touch, a pinch, or a tight hold are easily recognised."], "caption": "Figure 5. Four different grips with a SmartToken.", "local_uri": ["7dca0f56046ece7e8b7c7ae2f371891567ac0b8b_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "SmartTokens: Embedding Motion and Grip Sensing in Small Tangible Objects", "pdf_hash": "7dca0f56046ece7e8b7c7ae2f371891567ac0b8b", "year": 2015, "venue": "UIST", "alt_text": "A short strip illustrating the mail notification system. The figure illustrates, in a comic strip fashion, our notification system implementing the SmartTokens as physical handles for notification such as mails. It also shows how this system can be easily integrated in existing environments.", "levels": null, "corpus_id": 16073939, "sentences": ["A short strip illustrating the mail notification system.", "The figure illustrates, in a comic strip fashion, our notification system implementing the SmartTokens as physical handles for notification such as mails.", "It also shows how this system can be easily integrated in existing environments."], "caption": "\ufb01cation and task management interface inspired from Durrell Bishop\u2019s marble answering machine [21].", "local_uri": ["7dca0f56046ece7e8b7c7ae2f371891567ac0b8b_Image_012.png"], "annotated": false, "compound": false}
{"title": "Guess the Data: Data Work to Understand How People Make Sense of and Use Simple Sensor Data from Homes", "pdf_hash": "6da504c2201d0795a9dc76732ba6f20494fd2dda", "year": 2020, "venue": "CHI", "alt_text": "In the screenshot of light graphs is a plateau in the value of the sensor in the hall from 8pm to 10pm clearly visible.", "levels": [[1]], "corpus_id": 218483536, "sentences": ["In the screenshot of light graphs is a plateau in the value of the sensor in the hall from 8pm to 10pm clearly visible."], "caption": "", "local_uri": ["6da504c2201d0795a9dc76732ba6f20494fd2dda_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Guess the Data: Data Work to Understand How People Make Sense of and Use Simple Sensor Data from Homes", "pdf_hash": "6da504c2201d0795a9dc76732ba6f20494fd2dda", "year": 2020, "venue": "CHI", "alt_text": "Screenshot of annotation message shown with the following text:\n\u201c[..] you can clearly see in the hallway that I have arrived home from work. It's funny that on the first day of the study we noticed that I forgot to turn off the light in the hallway (it didn't seem to be the first time) and stayed for 2 hours in the working/bicycle room. Only when my friend arrived home at 10 pm was the light turned off.\u201d", "levels": [[-1], [-1], [-1]], "corpus_id": 218483536, "sentences": ["Screenshot of annotation message shown with the following text:\n\u201c[..] you can clearly see in the hallway that I have arrived home from work.", "It's funny that on the first day of the study we noticed that I forgot to turn off the light in the hallway (it didn't seem to be the first time) and stayed for 2 hours in the working/bicycle room.", "Only when my friend arrived home at 10 pm was the light turned off.\u201d"], "caption": "Figure 3. Individual data work example 2; (a) screenshot of light sensors; (b) corresponding annotation, translated II.A.", "local_uri": ["6da504c2201d0795a9dc76732ba6f20494fd2dda_Image_004.gif"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing in the Network of Relations for Species Conservation: The Playful Tingtibi Community Birdhouse", "pdf_hash": "757ebd971c71746c18b3a33994bc0eaaac1c8efe", "year": 2020, "venue": "CHI", "alt_text": "A picture containing indoor, photo, computer, person  Description automatically generated", "levels": [[-1]], "corpus_id": 218483393, "sentences": ["A picture containing indoor, photo, computer, person  Description automatically generated"], "caption": "Figure 2 Children exploring and taking turn to listen to different bird calls (left), A local man playing local soundmarks during the community trial (center), and foresters and teachers listening to the endangered species in soundscapes while children were engaged in cocreating social play themed on endangered species habitat protection (right)", "local_uri": ["757ebd971c71746c18b3a33994bc0eaaac1c8efe_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Designing in the Network of Relations for Species Conservation: The Playful Tingtibi Community Birdhouse", "pdf_hash": "757ebd971c71746c18b3a33994bc0eaaac1c8efe", "year": 2020, "venue": "CHI", "alt_text": "Children at local school presenting their codesigned social play to save endangered species", "levels": null, "corpus_id": 218483393, "sentences": ["Children at local school presenting their codesigned social play to save endangered species"], "caption": "Figure 4: Children presenting and discussing social play: Saving WBH Habitat", "local_uri": ["757ebd971c71746c18b3a33994bc0eaaac1c8efe_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Designing in the Network of Relations for Species Conservation: The Playful Tingtibi Community Birdhouse", "pdf_hash": "757ebd971c71746c18b3a33994bc0eaaac1c8efe", "year": 2020, "venue": "CHI", "alt_text": "Two children at deployment site 2 cocreating card games and playing together with tangible cards on the birdhouse", "levels": null, "corpus_id": 218483393, "sentences": ["Two children at deployment site 2 cocreating card games and playing together with tangible cards on the birdhouse"], "caption": "Figure 5. Two children at deployment site 2 playing with tangible bird cards on the birdhouse", "local_uri": ["757ebd971c71746c18b3a33994bc0eaaac1c8efe_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Window Management Interactions in AR Headset + Smartphone Interface", "pdf_hash": "5b9b1507d32f3c9b2ecd8b04d718016ff33179b9", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 4: \"(a) A user is shaking the smartphone to the left side. (b) A user is shaking the smartphone to the left side while pressing on the smartphone screen at the same time. (c) A user is using the smartphone to touch the virtual window while pressing the edge of the smartphon screen at the same time.\"", "levels": null, "corpus_id": 218482693, "sentences": ["Figure 4: \"(a) A user is shaking the smartphone to the left side.", "(b) A user is shaking the smartphone to the left side while pressing on the smartphone screen at the same time. (c) A user is using the smartphone to touch the virtual window while pressing the edge of the smartphon screen at the same time.\""], "caption": "", "local_uri": ["5b9b1507d32f3c9b2ecd8b04d718016ff33179b9_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality", "pdf_hash": "43213aad8d504874b41c6748fc3c67f7ae75d3d4", "year": 2020, "venue": "CHI", "alt_text": "Figure 4. Variations in pin length depending on their position: 3, 9, and 14 mm. A rendered CG image of the worm gear mechanism is overlaid.", "levels": null, "corpus_id": 218482992, "sentences": ["Figure 4.", "Variations in pin length depending on their position: 3, 9, and 14 mm.", "A rendered CG image of the worm gear mechanism is overlaid."], "caption": "Figure 4. Variations in pin length depending on their position: 3, 9, and 14 mm. A rendered CG image of the worm gear mechanism is overlaid.", "local_uri": ["43213aad8d504874b41c6748fc3c67f7ae75d3d4_Image_010.png"], "annotated": false, "compound": false}
{"title": "PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality", "pdf_hash": "43213aad8d504874b41c6748fc3c67f7ae75d3d4", "year": 2020, "venue": "CHI", "alt_text": "Figure 6. Shape surface represented by the blue areas (a circle here). Each pin length is determined so that the midpoint on the outer edge of each pin is close to the surface of the target shape.", "levels": null, "corpus_id": 218482992, "sentences": ["Figure 6.", "Shape surface represented by the blue areas (a circle here).", "Each pin length is determined so that the midpoint on the outer edge of each pin is close to the surface of the target shape."], "caption": "", "local_uri": ["43213aad8d504874b41c6748fc3c67f7ae75d3d4_Image_012.png"], "annotated": false, "compound": false}
{"title": "PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality", "pdf_hash": "43213aad8d504874b41c6748fc3c67f7ae75d3d4", "year": 2020, "venue": "CHI", "alt_text": "Figure 5. Overview of the software and hardware architecture of PoCoPo. Four types of custom two-layer PCBs were designed for PoCoPo. The microcontroller (Teensy 3.6) and a Bluetooth module are attached to the main board. The motor driver board is connected to the main board via I2C. The board with two multiplexers (multiplexer board) is used to switch the optical encoders to read by time division. The encoder board is placed at the bottom of the pin to measure the length of its extrusion.", "levels": null, "corpus_id": 218482992, "sentences": ["Figure 5.", "Overview of the software and hardware architecture of PoCoPo.", "Four types of custom two-layer PCBs were designed for PoCoPo.", "The microcontroller (Teensy 3.6) and a Bluetooth module are attached to the main board.", "The motor driver board is connected to the main board via I2C.", "The board with two multiplexers (multiplexer board) is used to switch the optical encoders to read by time division.", "The encoder board is placed at the bottom of the pin to measure the length of its extrusion."], "caption": "Figure 5. Overview of the software and hardware architecture of PoCoPo. Four types of custom two-layer PCBs were designed for PoCoPo. The microcontroller (Teensy 3.6) and a Bluetooth module are attached to the main board. The motor driver board is connected to the main board via I2C. The board with two multiplexers (multiplexer board) is used to switch the optical encoders to read by time division. The encoder board is placed at the bottom of the pin to measure the length of its extrusion.", "local_uri": ["43213aad8d504874b41c6748fc3c67f7ae75d3d4_Image_013.png"], "annotated": false, "compound": false}
{"title": "PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality", "pdf_hash": "43213aad8d504874b41c6748fc3c67f7ae75d3d4", "year": 2020, "venue": "CHI", "alt_text": "Figure 7. Example applications of PoCoPo regarding the rendering of static objects. The user can freely hold a: a glass (linear surface), b: a matryoshka (convex surface), or c: a trophy (concave surface) on the table in VR.", "levels": null, "corpus_id": 218482992, "sentences": ["Figure 7.", "Example applications of PoCoPo regarding the rendering of static objects.", "The user can freely hold a: a glass (linear surface), b: a matryoshka (convex surface), or c: a trophy (concave surface) on the table in VR."], "caption": "Figure 7. Example applications of PoCoPo regarding the rendering of static objects. The user can freely hold a: a glass (linear surface), b: a matryoshka (convex surface), or c: a trophy (concave surface) on the table in VR.", "local_uri": ["43213aad8d504874b41c6748fc3c67f7ae75d3d4_Image_015.png"], "annotated": false, "compound": false}
{"title": "PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality", "pdf_hash": "43213aad8d504874b41c6748fc3c67f7ae75d3d4", "year": 2020, "venue": "CHI", "alt_text": "Figure 8. Example applications of PoCoPo regarding the rendering of dynamic objects. The user can freely hold animals in VR. a: Popping up of the pins when holding a hedgehog. b: Expanding or shrinking of the device representing hamster pulse. c: Movement of the pins corresponding to the movement of a snake.", "levels": null, "corpus_id": 218482992, "sentences": ["Figure 8.", "Example applications of PoCoPo regarding the rendering of dynamic objects.", "The user can freely hold animals in VR.", "a: Popping up of the pins when holding a hedgehog. b: Expanding or shrinking of the device representing hamster pulse.", "c: Movement of the pins corresponding to the movement of a snake."], "caption": "Figure 8. Example applications of PoCoPo regarding the rendering of dynamic objects. The user can freely hold animals in VR. a: Popping up of the pins when holding a hedgehog. b: Expanding or shrinking of the device representing hamster pulse. c: Movement of the pins correspond- ing to the movement of a snake.", "local_uri": ["43213aad8d504874b41c6748fc3c67f7ae75d3d4_Image_016.png"], "annotated": false, "compound": false}
{"title": "Mosaic: collaborative ways for older adults to use their expertise through information technologies", "pdf_hash": "dd704c54a93341d30ee760a8f3c6e3bd149cfd2f", "year": 2014, "venue": "ASAC", "alt_text": "An older adult is using a computer to guide a younger worker on the field. The worker is using wearables and cutting down a tree.", "levels": null, "corpus_id": 42925000, "sentences": ["An older adult is using a computer to guide a younger worker on the field.", "The worker is using wearables and cutting down a tree."], "caption": "", "local_uri": ["dd704c54a93341d30ee760a8f3c6e3bd149cfd2f_Image_007.png"], "annotated": false, "compound": false}
{"title": "Online Privacy Heuristics that Predict Information Disclosure", "pdf_hash": "0e331cc5b7070a1f782326c15a958b2f916dd39f", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"Notification message on a social networking site\" Figure 3: \"Reciprocity cue in a mobile chatting app\"", "levels": null, "corpus_id": 218483316, "sentences": ["Figure 2: \"Notification message on a social networking site\" Figure 3: \"Reciprocity cue in a mobile chatting app\""], "caption": "Figure 2. Notification message on a social networking site.", "local_uri": ["0e331cc5b7070a1f782326c15a958b2f916dd39f_Image_002.jpg", "0e331cc5b7070a1f782326c15a958b2f916dd39f_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "Online Privacy Heuristics that Predict Information Disclosure", "pdf_hash": "0e331cc5b7070a1f782326c15a958b2f916dd39f", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: \"Detailed terms and conditions about privacy policy [left]\" Figure 8: \"Virtual assistant in smartphone [right]\"", "levels": null, "corpus_id": 218483316, "sentences": ["Figure 7: \"Detailed terms and conditions about privacy policy [left]\" Figure 8: \"Virtual assistant in smartphone [right]\""], "caption": "Figure 7. Detailed terms and conditions about privacy policy", "local_uri": ["0e331cc5b7070a1f782326c15a958b2f916dd39f_Image_007.png", "0e331cc5b7070a1f782326c15a958b2f916dd39f_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "The TalkingBox.: Revealing Strengths of Adults with Severe Cognitive Disabilities", "pdf_hash": "d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 1 (top-left) - A picture containing room, table and memory matching tiles. Chris is putting the memory tiles in the box.  (top-right) - A picture containing the TalkingBox with RFID tiles laid down on the table.   bottom-left - A picture containing person, sitting, table and TalkingBox. Chris waiting for his turn to put the cards inside the box. He is carefully listening to the sound coming out from the box.", "levels": null, "corpus_id": 225947898, "sentences": ["Figure 1 (top-left) - A picture containing room, table and memory matching tiles.", "Chris is putting the memory tiles in the box.", "(top-right) - A picture containing the TalkingBox with RFID tiles laid down on the table.", "bottom-left - A picture containing person, sitting, table and TalkingBox.", "Chris waiting for his turn to put the cards inside the box.", "He is carefully listening to the sound coming out from the box."], "caption": "", "local_uri": ["d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The TalkingBox.: Revealing Strengths of Adults with Severe Cognitive Disabilities", "pdf_hash": "d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c", "year": 2020, "venue": "ASSETS", "alt_text": "(Left) - A picture containing two people. A peer is trying to help Chris to find the matching card.", "levels": null, "corpus_id": 225947898, "sentences": ["(Left) - A picture containing two people.", "A peer is trying to help Chris to find the matching card."], "caption": "", "local_uri": ["d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "The TalkingBox.: Revealing Strengths of Adults with Severe Cognitive Disabilities", "pdf_hash": "d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c", "year": 2020, "venue": "ASSETS", "alt_text": "(right) - A picture containing young adults. A person is approaching the table to play the game.", "levels": null, "corpus_id": 225947898, "sentences": ["(right) - A picture containing young adults.", "A person is approaching the table to play the game."], "caption": "", "local_uri": ["d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "The TalkingBox.: Revealing Strengths of Adults with Severe Cognitive Disabilities", "pdf_hash": "d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c", "year": 2020, "venue": "ASSETS", "alt_text": "A close up of different RFID cards with different signs and symbols. in the image we can see one tile per matching pair. Every card is different in this set. Turn left and turn right cards can be easily confused. Similarly, the two tiles with variety of different street signs are hard to discriminate without deep concentration and focus.", "levels": null, "corpus_id": 225947898, "sentences": ["A close up of different RFID cards with different signs and symbols.", "in the image we can see one tile per matching pair.", "Every card is different in this set.", "Turn left and turn right cards can be easily confused.", "Similarly, the two tiles with variety of different street signs are hard to discriminate without deep concentration and focus."], "caption": "Figure 2: A peer noticed that Chris has a problem fnding a matching tile. He reaches out to fip the card and show it to Chris (right) - A peer is sitting next to Chris repeating the activated words", "local_uri": ["d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "The TalkingBox.: Revealing Strengths of Adults with Severe Cognitive Disabilities", "pdf_hash": "d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c", "year": 2020, "venue": "ASSETS", "alt_text": "A picture containing two persons with disabilities. They are trying to play the game together.", "levels": null, "corpus_id": 225947898, "sentences": ["A picture containing two persons with disabilities.", "They are trying to play the game together."], "caption": "", "local_uri": ["d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "The TalkingBox.: Revealing Strengths of Adults with Severe Cognitive Disabilities", "pdf_hash": "d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c", "year": 2020, "venue": "ASSETS", "alt_text": "A group of people sitting at a table. Frontline manager approach the table to see how Chris plays the game.", "levels": null, "corpus_id": 225947898, "sentences": ["A group of people sitting at a table.", "Frontline manager approach the table to see how Chris plays the game."], "caption": "", "local_uri": ["d344d1edb6a8eac1eb94fa29ef1d6f3c0d20e41c_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Supporting Circuit Design with a Block-Based, Generator Language", "pdf_hash": "cba4b8ca6d201d299ddf7ffac0ae0a046c37035b", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 7: The Simon device, with 4 arcade dome buttons connected to a circuit board, all on a piece of laser-cut plywood. The circuit board has a socketed Nucleo microcontroller board.", "levels": null, "corpus_id": 218482807, "sentences": ["Figure 7: The Simon device, with 4 arcade dome buttons connected to a circuit board, all on a piece of laser-cut plywood.", "The circuit board has a socketed Nucleo microcontroller board."], "caption": "", "local_uri": ["cba4b8ca6d201d299ddf7ffac0ae0a046c37035b_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Supporting Circuit Design with a Block-Based, Generator Language", "pdf_hash": "cba4b8ca6d201d299ddf7ffac0ae0a046c37035b", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 8: The datalogger circuit board, with a discrete microcontroller chip (QFP-48) at the center and various components around it including a SD card, USB-C receptacle,  CAN transceiver, and supercapacitor.", "levels": [[-1]], "corpus_id": 218482807, "sentences": ["Figure 8: The datalogger circuit board, with a discrete microcontroller chip (QFP-48) at the center and various components around it including a SD card, USB-C receptacle,  CAN transceiver, and supercapacitor."], "caption": "", "local_uri": ["cba4b8ca6d201d299ddf7ffac0ae0a046c37035b_Image_013.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Self-Expression by Design: Co-Designing the ExpressiBall with Minimally-Verbal Children on the Autism Spectrum", "pdf_hash": "61e4009ed16cad35eb027788332d871e8d8bb9d2", "year": 2020, "venue": "CHI", "alt_text": "A table which describes six modalities of self-expression. Modality: Words(this is a fundamental modality). Auto-expressive example: Conveying interests (\u201cgloves\u201d), socio-expressive example Describing actions (\u201cbounce\u201d). Modality: Sounds (this is a fundamental modality), Auto-expressive example: Self-soothing (e.g. low humming), socio-expressive example: Attention-seeking (\u201cuh uh!\u201d). Modality: Bodily movement (this is a fundamental modality), Auto-expressive example: Dancing with the ball, socio-expressive example: Throwing the ball to a peer. Modality: Touch and Gesture (this is a fundamental modality), Auto-expressive example: Stroking the ball, socio-expressive example: Pointing to the ball. Modality: Play (this is an integrative modality), Auto-expressive example: Throwing ball up for peripheral vision stimulation, socio-expressive example: Independently initiated turn taking games. Modality: Creativity (this is an integrative modality), Auto-expressive example: Making up a song in the ball, socio-expressive example: Collaborative drawing on the ball.", "levels": null, "corpus_id": 218482476, "sentences": ["A table which describes six modalities of self-expression.", "Modality: Words(this is a fundamental modality).", "Auto-expressive example: Conveying interests (\u201cgloves\u201d), socio-expressive example Describing actions (\u201cbounce\u201d).", "Modality: Sounds (this is a fundamental modality), Auto-expressive example: Self-soothing (e.g. low humming), socio-expressive example: Attention-seeking (\u201cuh uh!\u201d).", "Modality: Bodily movement (this is a fundamental modality), Auto-expressive example: Dancing with the ball, socio-expressive example: Throwing the ball to a peer.", "Modality: Touch and Gesture (this is a fundamental modality), Auto-expressive example: Stroking the ball, socio-expressive example: Pointing to the ball.", "Modality: Play (this is an integrative modality), Auto-expressive example: Throwing ball up for peripheral vision stimulation, socio-expressive example: Independently initiated turn taking games.", "Modality: Creativity (this is an integrative modality), Auto-expressive example: Making up a song in the ball, socio-expressive example: Collaborative drawing on the ball."], "caption": "Figure 1. Table of Modalities of Self-Expression with Examples", "local_uri": ["61e4009ed16cad35eb027788332d871e8d8bb9d2_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Self-Expression by Design: Co-Designing the ExpressiBall with Minimally-Verbal Children on the Autism Spectrum", "pdf_hash": "61e4009ed16cad35eb027788332d871e8d8bb9d2", "year": 2020, "venue": "CHI", "alt_text": "Figure 2 shows three images in succession. 2a shows a child holding the polystyrene ball with a small white button speaker attached to it and he is pressing it with his left hand. 2b shows a child and an adult sitting on the floor of the school gym, the adult is holding the prototype ball and the child is holding a pen and drawing on it. 2c shows child and therapist on a seesaw with the ball on the seesaw between them.", "levels": null, "corpus_id": 218482476, "sentences": ["Figure 2 shows three images in succession.", "2a shows a child holding the polystyrene ball with a small white button speaker attached to it and he is pressing it with his left hand.", "2b shows a child and an adult sitting on the floor of the school gym, the adult is holding the prototype ball and the child is holding a pen and drawing on it.", "2c shows child and therapist on a seesaw with the ball on the seesaw between them."], "caption": "Figure 2. Prototype 1 images showing; a) Freddy pressing the button speaker, b) Andy drawing on the ball in-situ and c) Andy and the therapist on the seesaw with the ball", "local_uri": ["61e4009ed16cad35eb027788332d871e8d8bb9d2_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Self-Expression by Design: Co-Designing the ExpressiBall with Minimally-Verbal Children on the Autism Spectrum", "pdf_hash": "61e4009ed16cad35eb027788332d871e8d8bb9d2", "year": 2020, "venue": "CHI", "alt_text": "Figure 3 shows three images in succession. 3a shows Prototype 2 in the lab. It is a soft fabric ball with polka dot blue fabric and a black hessian circle resembling a microphone. 3b shows The researcher is holding the ball and Hugh is sitting on a chair, pressing his mouth to the hessian material. 3c Hugh recording into the ball via his talker. The researcher is holding the ball which is lit up blue and Hugh is pressing \u201cmouth\u201d on his taker and looking at the ball.", "levels": null, "corpus_id": 218482476, "sentences": ["Figure 3 shows three images in succession.", "3a shows Prototype 2 in the lab.", "It is a soft fabric ball with polka dot blue fabric and a black hessian circle resembling a microphone.", "3b shows The researcher is holding the ball and Hugh is sitting on a chair, pressing his mouth to the hessian material.", "3c Hugh recording into the ball via his talker.", "The researcher is holding the ball which is lit up blue and Hugh is pressing \u201cmouth\u201d on his taker and looking at the ball."], "caption": "Figure 3. Prototype 2; a) deconstructed in the lab, b) Hugh touching the ball to lips c) recording into the ball via his talker", "local_uri": ["61e4009ed16cad35eb027788332d871e8d8bb9d2_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Self-Expression by Design: Co-Designing the ExpressiBall with Minimally-Verbal Children on the Autism Spectrum", "pdf_hash": "61e4009ed16cad35eb027788332d871e8d8bb9d2", "year": 2020, "venue": "CHI", "alt_text": "Figure 4 shows three images in succession of prototype 3. 4a shows the high-fidelity prototype deconstructed in the lab, with accelerometer and LEDs on show. 4b shows CAD imagery of the internal housing for the technology. 4c shows Rory passing the prototype to Liz who is sitting on the trampoline in the outdoor play area of the school.", "levels": null, "corpus_id": 218482476, "sentences": ["Figure 4 shows three images in succession of prototype 3.", "4a shows the high-fidelity prototype deconstructed in the lab, with accelerometer and LEDs on show.", "4b shows CAD imagery of the internal housing for the technology.", "4c shows Rory passing the prototype to Liz who is sitting on the trampoline in the outdoor play area of the school."], "caption": "Figure 4. Prototype 3; a) deconstructed, b) in CAD imagery, and", "local_uri": ["61e4009ed16cad35eb027788332d871e8d8bb9d2_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Refuge Tech: An Assets-Based Approach to Refugee Resettlement", "pdf_hash": "0d0b85da3cd5196fe27962b2983b4741741d8189", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "https://lh5.googleusercontent.com/y3wxA7rGD3BAXS9HaV0EbrurxzrJ3FgNq4sYmY00wi6PDUSTlH-iqA_60yOtFiXYTxLOEz0Ma6Ev9vauv-loe8arAioHq6_d4Ol-dDp0cHmYfcVBdWK6qevzGxz4fjA8mqOLqp6C", "levels": null, "corpus_id": 5070489, "sentences": ["https://lh5.googleusercontent.com/y3wxA7rGD3BAXS9HaV0EbrurxzrJ3FgNq4sYmY00wi6PDUSTlH-iqA_60yOtFiXYTxLOEz0Ma6Ev9vauv-loe8arAioHq6_d4Ol-dDp0cHmYfcVBdWK6qevzGxz4fjA8mqOLqp6C"], "caption": "", "local_uri": ["0d0b85da3cd5196fe27962b2983b4741741d8189_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Refuge Tech: An Assets-Based Approach to Refugee Resettlement", "pdf_hash": "0d0b85da3cd5196fe27962b2983b4741741d8189", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "https://lh3.googleusercontent.com/rNKGUvOaUoor0LpYEnkdJaOmbdYBSmIpSzdz82mLGkkyDwTG9az48AMkL7KnEYEMakLyjUAN6HHAEkxEi2Ao4oJf-nboigONmvceKrtek1UE8rL_CCo0am6W_a_S8gt7aJm39Wds", "levels": null, "corpus_id": 5070489, "sentences": ["https://lh3.googleusercontent.com/rNKGUvOaUoor0LpYEnkdJaOmbdYBSmIpSzdz82mLGkkyDwTG9az48AMkL7KnEYEMakLyjUAN6HHAEkxEi2Ao4oJf-nboigONmvceKrtek1UE8rL_CCo0am6W_a_S8gt7aJm39Wds"], "caption": "", "local_uri": ["0d0b85da3cd5196fe27962b2983b4741741d8189_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Refuge Tech: An Assets-Based Approach to Refugee Resettlement", "pdf_hash": "0d0b85da3cd5196fe27962b2983b4741741d8189", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "https://lh6.googleusercontent.com/oleIKB4t5M1iszo1HBiZ4trk74yq4KmZf3sHCyK9vCMrXV2hQxDC0tj8Sk94Fu2Fv7H3jVYCwt_6Ck6tALczXTyPiEyU_6Mj53S28znG_-wzp9mHm-2f6NWGFQJZkSAullNw7zVQ", "levels": null, "corpus_id": 5070489, "sentences": ["https://lh6.googleusercontent.com/oleIKB4t5M1iszo1HBiZ4trk74yq4KmZf3sHCyK9vCMrXV2hQxDC0tj8Sk94Fu2Fv7H3jVYCwt_6Ck6tALczXTyPiEyU_6Mj53S28znG_-wzp9mHm-2f6NWGFQJZkSAullNw7zVQ"], "caption": "", "local_uri": ["0d0b85da3cd5196fe27962b2983b4741741d8189_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Beep Beep: Building Trust with Sound", "pdf_hash": "a563be6f98d7e976d0cb77f5242eb35cee37b3f4", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "three illustrations of devices that provide sound feedback: an elevator, a self-driving car and a smartphone", "levels": null, "corpus_id": 218483640, "sentences": ["three illustrations of devices that provide sound feedback: an elevator, a self-driving car and a smartphone"], "caption": "", "local_uri": ["a563be6f98d7e976d0cb77f5242eb35cee37b3f4_Image_002.png"], "annotated": false, "compound": false}
{"title": "Statewide Cycloplan: Bicycle Planning Tool & Participatory GIS", "pdf_hash": "435100c7f692f6b7b9b33d95c0cfb3204dc1c1cc", "year": 2015, "venue": "", "alt_text": "This figure shows the landing page screen for Cyclopath. The window on the left enables users to request a bike route. The window on the right contains a zoomable map of the state of Minnesota, with major bicycle trails emphasized.", "levels": null, "corpus_id": 127761451, "sentences": ["This figure shows the landing page screen for Cyclopath.", "The window on the left enables users to request a bike route.", "The window on the right contains a zoomable map of the state of Minnesota, with major bicycle trails emphasized."], "caption": "Figure 1 Current Cyclopath Landing Page.", "local_uri": ["435100c7f692f6b7b9b33d95c0cfb3204dc1c1cc_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Statewide Cycloplan: Bicycle Planning Tool & Participatory GIS", "pdf_hash": "435100c7f692f6b7b9b33d95c0cfb3204dc1c1cc", "year": 2015, "venue": "", "alt_text": "This figure illustrates the \u201csegmentization\u201d problem. It highlights a section of road that appears to intersect many other road segments. However, we do not know a priori whether these are actual intersections, or instead rather this road section might cross over other roadways on a bridge or under in a tunnel.", "levels": null, "corpus_id": 127761451, "sentences": ["This figure illustrates the \u201csegmentization\u201d problem.", "It highlights a section of road that appears to intersect many other road segments.", "However, we do not know a priori whether these are actual intersections, or instead rather this road section might cross over other roadways on a bridge or under in a tunnel."], "caption": "", "local_uri": ["435100c7f692f6b7b9b33d95c0cfb3204dc1c1cc_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Can Videogame Players Inform Better Scientific Visualization'", "pdf_hash": "d94acb43f3017f98e959fa2357eb426c8cb84ae8", "year": 2016, "venue": "CHI PLAY", "alt_text": "https://lh5.googleusercontent.com/Lp8WpUNC0zQdP_uCEE3Be6U2VZe4yGo_gnQGyxqLojqWHHBvZr30PIXEBVoxQy-lVfr83vnq5p_-I4HNQvnBtVh_i3SZX05DURU65YSWLg2Rprzs3FCADjf2VsdkxDkS0xj8C52A", "levels": null, "corpus_id": 15621238, "sentences": ["https://lh5.googleusercontent.com/Lp8WpUNC0zQdP_uCEE3Be6U2VZe4yGo_gnQGyxqLojqWHHBvZr30PIXEBVoxQy-lVfr83vnq5p_-I4HNQvnBtVh_i3SZX05DURU65YSWLg2Rprzs3FCADjf2VsdkxDkS0xj8C52A"], "caption": "", "local_uri": ["d94acb43f3017f98e959fa2357eb426c8cb84ae8_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "The picture shows the simplified font design for BLV users. All letters and numbers are composed of three basic shapes, straight lines, circles, and hooks. For each character, the starting point of each stroke as well as the sequence to write the basic shapes are illustrated. For example, letter b is composed of a long vertical line with a right semicircle at the lower half of the line. The starting point is the top of the vertical line, the first part is to write downwards to write a vertical line, and the second part is to write the right semicircle from the bottom of the shape, and then close the semicircle on the vertical line.", "levels": null, "corpus_id": 233987356, "sentences": ["The picture shows the simplified font design for BLV users.", "All letters and numbers are composed of three basic shapes, straight lines, circles, and hooks.", "For each character, the starting point of each stroke as well as the sequence to write the basic shapes are illustrated.", "For example, letter b is composed of a long vertical line with a right semicircle at the lower half of the line.", "The starting point is the top of the vertical line, the first part is to write downwards to write a vertical line, and the second part is to write the right semicircle from the bottom of the shape, and then close the semicircle on the vertical line."], "caption": "", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "This picture presents the frontend and backend of LightWrite. On the left is the frontend of LightWrite, which consists of three different modules, basic stroke training module, character learning module and practicing/testing/free-writing module. In character learning module, the upper and lower part of left edge is used to navigate to the previous and next letter, and the right edge is used to clean the current handwriting and restart learning the character. In Practicing/Testing/Free-Writing Module, the left edge is used to trigger upload and hint(only in practicing module), and the left edge is to clean the screen. All modules except for the basic stroke modules talks to the backend to receive character recognition result or suggestions to improve handwriting. On the right is the backend of LightWrite, with two functions of recognition and suggestion. The recognition is done by a four-layer CNN model. The suggestions are done by an ad-hoc algorithm to provide suggestions such as \"please write the vertical line straight\", and \"please move the ending stroke to the right\".", "levels": null, "corpus_id": 233987356, "sentences": ["This picture presents the frontend and backend of LightWrite.", "On the left is the frontend of LightWrite, which consists of three different modules, basic stroke training module, character learning module and practicing/testing/free-writing module.", "In character learning module, the upper and lower part of left edge is used to navigate to the previous and next letter, and the right edge is used to clean the current handwriting and restart learning the character.", "In Practicing/Testing/Free-Writing Module, the left edge is used to trigger upload and hint(only in practicing module), and the left edge is to clean the screen.", "All modules except for the basic stroke modules talks to the backend to receive character recognition result or suggestions to improve handwriting.", "On the right is the backend of LightWrite, with two functions of recognition and suggestion.", "The recognition is done by a four-layer CNN model.", "The suggestions are done by an ad-hoc algorithm to provide suggestions such as \"please write the vertical line straight\", and \"please move the ending stroke to the right\"."], "caption": "", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "The picture shows the group for 26 letters. There are 5 or 6 letters on each group. In group 1, there are l, i, j, t, f, with j introduced based on the shape of i, and f based on the shape of t. In group2 are v, w, x, z, and s, with w based on v. A helper shape in horizontal symmetry of z is introduced, and s is based on this helper shape. In group3 are b, p, d, a, q, g, with p, d and a based on b, and q based on a, and g based on q. In group4 are h, n, m , u and y, with n based on h, m and u based on n, and y based on u. In group 5 are o, c, e, r, and k, with c based on o, and e based on c. r was based on a helper shape with the curve of the last part of r turned into a straight line for better understanding. Group 1 to 5 are from easy to difficult.", "levels": null, "corpus_id": 233987356, "sentences": ["The picture shows the group for 26 letters.", "There are 5 or 6 letters on each group.", "In group 1, there are l, i, j, t, f, with j introduced based on the shape of i, and f based on the shape of t. In group2 are v, w, x, z, and s, with w based on v. A helper shape in horizontal symmetry of z is introduced, and s is based on this helper shape.", "In group3 are b, p, d, a, q, g, with p, d and a based on b, and q based on a, and g based on q. In group4 are h, n, m , u and y, with n based on h, m and u based on n, and y based on u. In group 5 are o, c, e, r, and k, with c based on o, and e based on c. r was based on a helper shape with the curve of the last part of r turned into a straight line for better understanding.", "Group 1 to 5 are from easy to difficult."], "caption": "Figure 4: Seven basic strokes provided in the basic stroke learning module of LightWrite.", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "Box plot of the time for the participants to learn 26 letters(in seconds). The median value for each letter varies. Letter o has the lowest median(20.095), and letter f has the largest median(81.526), and most(19 letters) are below 50 seconds. The ranges from upper quartile to lower quartile also varies for each letter. Letter a, c, o has small quartile ranges but a few outliers, and letter f, n, q, s has large quartile ranges.", "levels": [[1], [3], [2], [3], [3]], "corpus_id": 233987356, "sentences": ["Box plot of the time for the participants to learn 26 letters(in seconds).", "The median value for each letter varies.", "Letter o has the lowest median(20.095), and letter f has the largest median(81.526), and most(19 letters) are below 50 seconds.", "The ranges from upper quartile to lower quartile also varies for each letter.", "Letter a, c, o has small quartile ranges but a few outliers, and letter f, n, q, s has large quartile ranges."], "caption": "", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "Bar plot of participants agreement on the the 11 statements. The first four statements regarding the overall idea of learning handwriting on smartphone are: 1, \"It is meaningful to me to learn handwritten letters and digits on a smartphone.\"; 2, \"I am willing to learn handwritten letters and digits on a smartphone.\"; 3, \"Learning to write with my finger helps me learn to write with pens.\"; 4, \"I feel a sense of accomplishment by learning to write letters and digits.\". Statements 5 to 11 are about the experience with LighWrite: 5, \"LightWrite can successfully teach me how to write letters and digits.\"; 6, \"It is easy to learn the usage of LightWrite.\"; 7, \"It is easy to memorize the usage of LightWrite.\"; 8, \"I will be happy to download LightWrite and use it to learn to handwrite if it is launched in the app store.\"; 9, \"The touch-vibration feedback in basic stroke learning mode can help me better understand and write the basic strokes.\"; 10, \"Voice instructions in character learning mode can help me understand the shape of the characters.\"; 11, \"Voice feedback provided in the character learning module can let me know how to write better.\". All statements received an average score above 6.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987356, "sentences": ["Bar plot of participants agreement on the the 11 statements.", "The first four statements regarding the overall idea of learning handwriting on smartphone are: 1, \"It is meaningful to me to learn handwritten letters and digits on a smartphone.\";", "2, \"I am willing to learn handwritten letters and digits on a smartphone.\";", "3, \"Learning to write with my finger helps me learn to write with pens.\";", "4, \"I feel a sense of accomplishment by learning to write letters and digits.\".", "Statements 5 to 11 are about the experience with LighWrite: 5, \"LightWrite can successfully teach me how to write letters and digits.\";", "6, \"It is easy to learn the usage of LightWrite.\";", "7, \"It is easy to memorize the usage of LightWrite.\";", "8, \"I will be happy to download LightWrite and use it to learn to handwrite if it is launched in the app store.\";", "9, \"The touch-vibration feedback in basic stroke learning mode can help me better understand and write the basic strokes.\";", "10, \"Voice instructions in character learning mode can help me understand the shape of the characters.\";", "11, \"Voice feedback provided in the character learning module can let me know how to write better.\".", "All statements received an average score above 6."], "caption": "", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_009.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "For the box plot, the median for three stages are 1, 7 and 22 respectively. The range between the lower and the upper quartile is the smallest for stage 0, and is the largest for stage 1. The line plot for each participants' performance for three stages indicate that all participants improved from stage 0 to stage 2.", "levels": [[2], [2], [3]], "corpus_id": 233987356, "sentences": ["For the box plot, the median for three stages are 1, 7 and 22 respectively.", "The range between the lower and the upper quartile is the smallest for stage 0, and is the largest for stage 1.", "The line plot for each participants' performance for three stages indicate that all participants improved from stage 0 to stage 2."], "caption": "", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_010.png"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "pdf_hash": "1bd67672e9376feaac80770695717b13ece2c47b", "year": 2021, "venue": "CHI", "alt_text": "The picture demonstrates two writing samples for stage 0 and 2 from participant number 10 and participant number 12. In stage 0, participant 10 tried writing m but ended up with a shape similar to a shape in horizontal symmetry to letter n, and in stage 2 could write recognizable letter m. For participant 12, they skipped letter k in the test of stage 0, but could write a recognizable k at stage 2.", "levels": null, "corpus_id": 233987356, "sentences": ["The picture demonstrates two writing samples for stage 0 and 2 from participant number 10 and participant number 12.", "In stage 0, participant 10 tried writing m but ended up with a shape similar to a shape in horizontal symmetry to letter n, and in stage 2 could write recognizable letter m. For participant 12, they skipped letter k in the test of stage 0, but could write a recognizable k at stage 2."], "caption": "", "local_uri": ["1bd67672e9376feaac80770695717b13ece2c47b_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "An Oldy's Lament: Poem of Resistance and Resilience of the 'Othered' in Technology Colonisation", "pdf_hash": "764e9e69243828410a6bfc18fe27f3841d7f5ba5", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Julie sitting on her side of the table, looking at her co-writers, raising both hands explaining her point in the discussion.", "levels": null, "corpus_id": 218482651, "sentences": ["Julie sitting on her side of the table, looking at her co-writers, raising both hands explaining her point in the discussion."], "caption": "", "local_uri": ["764e9e69243828410a6bfc18fe27f3841d7f5ba5_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing", "pdf_hash": "dbe2937c5aa448cf59546dbf1fd1e8931f9ac20c", "year": 2017, "venue": "UIST", "alt_text": "Figure 7. Confusion matrices of cross validation and leave-one-session-out evaluation", "levels": null, "corpus_id": 35563511, "sentences": ["Figure 7.", "Confusion matrices of cross validation and leave-one-session-out evaluation"], "caption": "Figure 7. Confusion matrices. Left: cross validation accuracies; Right: leave-one-session-out accuracies.", "local_uri": ["dbe2937c5aa448cf59546dbf1fd1e8931f9ac20c_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "FoveAR: Combining an Optically See-Through Near-Eye Display with Projector-Based Spatial Augmented Reality", "pdf_hash": "cc1b11debe50be088e92c4431947e4e36dc50e84", "year": 2015, "venue": "UIST", "alt_text": "Figure consists of 4 subfigures. A) An example optically see-through display glasses showing Lumus DK-32. B) When wearing AR glasses the viewer is presented with a limited field of view, impacting their sense of immersion and presence. C) FoveAR extends the glasses experience by adding a view-dependent projection in the environment. D) shows what the projector is projecting in the periphery.  Note: All first person images in this paper were photographed through the glasses.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 34568739, "sentences": ["Figure consists of 4 subfigures.", "A) An example optically see-through display glasses showing Lumus DK-32.", "B) When wearing AR glasses the viewer is presented with a limited field of view, impacting their sense of immersion and presence.", "C) FoveAR extends the glasses experience by adding a view-dependent projection in the environment.", "D) shows what the projector is projecting in the periphery.", "Note: All first person images in this paper were photographed through the glasses."], "caption": "Figure 1. When using optically see-through display glasses (A), the viewer is presented with a limited field of view, impacting their sense of immersion and presence (B). FoveAR extends the glasses experience (C) by adding a view-dependent projection in the environment (D). Note: All first person images in this paper were photographed through the glasses. Due to the relative brightness of the glasses display and limited dynamic range of the camera, projected content appears subjectively dimmer than in reality.", "local_uri": ["cc1b11debe50be088e92c4431947e4e36dc50e84_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FoveAR: Combining an Optically See-Through Near-Eye Display with Projector-Based Spatial Augmented Reality", "pdf_hash": "cc1b11debe50be088e92c4431947e4e36dc50e84", "year": 2015, "venue": "UIST", "alt_text": "The system diagram of the sensor tape, showing components of the master and slave nodes. The master node contains an Atmega328 microcontroller, a Bluetooth module, and a USB transceiver. Each slave contains an Atmega328 microcontroller, proximity sensor, 6-axis inertial measurement unit, and a light sensor. Each node is connected to the next by five wires: power, ground, and three communication wires for the global I2C bus and a peer-to-peer.", "levels": null, "corpus_id": 34568739, "sentences": ["The system diagram of the sensor tape, showing components of the master and slave nodes.", "The master node contains an Atmega328 microcontroller, a Bluetooth module, and a USB transceiver.", "Each slave contains an Atmega328 microcontroller, proximity sensor, 6-axis inertial measurement unit, and a light sensor.", "Each node is connected to the next by five wires: power, ground, and three communication wires for the global I2C bus and a peer-to-peer."], "caption": "Figure 2. FoveAR hardware: A) Lumus DK-32 display with retro-reflective markers, B) projector and Kinect mounted on the ceiling, and C) OptiTrack Flex 3 motion tracking system.", "local_uri": ["cc1b11debe50be088e92c4431947e4e36dc50e84_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "FoveAR: Combining an Optically See-Through Near-Eye Display with Projector-Based Spatial Augmented Reality", "pdf_hash": "cc1b11debe50be088e92c4431947e4e36dc50e84", "year": 2015, "venue": "UIST", "alt_text": "Communication flowchart, showing how the master and the slaves interact. The initial slave address assignments are done over a peer-to-peer network. Later communications use the global I2C bus allowing the slaves nodes to send and receive data to the master.", "levels": [[-1], [-1], [-1]], "corpus_id": 34568739, "sentences": ["Communication flowchart, showing how the master and the slaves interact.", "The initial slave address assignments are done over a peer-to-peer network.", "Later communications use the global I2C bus allowing the slaves nodes to send and receive data to the master."], "caption": "Figure 3. Three FoveAR experiences: A) 3D helicopter animation, B) AR shooter game; C-D) 3D life-size telepresence.", "local_uri": ["cc1b11debe50be088e92c4431947e4e36dc50e84_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FoveAR: Combining an Optically See-Through Near-Eye Display with Projector-Based Spatial Augmented Reality", "pdf_hash": "cc1b11debe50be088e92c4431947e4e36dc50e84", "year": 2015, "venue": "UIST", "alt_text": "Illustration of four content combinations demonstrated in FoveAR. Left side shows the images presented in the glasses, the middle shows what the projector is displaying and the right shows how it looks when combined to the user.", "levels": null, "corpus_id": 34568739, "sentences": ["Illustration of four content combinations demonstrated in FoveAR.", "Left side shows the images presented in the glasses, the middle shows what the projector is displaying and the right shows how it looks when combined to the user."], "caption": "Figure 4. Four content combinations demonstrated in FoveAR.", "local_uri": ["cc1b11debe50be088e92c4431947e4e36dc50e84_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Camera Distances and Shot Sizes in Cinematic Virtual Reality", "pdf_hash": "b4b7cfb5d19875d870faef4c4511b57cb0982593", "year": 2021, "venue": "IMX", "alt_text": "Shots of video 01, from left to right: intimate distance (0.45 m), personal distance (1 m), social distance (2.50 m), public distance (5 m). In these images a flter was used for anonymization.", "levels": null, "corpus_id": 235612765, "sentences": ["Shots of video 01, from left to right: intimate distance (0.45 m), personal distance (1 m), social distance (2.50 m), public distance (5 m).", "In these images a flter was used for anonymization."], "caption": "Figure 3: Shots of video 01, from left to right: intimate dis- tance (0.45 m), personal distance (1 m), social distance (2.50 m), public distance (5 m). In these images a flter was used for anonymization.", "local_uri": ["b4b7cfb5d19875d870faef4c4511b57cb0982593_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Camera Distances and Shot Sizes in Cinematic Virtual Reality", "pdf_hash": "b4b7cfb5d19875d870faef4c4511b57cb0982593", "year": 2021, "venue": "IMX", "alt_text": "Presence, Simulator Sickness and Experience scores with signifcant diferences between the test conditions (Likert scale from 0 to 6).", "levels": null, "corpus_id": 235612765, "sentences": ["Presence, Simulator Sickness and Experience scores with signifcant diferences between the test conditions (Likert scale from 0 to 6)."], "caption": "Figure 4: Presence, Simulator Sickness and Experience scores with signifcant diferences between the test condi- tions (Likert scale from 0 to 6).", "local_uri": ["b4b7cfb5d19875d870faef4c4511b57cb0982593_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Camera Distances and Shot Sizes in Cinematic Virtual Reality", "pdf_hash": "b4b7cfb5d19875d870faef4c4511b57cb0982593", "year": 2021, "venue": "IMX", "alt_text": "Relation to the scores with signifcant diferences between the test conditions (Likert scale from 0 to 6).", "levels": null, "corpus_id": 235612765, "sentences": ["Relation to the scores with signifcant diferences between the test conditions (Likert scale from 0 to 6)."], "caption": "Figure 6: Relation to the scores with signifcant diferences between the test conditions (Likert scale from 0 to 6).", "local_uri": ["b4b7cfb5d19875d870faef4c4511b57cb0982593_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Camera Distances and Shot Sizes in Cinematic Virtual Reality", "pdf_hash": "b4b7cfb5d19875d870faef4c4511b57cb0982593", "year": 2021, "venue": "IMX", "alt_text": "Scores of the items with signifcantly diferent results (Likert scale from 0 to 6)", "levels": null, "corpus_id": 235612765, "sentences": ["Scores of the items with signifcantly diferent results (Likert scale from 0 to 6)"], "caption": "", "local_uri": ["b4b7cfb5d19875d870faef4c4511b57cb0982593_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Challenges in Designing Visual Analytics for Environmental Acoustic Monitoring", "pdf_hash": "87b2bca415d18c4a571cad1ffb157ff2479622e8", "year": 2016, "venue": "Conference on Designing Interactive Systems", "alt_text": "This figures show the context of big environmental sound data collected to investigate various ecological questions", "levels": [[-1]], "corpus_id": 10217417, "sentences": ["This figures show the context of big environmental sound data collected to investigate various ecological questions"], "caption": "", "local_uri": ["87b2bca415d18c4a571cad1ffb157ff2479622e8_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Auth+Track: Enabling Authentication Free Interaction on Smartphone by Continuous User Tracking", "pdf_hash": "e803755c483f1cc518e573ed9608ae8a7b749b6d", "year": 2021, "venue": "CHI", "alt_text": "Left: The user is authenticating with the phone. Right: Three real-life scenarios: using the phone, gripping the phone while walking,  and putting the phone aside.", "levels": null, "corpus_id": 233987014, "sentences": ["Left: The user is authenticating with the phone.", "Right: Three real-life scenarios: using the phone, gripping the phone while walking,  and putting the phone aside."], "caption": "Figure 1: (a) A user is authenticating and the Auth+Track system begins keeping track of the authenticated user. (b) The Auth+Track system continuously tracks the authenti- cated user in multiple scenes: The user is working while the phone is placed on table; the user is using the phone; the user is gripping the phone and walking. When the user leaves the sensing range of the Auth+Track system, the phone au- tomatically locks.", "local_uri": ["e803755c483f1cc518e573ed9608ae8a7b749b6d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Auth+Track: Enabling Authentication Free Interaction on Smartphone by Continuous User Tracking", "pdf_hash": "e803755c483f1cc518e573ed9608ae8a7b749b6d", "year": 2021, "venue": "CHI", "alt_text": "The traditional authentication model consists of three states: 1) User on, 2) Idle, and 3) Locked. The transition rules among the three states include: no operation (1 to 2), operation (2 to 1), operation or biometric signal (1 to 1), active lock behavior (1 to 3), successful authentication (3 to 1), waiting for T seconds (2 to 3), and failed authentication (3 to 3). The Auth+Track model consists of three states: 1) User on, 2) User around, 3) User off, and 4) Locked. The transition rules among the four states include: no operation and tracking succeeds (1 to 2), operation and tracking succeeds (2 to 1), operation and tracking succeeds (1 to 1), tracking succeeds (2 to 2), user leaves (tracking fails) (1 to 3), operation (3 to 3), active lock behavior (1 to 4), successful authentication (4 to 1), and waiting for T seconds (3 to 4).", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987014, "sentences": ["The traditional authentication model consists of three states: 1) User on, 2) Idle, and 3) Locked.", "The transition rules among the three states include: no operation (1 to 2), operation (2 to 1), operation or biometric signal (1 to 1), active lock behavior (1 to 3), successful authentication (3 to 1), waiting for T seconds (2 to 3), and failed authentication (3 to 3).", "The Auth+Track model consists of three states: 1) User on, 2) User around, 3) User off, and 4) Locked.", "The transition rules among the four states include: no operation and tracking succeeds (1 to 2), operation and tracking succeeds (2 to 1), operation and tracking succeeds (1 to 1), tracking succeeds (2 to 2), user leaves (tracking fails) (1 to 3), operation (3 to 3), active lock behavior (1 to 4), successful authentication (4 to 1), and waiting for T seconds (3 to 4)."], "caption": "Existing auth model\u2019s state transition graph                                     (b) The Auth+Track model\u2019s state transition graphFigure 2: Authentication Model Comparison. (b) Auth+Track splits the \u201cIdle\u201d state in (a) traditional authentication model into two states \u2013 \u201cUser Around\u201d and \u201cUser Of\u201d \u2013 to distinguish a user\u2019s status as being around-device or being absent.", "local_uri": ["e803755c483f1cc518e573ed9608ae8a7b749b6d_Image_008.jpg", "e803755c483f1cc518e573ed9608ae8a7b749b6d_Image_009.jpg"], "annotated": false, "compound": true}
{"title": "Auth+Track: Enabling Authentication Free Interaction on Smartphone by Continuous User Tracking", "pdf_hash": "e803755c483f1cc518e573ed9608ae8a7b749b6d", "year": 2021, "venue": "CHI", "alt_text": "Left: A 160-degree fisheye camera is fixed to the top of the phone. Right: A sample frame captured by the camera, the bottom region of which covers the gripping hand.", "levels": null, "corpus_id": 233987014, "sentences": ["Left: A 160-degree fisheye camera is fixed to the top of the phone.", "Right: A sample frame captured by the camera, the bottom region of which covers the gripping hand."], "caption": "", "local_uri": ["e803755c483f1cc518e573ed9608ae8a7b749b6d_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Auth+Track: Enabling Authentication Free Interaction on Smartphone by Continuous User Tracking", "pdf_hash": "e803755c483f1cc518e573ed9608ae8a7b749b6d", "year": 2021, "venue": "CHI", "alt_text": "A frame in which the user is gripping the phone is on the top left. The near field hand region is first extracted for hand status classification. Meanwhile, a CNN-based model is used to label all the body keypoints in the frame. The features is then used to calculate high-level semantic information.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987014, "sentences": ["A frame in which the user is gripping the phone is on the top left.", "The near field hand region is first extracted for hand status classification.", "Meanwhile, a CNN-based model is used to label all the body keypoints in the frame.", "The features is then used to calculate high-level semantic information."], "caption": "", "local_uri": ["e803755c483f1cc518e573ed9608ae8a7b749b6d_Image_021.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Auth+Track: Enabling Authentication Free Interaction on Smartphone by Continuous User Tracking", "pdf_hash": "e803755c483f1cc518e573ed9608ae8a7b749b6d", "year": 2021, "venue": "CHI", "alt_text": "Subjective ratings in 7 aspects: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, Frustration, and Willing-to-use. With- Auth+Track outperforms without- in temporal demand, performance, and willing-to-use.", "levels": null, "corpus_id": 233987014, "sentences": ["Subjective ratings in 7 aspects: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, Frustration, and Willing-to-use.", "With- Auth+Track outperforms without- in temporal demand, performance, and willing-to-use."], "caption": "", "local_uri": ["e803755c483f1cc518e573ed9608ae8a7b749b6d_Image_028.jpg"], "annotated": false, "compound": false}
{"title": "Scones: towards conversational authoring of sketches", "pdf_hash": "13f863ee97aeea1510acd9b7b76996f6bcab0845", "year": 2020, "venue": "IUI", "alt_text": "This figure shows the generation process of each scene object by the Transformer model of the Composition Proposer. For each scene, we pass in a start token, the object representation, the end token, the text tokens of the instructions respectively, then generate the final scene sequentially by passing in the current generated object to obtain the next object.", "levels": null, "corpus_id": 210939535, "sentences": ["This figure shows the generation process of each scene object by the Transformer model of the Composition Proposer.", "For each scene, we pass in a start token, the object representation, the end token, the text tokens of the instructions respectively, then generate the final scene sequentially by passing in the current generated object to obtain the next object."], "caption": "Figure 3: The Scene Layout Generation Process using the Transformer Model of the Composition Proposer.", "local_uri": ["13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_007.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_008.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_013.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_014.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_015.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_016.png"], "annotated": false, "compound": true}
{"title": "Scones: towards conversational authoring of sketches", "pdf_hash": "13f863ee97aeea1510acd9b7b76996f6bcab0845", "year": 2020, "venue": "IUI", "alt_text": "This figure shows examples of multiple kinds of trees generated by the Object Generator. In each row, from left to right we show the clip art used to guide the generation, the mask that is generated from the clip art, and the generated sketch. On the top row, the clip art shows a cartoon-like pointy pine tree. The mask generated is largely triangular. And the generated sketches show various stroke-based trees with triangular leaves. The second row has a clip art of a cartoon-like oak tree, the mask appears to be an oval shape with curved edges, and rectangular trunk sticking out from the bottom. The generated sketches also exhibit curved strokes for the leaves of the trees, and a solid trunk in the middle at the bottom. The final row is an apple tree with round leaves, with a mask that looks like a circle with a rectangular trunk sticking out from the bottom. The resultant sketches all demonstrate similar characteristics, with a circular stroke depicting the leaves of the tree, closing with straight trunks at the bottom.", "levels": null, "corpus_id": 210939535, "sentences": ["This figure shows examples of multiple kinds of trees generated by the Object Generator.", "In each row, from left to right we show the clip art used to guide the generation, the mask that is generated from the clip art, and the generated sketch.", "On the top row, the clip art shows a cartoon-like pointy pine tree.", "The mask generated is largely triangular. And the generated sketches show various stroke-based trees with triangular leaves.", "The second row has a clip art of a cartoon-like oak tree, the mask appears to be an oval shape with curved edges, and rectangular trunk sticking out from the bottom.", "The generated sketches also exhibit curved strokes for the leaves of the trees, and a solid trunk in the middle at the bottom.", "The final row is an apple tree with round leaves, with a mask that looks like a circle with a rectangular trunk sticking out from the bottom.", "The resultant sketches all demonstrate similar characteristics, with a circular stroke depicting the leaves of the tree, closing with straight trunks at the bottom."], "caption": "", "local_uri": ["13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_037.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_038.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_039.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_040.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_041.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_042.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_043.jpg", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_044.jpg", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_045.jpg"], "annotated": false, "compound": true}
{"title": "Scones: towards conversational authoring of sketches", "pdf_hash": "13f863ee97aeea1510acd9b7b76996f6bcab0845", "year": 2020, "venue": "IUI", "alt_text": "This figure shows examples of racquets generated by the Object Generator. In each row, from left to right we show the clip art used to guide the generation, the mask that is generated from the clip art, and the generated sketch. On the top row, the clip art shows a cartoon-like racquet with head tilting to the right, and the bottom row shows the same racquet point to the left. The mask generated is oval-shaped for the head of the racquet, and rectangular for the handle of the racquet, tilting right for the first row, and left for the second row. The generated sketches follow this pattern with sketch strokes of racquets facing right in the top row, and left in the bottom row.", "levels": null, "corpus_id": 210939535, "sentences": ["This figure shows examples of racquets generated by the Object Generator.", "In each row, from left to right we show the clip art used to guide the generation, the mask that is generated from the clip art, and the generated sketch.", "On the top row, the clip art shows a cartoon-like racquet with head tilting to the right, and the bottom row shows the same racquet point to the left.", "The mask generated is oval-shaped for the head of the racquet, and rectangular for the handle of the racquet, tilting right for the first row, and left for the second row.", "The generated sketches follow this pattern with sketch strokes of racquets facing right in the top row, and left in the bottom row."], "caption": "", "local_uri": ["13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_047.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_048.jpg", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_049.jpg", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_052.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_053.jpg", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_054.jpg", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_056.png", "13f863ee97aeea1510acd9b7b76996f6bcab0845_Image_057.jpg"], "annotated": false, "compound": true}
{"title": "Cross-campus collaboration: A scientometric and network case study of publication activity across two campuses of a single institution", "pdf_hash": "0d929f3ecdf4dc552aeefb5768c024403a0ad352", "year": 2013, "venue": "J. Assoc. Inf. Sci. Technol.", "alt_text": "Figure 1 Histograms for number of single\u2010campus articles (1999\u20132009; left). Histogram for number of cross\u2010campus articles (1999\u20132009; right).", "levels": null, "corpus_id": 6284837, "sentences": ["Figure 1 Histograms for number of single\u2010campus articles (1999\u20132009; left).", "Histogram for number of cross\u2010campus articles (1999\u20132009; right)."], "caption": "Figure 1 Histograms for number of single-campus articles (1999\u20132009; left). Histogram for number of cross- campus articles (1999\u20132009; right).\u200c", "local_uri": ["0d929f3ecdf4dc552aeefb5768c024403a0ad352_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Where Are My Parents?: Information Needs of Hospitalized Children", "pdf_hash": "323c894b7b83f14fd22b8d9b37401bf1de2bfb8e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Photo of three Styrofoam walkie-talkies. One is labeled \"doctor,\" one is labeled \"parent,\" and one is labeled \"kid.\"", "levels": null, "corpus_id": 218482717, "sentences": ["Photo of three Styrofoam walkie-talkies.", "One is labeled \"doctor,\" one is labeled \"parent,\" and one is labeled \"kid.\""], "caption": "", "local_uri": ["323c894b7b83f14fd22b8d9b37401bf1de2bfb8e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Where Are My Parents?: Information Needs of Hospitalized Children", "pdf_hash": "323c894b7b83f14fd22b8d9b37401bf1de2bfb8e", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Photo of straws that form an upside down square \"U\" shape, leading from one bottle covered with paper to another bottle covered with paper.", "levels": null, "corpus_id": 218482717, "sentences": ["Photo of straws that form an upside down square \"U\" shape, leading from one bottle covered with paper to another bottle covered with paper."], "caption": "", "local_uri": ["323c894b7b83f14fd22b8d9b37401bf1de2bfb8e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "An Accessible CAD Workflow Using Programming of 3D Models and Preview Rendering in A 2.5D Shape Display", "pdf_hash": "dd815cd78c8ec36d7ee8df967ba0df2dba841519", "year": 2018, "venue": "ASSETS", "alt_text": "A teaser image showing four images. Left is a section titled 'Code' and an image of the text editor with the 'Render to shapeShift' button highlighted. In the middle is a section titled 'Preview' and an image of a mug digitally rendered and next to it an image of the same mug rendered in the shape display. Right is a section titled '3D Print' and an image of a 3D printed white mug.", "levels": null, "corpus_id": 52059321, "sentences": ["A teaser image showing four images.", "Left is a section titled 'Code' and an image of the text editor with the 'Render to shapeShift' button highlighted.", "In the middle is a section titled 'Preview' and an image of a mug digitally rendered and next to it an image of the same mug rendered in the shape display.", "Right is a section titled '3D Print' and an image of a 3D printed white mug."], "caption": "Figure 1. We propose an accessible CAD work\ufb02ow where 3D models are generated through OpenSCAD, a script-based 3D modeling tool, and rendered at interactive speeds in shapeShift, a 2.5D shape display consisting of a grid of 12 \u00d7 24 actuated pins.", "local_uri": ["dd815cd78c8ec36d7ee8df967ba0df2dba841519_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "An Accessible CAD Workflow Using Programming of 3D Models and Preview Rendering in A 2.5D Shape Display", "pdf_hash": "dd815cd78c8ec36d7ee8df967ba0df2dba841519", "year": 2018, "venue": "ASSETS", "alt_text": "An image consisting of four photos arranged in a 2x2 grid. Top row: digital rendering of the 3D model (left) and model when first rendered on the display (right). Bottom row: the model rendered on the display and rotated 90 degrees around the positive x-axis (left), and around the positive y-axis (right)", "levels": null, "corpus_id": 52059321, "sentences": ["An image consisting of four photos arranged in a 2x2 grid.", "Top row: digital rendering of the 3D model (left) and model when first rendered on the display (right).", "Bottom row: the model rendered on the display and rotated 90 degrees around the positive x-axis (left), and around the positive y-axis (right)"], "caption": "Figure 2. Top: digital rendering of the 3D model (left) and model when \ufb01rst rendered on the display (right). Bottom: the model rendered on the display and rotated 90\u25e6 around the positive x-axis (left), and around the positive y-axis (right)", "local_uri": ["dd815cd78c8ec36d7ee8df967ba0df2dba841519_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Birdsound: enticing urban dwellers to engage with local birds around their home", "pdf_hash": "c541253c864b03702edb68d525b6a5d72c728706", "year": 2017, "venue": "OZCHI", "alt_text": "https://lh3.googleusercontent.com/JB46DiuaDeF_lCAIsmYGjq4FvbDaJ3C1eDrRyR9BJQwFoFZXQLBvyOjYggmzVUCK3GnwsZxJ4R2OcOepxk03e_hdOTNhlPRxQo3wrGJW6Z-uWupWE0VY9T0bEk70fxKcJA5QgLzdl4s", "levels": null, "corpus_id": 19786350, "sentences": ["https://lh3.googleusercontent.com/JB46DiuaDeF_lCAIsmYGjq4FvbDaJ3C1eDrRyR9BJQwFoFZXQLBvyOjYggmzVUCK3GnwsZxJ4R2OcOepxk03e_hdOTNhlPRxQo3wrGJW6Z-uWupWE0VY9T0bEk70fxKcJA5QgLzdl4s"], "caption": "", "local_uri": ["c541253c864b03702edb68d525b6a5d72c728706_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "User-defined Swarm Robot Control", "pdf_hash": "1ecdb37f4be9ebde700f0513bb99ab6dd4836c21", "year": 2020, "venue": "CHI", "alt_text": "Figure. 2:  Left side has the initial state and right side has the final state of the robots. On the left there are 20 robots in a tight circle formation near a person and on the right, there are the same 20 robots in a bigger circle formation near the person.", "levels": null, "corpus_id": 218483365, "sentences": ["Figure. 2:  Left side has the initial state and right side has the final state of the robots.", "On the left there are 20 robots in a tight circle formation near a person and on the right, there are the same 20 robots in a bigger circle formation near the person."], "caption": "", "local_uri": ["1ecdb37f4be9ebde700f0513bb99ab6dd4836c21_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "User-defined Swarm Robot Control", "pdf_hash": "1ecdb37f4be9ebde700f0513bb99ab6dd4836c21", "year": 2020, "venue": "CHI", "alt_text": "Figure. 3: A person is gesturing at 20 robots far away in a large circle formation. Behind the robots are a television monitor and camera to displaying pictorial prompt and recording the interaction respectively.", "levels": null, "corpus_id": 218483365, "sentences": ["Figure. 3: A person is gesturing at 20 robots far away in a large circle formation.", "Behind the robots are a television monitor and camera to displaying pictorial prompt and recording the interaction respectively."], "caption": "Figure 3. Setup for the control elicitation study: After being prompted through a television monitor, participants interact with 20 robots on a table while standing.", "local_uri": ["1ecdb37f4be9ebde700f0513bb99ab6dd4836c21_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Beyond Flat Displays", "pdf_hash": "8f525c156675e8ec433d272f4f5a5fe8f64f6d42", "year": 2016, "venue": "", "alt_text": "FIGURE 4. Capacitive sensor circuit printed on molded PA6 tank. Surface mount devices connected and interconnects printed using Aerosol Jet process. Project funded by: Bayerische Forschungsstiftung. Project Partners. (Photo courtesy of Neotech Services)", "levels": null, "corpus_id": 184665237, "sentences": ["FIGURE 4.", "Capacitive sensor circuit printed on molded PA6 tank.", "Surface mount devices connected and interconnects printed using Aerosol Jet process.", "Project funded by: Bayerische Forschungsstiftung. Project Partners. (Photo courtesy of Neotech Services)"], "caption": "Solidoodle 2 (<$500 printer) (Photo courtesy of gizmag.com)", "local_uri": ["8f525c156675e8ec433d272f4f5a5fe8f64f6d42_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "How to Trick AI: Users' Strategies for Protecting Themselves from Automatic Personality Assessment", "pdf_hash": "d6b3f3fd502be27b698065d14dc97a1bc56b5792", "year": 2020, "venue": "CHI", "alt_text": "Figure 1 depicts a spider web for each of our 21 participants, which shows the BFI-2 profile (grey) as well as the chatbot-generated actual (blue) and falsified profiles (red); the participant IDs associate the profiles to the quotes provided in the text. The large spider web shows the mean values over all participants. The five spider web corners represent the Big Five personality traits.", "levels": [[-1], [-1], [-1]], "corpus_id": 218483563, "sentences": ["Figure 1 depicts a spider web for each of our 21 participants, which shows the BFI-2 profile (grey) as well as the chatbot-generated actual (blue) and falsified profiles (red); the participant IDs associate the profiles to the quotes provided in the text.", "The large spider web shows the mean values over all participants.", "The five spider web corners represent the Big Five personality traits."], "caption": "Figure 1. Our study consisted of three phases: (A) We opened the study with questionnaires and a \ufb01rst interview. (B) In the \ufb01rst chatbot interaction, our participants chatted naturally with the chatbot and reviewed their personality pro\ufb01le. We collected insights by means of questionnaires and an interview.", "local_uri": ["d6b3f3fd502be27b698065d14dc97a1bc56b5792_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How to Trick AI: Users' Strategies for Protecting Themselves from Automatic Personality Assessment", "pdf_hash": "d6b3f3fd502be27b698065d14dc97a1bc56b5792", "year": 2020, "venue": "CHI", "alt_text": "Figure 3 shows that participants perceived accuracy of their actual profile is low for conscientiousness, rather low for openness, extraversion, and neuroticism but high for agreeableness.", "levels": null, "corpus_id": 218483563, "sentences": ["Figure 3 shows that participants perceived accuracy of their actual profile is low for conscientiousness, rather low for openness, extraversion, and neuroticism but high for agreeableness."], "caption": "", "local_uri": ["d6b3f3fd502be27b698065d14dc97a1bc56b5792_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "How to Trick AI: Users' Strategies for Protecting Themselves from Automatic Personality Assessment", "pdf_hash": "d6b3f3fd502be27b698065d14dc97a1bc56b5792", "year": 2020, "venue": "CHI", "alt_text": "Figure 2 shows our study procedure. Our study consisted of three phases: (A) We opened the study with questionnaires and a first interview. (B) In the first chatbot interaction, our participants chatted naturally with the chatbot and reviewed their personality profile. We collected insights by means of questionnaires and an interview. (C) The second interaction was equal to the first one, but participants had to strategically manipulate the chatbot to disguise their personality.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483563, "sentences": ["Figure 2 shows our study procedure.", "Our study consisted of three phases: (A) We opened the study with questionnaires and a first interview. (", "B) In the first chatbot interaction, our participants chatted naturally with the chatbot and reviewed their personality profile.", "We collected insights by means of questionnaires and an interview. (", "C) The second interaction was equal to the first one, but participants had to strategically manipulate the chatbot to disguise their personality."], "caption": "Figure 2. Each spider web shows the BFI-2 pro\ufb01le (grey) as well as the chatbot-generated actual (blue) and falsi\ufb01ed pro\ufb01les (red) of one partic- ipant; the participant IDs associate the pro\ufb01les to the quotes provided in the text. The large spider web shows the mean values over all partici- pants. The spider web corners represent the Big Five personality traits.", "local_uri": ["d6b3f3fd502be27b698065d14dc97a1bc56b5792_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "How to Trick AI: Users' Strategies for Protecting Themselves from Automatic Personality Assessment", "pdf_hash": "d6b3f3fd502be27b698065d14dc97a1bc56b5792", "year": 2020, "venue": "CHI", "alt_text": "The boxplots show the difference between the actual and the falsified profiles in percent. The highest mean difference was achieved for agreeableness on average.", "levels": [[1], [2]], "corpus_id": 218483563, "sentences": ["The boxplots show the difference between the actual and the falsified profiles in percent.", "The highest mean difference was achieved for agreeableness on average."], "caption": "Figure 4. The boxplots show the absolute difference between the actual", "local_uri": ["d6b3f3fd502be27b698065d14dc97a1bc56b5792_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "HoverSpace - Analyses of the Perceived Spatial Affordances of Hover Interaction Above Tabletop Surfaces", "pdf_hash": "d5fb83ada0dc1ac5b566d0694e9ea3aa5bc02ed4", "year": 2015, "venue": "INTERACT", "alt_text": "https://lh4.googleusercontent.com/J1vq9XOOFja_9zmGaqSLIm-MfEgSOiF0dv2l6NZOkoSCd26Q4nq7KCUBug5JxIJWzc7yqOCW3VR0_BARU2RdTyENYcMwJ1GL9LxrwpX_g_yOQbXc1YeFPpI6na3wFKD-8A", "levels": [[0]], "corpus_id": 6597295, "sentences": ["https://lh4.googleusercontent.com/J1vq9XOOFja_9zmGaqSLIm-MfEgSOiF0dv2l6NZOkoSCd26Q4nq7KCUBug5JxIJWzc7yqOCW3VR0_BARU2RdTyENYcMwJ1GL9LxrwpX_g_yOQbXc1YeFPpI6na3wFKD-8A"], "caption": "", "local_uri": ["d5fb83ada0dc1ac5b566d0694e9ea3aa5bc02ed4_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HoverSpace - Analyses of the Perceived Spatial Affordances of Hover Interaction Above Tabletop Surfaces", "pdf_hash": "d5fb83ada0dc1ac5b566d0694e9ea3aa5bc02ed4", "year": 2015, "venue": "INTERACT", "alt_text": "https://lh4.googleusercontent.com/FTkFQwNWXqDn-zAB3QyUA-mLq6YiUjeEykx6IP9ICHNgFLAOOjDAtP4c6BoVG7kQnDh5tHOpoAAi_MOmPfbplKgOmfxZ-sids0D-2Dg_qlW_8QbWzF6cIkA9bvsVgqE6dA", "levels": null, "corpus_id": 6597295, "sentences": ["https://lh4.googleusercontent.com/FTkFQwNWXqDn-zAB3QyUA-mLq6YiUjeEykx6IP9ICHNgFLAOOjDAtP4c6BoVG7kQnDh5tHOpoAAi_MOmPfbplKgOmfxZ-sids0D-2Dg_qlW_8QbWzF6cIkA9bvsVgqE6dA"], "caption": "(b)Fig. 4. Illustrations of the drawn hover volumes of participants for the (a) sphere target shape and the (b) cube target shape. The y-axis indicates the direction orthogonally to the tabletop surface at y=0, the z-axis increases towards the opposite end of the tabletop, and the x-axis is oriented laterally to the right of the tabletop. The drawing patterns of outlining the circular or rectangular regions and then filling the regions with zigzag patterns could be observed for many participants.We observed a difference between the cube target shape and the sphere target shape. For the sphere, all participants indicated a round hover space, often drawing circles at various distances from the object (see Figure 4(a)). Conversely, for the cube shape, all participants indicated a rectangular hover space, often drawing a rectangular outline and then used zigzag pattern to fill the area (see Figure 4(b)). The comments of the participants during debriefing also reflect this behavior.", "local_uri": ["d5fb83ada0dc1ac5b566d0694e9ea3aa5bc02ed4_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "pdf_hash": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: This Figure illustrates the concept of synthesized social signals (S3s). At the top of the Figure, there are screenshots of many tweets of Bob (a fake account we used as an example). The tweets contain phrases like \"Morons\" and \"You know you are an asshole...\". There are arrows flowing from the tweets into letters \"A1, A2 ... An\" (which are below the arrows). This indicates that the tweets are fed into the algorithms A1, A2 ... An and the algorithms output information about Bob based on Bob's account history. Then, the information is rendered into Bob's Twitter profile as S3s. Bob's Twitter profile is located at the bottom of the Figure, below the arrows pointing from the algorithms A1, A2 ... An. Bob's profile border is turned red with two tags below the profile image each saying \"toxicity\" and \"misinformation\". The red border of the profile image and the tags are the S3s that indicate Bob is an account that is potentially toxic and also misinformation-spreading.", "levels": null, "corpus_id": 218482477, "sentences": ["Figure 1: This Figure illustrates the concept of synthesized social signals (S3s).", "At the top of the Figure, there are screenshots of many tweets of Bob (a fake account we used as an example).", "The tweets contain phrases like \"Morons\" and \"You know you are an asshole...\".", "There are arrows flowing from the tweets into letters \"A1, A2 ... An\" (which are below the arrows).", "This indicates that the tweets are fed into the algorithms A1, A2 ... An and the algorithms output information about Bob based on Bob's account history.", "Then, the information is rendered into Bob's Twitter profile as S3s.", "Bob's Twitter profile is located at the bottom of the Figure, below the arrows pointing from the algorithms A1, A2 ... An.", "Bob's profile border is turned red with two tags below the profile image each saying \"toxicity\" and \"misinformation\".", "The red border of the profile image and the tags are the S3s that indicate Bob is an account that is potentially toxic and also misinformation-spreading."], "caption": "ABSTRACT", "local_uri": ["666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "pdf_hash": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: The Figure shows a Facebook profile of Jane. At the top there is the cover image, while the profile picture is at the top left. Underneath the profile image, there is Jane's bio which says \"PhD student who loves research\". Underneath the bio, there are the number of followers (77), number of friends (819), number of mutual friends (7), profile pics of mutual friends, work information (graduate research assistant), school information (University of Michigan), location information (Ann Arbor), and family information (1 family member).", "levels": null, "corpus_id": 218482477, "sentences": ["Figure 2: The Figure shows a Facebook profile of Jane.", "At the top there is the cover image, while the profile picture is at the top left.", "Underneath the profile image, there is Jane's bio which says \"PhD student who loves research\".", "Underneath the bio, there are the number of followers (77), number of friends (819), number of mutual friends (7), profile pics of mutual friends, work information (graduate research assistant), school information (University of Michigan), location information (Ann Arbor), and family information (1 family member)."], "caption": "Figure 2. Example social signals on Facebook: a) pro\ufb01le image, b) cover image, c) bio, d) # of followers, e) # of friends (# of mutual friends),", "local_uri": ["666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "pdf_hash": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: The Figure contains Sig's modal which shows the five top toxic tweets of the account that has been flagged as toxic based on Sig's S3s. At the top, it says \"About 13% of @bob's recent tweets are likely to be toxic. Below we show up to 5 toxic tweet(s) of @bob as examples\". Below it, it says \"This does not necessarily guarantee that this account is toxic. We acknowledge that your definition of 'toxic' might be different. Here a 'toxic tweet' is defined as 'a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion' according to Perspective API\".  At the bottom, the Figure contains some of the 5 toxic tweets, such as \"You know you are an asshole @stranngerstran18? Your tweets are garbage\". There is a timestamp right below each tweet.", "levels": null, "corpus_id": 218482477, "sentences": ["Figure 4: The Figure contains Sig's modal which shows the five top toxic tweets of the account that has been flagged as toxic based on Sig's S3s.", "At the top, it says \"About 13% of @bob's recent tweets are likely to be toxic.", "Below we show up to 5 toxic tweet(s) of @bob as examples\". Below it, it says \"This does not necessarily guarantee that this account is toxic.", "We acknowledge that your definition of 'toxic' might be different.", "Here a 'toxic tweet' is defined as 'a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion' according to Perspective API\".", "At the bottom, the Figure contains some of the 5 toxic tweets, such as \"You know you are an asshole @stranngerstran18?", "Your tweets are garbage\".", "There is a timestamp right below each tweet."], "caption": "", "local_uri": ["666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "pdf_hash": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: The Figure illustrates how Sig works on Twitter's notifications tab. There are 3 rows in the screenshot of a notifications tab. The first row contains 4 profile pics with the sentence \"Alice and 3 others followed you\" below the profile pics. The second row contains one profile pic and the sentence \"Catherine liked your Retweet\". The last row contains 3 profile pics and the sentence \"John Smith and 2 others followed you\". The profile pics of Alice (left at first row) and John Smith (left at third row) turned red, indicating that at least one S3 was triggered for these two accounts. The profile pics of the two accounts on the right of the first row have blue, dotted borders, indicating that Sig is currently computing the S3s of these accounts. The remaining profile pics' borders are not changed, indicating that the accounts' S3s were not over the threshold so the blue, dotted border disappeared after Sig finished computing.", "levels": null, "corpus_id": 218482477, "sentences": ["Figure 3: The Figure illustrates how Sig works on Twitter's notifications tab.", "There are 3 rows in the screenshot of a notifications tab.", "The first row contains 4 profile pics with the sentence \"Alice and 3 others followed you\" below the profile pics.", "The second row contains one profile pic and the sentence \"Catherine liked your Retweet\".", "The last row contains 3 profile pics and the sentence \"John Smith and 2 others followed you\".", "The profile pics of Alice (left at first row) and John Smith (left at third row) turned red, indicating that at least one S3 was triggered for these two accounts.", "The profile pics of the two accounts on the right of the first row have blue, dotted borders, indicating that Sig is currently computing the S3s of these accounts.", "The remaining profile pics' borders are not changed, indicating that the accounts' S3s were not over the threshold so the blue, dotted border disappeared after Sig finished computing."], "caption": "Figure 3. Pro\ufb01le border color is used to render S3s in noti\ufb01ca- tion/timeline page of Twitter. A red border indicates that at least one S3 has been triggered, and the account may be risky to interact with (ex- amples: Alice and John, in \ufb01rst and third row). A blue, double-lined border indicates that Sig is currently computing S3s for the account (ex- amples on the right in the \ufb01rst row). If no S3s are triggered, the blue border disappears after computation (examples in second and right of third row).", "local_uri": ["666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "pdf_hash": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: The Figure shows Sig's sliders that users can use to set the thresholds for toxicity and misinformation S3s. At the right top, there is the Sig's icon (which appears when you install Sig). In the Figure the icon is pressed, with the popup, which contains the sliders, showing below the icon. There is one slider per toxicity S3 and misinformation S3. It says \"Maximum toxic tweet frequency you would allow: 8%\" above the slider for toxicity S3 and \"Maximum misinfo. tweet frequency you would allow: 2%\" above the slider for misinformation S3.  At the bottom, there is a Log out button.", "levels": null, "corpus_id": 218482477, "sentences": ["Figure 5: The Figure shows Sig's sliders that users can use to set the thresholds for toxicity and misinformation S3s.", "At the right top, there is the Sig's icon (which appears when you install Sig).", "In the Figure the icon is pressed, with the popup, which contains the sliders, showing below the icon.", "There is one slider per toxicity S3 and misinformation S3.", "It says \"Maximum toxic tweet frequency you would allow: 8%\" above the slider for toxicity S3 and \"Maximum misinfo.", "tweet frequency you would allow: 2%\" above the slider for misinformation S3.", "At the bottom, there is a Log out button."], "caption": "Figure 5. Users can adjust the sliders to set their own thresholds for each S3 in Sig. In the \ufb01eld deployment, participants could choose thresholds for toxicity and misinformation.", "local_uri": ["666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "pdf_hash": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: This Figure contains a 2D stacked column chart showing the results of the post-study survey (which was conducted after the field study).  In the x-axis, there are 4 questions/sentences which were given to the participants using a 1-5 Likert scale. The y-axis is the average of the rating per each question/sentence. Furthermore, each bar per question/sentence shows the distribution of the ratings. The ratio of each rating (from 1 to 5) is colored differently.  The first sentence is \"1) Sig helped me feel more comfortable when deciding whether to interact with strangers on Twitter\". Participants overall gave an average rating of 3.9/5 for this sentence. 4 people gave a 3, 4 people gave a 4, and 3 people gave a 5.  Next, for \"2) How would you rate your overall experience of using Sig?\" the average rating was 4 out of 5. 2 people gave a 3, 7 people gave a 4, and 2 people gave a 5.  For \"3) Was Sig's visualization of various social signals easy to understand?\" the average score was 4.7 out of 5. 3 people gave a 4 and 8 people gave a 5. Finally, for \"4) Would you be interested in using a refined version of Sig in the future?\", the average rating was 4.7 out of 5. 1 person gave a 2, 2 people gave a 3, 2 people gave a 4, and 6 people gave a 5.", "levels": [[1], [1], [1], [1], [1], [1], [2], [2], [1], [2], [2], [1], [2], [2], [1], [2], [2]], "corpus_id": 218482477, "sentences": ["Figure 6: This Figure contains a 2D stacked column chart showing the results of the post-study survey (which was conducted after the field study).", "In the x-axis, there are 4 questions/sentences which were given to the participants using a 1-5 Likert scale.", "The y-axis is the average of the rating per each question/sentence.", "Furthermore, each bar per question/sentence shows the distribution of the ratings.", "The ratio of each rating (from 1 to 5) is colored differently.", "The first sentence is \"1) Sig helped me feel more comfortable when deciding whether to interact with strangers on Twitter\".", "Participants overall gave an average rating of 3.9/5 for this sentence.", "4 people gave a 3, 4 people gave a 4, and 3 people gave a 5.", "Next, for \"2) How would you rate your overall experience of using Sig?\"", "the average rating was 4 out of 5.", "2 people gave a 3, 7 people gave a 4, and 2 people gave a 5.", "For \"3) Was Sig's visualization of various social signals easy to understand?\"", "the average score was 4.7 out of 5.", "3 people gave a 4 and 8 people gave a 5.", "Finally, for \"4) Would you be interested in using a refined version of Sig in the future?\",", "the average rating was 4.7 out of 5.", "1 person gave a 2, 2 people gave a 3, 2 people gave a 4, and 6 people gave a 5."], "caption": "Figure 6. Results of the survey taken by participants after the \ufb01eld study. Each question appeared on a 1-5 Likert scale.", "local_uri": ["666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Consensus Building in Collaborative Sequencing with Visual Awareness", "pdf_hash": "233dd54bcbf3e6a151dce13a4d01a531a0648b04", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: \"Two screens from the travel itinerary planning prototype. Top shows the individual screen with the constructed sequence on the left, a list of locations on the middle, and a map on the right. Bottom shows the collaborative screen which shows three group members' sequences on the left and a list of locations on the right.\"", "levels": null, "corpus_id": 218482722, "sentences": ["Figure 1: \"Two screens from the travel itinerary planning prototype.", "Top shows the individual screen with the constructed sequence on the left, a list of locations on the middle, and a map on the right.", "Bottom shows the collaborative screen which shows three group members' sequences on the left and a list of locations on the right.\""], "caption": "Consensus Building in Collaborative Sequencing with Visual Awareness", "local_uri": ["233dd54bcbf3e6a151dce13a4d01a531a0648b04_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Consensus Building in Collaborative Sequencing with Visual Awareness", "pdf_hash": "233dd54bcbf3e6a151dce13a4d01a531a0648b04", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2: \"Visual representation of a sequence containing three locations. Locations are represented by rectangles ordered vertically with an arrow connecting each rectangle to the next one below it.\"", "levels": null, "corpus_id": 218482722, "sentences": ["Figure 2: \"Visual representation of a sequence containing three locations.", "Locations are represented by rectangles ordered vertically with an arrow connecting each rectangle to the next one below it.\""], "caption": "", "local_uri": ["233dd54bcbf3e6a151dce13a4d01a531a0648b04_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Consensus Building in Collaborative Sequencing with Visual Awareness", "pdf_hash": "233dd54bcbf3e6a151dce13a4d01a531a0648b04", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3: \"Section of the map displayed in the prototype with two types of circular markers overlayed that represent locations in their geographical positions. The big markers represent locations included in the sequence and small markers represent those that have not been included.\"", "levels": [[-1], [-1]], "corpus_id": 218482722, "sentences": ["Figure 3: \"Section of the map displayed in the prototype with two types of circular markers overlayed that represent locations in their geographical positions.", "The big markers represent locations included in the sequence and small markers represent those that have not been included.\""], "caption": "", "local_uri": ["233dd54bcbf3e6a151dce13a4d01a531a0648b04_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Consensus Building in Collaborative Sequencing with Visual Awareness", "pdf_hash": "233dd54bcbf3e6a151dce13a4d01a531a0648b04", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 5: \"Section of the list of locations displayed in the prototype. Location entries are ordered vertically and each entry shows an image of the location and information of the location, including the category, the cost, the name of the location and a short paragraph description.", "levels": [[-1], [-1]], "corpus_id": 218482722, "sentences": ["Figure 5: \"Section of the list of locations displayed in the prototype.", "Location entries are ordered vertically and each entry shows an image of the location and information of the location, including the category, the cost, the name of the location and a short paragraph description."], "caption": "", "local_uri": ["233dd54bcbf3e6a151dce13a4d01a531a0648b04_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Consensus Building in Collaborative Sequencing with Visual Awareness", "pdf_hash": "233dd54bcbf3e6a151dce13a4d01a531a0648b04", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 8: \"A double bar graph showing the average survey responses for both the Treatment and Control condition. The vertical axis is labelled with the metric measured by the survey questions. 'Perceived Efficiency' and 'Perceived Effectiveness' were both higher in the Treatment condition when compared to the Control condition, and 'Aggregated NASA-TLX' was lower in the Treatment condition when compared to the Control condition.\"", "levels": [[1], [1], [2]], "corpus_id": 218482722, "sentences": ["Figure 8: \"A double bar graph showing the average survey responses for both the Treatment and Control condition.", "The vertical axis is labelled with the metric measured by the survey questions. '", "Perceived Efficiency' and 'Perceived Effectiveness' were both higher in the Treatment condition when compared to the Control condition, and 'Aggregated NASA-TLX' was lower in the Treatment condition when compared to the Control condition.\""], "caption": "", "local_uri": ["233dd54bcbf3e6a151dce13a4d01a531a0648b04_Image_018.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Time Estimation Bias in Knowledge Work: Tasks With Fewer Time Constraints Are More Error-Prone", "pdf_hash": "af1aae9b76b4edd2da0b86e3efe98cf3b11b36b5", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "The association between planned (estimated) work duration and actual workday duration for each participant. The blue line shows a perfect relationship with no bias. Data points below the line indicate that the planned duration was longer than the actual one.", "levels": [[1], [3], [3]], "corpus_id": 218483394, "sentences": ["The association between planned (estimated) work duration and actual workday duration for each participant.", "The blue line shows a perfect relationship with no bias.", "Data points below the line indicate that the planned duration was longer than the actual one."], "caption": "Research report180 min89 minMeeting60 min60 minTranscribe120 min0 minEmail and communications60 min79 minCoding task60 min145 min", "local_uri": ["af1aae9b76b4edd2da0b86e3efe98cf3b11b36b5_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FootUI: Assisting People with Upper Body Motor Impairments to Use Smartphones with Foot Gestures on the Bed", "pdf_hash": "d6fe18eeac604fcdad9b63e99edf7707db2d7f43", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A user reclines on the bed and uses smartphones without hands. A smartphone is fixed on the smartphone holder and the user's feet are within the view of the phone camera. The camera is always on to track the user's feet. The movement of the right foot toe is mapped to the cursor on the smartphone interface, and some special foot gestures can trigger the shortcut actions of the smartphone.", "levels": null, "corpus_id": 233987018, "sentences": ["A user reclines on the bed and uses smartphones without hands.", "A smartphone is fixed on the smartphone holder and the user's feet are within the view of the phone camera.", "The camera is always on to track the user's feet.", "The movement of the right foot toe is mapped to the cursor on the smartphone interface, and some special foot gestures can trigger the shortcut actions of the smartphone."], "caption": "", "local_uri": ["d6fe18eeac604fcdad9b63e99edf7707db2d7f43_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "FootUI: Assisting People with Upper Body Motor Impairments to Use Smartphones with Foot Gestures on the Bed", "pdf_hash": "d6fe18eeac604fcdad9b63e99edf7707db2d7f43", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Foot gesture set containing 3 dynamic continuous gestures which are \"heel rotation\", \"ankle flexion\" and \"leg flexion\" and 8 discrete gestures which are \"tap\", \"double tap\", \"kick up\", \"rotate right\", \"rotate left\", \"heel to heel\", \"toe to toe\" and \"close up\".", "levels": null, "corpus_id": 233987018, "sentences": ["Foot gesture set containing 3 dynamic continuous gestures which are \"heel rotation\", \"ankle flexion\" and \"leg flexion\" and 8 discrete gestures which are \"tap\", \"double tap\", \"kick up\", \"rotate right\", \"rotate left\", \"heel to heel\", \"toe to toe\" and \"close up\"."], "caption": "", "local_uri": ["d6fe18eeac604fcdad9b63e99edf7707db2d7f43_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "FootUI: Assisting People with Upper Body Motor Impairments to Use Smartphones with Foot Gestures on the Bed", "pdf_hash": "d6fe18eeac604fcdad9b63e99edf7707db2d7f43", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "For both feet, the fingertip, center and other feature points are extracted, the coordinates are used in classification", "levels": [[-1]], "corpus_id": 233987018, "sentences": ["For both feet, the fingertip, center and other feature points are extracted, the coordinates are used in classification"], "caption": "", "local_uri": ["d6fe18eeac604fcdad9b63e99edf7707db2d7f43_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FootUI: Assisting People with Upper Body Motor Impairments to Use Smartphones with Foot Gestures on the Bed", "pdf_hash": "d6fe18eeac604fcdad9b63e99edf7707db2d7f43", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Mapping from foot gestures to smartphone operations including basic operation and shortcuts. Basic operation contains pointing (combination of \"right foot heel rotation\", \"right foot ankle flexion\" and \"right leg flexion\"), trigger (\"left foot tap\"), and slide (\"right foot heel rotation\"). Shortcuts contains screenshot (\"heel to heel\"), app switch/backstage (\"close up\"), volume (the combination of \"left foot rotate left\" and \"right foot rotate right\"), brightness (the combination of \"left foot tap\" and \"right foot tap\"), notice (\"left foot kick up\"), screen switch (both feet rotate to the same direction), shortcut menu (\"left foot double tap\"), home (\"toe to toe\") and back(\"left foot rotate left\")", "levels": null, "corpus_id": 233987018, "sentences": ["Mapping from foot gestures to smartphone operations including basic operation and shortcuts.", "Basic operation contains pointing (combination of \"right foot heel rotation\", \"right foot ankle flexion\" and \"right leg flexion\"), trigger (\"left foot tap\"), and slide (\"right foot heel rotation\").", "Shortcuts contains screenshot (\"heel to heel\"), app switch/backstage (\"close up\"), volume (the combination of \"left foot rotate left\" and \"right foot rotate right\"), brightness (the combination of \"left foot tap\" and \"right foot tap\"), notice (\"left foot kick up\"), screen switch (both feet rotate to the same direction), shortcut menu (\"left foot double tap\"), home (\"toe to toe\") and back(\"left foot rotate left\")"], "caption": "", "local_uri": ["d6fe18eeac604fcdad9b63e99edf7707db2d7f43_Image_006.png"], "annotated": false, "compound": false}
{"title": "FootUI: Assisting People with Upper Body Motor Impairments to Use Smartphones with Foot Gestures on the Bed", "pdf_hash": "d6fe18eeac604fcdad9b63e99edf7707db2d7f43", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The user interface of FootUI containing cursor, shortcut menu, scroll button and camera view.", "levels": null, "corpus_id": 233987018, "sentences": ["The user interface of FootUI containing cursor, shortcut menu, scroll button and camera view."], "caption": "Figure 6: User interface of FootUI", "local_uri": ["d6fe18eeac604fcdad9b63e99edf7707db2d7f43_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Tiltcasting: 3D Interaction on Large Displays using a Mobile Device", "pdf_hash": "2536bf533421f8889f37eba9f6adad4f77f4c866", "year": 2015, "venue": "UIST", "alt_text": "First-person view of a phone being held like a game controller with a large screen in the background.", "levels": null, "corpus_id": 17453484, "sentences": ["First-person view of a phone being held like a game controller with a large screen in the background."], "caption": "Figure 1. Tiltcasting enables 3D interaction with nearby large displays via a Mobile Device.", "local_uri": ["2536bf533421f8889f37eba9f6adad4f77f4c866_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Tiltcasting: 3D Interaction on Large Displays using a Mobile Device", "pdf_hash": "2536bf533421f8889f37eba9f6adad4f77f4c866", "year": 2015, "venue": "UIST", "alt_text": "Visualization of the occlusion removal mechanism. Shows a person using Tiltcast.", "levels": null, "corpus_id": 17453484, "sentences": ["Visualization of the occlusion removal mechanism.", "Shows a person using Tiltcast."], "caption": "Figure 2. Tiltcasting\u2019s occlusion removal mechanism. a) Yellow sphere occluded by blue sphere, interaction plane is in vertical position. b) Tilt\u00ad ing the interaction plane hides three blue objects, revealing the target.", "local_uri": ["2536bf533421f8889f37eba9f6adad4f77f4c866_Image_002.jpg", "2536bf533421f8889f37eba9f6adad4f77f4c866_Image_003.jpg"], "annotated": false, "compound": true}
{"title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions", "pdf_hash": "7cc25eed41bc78c0af8b1e3b57437da39d1e220c", "year": 2020, "venue": "CHI", "alt_text": "The left part shows the interface of the WoZ platform. In the center of the interface, a red button and a green button is shown with the explanation of the button to send wrong or correct responses. The right part shows the picture of the mock-up smart speaker. The camera, button, and phone on the speaker are pointed out.", "levels": null, "corpus_id": 218483630, "sentences": ["The left part shows the interface of the WoZ platform.", "In the center of the interface, a red button and a green button is shown with the explanation of the button to send wrong or correct responses.", "The right part shows the picture of the mock-up smart speaker.", "The camera, button, and phone on the speaker are pointed out."], "caption": "Figure 2. The experiment platform of Study 1. (a) The interface for the experimenter (wizard) to select and send the response to the user, which is run on a laptop. (b) The mock-up of the smart speaker. The camera captures the user\u2019s facial expression and is connected to the laptop. The phone receives the response in text and plays it to the user in audio. A button is used to interrupt conversations in Study 2.", "local_uri": ["7cc25eed41bc78c0af8b1e3b57437da39d1e220c_Image_003.png"], "annotated": false, "compound": false}
{"title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions", "pdf_hash": "7cc25eed41bc78c0af8b1e3b57437da39d1e220c", "year": 2020, "venue": "CHI", "alt_text": "The average value with the standard deviation of the presence and intensity of seventeen action units are shown in histograms. The values for correct responses, wrong responses, consistently detected responses are labeled with the color of black, orange and yellow respectively.", "levels": [[1], [1]], "corpus_id": 218483630, "sentences": ["The average value with the standard deviation of the presence and intensity of seventeen action units are shown in histograms.", "The values for correct responses, wrong responses, consistently detected responses are labeled with the color of black, orange and yellow respectively."], "caption": "Figure 3. The average intensity (0-5) and presence scores (0-1) of seventeen action units calculated by OpenFace toolkit. \"Correct\" stands for the results of all correct responses, \"Wrong\" stands for that of all incorrect responses and \"Wrong_Consistent\" stands for results of 35 incorrect responses that were consistently detected by human judges. The error bars represent SEMs, n = 12)", "local_uri": ["7cc25eed41bc78c0af8b1e3b57437da39d1e220c_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions", "pdf_hash": "7cc25eed41bc78c0af8b1e3b57437da39d1e220c", "year": 2020, "venue": "CHI", "alt_text": "67 facial landmarks are marked on a cartoon face. A black line connects the position of two eyes. Two red circles represent the positions of the eyebrows. One green circle represents the position of the nose. Two yellow lines represent the vertical distances between the eyebrows and the nose.", "levels": null, "corpus_id": 218483630, "sentences": ["67 facial landmarks are marked on a cartoon face.", "A black line connects the position of two eyes.", "Two red circles represent the positions of the eyebrows.", "One green circle represents the position of the nose.", "Two yellow lines represent the vertical distances between the eyebrows and the nose."], "caption": "Figure 5. Illustration of feature calculation for Raise Eyebrows. 67 fa- cial landmarks were extracted by OpenFace toolkit. We calculated Dis- tance_Left and Distance_Right as the features.", "local_uri": ["7cc25eed41bc78c0af8b1e3b57437da39d1e220c_Image_006.png"], "annotated": false, "compound": false}
{"title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions", "pdf_hash": "7cc25eed41bc78c0af8b1e3b57437da39d1e220c", "year": 2020, "venue": "CHI", "alt_text": "The left part shows the histogram of the averaged interruption times of four techniques in four colors. The right part shows the histogram of subjective scores for four dimensions. Four colors represent the scores of four different interruption techniques.", "levels": [[1], [1], [1]], "corpus_id": 218483630, "sentences": ["The left part shows the histogram of the averaged interruption times of four techniques in four colors.", "The right part shows the histogram of subjective scores for four dimensions.", "Four colors represent the scores of four different interruption techniques."], "caption": "Figure 6. Left) Interruption time with different interruption techniques and the error bars represent the standard deviations. Right) The subjective scores from users on different dimensions of user experience and the numbers were the average values of the scores.", "local_uri": ["7cc25eed41bc78c0af8b1e3b57437da39d1e220c_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions", "pdf_hash": "7cc25eed41bc78c0af8b1e3b57437da39d1e220c", "year": 2020, "venue": "CHI", "alt_text": "This figure shows the histogram of subjective scores for four dimensions. The scores from 1 to 7 are marked by 7 different colors.", "levels": [[1], [1]], "corpus_id": 218483630, "sentences": ["This figure shows the histogram of subjective scores for four dimensions.", "The scores from 1 to 7 are marked by 7 different colors."], "caption": "Figure 7. Subjective scores from users on experience of different con- versation recovery strategies. \"I\", \"C\", \"A\" and \"R\" stand for the strategies of Interruption Only, Error Con\ufb01rmation, Alternative Response, Repeat Request respectively.", "local_uri": ["7cc25eed41bc78c0af8b1e3b57437da39d1e220c_Image_008.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Human-Nature Relations in Urban Gardens: Explorations with Camera Traps", "pdf_hash": "c444266915f8000619d9e5d9bc6665af2340eab1", "year": 2021, "venue": "CHI", "alt_text": "Photo of Possum inside a chicken coop, shot in black and white. Possum is staring into the camera/at the viewer.", "levels": null, "corpus_id": 233986979, "sentences": ["Photo of Possum inside a chicken coop, shot in black and white.", "Possum is staring into the camera/at the viewer."], "caption": "", "local_uri": ["c444266915f8000619d9e5d9bc6665af2340eab1_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "The front of the business card is white. At the top right of the business card is Wobble\u2019s logo written once in a print font and below that is Wobble written in a braille font. The name, Pari Gabriel, is written in large font across the middle of the business card with the job title, Diversity Engineer and email address, pari@wobble.com.\n\nThe back of the business card is purple with a white Wobble logo and QR code.", "levels": null, "corpus_id": 195259240, "sentences": ["The front of the business card is white.", "At the top right of the business card is Wobble\u2019s logo written once in a print font and below that is Wobble written in a braille font.", "The name, Pari Gabriel, is written in large font across the middle of the business card with the job title, Diversity Engineer and email address, pari@wobble.com.", "The back of the business card is purple with a white Wobble logo and QR code."], "caption": "We examine this critique", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "On the left, Wobble's logo is printed on a tote bag.  On the right, there are 8 color variations of Wobble's logo.  8 color variations according to different types of color blindness. The 8 variations show how the logo would look like for people with color blindness.", "levels": null, "corpus_id": 195259240, "sentences": ["On the left, Wobble's logo is printed on a tote bag.", "On the right, there are 8 color variations of Wobble's logo.", "8 color variations according to different types of color blindness.", "The 8 variations show how the logo would look like for people with color blindness."], "caption": "", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "2 boxes show how the paper towel dispenser could assist people  1. A person is confused and has drippy hands. The device is charging at the person from around the corner, saying: \"Don't worry! I'm tracking you... er... coming to you!\"  2. A person is running away from the dispenser, which is charging from inside the restroom into the office, saying: \"Found ya! Drying your hands on your pants again, huh?!\"", "levels": null, "corpus_id": 195259240, "sentences": ["2 boxes show how the paper towel dispenser could assist people  1.", "A person is confused and has drippy hands.", "The device is charging at the person from around the corner, saying: \"Don't worry! I'm tracking you... er... coming to you!\"  2.", "A person is running away from the dispenser, which is charging from inside the restroom into the office, saying: \"Found ya! Drying your hands on your pants again, huh?!\""], "caption": "", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "The figure shows the design of the paper towel dispenser as if it was a 3D sketch made by a designer in a software design tool.  The figure also shows the paper towel dispenser is outlined in sketch lines. A purple rectangle represents the sensor located near the bottom of the dispenser where paper towels are dispensed. The dispenser is spilling out 3 sheets of paper towel. Below we will describe the pattern printed on each paper towel.  2. The towel design is based on a painting by the famous gay painter Keith Haring. It has a yellow background and multiple human figures with a red outline spread around the figure. The human figures' heads have a black x mark inside each of them.  Foregrounded on the hand towel is a message that reads \"Have you designed diversity today?\"", "levels": null, "corpus_id": 195259240, "sentences": ["The figure shows the design of the paper towel dispenser as if it was a 3D sketch made by a designer in a software design tool.", "The figure also shows the paper towel dispenser is outlined in sketch lines.", "A purple rectangle represents the sensor located near the bottom of the dispenser where paper towels are dispensed.", "The dispenser is spilling out 3 sheets of paper towel.", "Below we will describe the pattern printed on each paper towel.", "2.", "The towel design is based on a painting by the famous gay painter Keith Haring.", "It has a yellow background and multiple human figures with a red outline spread around the figure.", "The human figures' heads have a black x mark inside each of them.", "Foregrounded on the hand towel is a message that reads \"Have you designed diversity today?\""], "caption": "", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "The figure shows two UI\u2019s of an email client app.\n\nThe left side shows an email from Head of HR, copied below.\n\"To: \u201cWobble All:\u201d all@wobble.com\nFrom: \u201cHead of HR\u201d <headofhr@wobble.com>\nSubject: Register to Participate in Interview Rounds\n\nIt\u2019s that time of year. Our fabulous recruiters have visited hundreds of college campuses, and we\u2019ve got 1000 candidates visiting campus for interviews! In an effort to gather additional perspectives during the interview process, all candidates will meet with one volunteer interviewer in addition to speaking with their respective team and HR. This could be you! Register to participate in an interview round and help us expand Wobble\u2019s mission to the next generation of awesome employees! Register via the \u2018services\u2019 tab on your internal HR portal.\nJames Rice Head of HR wobble.com\"\n\nAt the bottom are a reply and forward buttons and the forward button is being clicked. An arrow connects the selected forward button to the second UI. Close to the arrow, there is a rectangle with round corners labeled \u201cdiverse employees\u201d indicating they are the recipient of the forwarded email. The second UI  includes an email from the Chief  Diversity Officer, copied below.\n\n\"Dear Diverse Employees!\nPer our new company policy, which was written with your generous feedback, we want to make sure all our interviewees witness what a great and diverse company we are.\nWe are excited to include one diverse employee on each interview panel in addition to the lead interviewer. Each of you should register for at least two interviews.\nIt will take some extra time, but helping to diversify Wobble is worth it!\nThank you,\nJack Price Chief Diversity Officer\"", "levels": null, "corpus_id": 195259240, "sentences": ["The figure shows two UI\u2019s of an email client app.", "The left side shows an email from Head of HR, copied below.", "\"To: \u201cWobble All:\u201d all@wobble.com\nFrom: \u201cHead of HR\u201d <headofhr@wobble.com>\nSubject: Register to Participate in Interview Rounds\n\nIt\u2019s that time of year.", "Our fabulous recruiters have visited hundreds of college campuses, and we\u2019ve got 1000 candidates visiting campus for interviews! In an effort to gather additional perspectives during the interview process, all candidates will meet with one volunteer interviewer in addition to speaking with their respective team and HR.", "This could be you! Register to participate in an interview round and help us expand Wobble\u2019s mission to the next generation of awesome employees! Register via the \u2018services\u2019 tab on your internal HR portal.", "James Rice Head of HR wobble.com\"\n\nAt the bottom are a reply and forward buttons and the forward button is being clicked.", "An arrow connects the selected forward button to the second UI.", "Close to the arrow, there is a rectangle with round corners labeled \u201cdiverse employees\u201d indicating they are the recipient of the forwarded email.", "The second UI  includes an email from the Chief  Diversity Officer, copied below.", "\"Dear Diverse Employees!\nPer our new company policy, which was written with your generous feedback, we want to make sure all our interviewees witness what a great and diverse company we are.", "We are excited to include one diverse employee on each interview panel in addition to the lead interviewer.", "Each of you should register for at least two interviews.", "It will take some extra time, but helping to diversify Wobble is worth it!\nThank you,\nJack Price Chief Diversity Officer\""], "caption": "recruitment process. This goal is why we created the One Diverse Wobbler per interview policy!", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "Image depicting Wobble's location in San Francisco, California in the heart of the bustling, innovative mission district.", "levels": null, "corpus_id": 195259240, "sentences": ["Image depicting Wobble's location in San Francisco, California in the heart of the bustling, innovative mission district."], "caption": "", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Design and Diversity?: Speculations on What Could Go Wrong", "pdf_hash": "108b0f3c6e2043c4bd1698140797f32ddb7aaf61", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "Light pole with a job application flyer  Photo of a dirty light pole with old flyers attached. On top of these is a new flyer. The text reads: \"Contract work opportunities @ Wobble, the most diverse Silicon Valley company!\" \"Glassdoor 2017 best places to work - employees' choice\" \"We have the best perks!*\" \"Work hard, play hard environment; On-campus dining; Astro-turf floors; Community game room\" \"Go to wobb.le/contract-jobs\" \"*Contract employees ineligible for on-campus dining and beer-Fridays.\"", "levels": null, "corpus_id": 195259240, "sentences": ["Light pole with a job application flyer  Photo of a dirty light pole with old flyers attached.", "On top of these is a new flyer.", "The text reads: \"Contract work opportunities @ Wobble, the most diverse Silicon Valley company!\" \"Glassdoor 2017 best places to work - employees' choice\" \"We have the best perks!*\" \"Work hard, play hard environment; On-campus dining; Astro-turf floors; Community game room\" \"Go to wobb.le/contract-jobs\" \"*Contract employees ineligible for on-campus dining and beer-Fridays.\""], "caption": "", "local_uri": ["108b0f3c6e2043c4bd1698140797f32ddb7aaf61_Image_016.png"], "annotated": false, "compound": false}
{"title": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "pdf_hash": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"An avatar walking through a virtual wall and the user in the real environment.\"", "levels": null, "corpus_id": 218483389, "sentences": ["Figure 1: \"An avatar walking through a virtual wall and the user in the real environment.\""], "caption": "Figure 1. An avatar walking through a virtual wall (left) and the user in the real environment (right).", "local_uri": ["b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "pdf_hash": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"Four types of avatars were used in the experiment. For realistic avatars, gender-matching avatars were used.\"", "levels": null, "corpus_id": 218483389, "sentences": ["Figure 2: \"Four types of avatars were used in the experiment.", "For realistic avatars, gender-matching avatars were used.\""], "caption": "Figure 2. Four types of avatars were used in the experiment. For realistic avatars, gender-matching avatars were used (left: male, right: female).", "local_uri": ["b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "pdf_hash": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"A participant equipped with experimental apparatus and a wearable sensor used to estimate participants' SCR to threat.\"", "levels": null, "corpus_id": 218483389, "sentences": ["Figure 3: \"A participant equipped with experimental apparatus and a wearable sensor used to estimate participants' SCR to threat.\""], "caption": "", "local_uri": ["b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "pdf_hash": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: \"Room layouts: S and G represent the start and goal, respectively. The lines represent the walls. The buttons are numbered and must be pushed sequentially to activate the teleporter at the goal in each room. The dashed lines in room 3 represent the sliding walls that closed when the participants entered the yellow- and green-colored areas. The gray line in room 4 represents the wall through which electric lightning was passed when the participants approached it. On pushing the blue buttons in room 4, the participants were presented with the threatening stimuli. Room characteristics: In Room 1, the participants could easily push the button on the other side of the wall if they penetrated their hand. However, they also had the option to pass their hand under the wall and walk around. In Room 2, the participants had to walk back and forth several times but could take a shortcut if they walked through the wall. In Room 3, the buttons could only be pushed if the participants walked through the sliding wall, which closed when they approached. In Room 4, threatening stimuli such as electric lightning passing throughout the wall (left) and a bunch of needles popping out from the wall (right) were presented.\"", "levels": null, "corpus_id": 218483389, "sentences": ["Figure 4: \"Room layouts: S and G represent the start and goal, respectively.", "The lines represent the walls.", "The buttons are numbered and must be pushed sequentially to activate the teleporter at the goal in each room.", "The dashed lines in room 3 represent the sliding walls that closed when the participants entered the yellow- and green-colored areas.", "The gray line in room 4 represents the wall through which electric lightning was passed when the participants approached it.", "On pushing the blue buttons in room 4, the participants were presented with the threatening stimuli.", "Room characteristics: In Room 1, the participants could easily push the button on the other side of the wall if they penetrated their hand.", "However, they also had the option to pass their hand under the wall and walk around.", "In Room 2, the participants had to walk back and forth several times but could take a shortcut if they walked through the wall.", "In Room 3, the buttons could only be pushed if the participants walked through the sliding wall, which closed when they approached.", "In Room 4, threatening stimuli such as electric lightning passing throughout the wall (left) and a bunch of needles popping out from the wall (right) were presented.\""], "caption": "Figure 4. (Top) Room layouts. S and G represent the start and goal, respectively. The lines represent the walls. The buttons are numbered and must be pushed sequentially to activate the teleporter at the goal in each room. The dashed lines in room 3 represent the sliding walls that closed when the participants entered the yellow- and green-colored areas. The gray line in room 4 represents the wall through which electric lightning was passed when the participants approached it. On pushing the blue buttons in room 4, the participants were presented with the threatening stimuli. (Bottom) Room characteristics. In Room 1, the participants could easily push the button on the other side of the wall if they penetrated their hand. However, they also had the option to pass their hand under the wall and walk around. In Room 2, the participants had to walk back and forth several times but could take a shortcut if they walked through the wall. In Room 3, the buttons could only be pushed if the participants walked through the sliding wall, which closed when they approached. In Room 4, threatening stimuli such as electric lightning passing throughout the wall (left) and a bunch of needles popping out from the wall (right) were presented.", "local_uri": ["b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "pdf_hash": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: \"Distribution of participants' behavior in rooms 1, 2, and 3 according to avatar anthropomorphism and visibility.\"", "levels": [[1]], "corpus_id": 218483389, "sentences": ["Figure 5: \"Distribution of participants' behavior in rooms 1, 2, and 3 according to avatar anthropomorphism and visibility.\""], "caption": "Figure 5. Distribution of participants\u2019 behavior in rooms 1, 2, and 3 according to avatar anthropomorphism and visibility. C, H, R, and F represent Controller, Human Hand, Robot, and Full-body Human, respectively. The shown numbers are the percentage out of 23 participants for each condition.", "local_uri": ["b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "pdf_hash": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: \"Letter-value box plot of the time until the participants first walked through the wall in room 3, room 4, and across rooms according to avatar anthropomorphism and visibility. \"", "levels": [[1]], "corpus_id": 218483389, "sentences": ["Figure 7: \"Letter-value box plot of the time until the participants first walked through the wall in room 3, room 4, and across rooms according to avatar anthropomorphism and visibility. \""], "caption": "Figure 7. Letter-value box plot1 with data points of the time until the participants \ufb01rst walked through the wall in room 3 (left), room 4 (center), and across rooms (right) according to avatar anthropomorphism and visibility. C, H, R, and F represent Controller, Human Hand, Robot, and Full-body Human, respectively. In the cross-room results, the data points are colored according to the rooms where the participants \ufb01rst walked through the wall.", "local_uri": ["b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_007.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches", "pdf_hash": "8fcff5229f4e115a16dc33daf40a87f8a427ba9e", "year": 2017, "venue": "UIST", "alt_text": "Three illustrations of RetroShape, simulating a bouncing ball, a rolling ball, and multiple strikes on the ground.", "levels": null, "corpus_id": 2323656, "sentences": ["Three illustrations of RetroShape, simulating a bouncing ball, a rolling ball, and multiple strikes on the ground."], "caption": "Figure 1. RetroShape aims to extend the visual scene to 2.5D physical space by a deformable display on its rear surface. Our RetroShape prototype equips 4\u00d74 taxels, which can simulate (a) a bouncing ball on an elastic surface, (b) ball rolling, or (c) multiple strikes on the ground.", "local_uri": ["8fcff5229f4e115a16dc33daf40a87f8a427ba9e_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches", "pdf_hash": "8fcff5229f4e115a16dc33daf40a87f8a427ba9e", "year": 2017, "venue": "UIST", "alt_text": "The RetroShape prototype. A ball on the watch face is rendered by two by two taxels.", "levels": null, "corpus_id": 2323656, "sentences": ["The RetroShape prototype.", "A ball on the watch face is rendered by two by two taxels."], "caption": "Figure 3. The RetroShape prototype, which is composed by (a) sixteen servo motors and taxels and (b) a 2\u201d display on the front face.", "local_uri": ["8fcff5229f4e115a16dc33daf40a87f8a427ba9e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches", "pdf_hash": "8fcff5229f4e115a16dc33daf40a87f8a427ba9e", "year": 2017, "venue": "UIST", "alt_text": "The authoring interface for RetroShape applications. A user is dragging a GIF image onto the key frame editor view.", "levels": null, "corpus_id": 2323656, "sentences": ["The authoring interface for RetroShape applications.", "A user is dragging a GIF image onto the key frame editor view."], "caption": "", "local_uri": ["8fcff5229f4e115a16dc33daf40a87f8a427ba9e_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches", "pdf_hash": "8fcff5229f4e115a16dc33daf40a87f8a427ba9e", "year": 2017, "venue": "UIST", "alt_text": "The Unity games we developed, including bouncing balls, a whack-a-mole game, a space shooter game, and an FPS game.", "levels": null, "corpus_id": 2323656, "sentences": ["The Unity games we developed, including bouncing balls, a whack-a-mole game, a space shooter game, and an FPS game."], "caption": "", "local_uri": ["8fcff5229f4e115a16dc33daf40a87f8a427ba9e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Polymorphic Blocks: Unifying High-level Specification and Low-level Control for Circuit Board Design", "pdf_hash": "009797efee7c3ac18435c2e337ef3ac775ebbae8", "year": 2020, "venue": "UIST", "alt_text": "The datalogger PCB assembly, a PCB with attached (soldered) parts. A microcontroller (LPC1549JBD64, in QFP-64 package) sits at the center, with other components on the board including an USB Type-C receptacle, an SD card socket, surface-mount tactile switches, surface-mount LEDs, a coin cell in holder, and connectors.", "levels": [[-1], [-1]], "corpus_id": 222805024, "sentences": ["The datalogger PCB assembly, a PCB with attached (soldered) parts.", "A microcontroller (LPC1549JBD64, in QFP-64 package) sits at the center, with other components on the board including an USB Type-C receptacle, an SD card socket, surface-mount tactile switches, surface-mount LEDs, a coin cell in holder, and connectors."], "caption": "Figure 3. A more complex example: the datalogger PCB produced with our system, further explained in Section 5.2.", "local_uri": ["009797efee7c3ac18435c2e337ef3ac775ebbae8_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: Shows a diagram of the ProtoSpray process's two stages - 3D printing a base object and spraying of electro luminescent (EL) material on top. The rest of figure 1 shows 3 ProtoSprayed objects, a cube with an EL S-shaped pattern covering three surfaces, a hemisphere with 4 concentric rings of EL cells and a mobius strip with 7 arrows (4 of which are visible and illuminated).", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 1: Shows a diagram of the ProtoSpray process's two stages - 3D printing a base object and spraying of electro luminescent (EL) material on top.", "The rest of figure 1 shows 3 ProtoSprayed objects, a cube with an EL S-shaped pattern covering three surfaces, a hemisphere with 4 concentric rings of EL cells and a mobius strip with 7 arrows (4 of which are visible and illuminated)."], "caption": "Figure 1. ProtoSpray is the \ufb01rst fabrication technique that combines 3D printing with spray coating to create interactive displays of arbitrary shapes. It uses mixed material printing of electrodes to create solid objects which are sprayed with layers of electroluminescent ink. Our prototypes (three of them shown (a-c)) demonstrate how ProtoSpray enables the creation of displays with complex curvatures, going further than any work done before.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: Shows two diagrams of the material layers involved in constructing EL cells traditionally and through ProtoSpray. From top too bottom the 5 layers are \"substrate, base electrode, dielectric, emissive layer, surface electrode\". There are two adjacent diagrams showing traditional flat layering and ProtoSpray layering on a 3d object where the base electrode takes the form of channels through the object.", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 2: Shows two diagrams of the material layers involved in constructing EL cells traditionally and through ProtoSpray.", "From top too bottom the 5 layers are \"substrate, base electrode, dielectric, emissive layer, surface electrode\".", "There are two adjacent diagrams showing traditional flat layering and ProtoSpray layering on a 3d object where the base electrode takes the form of channels through the object."], "caption": "Figure 2. Traditional EL layering (left) compared to ProtoSpray layer- ing, applied to an object without base electode masking (right). In tradi- tional EL display creation, the base electrode is applied to the substrate as an ink. In ProtoSpray, without masking, the base object is 3D printed on a multimaterial printer with the base electrode embedded into the substrate. The other three layers are then sprayed onto the surface.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: shows a diagram showing the cross section of a \"channelled\" object to explain which parts of the channel are the cell size (area at the top of the channel), cell spacing (distance between adjacent channels), channel length (height of channel) and channel width (diameter of channel).", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 3: shows a diagram showing the cross section of a \"channelled\" object to explain which parts of the channel are the cell size (area at the top of the channel), cell spacing (distance between adjacent channels), channel length (height of channel) and channel width (diameter of channel)."], "caption": "Figure 3. Channelled base electrodes in a 3D printed object.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: Shows three separate EL cells that have been sprayed with different parts of the layering structure being masked for each (see caption). Each EL cell is in an S-shape", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 4: Shows three separate EL cells that have been sprayed with different parts of the layering structure being masked for each (see caption).", "Each EL cell is in an S-shape"], "caption": "Figure 4. Test 1: three EL cells with the same mask used on different lay- ers: a) base electrode masked; b) light-emitting layer masked; c) surface electrode masked.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: Shows 4 cells as in figure 3. The first and the third (a and c) are blurry and observably lower resolution. The cells represent different types of masking", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 5: Shows 4 cells as in figure 3.", "The first and the third (a and c) are blurry and observably lower resolution.", "The cells represent different types of masking"], "caption": "Figure 5. Test 2: four EL cells sprayed onto \ufb02at substrates using tech- niques to de\ufb01ne cell shapes on the base electrode: a) hand masking; b) contact masking; c) shadow masking; d) 3D printer-shaped cells.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: Shows 5 3D objects/strips which have each been 3d printed and have a single EL cell running their length. Each has a different shape on top of a basic strip: 90 degree triangle, 60 degree triangle, 30 degree triangle, circular wave and hemisphere", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 6: Shows 5 3D objects/strips which have each been 3d printed and have a single EL cell running their length.", "Each has a different shape on top of a basic strip: 90 degree triangle, 60 degree triangle, 30 degree triangle, circular wave and hemisphere"], "caption": "Figure 6. Test 3: EL cells on different topologies. Rectangular cells are printed/sprayed over different angles (90 degrees, 60 degrees and 30 degrees), a curved semicircular prism and a hemispherical surface", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: Shows another 4 strips with EL cells of varying brightness (addressed in text). These are incrementally wavey (dimensions also addressed in text)", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 7: Shows another 4 strips with EL cells of varying brightness (addressed in text).", "These are incrementally wavey (dimensions also addressed in text)"], "caption": "Figure 7. Test 4: four EL cells on curved topologies of regularly incre- menting size, lit up with different levels of success.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 9: shows 3 test cells that are segmented spherical shaped with El S-shaped cells on them. They appear pretty similar although the third (C) sample looks like it has higher resolution. Descriptions of what the cells represent can be found in caption and the text.", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 9: shows 3 test cells that are segmented spherical shaped with El S-shaped cells on them.", "They appear pretty similar although the third (C) sample looks like it has higher resolution.", "Descriptions of what the cells represent can be found in caption and the text."], "caption": "Figure 9. Test 6: Cells printed on different scales of 3D printed resolu- tions. a) unsanded on a 0.15mm layer height; b) sanded on a 0.15mm layer height; c) unsanded on a 0.06mm layer height", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes", "pdf_hash": "3a086964499a2e54ce9c94f09ba165eb9d3d75ef", "year": 2020, "venue": "CHI", "alt_text": "Figure 10: shows 4 smaller images. The first is a 7 segment display on a flat surface as a demonstrator. The paint around the 7 EL cells is very smooth - demonstrating no on surface electrodes and spraying in a single coat. The second image is the CAD design for the channelling for the 7 segment display, clearly showing channels that would be overlapping if they were in two dimensions. The third image shows a 6x6 pixelated array on which is shown the number seven with 12 of the pixels lit up to show this. The last image is a widget shaped like a power cell with 5 EL cells, 4 of which are lit up.", "levels": null, "corpus_id": 218483072, "sentences": ["Figure 10: shows 4 smaller images.", "The first is a 7 segment display on a flat surface as a demonstrator.", "The paint around the 7 EL cells is very smooth - demonstrating no on surface electrodes and spraying in a single coat.", "The second image is the CAD design for the channelling for the 7 segment display, clearly showing channels that would be overlapping if they were in two dimensions.", "The third image shows a 6x6 pixelated array on which is shown the number seven with 12 of the pixels lit up to show this.", "The last image is a widget shaped like a power cell with 5 EL cells, 4 of which are lit up."], "caption": "Figure 10. a) The demonstration of a con\ufb01gurable 7-segment display us- ing ProtoSpray; b) A diagram of the design for the channelling that is required for the 7-segment display, showing channels that would other- wise overlap on a 2D surface; c) A 6x6 con\ufb01gurable matrix display cre- ated using the ProtoSpray process. Pixels are 4mmx4mm squares with 2mm in-between them; d) ProtoSpray on a curved surface to create an interactive power level indicator, 3D printed in the shape of a battery.", "local_uri": ["3a086964499a2e54ce9c94f09ba165eb9d3d75ef_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "PaNDa-Glove: A Sensory Substitution Glove for Peripheral Neuropathy", "pdf_hash": "e6eac0e10ca7f9264f54dc7dd39281d833ea8430", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Receptive glove with force sensors at the fingertips and an infrared temperature sensor on the palm.", "levels": null, "corpus_id": 218482572, "sentences": ["Receptive glove with force sensors at the fingertips and an infrared temperature sensor on the palm."], "caption": "", "local_uri": ["e6eac0e10ca7f9264f54dc7dd39281d833ea8430_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "PTeacher: a Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback", "pdf_hash": "d3088b9265cc1ea435b6a5a0da90d3e36c013ddd", "year": 2021, "venue": "CHI", "alt_text": "This figure contains 4 part. The first part is describing the process that the user's speech will deliver to MDD. The second part is audio and visual database. The third part is audio-visual generator. The fourth part is the user interface of our system.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987453, "sentences": ["This figure contains 4 part.", "The first part is describing the process that the user's speech will deliver to MDD.", "The second part is audio and visual database.", "The third part is audio-visual generator.", "The fourth part is the user interface of our system."], "caption": "Figure 2: The working fow of the exaggerated audio-visual corrective feedback. Mispronunciation detection and diagnosis (MDD) systems are employed to diagnose pronunciation mistakes at sentence, word, and phoneme levels. Afterwards, person- alized feedback are provided by the audio and visual exaggeration generators based on the MDD results.", "local_uri": ["d3088b9265cc1ea435b6a5a0da90d3e36c013ddd_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Snapstream: Snapshot-based Interaction in Live Streaming for Visual Art", "pdf_hash": "ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9", "year": 2020, "venue": "CHI", "alt_text": "An example of visual art live streaming: The player shows the visual art the streamer is working on. Viewers interact with the streamer and other viewers via text-based chat.", "levels": null, "corpus_id": 214811453, "sentences": ["An example of visual art live streaming: The player shows the visual art the streamer is working on.", "Viewers interact with the streamer and other viewers via text-based chat."], "caption": "", "local_uri": ["ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Snapstream: Snapshot-based Interaction in Live Streaming for Visual Art", "pdf_hash": "ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9", "year": 2020, "venue": "CHI", "alt_text": "Overview of Snapstream: (a) The player. (b) Users can share a snapshot in chat. (c) Streamers can refer to and answer a specific snapshot using voice. When the streamer mentions certain snapshot by voice, it will be highlighted for all users. (d) Users can mention a specific snapshot. (e) Users can take a snapshot and (f) filter the chat.", "levels": null, "corpus_id": 214811453, "sentences": ["Overview of Snapstream: (a) The player. (b) Users can share a snapshot in chat. (c) Streamers can refer to and answer a specific snapshot using voice.", "When the streamer mentions certain snapshot by voice, it will be highlighted for all users. (d) Users can mention a specific snapshot. (e) Users can take a snapshot and (f) filter the chat."], "caption": "", "local_uri": ["ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Snapstream: Snapshot-based Interaction in Live Streaming for Visual Art", "pdf_hash": "ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9", "year": 2020, "venue": "CHI", "alt_text": "(a) Users can crop and annotate the snapshot with text and shapes. (b) Users can choose the shape of the mark and its color.", "levels": null, "corpus_id": 214811453, "sentences": ["(a) Users can crop and annotate the snapshot with text and shapes. (b) Users can choose the shape of the mark and its color."], "caption": "", "local_uri": ["ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Snapstream: Snapshot-based Interaction in Live Streaming for Visual Art", "pdf_hash": "ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9", "year": 2020, "venue": "CHI", "alt_text": "Different use cases of snapshots in the studies. (a)-top: a user annotated on the image with the red circle (snapshots cropped to fit the table).", "levels": null, "corpus_id": 214811453, "sentences": ["Different use cases of snapshots in the studies.", "(a)-top: a user annotated on the image with the red circle (snapshots cropped to fit the table)."], "caption": "", "local_uri": ["ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Snapstream: Snapshot-based Interaction in Live Streaming for Visual Art", "pdf_hash": "ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9", "year": 2020, "venue": "CHI", "alt_text": "(b)-bottom: a user added a background object with red rectangles (snapshots cropped to fit the table).", "levels": null, "corpus_id": 214811453, "sentences": ["(b)-bottom: a user added a background object with red rectangles (snapshots cropped to fit the table)."], "caption": "", "local_uri": ["ad6bb40c3f24e8e1cfcc8e4b6d154c3f2d1a68f9_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Weaving by Touch: A Case Analysis of Accessible Making", "pdf_hash": "36821cb86e9adc3ecb23694fe31e85fef758a728", "year": 2020, "venue": "CHI", "alt_text": "Image shows a clear glass display case with various woven products, including a rug, baby bibs, bookmarks, mug rugs, and cane holders. A folded cane is also placed beside the cane holders.", "levels": null, "corpus_id": 210939299, "sentences": ["Image shows a clear glass display case with various woven products, including a rug, baby bibs, bookmarks, mug rugs, and cane holders.", "A folded cane is also placed beside the cane holders."], "caption": "Figure 3. Display case of products created by weavers with vision im- pairments within the community of study, including a rug, baby bibs, bookmarks, mug rugs and cane holders.", "local_uri": ["36821cb86e9adc3ecb23694fe31e85fef758a728_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 1: The teaser figure of this paper, consisting of three pictures from left to right. (a) an AR glasses and an infrared camera installed on the nose bridge of it (from front perspective). (b) a women wears the devices and uses her index finger touching her cheek. (c) an example grayscale image which contains a user's face and hand.", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 1: The teaser figure of this paper, consisting of three pictures from left to right. (a) an AR glasses and an infrared camera installed on the nose bridge of it (from front perspective). (b) a women wears the devices and uses her index finger touching her cheek.", "(c) an example grayscale image which contains a user's face and hand."], "caption": "Figure 1: (a) An infrared camera fxed on the nose bridge of an AR glasses looking downward. (b) The space around the user\u2019s lower face can be captured by the camera in high resolution. (c) An image captured by the camera.", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 2: On the left is Epson Moverio BT-300 AR glasses, while the right side is Nreal Light AR glasses. The figure describes the look of deploying camera on the two devices.", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 2: On the left is Epson Moverio BT-300 AR glasses, while the right side is Nreal Light AR glasses.", "The figure describes the look of deploying camera on the two devices."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_004.png"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 3: Two sketches with each containing a sector area covering a user's hand and face while touching the nose (from the front and lateral perspective).", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 3: Two sketches with each containing a sector area covering a user's hand and face while touching the nose (from the front and lateral perspective)."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 4: There are two different images consisting of face and background (e.g., chest, table) which are captured by camera. (a) high intensity. (b) low intensity.", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 4: There are two different images consisting of face and background (e.g., chest, table) which are captured by camera. (a) high intensity.", "(b) low intensity."], "caption": "Figure 4: The high contrast between face and background, and the efects of setting (a) higher or (b) lower lighting power to the camera.", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_006.gif"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 5: The sketches of 21 hand-to-face gestures proposed in this paper. Each sketch contains only the face and hand. First row is 7 hand-to-cheek gestures. Second row has three columns: 1) The first column has six hand-to-nose gestures. 2) The second column has four hand-to-mouth gestures. 3) The third column contains four hand-to-chin gestures.", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 5: The sketches of 21 hand-to-face gestures proposed in this paper.", "Each sketch contains only the face and hand.", "First row is 7 hand-to-cheek gestures.", "Second row has three columns: 1) The first column has six hand-to-nose gestures. 2) The second column has four hand-to-mouth gestures.", "3) The third column contains four hand-to-chin gestures."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 6: Flowchart of algorithm pipeline of FaceSight. a). Capture image from the camera. b) Segmentation of the facial region (and hand). c) detect touch contact. d) recognize touch location. e) classify hand-to-face gestures with CNN. f) acquire input features, including gesture recognition, fingertip locating, nose deformation estimation.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987153, "sentences": ["Figure 6: Flowchart of algorithm pipeline of FaceSight.", "a).", "Capture image from the camera.", "b) Segmentation of the facial region (and hand).", "c) detect touch contact.", "d) recognize touch location.", "e) classify hand-to-face gestures with CNN.", "f) acquire input features, including gesture recognition, fingertip locating, nose deformation estimation."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 7: There are six pictures in this figure to describe the segmentation result. (a) input image captured by camera, while (b), (c), (d), (e), (f) contains only the hand, nose, mouth, left cheek, right cheek respectively in the image.", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 7: There are six pictures in this figure to describe the segmentation result.", "(a) input image captured by camera, while (b), (c), (d), (e), (f) contains only the hand, nose, mouth, left cheek, right cheek respectively in the image."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_009.png"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 8: There are six pictures in this figure. Each picture composes of an application's user interface, a user who is performing the corresponding gesture, and the real world environment.", "levels": null, "corpus_id": 233987153, "sentences": ["Figure 8: There are six pictures in this figure.", "Each picture composes of an application's user interface, a user who is performing the corresponding gesture, and the real world environment."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "pdf_hash": "851f09fb796711c334599647d45759535163d36d", "year": 2021, "venue": "CHI", "alt_text": "Figure 9: Bar graph showing the rating of users' subjective feelings from 1 to 7 on the Y axis against the six evaluation metrics on the X axis.", "levels": [[1]], "corpus_id": 233987153, "sentences": ["Figure 9: Bar graph showing the rating of users' subjective feelings from 1 to 7 on the Y axis against the six evaluation metrics on the X axis."], "caption": "", "local_uri": ["851f09fb796711c334599647d45759535163d36d_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "pdf_hash": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "year": 2020, "venue": "UIST", "alt_text": "A user interacting with an encountered-type haptic device using our framework REACH+ in a series of four panels. Panel A shows the user with a VR headset and a virtual control panel overlaid on a table in front of them. A tabletop mobile robot acts as the encountered-type haptic device. Panel B highlights the users predicted reach target and illustrates the estimated nearest reachable point of the device. Panel C shows the users virtual hand being shifted toward the virtual target as they begin reaching forward. Panel D shows the users physical hand arriving at the reachable point, contacting the haptic device as the virtual hand arrives at the virtual target several inches away.", "levels": null, "corpus_id": 222805049, "sentences": ["A user interacting with an encountered-type haptic device using our framework REACH+ in a series of four panels.", "Panel A shows the user with a VR headset and a virtual control panel overlaid on a table in front of them.", "A tabletop mobile robot acts as the encountered-type haptic device.", "Panel B highlights the users predicted reach target and illustrates the estimated nearest reachable point of the device.", "Panel C shows the users virtual hand being shifted toward the virtual target as they begin reaching forward.", "Panel D shows the users physical hand arriving at the reachable point, contacting the haptic device as the virtual hand arrives at the virtual target several inches away."], "caption": "Figure 1. (A) A user in VR interacts with a virtual control panel rendered by an encountered-type haptic device. Our framework: (B) predicts their intended target and arrival time, estimates a point pp reachable by the device, and then (C) redirects the user\u2019s hand to the reachable point to minimize visuo-haptic discrepancy and ensure (D) haptic feedback is rendered.", "local_uri": ["91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "pdf_hash": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "year": 2020, "venue": "UIST", "alt_text": "An image of the encountered type haptic device. On the left, an isometric view showing a 3D printed haptic proxy mounted to the top. On the right, a side view shows the diameter of the device is 28 cm, the height is 9 cm, and the proxy width is 3 cm.", "levels": null, "corpus_id": 222805049, "sentences": ["An image of the encountered type haptic device.", "On the left, an isometric view showing a 3D printed haptic proxy mounted to the top.", "On the right, a side view shows the diameter of the device is 28 cm, the height is 9 cm, and the proxy width is 3 cm."], "caption": "", "local_uri": ["91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "pdf_hash": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "year": 2020, "venue": "UIST", "alt_text": "The reach sequence of a physical hand and its virtual counterpart are shown. Both hands start at the same position p0. The virtual hand reaches toward a virtual target pv. The physical hand Hp is redirected toward a physical target pp by the application of a vector offset W at each point in time.", "levels": null, "corpus_id": 222805049, "sentences": ["The reach sequence of a physical hand and its virtual counterpart are shown.", "Both hands start at the same position p0.", "The virtual hand reaches toward a virtual target pv.", "The physical hand Hp is redirected toward a physical target pp by the application of a vector offset W at each point in time."], "caption": "Figure 7. Diagram of reach redirection [4]. At each timestep, virtual hand position is computed as a vector offset W from the physical hand position, based on the hand\u2019s displacement in the target direction d\u02c6.", "local_uri": ["91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "pdf_hash": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "year": 2020, "venue": "UIST", "alt_text": "Figure 9A shows the results of device position accuracy evaluation. It is a spatial plot showing 8 targets equally spaced in a circle. Measured data points of device final position are overlaid, showing a small average error of 3.5 mm. Figure 9B shows average measured device speed versus target direction relative to the EHD. For a desired speed of 20 cm/s, the average measured speed was 19.4 cm/s. For 25 cm/s, the measured average was 26.6 cm/s. For 30 cm/s, it was 31.5 cm/s. For 35 cm/s, it was 35.6 cm/s.", "levels": [[1], [1], [2, 1], [1], [2]], "corpus_id": 222805049, "sentences": ["Figure 9A shows the results of device position accuracy evaluation.", "It is a spatial plot showing 8 targets equally spaced in a circle.", "Measured data points of device final position are overlaid, showing a small average error of 3.5 mm.", "Figure 9B shows average measured device speed versus target direction relative to the EHD.", "For a desired speed of 20 cm/s, the average measured speed was 19.4 cm/s. For 25 cm/s, the measured average was 26.6 cm/s. For 30 cm/s, it was 31.5 cm/s. For 35 cm/s, it was 35.6 cm/s."], "caption": "", "local_uri": ["91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "pdf_hash": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "year": 2020, "venue": "UIST", "alt_text": "Figure 14A shows a plot of visuo-haptic latency in seconds versus device speed in cm/s. The mean latency for no redirection, limited REACH+, and REACH+ conditions are shown for each speed. The data is as follows. For 20 cm/s device speed: No Redirection = 0.22 s, Limited REACH+ = 0.13 s, REACH+ = 0.07 s. For 25 cm/s device speed: No Redirection = 0.06 s, Limited REACH+ = 0.04 s, REACH+ = 0.075 s. For 30 cm/s device speed: No Redirection = 0.02 s, Limited REACH+ = 0.03 s, REACH+ = 0.05 s. For 35 cm/s device speed: No Redirection = 0.04 s, Limited REACH+ = 0.03 s, REACH+ = 0.035 s. For 20 cm/s device speed, significant differences were found between no redirection and Limited REACH+ (p<0.001) and no redirection and REACH+ (p<0.001). Figure 14B shows a plot of on time arrival rate versus device speed in cm/s. The on time arrival rate of the EHD for no redirection, limited REACH+, and REACH+ conditions are shown for each speed. The data is as follows. For 20 cm/s device speed: No Redirection = 0.43, Limited REACH+ = 0.60, REACH+ = 0.71. For 25 cm/s device speed: No Redirection = 0.79, Limited REACH+ = 0.87, REACH+ = 0.77. For 30 cm/s device speed: No Redirection = 0.87, Limited REACH+ = 0.90, REACH+ = 0.80. For 35 cm/s device speed: No Redirection = 0.85, Limited REACH+ = 0.86, REACH+ = 0.85. For 20 cm/s device speed, significant differences were found between no redirection and Limited REACH+ (p<0.01) and no redirection and REACH+ (p<0.001).", "levels": [[1], [2], [2], [1], [2], [2], [2], [2], [2], [2]], "corpus_id": 222805049, "sentences": ["Figure 14A shows a plot of visuo-haptic latency in seconds versus device speed in cm/s. The mean latency for no redirection, limited REACH+, and REACH+ conditions are shown for each speed.", "The data is as follows.", "For 20 cm/s device speed: No Redirection = 0.22 s, Limited REACH+ = 0.13 s, REACH+ = 0.07 s. For 25 cm/s device speed: No Redirection = 0.06 s, Limited REACH+ = 0.04 s, REACH+ = 0.075 s. For 30 cm/s device speed: No Redirection = 0.02 s, Limited REACH+ = 0.03 s, REACH+ = 0.05 s. For 35 cm/s device speed: No Redirection = 0.04 s, Limited REACH+ = 0.03 s, REACH+ = 0.035 s. For 20 cm/s device speed, significant differences were found between no redirection and Limited REACH+ (p<0.001) and no redirection and REACH+ (p<0.001).", "Figure 14B shows a plot of on time arrival rate versus device speed in cm/s. The on time arrival rate of the EHD for no redirection, limited REACH+, and REACH+ conditions are shown for each speed.", "The data is as follows.", "For 20 cm/s device speed: No Redirection = 0.43, Limited REACH+ = 0.60, REACH+ = 0.71.", "For 25 cm/s device speed: No Redirection = 0.79, Limited REACH+ = 0.87, REACH+ = 0.77.", "For 30 cm/s device speed: No Redirection = 0.87, Limited REACH+ = 0.90, REACH+ = 0.80.", "For 35 cm/s device speed: No Redirection = 0.85, Limited REACH+ = 0.86, REACH+ = 0.85.", "For 20 cm/s device speed, significant differences were found between no redirection and Limited REACH+ (p<0.01) and no redirection and REACH+ (p<0.001)."], "caption": "", "local_uri": ["91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_013.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Biographical Prototypes: Reimagining Recognition and Disability in Design", "pdf_hash": "ee07ddedac7fea69527b2986818d2acbd17b31a3", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "A photograph depicting a conventional peeler with its handle covered in bright purple foam. Betsy Farber made the peeler more comfortable to hold by making the handle larger and easier to grip.", "levels": [[-1], [-1]], "corpus_id": 195259529, "sentences": ["A photograph depicting a conventional peeler with its handle covered in bright purple foam.", "Betsy Farber made the peeler more comfortable to hold by making the handle larger and easier to grip."], "caption": "Figure 1: Betsy Farber\u2019s OXO biographical prototype could illustrate her work by depicting the handle of a conventional peeler covered in foam or Play-Doh paired with a vignette describing her firsthand experience coming up with the idea to adapt the peeler to be comfortable for her grip.", "local_uri": ["ee07ddedac7fea69527b2986818d2acbd17b31a3_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Biographical Prototypes: Reimagining Recognition and Disability in Design", "pdf_hash": "ee07ddedac7fea69527b2986818d2acbd17b31a3", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "Participants working together to create biographical prototypes in small groups at the downtown public library.", "levels": [[-1]], "corpus_id": 195259529, "sentences": ["Participants working together to create biographical prototypes in small groups at the downtown public library."], "caption": "Figure 2: Participants working together to create biographical prototypes in small groups at the downtown public library.", "local_uri": ["ee07ddedac7fea69527b2986818d2acbd17b31a3_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Biographical Prototypes: Reimagining Recognition and Disability in Design", "pdf_hash": "ee07ddedac7fea69527b2986818d2acbd17b31a3", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "Photo from first workshop depicting disabled activists, designers, and developers creating a wide variety of biographical prototypes.", "levels": [[-1]], "corpus_id": 195259529, "sentences": ["Photo from first workshop depicting disabled activists, designers, and developers creating a wide variety of biographical prototypes."], "caption": "", "local_uri": ["ee07ddedac7fea69527b2986818d2acbd17b31a3_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Biographical Prototypes: Reimagining Recognition and Disability in Design", "pdf_hash": "ee07ddedac7fea69527b2986818d2acbd17b31a3", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "Photo from second workshop depicting disabled activists, designers, and developers creating a wide variety of biographical prototypes.", "levels": [[-1]], "corpus_id": 195259529, "sentences": ["Photo from second workshop depicting disabled activists, designers, and developers creating a wide variety of biographical prototypes."], "caption": "", "local_uri": ["ee07ddedac7fea69527b2986818d2acbd17b31a3_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Biographical Prototypes: Reimagining Recognition and Disability in Design", "pdf_hash": "ee07ddedac7fea69527b2986818d2acbd17b31a3", "year": 2019, "venue": "Conference on Designing Interactive Systems", "alt_text": "Photo from third workshop depicting disabled activists, designers, and developers creating a wide variety of biographical prototypes.", "levels": [[-1]], "corpus_id": 195259529, "sentences": ["Photo from third workshop depicting disabled activists, designers, and developers creating a wide variety of biographical prototypes."], "caption": "", "local_uri": ["ee07ddedac7fea69527b2986818d2acbd17b31a3_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "PantoGuide: A Haptic and Audio Guidance System To Support Tactile Graphics Exploration", "pdf_hash": "ae5bcaa963cb0b92a4e99ab7b0f0674c32d56e68", "year": 2020, "venue": "ASSETS", "alt_text": "Banner Image with three images labelled A, B, and C. a) A user wearing the haptic guidance device and touching a tactile graphic on top of a touchscreen. The device has labels for the tactor, pantograph mechanism, DC motor, magnetic encoder, wrist sleeve. The tactile graphic and touchscreen are also labelled.  b) A tactile graphic of a bar chart. c) A tactile graphic of a small marble or radius r sitting at rest at the top of an inclined plane with a loop de loop at the end of the incline plane.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 225093486, "sentences": ["Banner Image with three images labelled A, B, and C. a) A user wearing the haptic guidance device and touching a tactile graphic on top of a touchscreen.", "The device has labels for the tactor, pantograph mechanism, DC motor, magnetic encoder, wrist sleeve.", "The tactile graphic and touchscreen are also labelled.", "b) A tactile graphic of a bar chart.", "c) A tactile graphic of a small marble or radius r sitting at rest at the top of an inclined plane with a loop de loop at the end of the incline plane."], "caption": "Figure 1: a) PantoGuide is a system that provides haptic and audio guidance cues to a user while exploring a tactile graphic. The device has a tactor in contact with the user that moves in a 2D plane (red arrows) to provide skin-stretch feedback. We demonstrate its use in two applications: b) providing point-to-point directional guidance cues when a user explores diferent elements of a tactile bar chart, and c) providing continuous guidance on the trajectory of a moving marble", "local_uri": ["ae5bcaa963cb0b92a4e99ab7b0f0674c32d56e68_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Leveraging Text-Chart Links to Support Authoring of Data-Driven Articles with VizFlow", "pdf_hash": "bcc673a68e752f837250ae914624dedf05472e75", "year": 2021, "venue": "CHI", "alt_text": "A sequence of sixteen snapshots of a long scrollytelling article created with VizFlow. The sequence is complemented with a colour code designating the scope of each document section and the reflow mode associated to it.", "levels": null, "corpus_id": 233987085, "sentences": ["A sequence of sixteen snapshots of a long scrollytelling article created with VizFlow.", "The sequence is complemented with a colour code designating the scope of each document section and the reflow mode associated to it."], "caption": "", "local_uri": ["bcc673a68e752f837250ae914624dedf05472e75_Image_035.jpg"], "annotated": false, "compound": false}
{"title": "Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments", "pdf_hash": "c0d867d3d13cbd748d77e960b554f8ea2eb0860d", "year": 2020, "venue": "CHI", "alt_text": "In this figure, there are two screenshots depicted of what study and workshop participants saw during the evaluation (in the workshop) and the study. The left (called a) shows a street with the front of a red vehicle on the left and the same vehicle on the right. Across the street, there are multiple high buildings with some fire ladders attached. Two vehicles are also parked in the driveway. Straight ahead, there is a green column and an arrow is floating mid-air showing towards this green column. This column is a waypoint, indicating where participants of the study had to walk to. This was not present in the workshop. On the right side (called b) we see down the street to our left. Multiple vehicles are approaching, which are all red. On the sidewalk, there are some people talking to each other. On both sides of the street, there are high buildings. It is sunny.", "levels": null, "corpus_id": 216635205, "sentences": ["In this figure, there are two screenshots depicted of what study and workshop participants saw during the evaluation (in the workshop) and the study.", "The left (called a) shows a street with the front of a red vehicle on the left and the same vehicle on the right.", "Across the street, there are multiple high buildings with some fire ladders attached.", "Two vehicles are also parked in the driveway.", "Straight ahead, there is a green column and an arrow is floating mid-air showing towards this green column.", "This column is a waypoint, indicating where participants of the study had to walk to.", "This was not present in the workshop.", "On the right side (called b) we see down the street to our left.", "Multiple vehicles are approaching, which are all red.", "On the sidewalk, there are some people talking to each other.", "On both sides of the street, there are high buildings.", "It is sunny."], "caption": "Figure 1: VR simulation used in the workshop and study. (a) The view straight ahead and (b) the view to the left.", "local_uri": ["c0d867d3d13cbd748d77e960b554f8ea2eb0860d_Image_003.png"], "annotated": false, "compound": false}
{"title": "Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments", "pdf_hash": "c0d867d3d13cbd748d77e960b554f8ea2eb0860d", "year": 2020, "venue": "CHI", "alt_text": "Figure 4 shows the apparatus for the study. It is divided into the apparatus for seeing participants (a) and the apparatus for the participants with vision impairments. In subfigure a, a young man stands in a room. He wears a green T-shirt and beige shorts. On his head, there is a virtual reality headset and Sony headphones. In his right hand, he is holding a virtual reality controller from the HTC Vive Pro. There is a cable from the virtual reality headset to a non-visible computer. Directly in front of him is a quarter round on the floor. There is some free space and then there is another quarter round. In the background, there are some stacked chairs. In subfigure b, a hallway is depicted. On the floor, there are again two quarter rounds, fixated with adhesive tape. Three people are in this picture. On the very left, there is an experimenter holding a cable. In the middle, there is a blind study participant. She is wearing black trousers and a red pullover. On her head, there is a virtual reality headset and Sony headphones. She is holding a walking cane for the blind in her right hand. In the background, there is a second experimenter. He is crouched over a laptop.", "levels": null, "corpus_id": 216635205, "sentences": ["Figure 4 shows the apparatus for the study.", "It is divided into the apparatus for seeing participants (a) and the apparatus for the participants with vision impairments.", "In subfigure a, a young man stands in a room.", "He wears a green T-shirt and beige shorts.", "On his head, there is a virtual reality headset and Sony headphones.", "In his right hand, he is holding a virtual reality controller from the HTC Vive Pro.", "There is a cable from the virtual reality headset to a non-visible computer.", "Directly in front of him is a quarter round on the floor.", "There is some free space and then there is another quarter round.", "In the background, there are some stacked chairs.", "In subfigure b, a hallway is depicted.", "On the floor, there are again two quarter rounds, fixated with adhesive tape.", "Three people are in this picture.", "On the very left, there is an experimenter holding a cable.", "In the middle, there is a blind study participant.", "She is wearing black trousers and a red pullover.", "On her head, there is a virtual reality headset and Sony headphones.", "She is holding a walking cane for the blind in her right hand.", "In the background, there is a second experimenter.", "He is crouched over a laptop."], "caption": "Figure 4: Study setting (a) for seeing participants and (b) for VIP.", "local_uri": ["c0d867d3d13cbd748d77e960b554f8ea2eb0860d_Image_006.png"], "annotated": false, "compound": false}
{"title": "Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments", "pdf_hash": "c0d867d3d13cbd748d77e960b554f8ea2eb0860d", "year": 2020, "venue": "CHI", "alt_text": "This figure shows 2 plots for dominance (a and b). Subfigure a shows an interaction plot. On the x-axis, there are the two levels for the theme \"information content\" (high and low). On the y-axis, there is the dominance score from 4.45 to 4.80. A red line (red standing for two vehicles communicating) is starting left at approximately 4.65 and goes down to 4.45 five in the low content condition. A blue line (indicating one vehicle communicating) is also starting at approximately 4.65 but goes up to 4.80 for the low content condition.  Subfigure b shows for the x-axis the two levels of sight: seeing on the left and VIP on the right. For the y-axis, dominance is depicted from 4.3 to 4.9. The red line (indicating low information content) starts at 4.7 for the seeing going down to 4.3 for the VIP). The blue line indicating high information content, starts at 4.5 and goes up to 4.9 for the VIP.", "levels": [[1], [1], [1], [1], [2, 1], [2, 1], [1], [1], [2, 1], [2, 1]], "corpus_id": 216635205, "sentences": ["This figure shows 2 plots for dominance (a and b).", "Subfigure a shows an interaction plot.", "On the x-axis, there are the two levels for the theme \"information content\" (high and low).", "On the y-axis, there is the dominance score from 4.45 to 4.80.", "A red line (red standing for two vehicles communicating) is starting left at approximately 4.65 and goes down to 4.45 five in the low content condition.", "A blue line (indicating one vehicle communicating) is also starting at approximately 4.65 but goes up to 4.80 for the low content condition.", "Subfigure b shows for the x-axis the two levels of sight: seeing on the left and VIP on the right.", "For the y-axis, dominance is depicted from 4.3 to 4.9.", "The red line (indicating low information content) starts at 4.7 for the seeing going down to 4.3 for the VIP).", "The blue line indicating high information content, starts at 4.5 and goes up to 4.9 for the VIP."], "caption": "Figure 6: Interaction effects for dominance: (a) equal for high content independent of the number of communicating vehicles. With low content, dominance was higher when one vehicle was communicating. (b) Dominance for the VIP was low for low content and high with high content. Seeing participants reported similar dominance values for low and the high content.", "local_uri": ["c0d867d3d13cbd748d77e960b554f8ea2eb0860d_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Mapping the Potential and Pitfalls of \"Data Dividends\" as a Means of Sharing the Profits of Artificial Intelligence", "pdf_hash": "60edc823de283d0b39d6269be3c18877eb489d93", "year": 2019, "venue": "ArXiv", "alt_text": "Shows Gini indices for each combination of task, transform, and machine learning model.", "levels": [[1]], "corpus_id": 208526947, "sentences": ["Shows Gini indices for each combination of task, transform, and machine learning model."], "caption": "Figure 1. Gini index for data dividends across tasks and models. A lower Gini index means more equality. Colors distinguish different models. Each row shows a different task. Dotted lines show country-level 2016 income Gini indices.", "local_uri": ["60edc823de283d0b39d6269be3c18877eb489d93_Image_001.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mapping the Potential and Pitfalls of \"Data Dividends\" as a Means of Sharing the Profits of Artificial Intelligence", "pdf_hash": "60edc823de283d0b39d6269be3c18877eb489d93", "year": 2019, "venue": "ArXiv", "alt_text": "Shows that the Shift transform maintains a relatively equal distribution of dividends while the Absolute Value and Clipping transforms create very unequal, long-tail distributions with \"superstars\" who earn 25-50x larger dividends.", "levels": [[3]], "corpus_id": 208526947, "sentences": ["Shows that the Shift transform maintains a relatively equal distribution of dividends while the Absolute Value and Clipping transforms create very unequal, long-tail distributions with \"superstars\" who earn 25-50x larger dividends."], "caption": "Figure 2. Left: Histogram of loss changes for Adult Income dataset and logistic regression. Middle and Right: Corresponding dividends for Shift, Absolute Value and Clipping transforms, shown in dollars for a total Funding pool of $36k, the total number of contributors (i.e. so the mean dividend is $1). Counts are shown on a log scale.", "local_uri": ["60edc823de283d0b39d6269be3c18877eb489d93_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Pseudo-code to distinguish between erros and edits. The algorithm is described throghout section 3.3.", "levels": [[-1], [-1]], "corpus_id": 17352998, "sentences": ["Pseudo-code to distinguish between erros and edits.", "The algorithm is described throghout section 3.3."], "caption": "Figure 1. Algorithm to distinguish between an edit and an error.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "The figure shows four examples of backspaced and final text and how the algortihm classifies each of the input streams (whether error or edit).", "levels": [[-1]], "corpus_id": 17352998, "sentences": ["The figure shows four examples of backspaced and final text and how the algortihm classifies each of the input streams (whether error or edit)."], "caption": "Figure 2. Four input streams and how to distinguish errors from edits. a) backspaces are classified as errors, since all character corrections are adjacent; b) the backspaced text is a different and valid word, showing a change in mind/intent from black to brown; c) the spellchecker return a suggestion that is equal to the re-entered text, meaning that backspaces were errors; d) spellchecker does not return results, but words are significantly different, thus we assume it is an edit.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_012.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Participants slowly improve performance from week to week; however, for P2 there is a significant performance drop in week 3, followed by a steady improvement, and for P4 there is a sudden performance improvement in week 7.", "levels": null, "corpus_id": 17352998, "sentences": ["Participants slowly improve performance from week to week; however, for P2 there is a significant performance drop in week 3, followed by a steady improvement, and for P4 there is a sudden performance improvement in week 7."], "caption": "Figure 3. Words per minute over 8 weeks.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_023.png"], "annotated": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Error rates decrease over the 8-week period. Substitutions are clearly the most common error type.", "levels": [[3], [2]], "corpus_id": 17352998, "sentences": ["Error rates decrease over the 8-week period.", "Substitutions are clearly the most common error type."], "caption": "Figure 5. Total error rate for each type of error.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_026.png"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "There is a clear predominance of horizontal overlaps, particularly on keys N and M.", "levels": [[-1]], "corpus_id": 17352998, "sentences": ["There is a clear predominance of horizontal overlaps, particularly on keys N and M."], "caption": "Figure 6. Polygons encompass hit points within a standard deviation of", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_028.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Bubbles are clearly larger in week one (twice the size of week 8). Some keys such as S, E, and backspace have larger bubbles in week 1.", "levels": null, "corpus_id": 17352998, "sentences": ["Bubbles are clearly larger in week one (twice the size of week 8).", "Some keys such as S, E, and backspace have larger bubbles in week 1."], "caption": "Figure 7. A circle indicates a pause; size represents its duration. Left - week 1 for P1, Right - week 8 for P1.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_029.jpg"], "annotated": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Lift points are scattered over intended keys. There is no clear offset pattern. Keys nears edges have lower variability.", "levels": [[-1], [-1], [-1]], "corpus_id": 17352998, "sentences": ["Lift points are scattered over intended keys.", "There is no clear offset pattern.", "Keys nears edges have lower variability."], "caption": "Figure 8. Lift points for all participants in week 8.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_030.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Participants slowly improve performance from week to week as in laboratory settings; however, everyday typing performance is consistently higher than lab performance (average 1.5 times).", "levels": [[3, 2]], "corpus_id": 17352998, "sentences": ["Participants slowly improve performance from week to week as in laboratory settings; however, everyday typing performance is consistently higher than lab performance (average 1.5 times)."], "caption": "Figure 9. Words per minute for each participant over 12 weeks.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_035.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Investigating Laboratory and Everyday Typing Performance of Blind Users", "pdf_hash": "664b42914e065121362a835185aaa07e08ea0075", "year": 2017, "venue": "ACM Trans. Access. Comput.", "alt_text": "Uncorrected error rate is slightly higher in everyday typing tasks compared with laboratory performance. This results holds true for all participants.", "levels": [[2], [2]], "corpus_id": 17352998, "sentences": ["Uncorrected error rate is slightly higher in everyday typing tasks compared with laboratory performance.", "This results holds true for all participants."], "caption": "Figure 10. Uncorrected error rate for each participant over 12 weeks.", "local_uri": ["664b42914e065121362a835185aaa07e08ea0075_Image_036.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Pictograms of the movements performed by the music conductors and music conductor students. Each pictogram shows the hand, hand position, and movement which was performed. The presented pictograms visually summarise what is described in the text.", "levels": null, "corpus_id": 233987477, "sentences": ["Pictograms of the movements performed by the music conductors and music conductor students.", "Each pictogram shows the hand, hand position, and movement which was performed.", "The presented pictograms visually summarise what is described in the text."], "caption": "", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_002.png"], "annotated": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Picture of the setup used in the spatiality experiment. The picture shows four boxes, the biggest box on the left side (400x400 mm) and the smallest on the right side (100x100 mm). The other boxes have the dimensions of 200x200 mm and 300x300 mm. In front of the boxes are the cards with the positive password qualities that have been assigned to that box.", "levels": null, "corpus_id": 233987477, "sentences": ["Picture of the setup used in the spatiality experiment.", "The picture shows four boxes, the biggest box on the left side (400x400 mm) and the smallest on the right side (100x100 mm).", "The other boxes have the dimensions of 200x200 mm and 300x300 mm.", "In front of the boxes are the cards with the positive password qualities that have been assigned to that box."], "caption": "Figure 2: The setup of the experiment. Cards containing the positive password qualities have been assigned to the difer- ent spatialities.", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Picture of the 11 created mock-ups. All mock-ups were painted white and were designed to elicit one of the positive password gestures created by the music conductor(s) (students).", "levels": null, "corpus_id": 233987477, "sentences": ["Picture of the 11 created mock-ups.", "All mock-ups were painted white and were designed to elicit one of the positive password gestures created by the music conductor(s) (students)."], "caption": "", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Image of the three device mock-ups. On the left side, a mock-up called \"the twist\". The twist is a cylindrical design, which can be twisted. In the middle, a mock-up called the \"joystick\". Just as a real joystick it can be pushed and twisted in all directions. It has a small stick with a round knob attached to it. On the right side, a mock-up called \"the wave\". The wave is a slate with a wave shaped texture on it. People can move their fingers within the ridges of the mock-up.", "levels": null, "corpus_id": 233987477, "sentences": ["Image of the three device mock-ups.", "On the left side, a mock-up called \"the twist\".", "The twist is a cylindrical design, which can be twisted.", "In the middle, a mock-up called the \"joystick\".", "Just as a real joystick it can be pushed and twisted in all directions.", "It has a small stick with a round knob attached to it.", "On the right side, a mock-up called \"the wave\".", "The wave is a slate with a wave shaped texture on it.", "People can move their fingers within the ridges of the mock-up."], "caption": "", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_005.png"], "annotated": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Image of the next iteration and the interaction. For this iteration, the twist was redesigned to be completely cylindrical and consisting of 2 parts: a longer bottom half and a smaller top half. The bottom half can be grabbed with one hand and the top can be used to enter the password. The image consist of three parts. The left part shows how to start the password, by lifting the top part, as such indicating a start of the password. The middle part shows that by rotating, you can perform your password. Lastly, the right part shows how to enter the password, by pressing the top part down again.", "levels": null, "corpus_id": 233987477, "sentences": ["Image of the next iteration and the interaction.", "For this iteration, the twist was redesigned to be completely cylindrical and consisting of 2 parts: a longer bottom half and a smaller top half.", "The bottom half can be grabbed with one hand and the top can be used to enter the password.", "The image consist of three parts.", "The left part shows how to start the password, by lifting the top part, as such indicating a start of the password.", "The middle part shows that by rotating, you can perform your password.", "Lastly, the right part shows how to enter the password, by pressing the top part down again."], "caption": "", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_006.png"], "annotated": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Image of the 3 x 3 grid. On the complete left, the image shows the 3 bottom parts with different diameters (30, 60 and 120 mm), but the same height of 40 mm. The next row shows the caps with a diameter of 30 mm, but with a changing height (20, 40 and 80 mm). The same applies to the other rows, where the diameter of the caps are 60 or 120 mm.", "levels": null, "corpus_id": 233987477, "sentences": ["Image of the 3 x 3 grid.", "On the complete left, the image shows the 3 bottom parts with different diameters (30, 60 and 120 mm), but the same height of 40 mm.", "The next row shows the caps with a diameter of 30 mm, but with a changing height (20, 40 and 80 mm).", "The same applies to the other rows, where the diameter of the caps are 60 or 120 mm."], "caption": "", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Design Space of Embodied Passwords based on Muscle Memory", "pdf_hash": "7ff4c74a66fa2e6534a828be003211ee8b8c4c7d", "year": 2021, "venue": "CHI", "alt_text": "Image of the two functional prototypes. On the left: the bigger prototype, and on the right the smaller prototype. The image shows each prototype being held and used to enter a password.", "levels": null, "corpus_id": 233987477, "sentences": ["Image of the two functional prototypes.", "On the left: the bigger prototype, and on the right the smaller prototype.", "The image shows each prototype being held and used to enter a password."], "caption": "", "local_uri": ["7ff4c74a66fa2e6534a828be003211ee8b8c4c7d_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "pdf_hash": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "year": 2020, "venue": "CHI", "alt_text": "Four images labeled from (a) to (d) illustrate how to use a smartphone and smartwatch. (a) A man wears a head-mounted display and uses two hands to hold the smartphone with his thumb of his right hand touching the screen. A smartphone in landscape orientation shown below has its full screen colored grey, indicating touchable area. (b) A man wears a head-mounted display and uses his right hand to hold the smartphone with his thumb touching the screen. A smartphone in portrait orientation shown below has its upper half colored black and lower half colored grey. (c) A man wears a head-mounted display and a smartwatch on his left arm. He uses the index finger of the right hand to touch the smartwatch. A watch shown below has its full touchscreen colored grey. (d) A man wears a head-mounted display and a smartwatch on his right arm. He raises his arm and points forward. A watch shown below has its full touchscreen colored black.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218482459, "sentences": ["Four images labeled from (a) to (d) illustrate how to use a smartphone and smartwatch. (", "a) A man wears a head-mounted display and uses two hands to hold the smartphone with his thumb of his right hand touching the screen.", "A smartphone in landscape orientation shown below has its full screen colored grey, indicating touchable area. (", "b) A man wears a head-mounted display and uses his right hand to hold the smartphone with his thumb touching the screen.", "A smartphone in portrait orientation shown below has its upper half colored black and lower half colored grey. (", "c) A man wears a head-mounted display and a smartwatch on his left arm.", "He uses the index finger of the right hand to touch the smartwatch.", "A watch shown below has its full touchscreen colored grey. (", "d) A man wears a head-mounted display and a smartwatch on his right arm.", "He raises his arm and points forward.", "A watch shown below has its full touchscreen colored black."], "caption": "", "local_uri": ["7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "pdf_hash": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "year": 2020, "venue": "CHI", "alt_text": "VC is connected to Tap. VB and WD are both connected to all five pointing techniques (P2TR, P2TA, P1RR, W2TR, W1RR) respectively. Except W1RR is connected to Wrist Rotate, techniques are connected to Tap.", "levels": null, "corpus_id": 218482459, "sentences": ["VC is connected to Tap.", "VB and WD are both connected to all five pointing techniques (P2TR, P2TA, P1RR, W2TR, W1RR) respectively.", "Except W1RR is connected to Wrist Rotate, techniques are connected to Tap."], "caption": "Figure 4. Summarized pointing conditions generated by combinations of pointing paradigms and pointing techniques and matched with corre- sponding selection mechanisms.", "local_uri": ["7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_004.png"], "annotated": false, "compound": false}
{"title": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "pdf_hash": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "year": 2020, "venue": "CHI", "alt_text": "The cubic space is decorated with fabrics. A man is sitting on a chair, wearing the Microsoft Hololens. Through the viewport of Hololens, he can see a cyan target at the intersection edges of the front and left wall. The position of this cyan target indicates the highest position of the targets in the AR space. Besides, he can see a red large sphere(large target), a red arrow and a white arrow inside the viewport. These two arrows indicate the direction of the target and cursor outside of the viewport respectively. He can also see a red line. This red line connects the large sphere inside the viewport and another target on the wall right to the man. A small red target next to the viewport and on the front wall is shown as a size reference.", "levels": null, "corpus_id": 218482459, "sentences": ["The cubic space is decorated with fabrics.", "A man is sitting on a chair, wearing the Microsoft Hololens. Through the viewport of Hololens, he can see a cyan target at the intersection edges of the front and left wall.", "The position of this cyan target indicates the highest position of the targets in the AR space.", "Besides, he can see a red large sphere(large target), a red arrow and a white arrow inside the viewport.", "These two arrows indicate the direction of the target and cursor outside of the viewport respectively.", "He can also see a red line.", "This red line connects the large sphere inside the viewport and another target on the wall right to the man.", "A small red target next to the viewport and on the front wall is shown as a size reference."], "caption": "Figure 5. The experiment setup and real and virtual environments a participant can see through the Hololens.", "local_uri": ["7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "pdf_hash": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "year": 2020, "venue": "CHI", "alt_text": "A combination plot of a bar chart for mean error rate and a point chart using cross circle, coloured orange, for mean offset distance (in cm). The left y-axis indicates mean error rate and the right y-axis indicates the mean offset distance. The x-axis indicates the pointing condition. The first bar is VC, coloured grey. The 2nd, 3rd, and 4th bars, coloured yellow, represent pointing techniques with VB, and 5th, 6th, 7th bars, coloured blue, represent pointing techniques with WD. The error bar of each pointing conditions and the significant difference between each other are also visualised.", "levels": [[1], [1], [1], [1], [1], [1]], "corpus_id": 218482459, "sentences": ["A combination plot of a bar chart for mean error rate and a point chart using cross circle, coloured orange, for mean offset distance (in cm).", "The left y-axis indicates mean error rate and the right y-axis indicates the mean offset distance.", "The x-axis indicates the pointing condition.", "The first bar is VC, coloured grey.", "The 2nd, 3rd, and 4th bars, coloured yellow, represent pointing techniques with VB, and 5th, 6th, 7th bars, coloured blue, represent pointing techniques with WD.", "The error bar of each pointing conditions and the significant difference between each other are also visualised."], "caption": "Figure 7. Mean %Err (bar) and OD (cross circle) for pointing conditions. The corresponding error bars are visualized to indicate standard devia- tion. The statistical signi\ufb01cances evaluated by pairwise t-test are marked with stars (\u2217\u2217 = p < 0.01 and \u2217 = p < 0.05).", "local_uri": ["7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "pdf_hash": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "year": 2020, "venue": "CHI", "alt_text": "Bar chart of median completion time in milliseconds for the 7 pointing conditions in the smartphone study. The y-axis indicates median completion time and the x-axis indicates the pointing condition. The first bar is VC, coloured grey, which is faster than other pointing conditions. The 2nd, 3rd, and 4th bars, coloured yellow, represent pointing techniques with VB, which are faster than pointing techniques with WD (5th, 6th, 7th bar), coloured blue.  The significant difference between each other is also visualised.", "levels": [[1], [1], [2], [2], [1]], "corpus_id": 218482459, "sentences": ["Bar chart of median completion time in milliseconds for the 7 pointing conditions in the smartphone study.", "The y-axis indicates median completion time and the x-axis indicates the pointing condition.", "The first bar is VC, coloured grey, which is faster than other pointing conditions.", "The 2nd, 3rd, and 4th bars, coloured yellow, represent pointing techniques with VB, which are faster than pointing techniques with WD (5th, 6th, 7th bar), coloured blue.", "The significant difference between each other is also visualised."], "caption": "Figure 6. Median CT for pointing conditions. The statistical signi\ufb01- cances evaluated by pairwise t-test are marked with stars (\u2217\u2217 = p < 0.01 and \u2217 = p < 0.05).", "local_uri": ["7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "pdf_hash": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "year": 2020, "venue": "CHI", "alt_text": "Bar chart of mean score on each NASA TLX attribute for 7 pointing conditions in smartphone study. The y-axis indicates mean score and the x-axis indicates TLX attributes. For each attribute, each bar represents each pointing condition in the following sequence: VC, VBP2TR, VBP2TA, VBP1RR, WDP2TR, WDP2TA and WDP1RR. The error bar of each pointing conditions and the significant difference between each other are also visualised.", "levels": [[1], [1], [1], [1]], "corpus_id": 218482459, "sentences": ["Bar chart of mean score on each NASA TLX attribute for 7 pointing conditions in smartphone study.", "The y-axis indicates mean score and the x-axis indicates TLX attributes.", "For each attribute, each bar represents each pointing condition in the following sequence: VC, VBP2TR, VBP2TA, VBP1RR, WDP2TR, WDP2TA and WDP1RR.", "The error bar of each pointing conditions and the significant difference between each other are also visualised."], "caption": "Figure 8. The mean responses for the attributes of NASA TLX questionnaire. The standard error bar is visualized. The statistical signi\ufb01cances evaluated by Wilcoxon rank sum test are marked with stars (\u2217\u2217 = p < 0.01 and \u2217 = p < 0.05).", "local_uri": ["7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Constructive Visualization to Inform the Design and Exploration of Tactile Data Representations", "pdf_hash": "dda64d3fadc0eba6ce009dc5902f0eba451afcbf", "year": 2020, "venue": "ASSETS", "alt_text": "Left image shows a sketch of a table with: a box with four dividers labelled textured tokens, a set of four tokens and braille labels next to each token labeled as legend, 8 long bars labelled reference bars, a laptop and keyboard labelled laptop with spreadsheet, a sheet of paper labelled Braille table, and a set of short bars labelled Braille labels. Right image shows a sketch of a table with: a sketch of a bar graph labelled tactile graphic, a sheet of paper labelled Braille table, and a laptop and keyboard labelled laptop with spreadsheet.", "levels": [[-1], [-1]], "corpus_id": 226068689, "sentences": ["Left image shows a sketch of a table with: a box with four dividers labelled textured tokens, a set of four tokens and braille labels next to each token labeled as legend, 8 long bars labelled reference bars, a laptop and keyboard labelled laptop with spreadsheet, a sheet of paper labelled Braille table, and a set of short bars labelled Braille labels.", "Right image shows a sketch of a table with: a sketch of a bar graph labelled tactile graphic, a sheet of paper labelled Braille table, and a laptop and keyboard labelled laptop with spreadsheet."], "caption": "", "local_uri": ["dda64d3fadc0eba6ce009dc5902f0eba451afcbf_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Constructive Visualization to Inform the Design and Exploration of Tactile Data Representations", "pdf_hash": "dda64d3fadc0eba6ce009dc5902f0eba451afcbf", "year": 2020, "venue": "ASSETS", "alt_text": "Plot comparing frequency of coded gestures (lateral, enclosure, counting, contour, two hands, one hand) between the construction and consumption conditions for two questions.", "levels": [[1]], "corpus_id": 226068689, "sentences": ["Plot comparing frequency of coded gestures (lateral, enclosure, counting, contour, two hands, one hand) between the construction and consumption conditions for two questions."], "caption": "", "local_uri": ["dda64d3fadc0eba6ce009dc5902f0eba451afcbf_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Multi-touch Interactions on Small Input Devices", "pdf_hash": "62a947f8750273a3ea6bcf0e43fcc355762bf3cf", "year": 2016, "venue": "", "alt_text": "http://mtv.mtvnimages.com/onair/the_hills/season_4/images/photos/episodes/401/11-spencer-xbox.jpg?width=600", "levels": null, "corpus_id": 57178934, "sentences": ["http://mtv.mtvnimages.com/onair/the_hills/season_4/images/photos/episodes/401/11-spencer-xbox.jpg?width=600"], "caption": "focus is NOT on the device", "local_uri": ["62a947f8750273a3ea6bcf0e43fcc355762bf3cf_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Designing AI to Work WITH or FOR People?", "pdf_hash": "0735b46e892be58a7b46ad3cde31fdd96145ebe7", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A screenshot of the UI of IBM AutoAI system. To the top it shows a visualization with tree-based layout, a path represents a pipeline, a node represents a step that pipeline goes through (e.g., data cleaning or model training)", "levels": [[-1], [-1]], "corpus_id": 233987632, "sentences": ["A screenshot of the UI of IBM AutoAI system.", "To the top it shows a visualization with tree-based layout, a path represents a pipeline, a node represents a step that pipeline goes through (e.g., data cleaning or model training)"], "caption": "", "local_uri": ["0735b46e892be58a7b46ad3cde31fdd96145ebe7_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "pdf_hash": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "year": 2015, "venue": "UIST", "alt_text": "The Capricate fabrication pipeline consists of four subsequent steps: First, a 3D model is designed or downloaded. Second, a user designs touch-sensitive parts in a specialized design tool directly on the 3D model. Third, the 3D model is 3D printed. Fourth, user can either use the 3D printed object standalone with a micro controller or directly on a capacitive surface (e.g., a tablet) for interactive applications.", "levels": null, "corpus_id": 2217868, "sentences": ["The Capricate fabrication pipeline consists of four subsequent steps: First, a 3D model is designed or downloaded.", "Second, a user designs touch-sensitive parts in a specialized design tool directly on the 3D model.", "Third, the 3D model is 3D printed.", "Fourth, user can either use the 3D printed object standalone with a micro controller or directly on a capacitive surface (e.g., a tablet) for interactive applications."], "caption": "", "local_uri": ["1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "pdf_hash": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "year": 2015, "venue": "UIST", "alt_text": "Designing with Capricate: In a design tool, a user (a) chooses a sensor type, selects an approximate location on the 3D surface, and (b) fine-tunes the selection. The tool (c) automatically wires the sensors to endpoints, and (d) generates  files for conductive and non-conductive 3D parts for 3D printing.", "levels": [[-1], [-1]], "corpus_id": 2217868, "sentences": ["Designing with Capricate: In a design tool, a user (a) chooses a sensor type, selects an approximate location on the 3D surface, and (b) fine-tunes the selection.", "The tool (c) automatically wires the sensors to endpoints, and (d) generates  files for conductive and non-conductive 3D parts for 3D printing."], "caption": "Figure 2: Designing with Capricate: A user (a) chooses a sensor type, selects an approximate location on the surface, and (b) \ufb01ne-tunes the selection. The tool (c) automatically wires the sensors to endpoints, and (d) generates the \ufb01les for 3D printing.", "local_uri": ["1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "pdf_hash": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "year": 2015, "venue": "UIST", "alt_text": "Comparing the distribution of touch points (gray circles) on a custom star shape in 2D: (a) The standard touch grid layout requires many electrodes that are not uniformly distributed and have different sizes due to the star's shape. (b) Our surface touch electrodes technique uniformly distributes touch points inside the star's shape, resulting in more uniformly distributed touch electrodes with approximately equal size.", "levels": [[-1], [-1]], "corpus_id": 2217868, "sentences": ["Comparing the distribution of touch points (gray circles) on a custom star shape in 2D: (a) The standard touch grid layout requires many electrodes that are not uniformly distributed and have different sizes due to the star's shape. (", "b) Our surface touch electrodes technique uniformly distributes touch points inside the star's shape, resulting in more uniformly distributed touch electrodes with approximately equal size."], "caption": "", "local_uri": ["1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "pdf_hash": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "year": 2015, "venue": "UIST", "alt_text": "Cross-section of the Himalaya mountains model showing touch sensing with: (a) Curved electrodes on the surface, that closely follow the object's mountain surface. (b) Flat electrodes in the subsurface, that lie underneath the object's surface. The black parts are touch-sensitive.", "levels": null, "corpus_id": 2217868, "sentences": ["Cross-section of the Himalaya mountains model showing touch sensing with: (a) Curved electrodes on the surface, that closely follow the object's mountain surface.", "(b) Flat electrodes in the subsurface, that lie underneath the object's surface.", "The black parts are touch-sensitive."], "caption": "Figure 3: Cross-section of the Himalaya mountains model showing touch sensing with (a) curved electrodes on the sur\u00ad face, and (b) \ufb02at electrodes in the subsurface (black parts are touch-sensitive).", "local_uri": ["1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "pdf_hash": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "year": 2015, "venue": "UIST", "alt_text": "A series of example objects for three application scenarios: First, for physical input prototyping a hemispherical input device, a spring-based pushbutton, a physical slider, and a directional pad on a smartphone is shown. Second, for wearable computing, a doubly-curved bracelet on an arm, that detects a linear touch position, and a person wearing a touch-sensitive glass frame, that can recognize touch-gestures on its front and left side, is shown. Third, for printed tangible user interfaces, the name of a touched mountain is shown on the Himalaya mountains model via top-projection.", "levels": null, "corpus_id": 2217868, "sentences": ["A series of example objects for three application scenarios: First, for physical input prototyping a hemispherical input device, a spring-based pushbutton, a physical slider, and a directional pad on a smartphone is shown.", "Second, for wearable computing, a doubly-curved bracelet on an arm, that detects a linear touch position, and a person wearing a touch-sensitive glass frame, that can recognize touch-gestures on its front and left side, is shown.", "Third, for printed tangible user interfaces, the name of a touched mountain is shown on the Himalaya mountains model via top-projection."], "caption": "Figure 5: A series of example objects for (a) physical input prototyping, (b) wearable computing, and (c) printed TUIs.", "local_uri": ["1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "pdf_hash": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "year": 2015, "venue": "UIST", "alt_text": "Comparison of touch / no touch for varying overlay thicknesses of PLA.  A graph shows the asymptotic dependency between increasing overlay thicknesses (from 0 to 20 mm) and the Signal-to-noise ratio. For thicknesses up to 10 mm the Signal-to-noise ratio drastically decreases with increasing thickness (resulting in a high gradient), but lies above a minimal value of 5, indicating robust measurements. For higher thicknesses the gradient asymptotically decreases.  Furthermore, a table lists the measured average capacitance readings when touched and not touched by a finger (including standard deviations), and the respective Signal-to-noise ratio for varying overlay thicknesses of 0.25, 0.5, 1, 2, 10, and 15 mm.", "levels": [[1], [3, 2], [3, 2], [3], [0, 2, 1]], "corpus_id": 2217868, "sentences": ["Comparison of touch / no touch for varying overlay thicknesses of PLA.", "A graph shows the asymptotic dependency between increasing overlay thicknesses (from 0 to 20 mm) and the Signal-to-noise ratio.", "For thicknesses up to 10 mm the Signal-to-noise ratio drastically decreases with increasing thickness (resulting in a high gradient), but lies above a minimal value of 5, indicating robust measurements.", "For higher thicknesses the gradient asymptotically decreases.", "Furthermore, a table lists the measured average capacitance readings when touched and not touched by a finger (including standard deviations), and the respective Signal-to-noise ratio for varying overlay thicknesses of 0.25, 0.5, 1, 2, 10, and 15 mm."], "caption": "= 194.799, p < 0.001). For thicker overlays, the SNR falls below a minimal SNR of 5:1, which is generally considered as the lower bound for robust touch detection. Therefore, \ufb02at subsurface touch electrodes can be placed at most 10 mm un\u00ad derneath the outermost point on the surface; further the maxi\u00ad mum height difference (along the surface normal of the touch electrodes) between any two points on the surface cannot ex\u00ad ceed 10 mm. If these requirements cannot be met, the sensor should be implemented using the curved surface touch elec\u00ad trodes technique.", "local_uri": ["1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "\u201cData Strikes\u201d: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "pdf_hash": "41cbffad975874060d643c36c8bdb5c72637564e", "year": 2019, "venue": "WWW", "alt_text": "Shows an ontological diagram: a joint data strike/boycott consists of a pure data strike and traditional boycott. Pure data strikes have direct indirect effects and traditional boycotts also have direct and indirect effects.", "levels": [[-1], [-1]], "corpus_id": 153313841, "sentences": ["Shows an ontological diagram: a joint data strike/boycott consists of a pure data strike and traditional boycott.", "Pure data strikes have direct indirect effects and traditional boycotts also have direct and indirect effects."], "caption": "", "local_uri": ["41cbffad975874060d643c36c8bdb5c72637564e_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\u201cData Strikes\u201d: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "pdf_hash": "41cbffad975874060d643c36c8bdb5c72637564e", "year": 2019, "venue": "WWW", "alt_text": "Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced.", "levels": [[1], [3]], "corpus_id": 153313841, "sentences": ["Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes.", "As size of boycotts/data strikes increases, Surfaced Hits are reduced."], "caption": "", "local_uri": ["41cbffad975874060d643c36c8bdb5c72637564e_Image_004.jpg", "41cbffad975874060d643c36c8bdb5c72637564e_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "\u201cData Strikes\u201d: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "pdf_hash": "41cbffad975874060d643c36c8bdb5c72637564e", "year": 2019, "venue": "WWW", "alt_text": "Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced. Zoomed in on effects between 0.71 and 0.78.", "levels": [[1], [3], [2]], "corpus_id": 153313841, "sentences": ["Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes.", "As size of boycotts/data strikes increases, Surfaced Hits are reduced.", "Zoomed in on effects between 0.71 and 0.78."], "caption": "", "local_uri": ["41cbffad975874060d643c36c8bdb5c72637564e_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "\u201cData Strikes\u201d: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "pdf_hash": "41cbffad975874060d643c36c8bdb5c72637564e", "year": 2019, "venue": "WWW", "alt_text": "Plot of Effect of Surfaced Hits on ML-20M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced. Zoomed in on effects between 0.71 and 0.78.", "levels": [[1], [3], [3, 2]], "corpus_id": 153313841, "sentences": ["Plot of Effect of Surfaced Hits on ML-20M. Shows boycotts and data strikes.", "As size of boycotts/data strikes increases, Surfaced Hits are reduced.", "Zoomed in on effects between 0.71 and 0.78."], "caption": "", "local_uri": ["41cbffad975874060d643c36c8bdb5c72637564e_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "\u201cData Strikes\u201d: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "pdf_hash": "41cbffad975874060d643c36c8bdb5c72637564e", "year": 2019, "venue": "WWW", "alt_text": "Scatterplot shows that the uniqueness of a group, measured by implicit rating cosine distance from centroid, correlates with the Similar User Effect Ratio.", "levels": [[3, 1]], "corpus_id": 153313841, "sentences": ["Scatterplot shows that the uniqueness of a group, measured by implicit rating cosine distance from centroid, correlates with the Similar User Effect Ratio."], "caption": "", "local_uri": ["41cbffad975874060d643c36c8bdb5c72637564e_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Combining Embedded Computation and Image Tracking for Composing Tangible Augmented Reality", "pdf_hash": "e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 1: \"A system purely based on marker tracking (top) cannot augment blocks, which are occluded or perspectively distorted. The approach combining marker tracking and embedded computation (bottom) overlays both compositions correctly.\"", "levels": null, "corpus_id": 218483269, "sentences": ["Figure 1: \"A system purely based on marker tracking (top) cannot augment blocks, which are occluded or perspectively distorted.", "The approach combining marker tracking and embedded computation (bottom) overlays both compositions correctly.\""], "caption": "", "local_uri": ["e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Combining Embedded Computation and Image Tracking for Composing Tangible Augmented Reality", "pdf_hash": "e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2: \"Hardware inside the tangibles. For the microprocessor, the Wemos D1 Mini, which includes a WiFi module, is used. Each block contains a battery shield, a lithium-ion polymer rechargeable battery, and a USB charging cable.\"", "levels": null, "corpus_id": 218483269, "sentences": ["Figure 2: \"Hardware inside the tangibles.", "For the microprocessor, the Wemos D1 Mini, which includes a WiFi module, is used.", "Each block contains a battery shield, a lithium-ion polymer rechargeable battery, and a USB charging cable.\""], "caption": "", "local_uri": ["e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10_Image_004.png"], "annotated": false, "compound": false}
{"title": "Combining Embedded Computation and Image Tracking for Composing Tangible Augmented Reality", "pdf_hash": "e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 4: \"Setup for evaluation of tracking robustness against occlusions. One of two connected blocks is occluded using four different laser-cut wooden rectangles of width 10cm and heights corresponding to 25, 50, 75, and 100% occlusion.\"", "levels": null, "corpus_id": 218483269, "sentences": ["Figure 4: \"Setup for evaluation of tracking robustness against occlusions.", "One of two connected blocks is occluded using four different laser-cut wooden rectangles of width 10cm and heights corresponding to 25, 50, 75, and 100% occlusion.\""], "caption": "", "local_uri": ["e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Combining Embedded Computation and Image Tracking for Composing Tangible Augmented Reality", "pdf_hash": "e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 5: \"Setup for evaluating tracking robustness against perspective distortions, where the same blocks at the same distance are rotated step-wise by 10 degrees between -70 and +70 degrees.\"", "levels": null, "corpus_id": 218483269, "sentences": ["Figure 5: \"Setup for evaluating tracking robustness against perspective distortions, where the same blocks at the same distance are rotated step-wise by 10 degrees between -70 and +70 degrees.\""], "caption": "", "local_uri": ["e2cf5bc40a5c9fc63da63c18ecd8496d5a424d10_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Engaging with Nature Sounds & Citizen Science by Designing for Creative & Contextual Audio Encounters", "pdf_hash": "71b95b8287702b4dd8c43bbcb52655400d180c3c", "year": 2021, "venue": "CHI", "alt_text": "Figure 3 shows a spectrogram of a single Eastern bristlebird call, and then two copies of the spectrogram that people creatively sketched over.", "levels": null, "corpus_id": 233987898, "sentences": ["Figure 3 shows a spectrogram of a single Eastern bristlebird call, and then two copies of the spectrogram that people creatively sketched over."], "caption": "", "local_uri": ["71b95b8287702b4dd8c43bbcb52655400d180c3c_Image_007.png"], "annotated": false, "compound": false}
{"title": "Including Adults with Severe Intellectual Disabilities in Co-Design through Active Support", "pdf_hash": "943fc27eafa4685c8d0e74e658a97b56d396b0d4", "year": 2021, "venue": "CHI", "alt_text": "Figure 1. (Image on the left). A picture containing two people (a support worker and a person with disability). The person with disability is touching a symbol on the iPad which then activates sound on the Talking Board.  (Image on the right).  A picture containing the TalkingBoard, iPad and six different symbol card laid down on the table. The Board has six pannels made of conductive foil that activates sounds. LED light also indicate when the sound is activated. The six panels have a velcro dots enabling the user to place the images on the Tangible board. Beside the tangible layout the TalkingBoard can be controlled with the Ipad which is also laid down on the table.", "levels": null, "corpus_id": 233987568, "sentences": ["Figure 1. (Image on the left).", "A picture containing two people (a support worker and a person with disability).", "The person with disability is touching a symbol on the iPad which then activates sound on the Talking Board.", "(Image on the right).", "A picture containing the TalkingBoard, iPad and six different symbol card laid down on the table.", "The Board has six pannels made of conductive foil that activates sounds.", "LED light also indicate when the sound is activated.", "The six panels have a velcro dots enabling the user to place the images on the Tangible board.", "Beside the tangible layout the TalkingBoard can be controlled with the Ipad which is also laid down on the table."], "caption": "", "local_uri": ["943fc27eafa4685c8d0e74e658a97b56d396b0d4_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Including Adults with Severe Intellectual Disabilities in Co-Design through Active Support", "pdf_hash": "943fc27eafa4685c8d0e74e658a97b56d396b0d4", "year": 2021, "venue": "CHI", "alt_text": "A close up of the sensory vest. We can see the sensory vest laid down on the table with a pair of hands attaching different sensory objects to the vest. The vest is colorful but the color that it is most distributed is green.", "levels": null, "corpus_id": 233987568, "sentences": ["A close up of the sensory vest.", "We can see the sensory vest laid down on the table with a pair of hands attaching different sensory objects to the vest.", "The vest is colorful but the color that it is most distributed is green."], "caption": "", "local_uri": ["943fc27eafa4685c8d0e74e658a97b56d396b0d4_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "SolutionChat: Real-time Moderator Support for Chat-based Structured Discussion", "pdf_hash": "9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"The overview of SolutionChat: (A) agenda panel for showing the discussion structure, (B) current discussion stage and featured opinions, (C) stage divider, (D) button for add a message as a featured opinion, (E) inline message recommendations for moderators to add short reactions to discussants' opinions, and (F) block message recommendations with generic facilitation messages for moderators to use.\"", "levels": null, "corpus_id": 218483544, "sentences": ["Figure 1: \"The overview of SolutionChat: (A) agenda panel for showing the discussion structure, (B) current discussion stage and featured opinions, (C) stage divider, (D) button for add a message as a featured opinion, (E) inline message recommendations for moderators to add short reactions to discussants' opinions, and (F) block message recommendations with generic facilitation messages for moderators to use.\""], "caption": "Figure 1. The overview of SolutionChat: (A) agenda panel for showing the discussion structure, (B) current discussion stage and featured opinions,", "local_uri": ["9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "SolutionChat: Real-time Moderator Support for Chat-based Structured Discussion", "pdf_hash": "9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"The distribution of moderation message counts per support type. Moderators mainly provided pedagogical and managerial support to discussants during chat-based discussion.\"", "levels": [[1], [0]], "corpus_id": 218483544, "sentences": ["Figure 2: \"The distribution of moderation message counts per support type.", "Moderators mainly provided pedagogical and managerial support to discussants during chat-based discussion.\""], "caption": "Figure 2. A distribution of moderation message counts per support type. Moderators mainly provided pedagogical and managerial support to dis- cussants during chat-based discussion.", "local_uri": ["9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SolutionChat: Real-time Moderator Support for Chat-based Structured Discussion", "pdf_hash": "9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"Moderation message counts per condition. For AP+MR condition, user-generated messages and system-recommended messages are distinguished.\"", "levels": null, "corpus_id": 218483544, "sentences": ["Figure 3: \"Moderation message counts per condition.", "For AP+MR condition, user-generated messages and system-recommended messages are distinguished.\""], "caption": "Figure 3. Moderation message counts per condition. For AP+MR con- dition, user-generated messages and system-recommended messages are distinguished.", "local_uri": ["9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "SolutionChat: Real-time Moderator Support for Chat-based Structured Discussion", "pdf_hash": "9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: \"A distribution of moderation message counts per support type. Moderators mainly provided pedagogical and managerial support to discussants during chat-based discussion.\"", "levels": [[1], [0]], "corpus_id": 218483544, "sentences": ["Figure 4: \"A distribution of moderation message counts per support type.", "Moderators mainly provided pedagogical and managerial support to discussants during chat-based discussion.\""], "caption": "Figure 4. A distribution of moderation message counts per support type. Moderators mainly provided pedagogical and managerial support to dis- cussants during chat-based discussion.", "local_uri": ["9f6637a115774d3c04a3ae8e9dbef0d74e0b3d37_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ElectroRing: Subtle Pinch and Touch Detection with a Ring", "pdf_hash": "abf5272fbc52ad80cecc0d046e386a92d6978014", "year": 2021, "venue": "CHI", "alt_text": "Three photographs, each one showing a different example application. Each photo shows the user's hand with the ring as they are interacting with a GUI on a computer display.", "levels": [[-1], [-1]], "corpus_id": 233987524, "sentences": ["Three photographs, each one showing a different example application.", "Each photo shows the user's hand with the ring as they are interacting with a GUI on a computer display."], "caption": "", "local_uri": ["abf5272fbc52ad80cecc0d046e386a92d6978014_Image_034.jpg", "abf5272fbc52ad80cecc0d046e386a92d6978014_Image_035.jpg", "abf5272fbc52ad80cecc0d046e386a92d6978014_Image_036.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Motion-Based Video Games for Older Adults in Long-Term Care", "pdf_hash": "f1b4b0f5a17b3ca53cc50749e1dd834569df1c02", "year": 2014, "venue": "", "alt_text": "A screenshot showing the game, Cupcake Heaven. The image displays two lanes of candy and vegetables at the bottom and the top of the screen, and a middle lane in which the player can feed candy to the child that is displayed on the left side of the screen. The player avatar is a hand.", "levels": null, "corpus_id": 142544187, "sentences": ["A screenshot showing the game, Cupcake Heaven.", "The image displays two lanes of candy and vegetables at the bottom and the top of the screen, and a middle lane in which the player can feed candy to the child that is displayed on the left side of the screen.", "The player avatar is a hand."], "caption": "", "local_uri": ["f1b4b0f5a17b3ca53cc50749e1dd834569df1c02_Image_019.jpg"], "annotated": false, "compound": false}
{"title": "Motion-Based Video Games for Older Adults in Long-Term Care", "pdf_hash": "f1b4b0f5a17b3ca53cc50749e1dd834569df1c02", "year": 2014, "venue": "", "alt_text": "The player experience profile for Emma and Andrew shows that it is imbalanced, with the caregiver Andrew reporting higher levels of ease of use and competence, but low levels for communication and partner preference. In contrast, results for Emma, the older adult, show that she thought that Andrew and her communicated well and that she preferred playing with him, but that she did not perceive the game easy to use, and that she did not feel competent.", "levels": null, "corpus_id": 142544187, "sentences": ["The player experience profile for Emma and Andrew shows that it is imbalanced, with the caregiver Andrew reporting higher levels of ease of use and competence, but low levels for communication and partner preference.", "In contrast, results for Emma, the older adult, show that she thought that Andrew and her communicated well and that she preferred playing with him, but that she did not perceive the game easy to use, and that she did not feel competent."], "caption": "", "local_uri": ["f1b4b0f5a17b3ca53cc50749e1dd834569df1c02_Image_036.jpg"], "annotated": false, "compound": false}
{"title": "Motion-Based Video Games for Older Adults in Long-Term Care", "pdf_hash": "f1b4b0f5a17b3ca53cc50749e1dd834569df1c02", "year": 2014, "venue": "", "alt_text": "Player experience profile for Jim and Sarah, showing very balanced results for both older adult and caregiver, with all ratings being above three on a five point Likert scale except for competence result of caregiver Sarah, which is lower.", "levels": null, "corpus_id": 142544187, "sentences": ["Player experience profile for Jim and Sarah, showing very balanced results for both older adult and caregiver, with all ratings being above three on a five point Likert scale except for competence result of caregiver Sarah, which is lower."], "caption": "", "local_uri": ["f1b4b0f5a17b3ca53cc50749e1dd834569df1c02_Image_037.jpg"], "annotated": false, "compound": false}
{"title": "MEMEography: Understanding Users Through Internet Memes", "pdf_hash": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Image macro with the text: \"Planning out my shift like\". Underneath it shoes a Simpson comic image of a woman writing on a clipboard: \"0800 - Snow the patient. 1000 - Look at memes. 1200 - Order a takeout. 1850 - Wean sedation, pull foley, 40 of Lasix, give suppository\".", "levels": null, "corpus_id": 233987846, "sentences": ["Image macro with the text: \"Planning out my shift like\".", "Underneath it shoes a Simpson comic image of a woman writing on a clipboard: \"0800 - Snow the patient.", "1000 - Look at memes.", "1200 - Order a takeout. 1850 - Wean sedation, pull foley, 40 of Lasix, give suppository\"."], "caption": "", "local_uri": ["9683f0243631f6b8ecaa8bece24ea0fb848f2dde_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "MEMEography: Understanding Users Through Internet Memes", "pdf_hash": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Column chart of amount of posts (ranging from 0 to about 300) over the time span 2015 until 2020, where bars show increasing numbers of posts over time. Stacked bars differentiate still image posts from video posts. In all cases the amount of video posts is lower than the amount of still image posts.", "levels": [[3, 1], [1], [2]], "corpus_id": 233987846, "sentences": ["Column chart of amount of posts (ranging from 0 to about 300) over the time span 2015 until 2020, where bars show increasing numbers of posts over time.", "Stacked bars differentiate still image posts from video posts.", "In all cases the amount of video posts is lower than the amount of still image posts."], "caption": "", "local_uri": ["9683f0243631f6b8ecaa8bece24ea0fb848f2dde_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "MEMEography: Understanding Users Through Internet Memes", "pdf_hash": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Stacked area chart of word count cluster amounts over time (2020) where amounts of Nurse word cluster is the highest throughout time. COVID-19 word cluster amounts increase strongly around March, April and May and decrease afterwards.", "levels": [[2], [3]], "corpus_id": 233987846, "sentences": ["Stacked area chart of word count cluster amounts over time (2020) where amounts of Nurse word cluster is the highest throughout time.", "COVID-19 word cluster amounts increase strongly around March, April and May and decrease afterwards."], "caption": "", "local_uri": ["9683f0243631f6b8ecaa8bece24ea0fb848f2dde_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TinyBlackBox : Recovering the Lost Interactions of Mobile Inthe-Wild Studies", "pdf_hash": "3decef1ef1a3b654003d3fd379a060a800a05207", "year": 2015, "venue": "", "alt_text": "Screen shot of the web-based TBB Player. three sections, 1) the rendered screen cotent of the mobile deivce showing the app drawer page. 2) the page DOM tree structure and 3) the individual node information for the Facebook icon page element.", "levels": null, "corpus_id": 16686288, "sentences": ["Screen shot of the web-based TBB Player.", "three sections, 1) the rendered screen cotent of the mobile deivce showing the app drawer page. 2) the page DOM tree structure and 3) the individual node information for the Facebook icon page element."], "caption": "", "local_uri": ["3decef1ef1a3b654003d3fd379a060a800a05207_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities", "pdf_hash": "e79cd418e238686f30e4b8f2660ab7e729fbec17", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 1 shows two drill-and-practice website screenshots. The website to the left is Khan Academy. The website to the right is IXL Learning. The figure highlights the math problem panel and the student input panel on both websites. The figure also highlights the smart score panel on IXL Learning that displays the studen's performance score.", "levels": null, "corpus_id": 226069006, "sentences": ["Figure 1 shows two drill-and-practice website screenshots.", "The website to the left is Khan Academy.", "The website to the right is IXL Learning.", "The figure highlights the math problem panel and the student input panel on both websites.", "The figure also highlights the smart score panel on IXL Learning that displays the studen's performance score."], "caption": "", "local_uri": ["e79cd418e238686f30e4b8f2660ab7e729fbec17_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities", "pdf_hash": "e79cd418e238686f30e4b8f2660ab7e729fbec17", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 2 shows two digital math game screenshots. The game to the left is Slice Fractions, which has a woolly mammoth that gets blocked by a red lava cube. The player is required to slice an equal amount of blue ice cube to diminish the red cube so that the mammoth can pass. The game to the right is ST Math, which requires the player to do multiplication by selecting the number of shoes that fit the number of flamingos so that flamingos can wear the shoes in order to run away from the scene.", "levels": null, "corpus_id": 226069006, "sentences": ["Figure 2 shows two digital math game screenshots.", "The game to the left is Slice Fractions, which has a woolly mammoth that gets blocked by a red lava cube.", "The player is required to slice an equal amount of blue ice cube to diminish the red cube so that the mammoth can pass.", "The game to the right is ST Math, which requires the player to do multiplication by selecting the number of shoes that fit the number of flamingos so that flamingos can wear the shoes in order to run away from the scene."], "caption": "", "local_uri": ["e79cd418e238686f30e4b8f2660ab7e729fbec17_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities", "pdf_hash": "e79cd418e238686f30e4b8f2660ab7e729fbec17", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 3 is a bar graph that shows the number of participants who used a specific math e-learning tool. The data is sorted in descending order. 4 people used IXL Learning; 4 people used Khan Academy; 3 people used CMP3; 2 people used Flocabulary; 2 people used Kahoot!; 2 people used ST Math; 1 person used BrainPOP; 1 person used Prodigy; 1 person used DareDash; 1 person used Jungle Math; and 1 person used Slice Fractions.", "levels": [[1], [1], [2], [2]], "corpus_id": 226069006, "sentences": ["Figure 3 is a bar graph that shows the number of participants who used a specific math e-learning tool.", "The data is sorted in descending order.", "4 people used IXL Learning; 4 people used Khan Academy; 3 people used CMP3; 2 people used Flocabulary; 2 people used Kahoot!;", "2 people used ST Math; 1 person used BrainPOP; 1 person used Prodigy; 1 person used DareDash; 1 person used Jungle Math; and 1 person used Slice Fractions."], "caption": "", "local_uri": ["e79cd418e238686f30e4b8f2660ab7e729fbec17_Image_009.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "\u201cThey\u2019re blowing up my phone\u201d: Group Messaging Practices Among Adolescents", "pdf_hash": "06f810047de0edb8100e00e6054d4c6fc2ce9c44", "year": 2015, "venue": "", "alt_text": "Example message groups plotted along the three dimentions (focus, membership, and duration)", "levels": [[-1]], "corpus_id": 38862011, "sentences": ["Example message groups plotted along the three dimentions (focus, membership, and duration)"], "caption": "Figure 2: Example Messaging Group Dimensions", "local_uri": ["06f810047de0edb8100e00e6054d4c6fc2ce9c44_Image_002.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "This figure shows the demonstrations of JetController and a bar chart to display the impulse frequencies of several scenarios.", "levels": [[-1]], "corpus_id": 233987781, "sentences": ["This figure shows the demonstrations of JetController and a bar chart to display the impulse frequencies of several scenarios."], "caption": "", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "A schematic top view of user holding JetController, showing the blowing coverage of airflow.", "levels": null, "corpus_id": 233987781, "sentences": ["A schematic top view of user holding JetController, showing the blowing coverage of airflow."], "caption": "Figure 2: A schematic top view of user holding JetController. The blowing coverage of airfow follows the ofcial spec from the manufacturer websites.", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_003.png"], "annotated": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "This diagram contains our software and hardware. The software parts contains Magnitude Control and Direction Control. The hardware parts is consists of Air Supply, Pressure Regulators, and High-speed Valves.", "levels": null, "corpus_id": 233987781, "sentences": ["This diagram contains our software and hardware.", "The software parts contains Magnitude Control and Direction Control.", "The hardware parts is consists of Air Supply, Pressure Regulators, and High-speed Valves."], "caption": "", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "The figure shows that a larger but shorter tubing without nozzles tends to have higher output force.", "levels": null, "corpus_id": 233987781, "sentences": ["The figure shows that a larger but shorter tubing without nozzles tends to have higher output force."], "caption": "Figure 6: Maximum force under diferent confgurations", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "The chart displays the relationship between force and pressure for all 8 configurations.", "levels": [[1]], "corpus_id": 233987781, "sentences": ["The chart displays the relationship between force and pressure for all 8 configurations."], "caption": "", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "The chart shows that noise depends on force and the use of nozzles, not tubing size and tubing length.", "levels": [[3]], "corpus_id": 233987781, "sentences": ["The chart shows that noise depends on force and the use of nozzles, not tubing size and tubing length."], "caption": "", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "The bar charts show the maximum frequencies under different tubing configurations and different force magnitude.", "levels": [[1]], "corpus_id": 233987781, "sentences": ["The bar charts show the maximum frequencies under different tubing configurations and different force magnitude."], "caption": "Figure 9: Maximum impulse frequency at diferent force magnitudes for diferent tubing sizes and tubing lengths, cal- culated based on force rise time and fall time. (The 4.0N fre- quency is unavailable for 6mm x 250cm tubing because it could only achieve a maximum force of 3.3N.)", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_010.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "pdf_hash": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "year": 2021, "venue": "CHI", "alt_text": "The figure contains two force curves, 50Hz full impulses at 1.0N under 6mm x 100cm tubing and 20Hz full impulses at 4.0N under 8mm x 100cm tubing.", "levels": null, "corpus_id": 233987781, "sentences": ["The figure contains two force curves, 50Hz full impulses at 1.0N under 6mm x 100cm tubing and 20Hz full impulses at 4.0N under 8mm x 100cm tubing."], "caption": "", "local_uri": ["abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_011.png"], "annotated": false, "compound": false}
{"title": "AtaTouch: Robust Finger Pinch Detection for a VR Controller Using RF Return Loss", "pdf_hash": "dd3549175f1bb02664a8d004fc7c83d99a1a5735", "year": 2021, "venue": "CHI", "alt_text": "Figure 7: Simulated return-loss value by varying antenna angle, all for antenna length of 6 cm. The upper lines are for angle of 10 degrees, the middle lines are for the angle of 20 degrees, and the bottom lines are for the angle of 30 degrees.", "levels": null, "corpus_id": 233987159, "sentences": ["Figure 7: Simulated return-loss value by varying antenna angle, all for antenna length of 6 cm.", "The upper lines are for angle of 10 degrees, the middle lines are for the angle of 20 degrees, and the bottom lines are for the angle of 30 degrees."], "caption": "", "local_uri": ["dd3549175f1bb02664a8d004fc7c83d99a1a5735_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Designing and Evaluating Calmer, a Device for Simulating Maternal Skin-to-Skin Holding for Premature Infants", "pdf_hash": "1f8fbfbd0e959b0e7c96c925b85967f45c33a454", "year": 2020, "venue": "CHI", "alt_text": "a) Facilitated Tucking; b) Test setup of Calmer I; c) Image from the pilot study with Calmer I; d) Calmer II", "levels": null, "corpus_id": 218483377, "sentences": ["a) Facilitated Tucking; b) Test setup of Calmer I; c) Image from the pilot study with Calmer I; d) Calmer II"], "caption": "Figure 2 a) Facilitated Tucking; b) Test setup of Calmer I; c) Image from the pilot study with Calmer I; d) Calmer II.", "local_uri": ["1f8fbfbd0e959b0e7c96c925b85967f45c33a454_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "pdf_hash": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "(a) Left eye and right eye follow the stimulus on the display. There is left eye stimuli and right eye stimuli and those are move oppositely along the horizontal axis.   (b) The user perceived above stimulus as a single stimuli which moves forward or backward direction due to single binocular vision from the vergence eye movement.", "levels": null, "corpus_id": 218482654, "sentences": ["(a) Left eye and right eye follow the stimulus on the display.", "There is left eye stimuli and right eye stimuli and those are move oppositely along the horizontal axis.", "(b) The user perceived above stimulus as a single stimuli which moves forward or backward direction due to single binocular vision from the vergence eye movement."], "caption": "", "local_uri": ["4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "pdf_hash": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Here is two pictures. One is presented VR environment. (c) The user placed center of the room in the VR environment. At the left-side wall from the user, there is wall and desk, and a mirror. At the front-side from the user, there is TV and TED video is playing.  Below the picture, two dots are floating on the semi-transparent display", "levels": null, "corpus_id": 218482654, "sentences": ["Here is two pictures.", "One is presented VR environment.", "(c) The user placed center of the room in the VR environment.", "At the left-side wall from the user, there is wall and desk, and a mirror.", "At the front-side from the user, there is TV and TED video is playing.", "Below the picture, two dots are floating on the semi-transparent display"], "caption": "", "local_uri": ["4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "pdf_hash": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "There are two boxplots per participants. Green one is for the TV viewing condition and the orange one is for the Wall viewing condition.     P1. green boxplot is from 2.17 seconds to 4.51 seconds and the center position is 3.08 seconds. Orange boxplot is from 2.30 seconds to 4.81 seconds and the center position is 3.40 seconds.  P2. green boxplot is from 1.62 seconds to 2.69 seconds and the center position is 2.24 seconds. Orange boxplot is from 1.95 seconds to 4.13 seconds and the center position is 2.50 seconds.  P3. green boxplot is from 1.84 seconds to 2.77 seconds and the center position is 2.34 seconds. Orange boxplot is from 1.74 seconds to 2.40 seconds and the center position is 2.00 seconds.  P4. green boxplot is from 1.73 seconds to 3.35 seconds and the center position is 2.20 seconds. Orange boxplot is from 1.62 seconds to 2.86 seconds and the center position is 2.37 seconds.  P5. green boxplot is from 1.59 seconds to 2.36 seconds and the center position is 2.04 seconds. Orange boxplot is from 1.62 seconds to 2.20 seconds and the center position is 1.88 seconds.  P6. green boxplot is from 1.65 seconds to 2.10 seconds and the center position is 1.84seconds. Orange boxplot is from 1.76 seconds to 2.63 seconds and the center position is 2.16 seconds.  P7. green boxplot is from 2.04 seconds to 3.80 seconds and the center position is 2.56 seconds. Orange boxplot is from 2.58 seconds to 4.55 seconds and the center position is 3.50 seconds.", "levels": [[1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 218482654, "sentences": ["There are two boxplots per participants.", "Green one is for the TV viewing condition and the orange one is for the Wall viewing condition.", "P1.", "green boxplot is from 2.17 seconds to 4.51 seconds and the center position is 3.08 seconds.", "Orange boxplot is from 2.30 seconds to 4.81 seconds and the center position is 3.40 seconds.", "P2.", "green boxplot is from 1.62 seconds to 2.69 seconds and the center position is 2.24 seconds.", "Orange boxplot is from 1.95 seconds to 4.13 seconds and the center position is 2.50 seconds.", "P3.", "green boxplot is from 1.84 seconds to 2.77 seconds and the center position is 2.34 seconds.", "Orange boxplot is from 1.74 seconds to 2.40 seconds and the center position is 2.00 seconds.", "P4.", "green boxplot is from 1.73 seconds to 3.35 seconds and the center position is 2.20 seconds.", "Orange boxplot is from 1.62 seconds to 2.86 seconds and the center position is 2.37 seconds.", "P5.", "green boxplot is from 1.59 seconds to 2.36 seconds and the center position is 2.04 seconds.", "Orange boxplot is from 1.62 seconds to 2.20 seconds and the center position is 1.88 seconds.", "P6.", "green boxplot is from 1.65 seconds to 2.10 seconds and the center position is 1.84seconds.", "Orange boxplot is from 1.76 seconds to 2.63 seconds and the center position is 2.16 seconds.", "P7.", "green boxplot is from 2.04 seconds to 3.80 seconds and the center position is 2.56 seconds.", "Orange boxplot is from 2.58 seconds to 4.55 seconds and the center position is 3.50 seconds."], "caption": "", "local_uri": ["4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "pdf_hash": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Y axis means participant (numbered with 'P') and X axis means score. There are two bars per participants in horizontal direction. The length of the bar represent the SUS survey scores.   P1. TV condition score is 47.5 and Wall condition score is 35.  P2. TV condition score is 80 and Wall condition score is 75.  P3. TV condition score is 87.5 and Wall condition score is 85.  P4. TV condition score is 82.5 and Wall condition score is 80.  P5. TV condition score is 65 and Wall condition score is 62.5.  P6. TV condition score is 97.5 and Wall condition score is 100.  P7. TV condition score is 87.5 and Wall condition score is 82.5.", "levels": [[1], [1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 218482654, "sentences": ["Y axis means participant (numbered with 'P') and X axis means score.", "There are two bars per participants in horizontal direction.", "The length of the bar represent the SUS survey scores.", "P1.", "TV condition score is 47.5 and Wall condition score is 35.", "P2.", "TV condition score is 80 and Wall condition score is 75.", "P3.", "TV condition score is 87.5 and Wall condition score is 85.", "P4.", "TV condition score is 82.5 and Wall condition score is 80.", "P5.", "TV condition score is 65 and Wall condition score is 62.5.", "P6.", "TV condition score is 97.5 and Wall condition score is 100.", "P7.", "TV condition score is 87.5 and Wall condition score is 82.5."], "caption": "", "local_uri": ["4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "pdf_hash": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "(a) There are two bar graph and two line graph. X-axis labeled as Phase and it is from 0 to 2 pi. Both barr graphs has similar heights across the x-axis and both line graph has peak at the 3/4 pi position and 15/8 pi position, and the peak at the 3/4 pi position is more higher than the peak at the 15/8 pi position  (b) cosine x graph and minus cosine x graph are ploated across the x-axis. from 0 to 2 pi", "levels": [[1], [1], [3, 2, 1], [1]], "corpus_id": 218482654, "sentences": ["(a) There are two bar graph and two line graph.", "X-axis labeled as Phase and it is from 0 to 2 pi.", "Both barr graphs has similar heights across the x-axis and both line graph has peak at the 3/4 pi position and 15/8 pi position, and the peak at the 3/4 pi position is more higher than the peak at the 15/8 pi position  (b) cosine x graph and minus cosine x graph are ploated across the x-axis.", "from 0 to 2 pi"], "caption": "", "local_uri": ["4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "On-Face Olfactory Interfaces", "pdf_hash": "eac27bcb03414777025ff11b23c5dade8f5f2bca", "year": 2020, "venue": "CHI", "alt_text": "We explored different form factors that can be used as a nose-ring (4), mouth piercing (1) for olfactory-gustatory interfaces as well as jewelry-like accessories placed directly on the skin (2) (3), integrated with tattoos or clipped on glasses, hats or sewn into fabrics. The cable that connects to the PCB is merged with the metallic tattoo (6) and can be hidden behind the ear, in the back part of the neck (5), or as a hair clip.", "levels": null, "corpus_id": 218482900, "sentences": ["We explored different form factors that can be used as a nose-ring (4), mouth piercing (1) for olfactory-gustatory interfaces as well as jewelry-like accessories placed directly on the skin (2) (3), integrated with tattoos or clipped on glasses, hats or sewn into fabrics.", "The cable that connects to the PCB is merged with the metallic tattoo (6) and can be hidden behind the ear, in the back part of the neck (5), or as a hair clip."], "caption": "Figure 1. We explored different form factors that can be used as a nose-ring (4), mouth piercing (1) for olfactory-gustatory interfaces as well as jewelry- like accessories placed directly on the skin (2) (3), integrated with tattoos or clipped on glasses, hats or sewn into fabrics. The cable that connects to the PCB is merged with the metallic tattoo (6) and can be hidden behind the ear, in the back part of the neck (5), or as a hair clip.", "local_uri": ["eac27bcb03414777025ff11b23c5dade8f5f2bca_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "On-Face Olfactory Interfaces", "pdf_hash": "eac27bcb03414777025ff11b23c5dade8f5f2bca", "year": 2020, "venue": "CHI", "alt_text": "Prototypes that we used for the user study. 1) \"Glasses\" prototype, 2) \"Nose\" prototype, and 3) Olfactory necklace. Participants wear the PCB board and battery on their left ear for both on-face designs while hooking the holder at the back part of the cloth for the necklace.", "levels": null, "corpus_id": 218482900, "sentences": ["Prototypes that we used for the user study.", "1) \"Glasses\" prototype, 2) \"Nose\" prototype, and 3) Olfactory necklace.", "Participants wear the PCB board and battery on their left ear for both on-face designs while hooking the holder at the back part of the cloth for the necklace."], "caption": "Figure 2. Prototypes that we used for the user study. 1) \"Glasses\" prototype, 2) \"Nose\" prototype, and 3) Olfactory necklace. Participants wear the PCB board and battery on their left ear for both on-face designs while hooking the holder at the back part of the cloth for the necklace.", "local_uri": ["eac27bcb03414777025ff11b23c5dade8f5f2bca_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "On-Face Olfactory Interfaces", "pdf_hash": "eac27bcb03414777025ff11b23c5dade8f5f2bca", "year": 2020, "venue": "CHI", "alt_text": "We designed two modular scent delivery holders, A) one-piece structure, and B) multi-part decorative structure. C) Design explorations of the scent release mechanism based on 1) Angle between the piezo and the tube. 2) Length and shape of the tube, 3) Assembly of multiple scent release, 4) Clip-on accessories or embeddings in jewelry and piercings. We adjusted these parameters based on the part of the face the prototype was designed to be placed on, ranging angles from 0 to 180 degrees. For example, if the scent release was placed along the lower lip, then we recommend using design 1.4. This design has an angle of 30 degrees, which will direct the scent towards the nose. In contrast, if the scent delivery is placed on the forehead, we used design 1.6.", "levels": null, "corpus_id": 218482900, "sentences": ["We designed two modular scent delivery holders, A) one-piece structure, and B) multi-part decorative structure.", "C) Design explorations of the scent release mechanism based on 1) Angle between the piezo and the tube.", "2) Length and shape of the tube, 3) Assembly of multiple scent release, 4) Clip-on accessories or embeddings in jewelry and piercings.", "We adjusted these parameters based on the part of the face the prototype was designed to be placed on, ranging angles from 0 to 180 degrees.", "For example, if the scent release was placed along the lower lip, then we recommend using design 1.4.", "This design has an angle of 30 degrees, which will direct the scent towards the nose.", "In contrast, if the scent delivery is placed on the forehead, we used design 1.6."], "caption": "Figure 3. We designed two modular scent delivery holders, A) one-piece structure, and B) multi-part decorative structure. C) Design explorations of the scent release mechanism based on 1) Angle between the piezo and the tube. 2) Length and shape of the tube, 3) Assembly of multiple scent release,", "local_uri": ["eac27bcb03414777025ff11b23c5dade8f5f2bca_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "On-Face Olfactory Interfaces", "pdf_hash": "eac27bcb03414777025ff11b23c5dade8f5f2bca", "year": 2020, "venue": "CHI", "alt_text": "On the right image: showcase of the prototypes with different materials and aesthetics. The asymmetrical pattern of the piezo cover is designed to move the attention away of the central circle and make it more aesthetically pleasing. Left image: A pair of participants during the user study. The \"wearer\" has a mirror to evaluate the aesthetics of the device (glasses) as well as its social acceptance. The observer was instructed to pay attention to the prototype to later asses the same.", "levels": null, "corpus_id": 218482900, "sentences": ["On the right image: showcase of the prototypes with different materials and aesthetics.", "The asymmetrical pattern of the piezo cover is designed to move the attention away of the central circle and make it more aesthetically pleasing.", "Left image: A pair of participants during the user study.", "The \"wearer\" has a mirror to evaluate the aesthetics of the device (glasses) as well as its social acceptance.", "The observer was instructed to pay attention to the prototype to later asses the same."], "caption": "Figure 4. On the right image: showcase of the prototypes with different materials and aesthetics. The asymmetrical pattern of the piezo cover is designed to move the attention away of the central circle and make it more aesthetically pleasing. Left image:A pair of participants during the user study. The \"wearer\" has a mirror to evaluate the aesthetics of the device (glasses) as well as its social acceptance. The observer was instructed to pay attention to the prototype to later asses the same.", "local_uri": ["eac27bcb03414777025ff11b23c5dade8f5f2bca_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "On-Face Olfactory Interfaces", "pdf_hash": "eac27bcb03414777025ff11b23c5dade8f5f2bca", "year": 2020, "venue": "CHI", "alt_text": "Likert Scale for 1 = Extremely Inappropriate or Extremely Uncomfortable and 9 = Extremely Appropriate or Extremely Comfort- able. Orange for the olfactory necklace, grey for the glasses, and blue for the nose prototype. Error bars correspond to \u00b11 S.D. On the top, so- cial acceptance for the necklace (M = 7.25, S.D = 2.75), glasses (M = 6.5, S.D = 1.56), nose (M = 4.7, S.D = 2.01). Comfort while wearing the nose prototype (M = 4.29, S.D = 1.88), necklace (M = 7.7, S.D=1.9), glasses (M = 6.37, S.D = 1.78). ***P-value <0.001, **P-value < 0.01.", "levels": [[1], [1], [2], [2], [2]], "corpus_id": 218482900, "sentences": ["Likert Scale for 1 = Extremely Inappropriate or Extremely Uncomfortable and 9 = Extremely Appropriate or Extremely Comfort- able.", "Orange for the olfactory necklace, grey for the glasses, and blue for the nose prototype.", "Error bars correspond to \u00b11 S.D. On the top, so- cial acceptance for the necklace (M = 7.25, S.D = 2.75), glasses (M = 6.5, S.D = 1.56), nose (M = 4.7, S.D = 2.01).", "Comfort while wearing the nose prototype (M = 4.29, S.D = 1.88), necklace (M = 7.7, S.D=1.9), glasses (M = 6.37, S.D = 1.78). ***", "P-value <0.001, **P-value < 0.01."], "caption": "\u00b1", "local_uri": ["eac27bcb03414777025ff11b23c5dade8f5f2bca_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "On-Face Olfactory Interfaces", "pdf_hash": "eac27bcb03414777025ff11b23c5dade8f5f2bca", "year": 2020, "venue": "CHI", "alt_text": "\"Moist\" - humidity felt on the face, \"Smell\" - intensity of the smell, \"Burst\" - visual spray, \"Sound\" - emitted when a burst is released. Error bars correspond to \u00b11 S.D. The wearers smelled the fragrance significantly more than the observers for all the prototypes (***P-value < 0.001). The bursts were significantly more visible for the nose prototype than for the necklace (***P-value < 0.001) and for the glasses *P < 0.05 (both the wearer and reviewer)", "levels": [[1], [2], [2]], "corpus_id": 218482900, "sentences": ["\"Moist\" - humidity felt on the face, \"Smell\" - intensity of the smell, \"Burst\" - visual spray, \"Sound\" - emitted when a burst is released.", "Error bars correspond to \u00b11 S.D. The wearers smelled the fragrance significantly more than the observers for all the prototypes (***P-value < 0.001).", "The bursts were significantly more visible for the nose prototype than for the necklace (***P-value < 0.001) and for the glasses *P < 0.05 (both the wearer and reviewer)"], "caption": "\u00b1", "local_uri": ["eac27bcb03414777025ff11b23c5dade8f5f2bca_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "From Reflection to Action: Combining Machine Learning with Expert Knowledge for Nutrition Goal Recommendations", "pdf_hash": "29b224f905d81046e8439b0801cacd2ec07a58ad", "year": 2021, "venue": "CHI", "alt_text": "A photograph of a foal core board with many cutout photos of breakfast foods printed on stock paper. The photos are grouped in piles of the same food item with a small printed label. Foods included fried eggs, white bread, strips of bacon, granola, biscuits, plantains, butter, cottage cheese, waffles, and maple syrup. Also visible is a plate showing a meal a participant has chosen, with one slice of toast, 3 strips of bacon, and 2 eggs.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987320, "sentences": ["A photograph of a foal core board with many cutout photos of breakfast foods printed on stock paper.", "The photos are grouped in piles of the same food item with a small printed label.", "Foods included fried eggs, white bread, strips of bacon, granola, biscuits, plantains, butter, cottage cheese, waffles, and maple syrup.", "Also visible is a plate showing a meal a participant has chosen, with one slice of toast, 3 strips of bacon, and 2 eggs."], "caption": "Supplementary Figure C. The \u201cvirtual bufet\u201d for breakfast meals.", "local_uri": ["29b224f905d81046e8439b0801cacd2ec07a58ad_Image_033.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "pdf_hash": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "year": 2020, "venue": "CHI PLAY", "alt_text": "The theoretical model of the Player Experience Inventory, measuring player experience at psychosocial and functional levels simultaneously", "levels": null, "corpus_id": 226228385, "sentences": ["The theoretical model of the Player Experience Inventory, measuring player experience at psychosocial and functional levels simultaneously"], "caption": "", "local_uri": ["28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "pdf_hash": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "year": 2020, "venue": "CHI PLAY", "alt_text": "After successful upload of CSV data, user will be able to see three different visualizations on basis of the data uploaded. These visualizations are gender pie chart, age based gender wise distribution bar chart and per construct box plots", "levels": [[-1], [-1]], "corpus_id": 226228385, "sentences": ["After successful upload of CSV data, user will be able to see three different visualizations on basis of the data uploaded.", "These visualizations are gender pie chart, age based gender wise distribution bar chart and per construct box plots"], "caption": "Figure 2. Overview of the data analysis module as presented by the PXI Bench. On the top right and left, visualizations illustrate demographic information. At the bottom, the actual analysis of the PXI data.", "local_uri": ["28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "pdf_hash": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "year": 2020, "venue": "CHI PLAY", "alt_text": "Screenshot of CSV Upload module of PXI Bench. It provides instructions to be followed for filling data in CSV file and also a template CSV with correct column names and one pre-filled entry also.", "levels": [[-1], [-1]], "corpus_id": 226228385, "sentences": ["Screenshot of CSV Upload module of PXI Bench.", "It provides instructions to be followed for filling data in CSV file and also a template CSV with correct column names and one pre-filled entry also."], "caption": "", "local_uri": ["28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "pdf_hash": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "year": 2020, "venue": "CHI PLAY", "alt_text": "The Benchmark data feature provides box plot for each construct. Age, genre, gender and game based filters can be altered to analyse the data in customized way.", "levels": [[-1], [-1]], "corpus_id": 226228385, "sentences": ["The Benchmark data feature provides box plot for each construct.", "Age, genre, gender and game based filters can be altered to analyse the data in customized way."], "caption": "", "local_uri": ["28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Passive haptic learning of Braille typing", "pdf_hash": "a2f502fdb0f5081b7411d752737e99684b8e671b", "year": 2014, "venue": "SEMWEB", "alt_text": "https://lh5.googleusercontent.com/moXqbitXgvp78loWsgtpBeDyylyG23twGW0FIM4nxyBqZZcrabzCmHWHy5o3GpdaWbKvJrcav0kRRdZqLbTgZE1w5-u8ugcAwZK9EuaqL4VeHzT-cqOrvV_7umxGXQ", "levels": null, "corpus_id": 1723573, "sentences": ["https://lh5.googleusercontent.com/moXqbitXgvp78loWsgtpBeDyylyG23twGW0FIM4nxyBqZZcrabzCmHWHy5o3GpdaWbKvJrcav0kRRdZqLbTgZE1w5-u8ugcAwZK9EuaqL4VeHzT-cqOrvV_7umxGXQ"], "caption": "", "local_uri": ["a2f502fdb0f5081b7411d752737e99684b8e671b_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Using Data to Approach the Unknown", "pdf_hash": "e96bad8bbbaa12e14f514008302b680184dc3efb", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Image shows a flowchart: \"Timing intercourse around ovulation\" precedes \"Ovulation induction or superovulation with medications + IUI\" that precedes \"IVF\"", "levels": [[-1]], "corpus_id": 230717488, "sentences": ["Image shows a flowchart: \"Timing intercourse around ovulation\" precedes \"Ovulation induction or superovulation with medications + IUI\" that precedes \"IVF\""], "caption": "", "local_uri": ["e96bad8bbbaa12e14f514008302b680184dc3efb_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Using Data to Approach the Unknown", "pdf_hash": "e96bad8bbbaa12e14f514008302b680184dc3efb", "year": 2020, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "The image shows a graph representation of the trajectories previously described, summarizing their fertility-related events, treatments and tracking activities, and the healthcare providers they worked with.  Claire: Fertility related events include 4 conceptions, 3 miscarriages, and one partial mole. Tracking activities include tracking period dates, lifestyle measures, temperature, symptoms, intercourse, ovulation prediction kits, and cervical mucus, using both a fertility app and a spreadsheet. Treatments include 3 IUIs. Healthcare providers include infertility specialist, REI, and a Chinese traditional doctor. She also quit both western and Chinese medicine before conceiving.  Anna: Fertility related events include 2 conceptions and 1 miscarriage. Tracking activities include tracking period dates, temperature, cervical mucus, intercourse, symptoms, ovulation prediction kits, and cervical position, using both a fertility app and a spreadsheet. Treatments include progesterone treatment, birth control for intermenstrual bleeding after the miscarriage, and 5 cycles of ovulation stimulation. Healthcare providers include only her midwife.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 230717488, "sentences": ["The image shows a graph representation of the trajectories previously described, summarizing their fertility-related events, treatments and tracking activities, and the healthcare providers they worked with.", "Claire: Fertility related events include 4 conceptions, 3 miscarriages, and one partial mole.", "Tracking activities include tracking period dates, lifestyle measures, temperature, symptoms, intercourse, ovulation prediction kits, and cervical mucus, using both a fertility app and a spreadsheet.", "Treatments include 3 IUIs.", "Healthcare providers include infertility specialist, REI, and a Chinese traditional doctor.", "She also quit both western and Chinese medicine before conceiving.", "Anna: Fertility related events include 2 conceptions and 1 miscarriage.", "Tracking activities include tracking period dates, temperature, cervical mucus, intercourse, symptoms, ovulation prediction kits, and cervical position, using both a fertility app and a spreadsheet.", "Treatments include progesterone treatment, birth control for intermenstrual bleeding after the miscarriage, and 5 cycles of ovulation stimulation.", "Healthcare providers include only her midwife."], "caption": "", "local_uri": ["e96bad8bbbaa12e14f514008302b680184dc3efb_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gaze Tracking for Eye-Hand Coordination Training Systems in Virtual Reality", "pdf_hash": "bf5a5fdd71822c36f869deacb4e45db1713a9b13", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Average distance of gaze (blue; the bottom curve) and cursor (green; the top curve) from target over time", "levels": [[1]], "corpus_id": 218483457, "sentences": ["Average distance of gaze (blue; the bottom curve) and cursor (green; the top curve) from target over time"], "caption": "Figure 7: Average distance of gaze (blue) and cursor (green) from target over time.", "local_uri": ["bf5a5fdd71822c36f869deacb4e45db1713a9b13_Image_013.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Gaze Tracking for Eye-Hand Coordination Training Systems in Virtual Reality", "pdf_hash": "bf5a5fdd71822c36f869deacb4e45db1713a9b13", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Average distance of cursor from target over time for conditions: mid-air (blue; denoted as superscript M; the top curve) and passive haptic feedback (green; denoted as superscript H; the bottom curve)", "levels": [[1]], "corpus_id": 218483457, "sentences": ["Average distance of cursor from target over time for conditions: mid-air (blue; denoted as superscript M; the top curve) and passive haptic feedback (green; denoted as superscript H; the bottom curve)"], "caption": "Figure 8: Average distance of cursor from target over time for mid-air (blue; superscript M ) and passive haptic feedback (green; superscript H ) conditions.", "local_uri": ["bf5a5fdd71822c36f869deacb4e45db1713a9b13_Image_014.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BackHand: Sensing Hand Gestures via Back of the Hand", "pdf_hash": "827573a74c097a0f83e79454e750d094d415b436", "year": 2015, "venue": "UIST", "alt_text": "The layout shows a total of 19 sensors with a base length of 6.3 mm distributed into a 2-row configuration on an artificial skin. The strain sensors should avoid physical contact with each other by leaving a spaced interval of 2 mm between the neighboring sensors in a row. Therefore, a number of 10 sensors fits the hand width of a normal person. The second row includes 9 sensors to increase resolution and to measure the physical properties along the vertical direction between each spaced interval of the first row.", "levels": null, "corpus_id": 14422938, "sentences": ["The layout shows a total of 19 sensors with a base length of 6.3 mm distributed into a 2-row configuration on an artificial skin.", "The strain sensors should avoid physical contact with each other by leaving a spaced interval of 2 mm between the neighboring sensors in a row.", "Therefore, a number of 10 sensors fits the hand width of a normal person.", "The second row includes 9 sensors to increase resolution and to measure the physical properties along the vertical direction between each spaced interval of the first row."], "caption": "Figure 3. The layout shows a total of 19 sensors with a base length of", "local_uri": ["827573a74c097a0f83e79454e750d094d415b436_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "BackHand: Sensing Hand Gestures via Back of the Hand", "pdf_hash": "827573a74c097a0f83e79454e750d094d415b436", "year": 2015, "venue": "UIST", "alt_text": "A customized 2-layer Arduino Nano shield circuit board used for signal conditioning is shown in (A) whose outward side is shown in (B) and inward side is shown in (C).", "levels": null, "corpus_id": 14422938, "sentences": ["A customized 2-layer Arduino Nano shield circuit board used for signal conditioning is shown in (A) whose outward side is shown in (B) and inward side is shown in (C)."], "caption": "Figure 5. A customized 2-layer Arduino Nano shield circuit board used for signal conditioning is shown in (A) whose outward side is shown in", "local_uri": ["827573a74c097a0f83e79454e750d094d415b436_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "BackHand: Sensing Hand Gestures via Back of the Hand", "pdf_hash": "827573a74c097a0f83e79454e750d094d415b436", "year": 2015, "venue": "UIST", "alt_text": "Study Setup: (A) monitor for on-screen instruction. (B) A button pushed by a participant to begin recording sensor readings while performing a gesture from the on-screen instruction. (C) A tilt platform for the arm to rest on. (D) Our prototype attached to the back of hand at specified locations.", "levels": null, "corpus_id": 14422938, "sentences": ["Study Setup: (A) monitor for on-screen instruction.", "(B) A button pushed by a participant to begin recording sensor readings while performing a gesture from the on-screen instruction. (C) A tilt platform for the arm to rest on. (D) Our prototype attached to the back of hand at specified locations."], "caption": "Figure 7. Study Setup: (A) monitor for on-screen instruction. (B) A button pushed by a participant to begin recording sensor readings while performing a gesture from the on-screen instruction. (C) A tilt platform for the arm to rest on. (D) Our prototype attached to the back of hand at speci\ufb01ed locations.", "local_uri": ["827573a74c097a0f83e79454e750d094d415b436_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "BackHand: Sensing Hand Gestures via Back of the Hand", "pdf_hash": "827573a74c097a0f83e79454e750d094d415b436", "year": 2015, "venue": "UIST", "alt_text": "The procedures of marking the 8 rows on the back of hand. (A) Locate the MCP joint positions and the head of ulna. (B) Evenly mark 8 points from the index MCP joint to the imaginary horizontal line that goes through the head of ulna. Evenly mark another 8 points from the pinky MCP joint to the head of ulna. Draw 8 lines on the back of hand. (C) 8 rows evenly spaced between the MCP joint and the head of ulna are marked where our prototype aligns.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 14422938, "sentences": ["The procedures of marking the 8 rows on the back of hand. (", "A) Locate the MCP joint positions and the head of ulna. (", "B) Evenly mark 8 points from the index MCP joint to the imaginary horizontal line that goes through the head of ulna.", "Evenly mark another 8 points from the pinky MCP joint to the head of ulna.", "Draw 8 lines on the back of hand. (", "C) 8 rows evenly spaced between the MCP joint and the head of ulna are marked where our prototype aligns."], "caption": "Figure 8. The procedures of marking the 8 rows on the back of hand. (A) Locate the MCP joint positions and the head of ulna. (B) Evenly mark 8 points from the index MCP joint to the imaginary horizontal line that goes through the head of ulna. Evenly mark another 8 points from the pinky MCP joint to the head of ulna. Draw 8 lines on the back of hand.", "local_uri": ["827573a74c097a0f83e79454e750d094d415b436_Image_010.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "BackHand: Sensing Hand Gestures via Back of the Hand", "pdf_hash": "827573a74c097a0f83e79454e750d094d415b436", "year": 2015, "venue": "UIST", "alt_text": "16 heat maps display all 16 gestures performed by one of the participants. Each entry is an average heat map of 10 trials of a gesture. The sensor reading patterns are significantly different across 16 gestures.", "levels": [[1], [2], [3]], "corpus_id": 14422938, "sentences": ["16 heat maps display all 16 gestures performed by one of the participants.", "Each entry is an average heat map of 10 trials of a gesture.", "The sensor reading patterns are significantly different across 16 gestures."], "caption": "Figure 9. 16 heat maps display all 16 gestures performed by one of the participants. Each entry is an average heat map of 10 trials of a gesture. The sensor reading patterns are signi\ufb01cantly different across 16 gestures.", "local_uri": ["827573a74c097a0f83e79454e750d094d415b436_Image_011.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "pdf_hash": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "year": 2020, "venue": "CHI", "alt_text": "Figure 1. Illustrations of gesture classes and examples. Shows how a hand interacts with the interactive cord to perform the 6 different gestures that are supported within the 3 gesture classes.  a) Continuous class. 1 example: Clockwise and counterclockwise twist gesture. b) Discrete motion class. 2 examples: Clockwise and counterclockwise flick gesture. Slide gesture. c) Discrete graaps class. 3 examples: Pinch, grab and pat gestures.", "levels": null, "corpus_id": 218483529, "sentences": ["Figure 1.", "Illustrations of gesture classes and examples. Shows how a hand interacts with the interactive cord to perform the 6 different gestures that are supported within the 3 gesture classes.", "a) Continuous class. 1 example: Clockwise and counterclockwise twist gesture. b) Discrete motion class.", "2 examples: Clockwise and counterclockwise flick gesture.", "Slide gesture. c) Discrete graaps class.", "3 examples: Pinch, grab and pat gestures."], "caption": "Figure 1. E-textile Microinteractions leverage the I/O Braid architecture for soft electronics to combine continuous twist sensing with casual, discrete gestures, such as Flick, Slide, Pinch, Grab and Pat. We demonstrate our hybrid interaction techniques through soft electronics that can be used for the control of consumer electronics, digital media and for interactive applications.", "local_uri": ["f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "pdf_hash": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "year": 2020, "venue": "CHI", "alt_text": "Figure 3. Plots with feature vectors for the different gestures and participants. The plot shows data from one repetition (out of 9) for the 12 participants (horizontal axis) for the 8 gestures (vertical axis). Each sub-image shows a plot of 16 overlaid feature vectors, which has been interpolated to 80 observations over time. Participants performed gestures without feedback and in their own style, which required user-dependent classification. Some potential issues can be seen in the time series:", "levels": [[0], [1], [1], [2, 1], [0], [3]], "corpus_id": 218483529, "sentences": ["Figure 3.", "Plots with feature vectors for the different gestures and participants.", "The plot shows data from one repetition (out of 9) for the 12 participants (horizontal axis) for the 8 gestures (vertical axis).", "Each sub-image shows a plot of 16 overlaid feature vectors, which has been interpolated to 80 observations over time.", "Participants performed gestures without feedback and in their own style, which required user-dependent classification.", "Some potential issues can be seen in the time series:"], "caption": "Figure 3: Based on the gesture elicitation, we choose to support three gesture classes (Flick, Slide, Grasp), which represent 83.3% of the elicited gestures. The plot shows data from one repetition (out of nine) for the 12 participants (horizontal axis) for the eight gestures (vertical axis). Each sub-image shows a plot of 16 overlaid feature vectors, which has been interpolated to 80 observations over time. Participants performed gestures without feedback and in their own style, which required user-dependent classification. Some potential issues can be seen in the time series:", "local_uri": ["f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "pdf_hash": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "year": 2020, "venue": "CHI", "alt_text": "Figure 6. Illustration of experimental task Illustration that shows how to perform the target matching with target ranges and the required 1 second hold within the lock zone. Shows also the reciprocal alternation to other side of the center line.", "levels": null, "corpus_id": 218483529, "sentences": ["Figure 6.", "Illustration of experimental task Illustration that shows how to perform the target matching with target ranges and the required 1 second hold within the lock zone.", "Shows also the reciprocal alternation to other side of the center line."], "caption": "", "local_uri": ["f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_007.png"], "annotated": false, "compound": false}
{"title": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "pdf_hash": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "year": 2020, "venue": "CHI", "alt_text": "Figure 7a. Time on task Boxplot with task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).   Figure 7b. Excess motion: Total trial vs last 1 second while locking on target. I/O Braid had more excess motion compared to Buttons and Scroll. The boxplots show median values with quartiles and min/max extent.   Figure 7c. Weighted average subjective feedback.  We mapped the 7-point Likert scale to a score in the range [-3, 3] for Ease of Use, Perceived Accuracy and Tactile Feel. We multiplied the score by the number of times the technique received that rating and computed an average for all the scores. The chart show favorable scores for I/O Braid and Scroll, whereas Buttons was the least popular.", "levels": [[1], [1], [2], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483529, "sentences": ["Figure 7a.", "Time on task Boxplot with task completion times for the three input devices.", "I/O Braid was faster than Buttons (statistical significance).", "Figure 7b.", "Excess motion: Total trial vs last 1 second while locking on target.", "I/O Braid had more excess motion compared to Buttons and Scroll.", "The boxplots show median values with quartiles and min/max extent.", "Figure 7c.", "Weighted average subjective feedback.", "We mapped the 7-point Likert scale to a score in the range [-3, 3] for Ease of Use, Perceived Accuracy and Tactile Feel.", "We multiplied the score by the number of times the technique received that rating and computed an average for all the scores.", "The chart show favorable scores for I/O Braid and Scroll, whereas Buttons was the least popular."], "caption": "Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "local_uri": ["f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_009.jpg", "f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_010.jpg", "f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "pdf_hash": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "year": 2020, "venue": "CHI", "alt_text": "Figure 9. Gestures for an interactive speaker cord.  A sequence of 12 annotated photos that show how gestures on the cord controls music playback.  The hand taps to toggle from pause to play. Continuous twisting decreases the volume. Slide switches from the Jazz playlist to the Electro playlist. Pat switches mode for the continuous control, such that it no longer controls volume, but instead enables seeking on the track. Continuous twisting fast forwards from 1 min 12 seconds to 1 min 31 seconds. A discrete clockwise flick skips to the next track. A discrete counterclockwise flick goes back to the previous track.", "levels": null, "corpus_id": 218483529, "sentences": ["Figure 9.", "Gestures for an interactive speaker cord.", "A sequence of 12 annotated photos that show how gestures on the cord controls music playback.", "The hand taps to toggle from pause to play.", "Continuous twisting decreases the volume.", "Slide switches from the Jazz playlist to the Electro playlist.", "Pat switches mode for the continuous control, such that it no longer controls volume, but instead enables seeking on the track.", "Continuous twisting fast forwards from 1 min 12 seconds to 1 min 31 seconds.", "A discrete clockwise flick skips to the next track.", "A discrete counterclockwise flick goes back to the previous track."], "caption": "Figure 9. Augmenting continuous twist control with discrete gestures for an interactive speaker cord. Discrete actions: Tap for play/pause, flicks for next/previous track. Slide advances to the next playlist. Remapping: Pat toggles between volume or fastforward control. Continuous twist provides fine control over volume or fastforwarding.", "local_uri": ["f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Social Media as a Design and Research Site in HCI: Mapping Out Opportunities and Envisioning Future Uses", "pdf_hash": "520574e92487666d503147e31e44ff016c0f065b", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Illustrative examples of using social media to inspire playful design. The left image features a screenshot of a phone with the Instagram UI showing the search of posts with the hashtag #playpotential. The right image shows two screenshots of a chat service where a user is interacting with a chatbot.", "levels": null, "corpus_id": 233986984, "sentences": ["Illustrative examples of using social media to inspire playful design.", "The left image features a screenshot of a phone with the Instagram UI showing the search of posts with the hashtag #playpotential.", "The right image shows two screenshots of a chat service where a user is interacting with a chatbot."], "caption": "", "local_uri": ["520574e92487666d503147e31e44ff016c0f065b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "'Not Too Much, Not Too Little' Wearables For Group Discussions", "pdf_hash": "084764ed3e6a3c3a48978aa19a540b755b61c92b", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "The devices look like hawaiian flowers, with a thick stem. There are four devices in the picture, each has a differnt color, and a different detail covering the vibration motor.", "levels": null, "corpus_id": 5066040, "sentences": ["The devices look like hawaiian flowers, with a thick stem.", "There are four devices in the picture, each has a differnt color, and a different detail covering the vibration motor."], "caption": "Figure 1: The L\u00e5gom social wearables prototypes.", "local_uri": ["084764ed3e6a3c3a48978aa19a540b755b61c92b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "The Effect of Computer-Generated Descriptions on Photo-Sharing Experiences of People with Visual Impairments", "pdf_hash": "269248eb8a44da5248cef840f7079b1294dbf237", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Fig. 1. Computer-generated descriptions in Facebook. The text in the white boxes is the descriptions that are read to a blind user by TalkBack. The descriptions are normally invisible. We show it visually to demonstrate the design.", "levels": null, "corpus_id": 19164681, "sentences": ["Fig. 1.", "Computer-generated descriptions in Facebook.", "The text in the white boxes is the descriptions that are read to a blind user by TalkBack.", "The descriptions are normally invisible.", "We show it visually to demonstrate the design."], "caption": "Fig. 1. Computer-generated descriptions in Facebook. The text in the white boxes is the descriptions that are read to a blind user by TalkBack. The descriptions are normally invisible.", "local_uri": ["269248eb8a44da5248cef840f7079b1294dbf237_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "The Effect of Computer-Generated Descriptions on Photo-Sharing Experiences of People with Visual Impairments", "pdf_hash": "269248eb8a44da5248cef840f7079b1294dbf237", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Fig. 2. The workflow of the photo-sharing feature: (a) Entry of the photo-sharing feature in Facebook; (b) A user can get photo description by setting focus on a photo; (c) After a user selects a photo to upload, by tapping this photo, she can still get the photo description; (d) When completing the photo selection, a user can double click the \u201cDone\u201d button to post the photos.", "levels": null, "corpus_id": 19164681, "sentences": ["Fig. 2.", "The workflow of the photo-sharing feature: (a) Entry of the photo-sharing feature in Facebook; (b) A user can get photo description by setting focus on a photo; (c) After a user selects a photo to upload, by tapping this photo, she can still get the photo description; (d) When completing the photo selection, a user can double click the \u201cDone\u201d button to post the photos."], "caption": "", "local_uri": ["269248eb8a44da5248cef840f7079b1294dbf237_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "The Effect of Computer-Generated Descriptions on Photo-Sharing Experiences of People with Visual Impairments", "pdf_hash": "269248eb8a44da5248cef840f7079b1294dbf237", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Fig. 3. The time that each participant spent browsing (green) and uploading photos (orange) during the week.", "levels": null, "corpus_id": 19164681, "sentences": ["Fig. 3.", "The time that each participant spent browsing (green) and uploading photos (orange) during the week."], "caption": "Fig. 3. The time that each participant spent browsing (green) and uploading photos (orange) during the week.", "local_uri": ["269248eb8a44da5248cef840f7079b1294dbf237_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "The Effect of Computer-Generated Descriptions on Photo-Sharing Experiences of People with Visual Impairments", "pdf_hash": "269248eb8a44da5248cef840f7079b1294dbf237", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Fig. 4. The number of times that each type of information was selected to be most useful.", "levels": null, "corpus_id": 19164681, "sentences": ["Fig. 4.", "The number of times that each type of information was selected to be most useful."], "caption": "", "local_uri": ["269248eb8a44da5248cef840f7079b1294dbf237_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Making Sense of Complex Running Metrics Using a Modified Running Shoe", "pdf_hash": "20c3ca908c26bfabb46db16fec7f9b9cdfa16996", "year": 2021, "venue": "CHI", "alt_text": "Three rows of eight outlines of the outsole of a running shoe. Row 1: concentric dots using the red-yellow-green scheme. Row 2: coloured areas with dark to light blue. Row 3: Coloured horizontal stripes.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 233987563, "sentences": ["Three rows of eight outlines of the outsole of a running shoe.", "Row 1: concentric dots using the red-yellow-green scheme.", "Row 2: coloured areas with dark to light blue.", "Row 3: Coloured horizontal stripes."], "caption": "The visualisation used in the fnal prototype. Foot strike type, pronation and shock are visualised on the outsole. The location of the most intensive point shows foot strike type (Y axis in the pic- ture) and the value of pronation (X axis in the picture). The colours map shock to community average shock with red representing rel- atively high shock, yellow showing medium shock and green show- ing low shock.Alternative colour version of 2aAlternative visualisation: foot strike type and impact Gs are pre- sented on the shoe sole and pronation is shown on the side of the shoe. The colour mapping relates the shock to community average shock. Location of the colourful stripe corresponds to foot strike type.", "local_uri": ["20c3ca908c26bfabb46db16fec7f9b9cdfa16996_Image_006.png", "20c3ca908c26bfabb46db16fec7f9b9cdfa16996_Image_007.png", "20c3ca908c26bfabb46db16fec7f9b9cdfa16996_Image_008.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Making Sense of Complex Running Metrics Using a Modified Running Shoe", "pdf_hash": "20c3ca908c26bfabb46db16fec7f9b9cdfa16996", "year": 2021, "venue": "CHI", "alt_text": "Desktop app: the map of the run on the left and a slider for choosing minipulating time on the right.", "levels": [[-1]], "corpus_id": 233987563, "sentences": ["Desktop app: the map of the run on the left and a slider for choosing minipulating time on the right."], "caption": "Figure 3: The desktop interface accompanying GraFeet. The user can view the run on a map and navigate to diferent parts of the run using the slider. The appropriate metrics are then shown on GraFeet. Average foot metric values can be shown by pressing the button. Thus, the interface provides both instantaneous metrics and an overview of the gathered data.", "local_uri": ["20c3ca908c26bfabb46db16fec7f9b9cdfa16996_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Making Sense of Complex Running Metrics Using a Modified Running Shoe", "pdf_hash": "20c3ca908c26bfabb46db16fec7f9b9cdfa16996", "year": 2021, "venue": "CHI", "alt_text": "Runner with their hand on the touchpad of a laptop, inverted running shoes lying next to the laptop.", "levels": null, "corpus_id": 233987563, "sentences": ["Runner with their hand on the touchpad of a laptop, inverted running shoes lying next to the laptop."], "caption": "a week and committed to a running regime (i.e. they all belonged to the user group defned by Knaving et al. [24]). There were no participants with professional running experience. Each participant was remunerated with EUR 10. Additionally, isotonic drink and water were available throughout the study.", "local_uri": ["20c3ca908c26bfabb46db16fec7f9b9cdfa16996_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "pdf_hash": "5894fd4581a79a7067102891bc3db5738195941f", "year": 2021, "venue": "CHI", "alt_text": "Different options for backspace granularity and caret control continuity. One deletion can cause either erase a character or a word. For caret control, one can either move the caret one letter at a time or move it continuously.", "levels": null, "corpus_id": 233987425, "sentences": ["Different options for backspace granularity and caret control continuity.", "One deletion can cause either erase a character or a word.", "For caret control, one can either move the caret one letter at a time or move it continuously."], "caption": "", "local_uri": ["5894fd4581a79a7067102891bc3db5738195941f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "pdf_hash": "5894fd4581a79a7067102891bc3db5738195941f", "year": 2021, "venue": "CHI", "alt_text": "Bar chart with error bars presents the correction time for four VR text revision techniques when dealing with different types of revision targets. Overall, revising targets that far from the end of the sentence takes more time than those targets near the end of the sentence.", "levels": [[1], [3]], "corpus_id": 233987425, "sentences": ["Bar chart with error bars presents the correction time for four VR text revision techniques when dealing with different types of revision targets.", "Overall, revising targets that far from the end of the sentence takes more time than those targets near the end of the sentence."], "caption": "", "local_uri": ["5894fd4581a79a7067102891bc3db5738195941f_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "pdf_hash": "5894fd4581a79a7067102891bc3db5738195941f", "year": 2021, "venue": "CHI", "alt_text": "Bar chart with error bars presents the caret control time for four VR text revision techniques when dealing with different types of revision targets. Overall, using the discrete caret control requires more time to navigate the caret than the continuous caret control.", "levels": [[1], [3]], "corpus_id": 233987425, "sentences": ["Bar chart with error bars presents the caret control time for four VR text revision techniques when dealing with different types of revision targets.", "Overall, using the discrete caret control requires more time to navigate the caret than the continuous caret control."], "caption": "", "local_uri": ["5894fd4581a79a7067102891bc3db5738195941f_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "pdf_hash": "5894fd4581a79a7067102891bc3db5738195941f", "year": 2021, "venue": "CHI", "alt_text": "Bar chart with error bars presents the backspace time for four VR text revision techniques when dealing with different types of revision targets. Overall, using the character-level bacskapce requires more time to delete characters than the word-level backspace.", "levels": [[1], [3]], "corpus_id": 233987425, "sentences": ["Bar chart with error bars presents the backspace time for four VR text revision techniques when dealing with different types of revision targets.", "Overall, using the character-level bacskapce requires more time to delete characters than the word-level backspace."], "caption": "", "local_uri": ["5894fd4581a79a7067102891bc3db5738195941f_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding and Supporting Trade-offs in the Design of Visualizations for Communication.", "pdf_hash": "232ed7c309cddf2c3a007fa2880174a3b9d4c891", "year": 2014, "venue": "", "alt_text": "https://lh3.googleusercontent.com/3J7sySSc7knFIKZbydofGg41fjNLMdKOaY3C06DD0rEQolAIgvuzJiATSWLdTaSzkEEmV0cNTmFOKQRl0EVHOtCspHjiIjrqqHTOdLwiO0ZULru9MR_qlMIFQg", "levels": [[0]], "corpus_id": 56674947, "sentences": ["https://lh3.googleusercontent.com/3J7sySSc7knFIKZbydofGg41fjNLMdKOaY3C06DD0rEQolAIgvuzJiATSWLdTaSzkEEmV0cNTmFOKQRl0EVHOtCspHjiIjrqqHTOdLwiO0ZULru9MR_qlMIFQg"], "caption": "Figure II- : Two means visualiied in a bar chart with error bars used to depict the range of possible values in which the mean is likely to fall in repeated trials.", "local_uri": ["232ed7c309cddf2c3a007fa2880174a3b9d4c891_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Understanding Individual and Collaborative Problem-Solving with Patient-Generated Data", "pdf_hash": "add7851f3459ebf1ee05b670cb73645d5705c167", "year": 2017, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Figure 2: \u00a9 Medtronic MiniMed. Inc. Example of a visualization used by providers. This is taken from a Medtronic insulin pump.", "levels": null, "corpus_id": 2005814, "sentences": ["Figure 2: \u00a9 Medtronic MiniMed. Inc. Example of a visualization used by providers.", "This is taken from a Medtronic insulin pump."], "caption": "", "local_uri": ["add7851f3459ebf1ee05b670cb73645d5705c167_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The word \"message\" in three smartfonts: a) Tricolor, b) Logobet, and c) Polkabet. It is presented in each smartfont twice: unblurred, and blurred.", "levels": null, "corpus_id": 8104451, "sentences": ["The word \"message\" in three smartfonts: a) Tricolor, b) Logobet, and c) Polkabet.", "It is presented in each smartfont twice: unblurred, and blurred."], "caption": "", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The Visibraille alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter.", "levels": null, "corpus_id": 8104451, "sentences": ["The Visibraille alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter."], "caption": "Visibraille alphabetPolkabet alphabetLogobet alphabetVisibraille 2 alphabetTricolor alphabet", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The Polkabet alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter.", "levels": null, "corpus_id": 8104451, "sentences": ["The Polkabet alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter."], "caption": "Polkabet alphabet", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The Logobet alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter.", "levels": null, "corpus_id": 8104451, "sentences": ["The Logobet alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter."], "caption": "Logobet alphabet", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The Visibraille 2 alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter.", "levels": null, "corpus_id": 8104451, "sentences": ["The Visibraille 2 alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter."], "caption": "Visibraille 2 alphabet", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The Tricolor alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter.", "levels": null, "corpus_id": 8104451, "sentences": ["The Tricolor alphabet, presented in a table showing each smartfont letter with its corresponding Latin letter."], "caption": "Tricolor alphabet", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "A table showing the mnemonics developed for Polkabet. The characters are organized into three rows and 6 columns. The top row consists of foods, the middle of animals, and the bottom of miscellaneous things. Each column has a different color: red, white, yellow, green, blue, and purple. Each square consists of an mnemonic object that falls into the row's category, and columns color. The top row is: tomato, ice crea, mac n cheese, zucchini, kool-aid, and eggplant. The second row is: lobster, dove, chick, alligator, (blue) jay, hippo. The bottom row is: nail polish, q-tips, sun, fern, ocean, violet.", "levels": null, "corpus_id": 8104451, "sentences": ["A table showing the mnemonics developed for Polkabet.", "The characters are organized into three rows and 6 columns.", "The top row consists of foods, the middle of animals, and the bottom of miscellaneous things.", "Each column has a different color: red, white, yellow, green, blue, and purple.", "Each square consists of an mnemonic object that falls into the row's category, and columns color.", "The top row is: tomato, ice crea, mac n cheese, zucchini, kool-aid, and eggplant.", "The second row is: lobster, dove, chick, alligator, (blue) jay, hippo.", "The bottom row is: nail polish, q-tips, sun, fern, ocean, violet."], "caption": "", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The Logobet alphabet with kerning. Adjacent letters share the same vertical space when possible. The alphabet takes up much less horizontal space than the version without kerning.", "levels": null, "corpus_id": 8104451, "sentences": ["The Logobet alphabet with kerning.", "Adjacent letters share the same vertical space when possible.", "The alphabet takes up much less horizontal space than the version without kerning."], "caption": "The alphabet in Logobet with kerning", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_012.jpg"], "annotated": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "A list of font heights, and representative corresponding smartfont letters. The font heights are: 40, 30, 22, 16, 12, 9, 7, 5, 4, 3, 2. There is one smartfont letter presented at size 40. For each subsequent (smaller) size, there is one more character presented than at the previous size.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 8104451, "sentences": ["A list of font heights, and representative corresponding smartfont letters.", "The font heights are: 40, 30, 22, 16, 12, 9, 7, 5, 4, 3, 2.", "There is one smartfont letter presented at size 40.", "For each subsequent (smaller) size, there is one more character presented than at the previous size."], "caption": "of characters appeared together on the virtual keyboard at least 33 times (mean 64.7). Since experiments were conducted remotely through web browsers, we did not control for display conditions or viewing factors such as retinal angle. However, this enables us to assess the confusability of our shapes \u201cin the wild,\u201d across a wide variety of display types and people. To minimize pixelation artifacts, participants were instructed to keep their web browsers at the default 100% zoom.", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_014.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "All 2x3 characters presented as a 2D graph, where similar characters are closer together. The selected characters for Visibraille 2 are darker than those not used for Visibraille 2.", "levels": [[-1], [-1]], "corpus_id": 8104451, "sentences": ["All 2x3 characters presented as a 2D graph, where similar characters are closer together.", "The selected characters for Visibraille 2 are darker than those not used for Visibraille 2."], "caption": "", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_016.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Reading and Learning Smartfonts", "pdf_hash": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "year": 2016, "venue": "UIST", "alt_text": "The colored (red, blue, and black) 2x3 characters used for Tricolor. They are presented as a 2D graph, where similar characters are closer together.", "levels": [[-1], [-1]], "corpus_id": 8104451, "sentences": ["The colored (red, blue, and black) 2x3 characters used for Tricolor.", "They are presented as a 2D graph, where similar characters are closer together."], "caption": "Figure 9: A D3 [11] force-directed layout of Tricolor attempts to locate similar pairs of letters near one another and also illustrates the colors chosen by our optimization algorithm.", "local_uri": ["3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_017.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Quantification of Users' Visual Attention During Everyday Mobile Device Interactions", "pdf_hash": "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: Our custom Android application recorded video snippets using the front-facing camera readily integrated into modern smartphones every time a user unlocked their device.", "levels": null, "corpus_id": 210927450, "sentences": ["Figure 2: Our custom Android application recorded video snippets using the front-facing camera readily integrated into modern smartphones every time a user unlocked their device."], "caption": "(a)                         (b)                         (c)                         (d)                         (e)", "local_uri": ["2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_003.jpg", "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_004.jpg", "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_005.jpg", "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_006.jpg", "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_007.jpg"], "annotated": false, "compound": true}
{"title": "Quantification of Users' Visual Attention During Everyday Mobile Device Interactions", "pdf_hash": "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: To annotate the recorded data, in our application, we further implemented an annotation game that allowed participants to quickly and effortlessly annotate images with eye contact labels in a crowd-sourcing fashion.", "levels": [[-1]], "corpus_id": 210927450, "sentences": ["Figure 4: To annotate the recorded data, in our application, we further implemented an annotation game that allowed participants to quickly and effortlessly annotate images with eye contact labels in a crowd-sourcing fashion."], "caption": "(a)                   (b)                    (c)", "local_uri": ["2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_009.jpg", "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_010.jpg", "2459f881d3d7d666c2a2be6ebb38f2eec9d01d2e_Image_011.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "De+re: a design concept for provoking meaningful interactive experiences", "pdf_hash": "420884634789b6a68800eaaf9301306f74dca749", "year": 2015, "venue": "MUM", "alt_text": "Photograph of the Hearsay audio device - A wooden box with two buttons and a speaker.", "levels": [[-1]], "corpus_id": 2241288, "sentences": ["Photograph of the Hearsay audio device - A wooden box with two buttons and a speaker."], "caption": "Figure 1. Hearsay speaker box with two illuminated buttons for playing random comments on news items.", "local_uri": ["420884634789b6a68800eaaf9301306f74dca749_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "De+re: a design concept for provoking meaningful interactive experiences", "pdf_hash": "420884634789b6a68800eaaf9301306f74dca749", "year": 2015, "venue": "MUM", "alt_text": "Illustration of the Hearsay audio device in use - A woman is pushing one of the buttons and turning her head towards the speaker.", "levels": null, "corpus_id": 2241288, "sentences": ["Illustration of the Hearsay audio device in use - A woman is pushing one of the buttons and turning her head towards the speaker."], "caption": "Figure 2. User pressing a button on the speaker to listen to a random newspaper reader comment.", "local_uri": ["420884634789b6a68800eaaf9301306f74dca749_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Getting the Healthcare We Want: The Use of Online \"Ask the Doctor\" Platforms in Practice", "pdf_hash": "66d850c662a18d21a3aa8a8927193d16aba055f8", "year": 2020, "venue": "CHI", "alt_text": "Figure 1:  \"Screenshots of WeDoctor mobile app (we take WeDoctor for example): (a) The main features of WeDoctor application; (b) A doctor's homepage with profile information and the form and price of consultation; (c) The number of followers and the popular science articles of the doctor. We've masked all identifiable information.\"", "levels": [[-1], [-1]], "corpus_id": 218482907, "sentences": ["Figure 1:  \"Screenshots of WeDoctor mobile app (we take WeDoctor for example): (a) The main features of WeDoctor application; (b) A doctor's homepage with profile information and the form and price of consultation; (c) The number of followers and the popular science articles of the doctor.", "We've masked all identifiable information.\""], "caption": "(b)                                     (c)Figure 1: Screenshots of WeDoctor mobile app (we take WeDoctor for example): (a) The main features of WeDoctor application;A doctor\u2019s homepage with pro\ufb01le information and the form and price of consultation; (c) The number of followers and the popular science articles of the doctor. We\u2019ve masked all identi\ufb01able information.early in the morning). In these cases, online AtD platforms provide a way to overcome these constraints.Our participants generally trusted these AtD platforms. They perceived them as formal and legitimate, comparing to social media and other online information venues; several partici- pants reported cases of how AtD helped clear their worries raised from online searches. Their perceived trustworthiness was related to their awareness that the AtD platforms were run by big and reputable companies, and doctors\u2019 informa- tion was transparent on the platforms, including the doctor\u2019s working hospital, department, title and so on. Several partici- pants reported their of\ufb02ine clinical visits validated the online diagnosis, making them trust the platforms even more.However, at the same time, our participants also demonstrated a clear awareness of the platforms\u2019 limits. Participants ac- knowledged that, many times, they still needed to rely on of\ufb02ine resources such as the pharmacy, lab tests, and a face-to- face diagnosis to complete their care. Moreover, limitations of the effectiveness of online communication were also ac- knowledged. P1 found that he had to spend quite some time describing various examination data to the doctor over the phone instead of just handing them over as he would in a face- to-face consultation. In order to send a picture to the doctor, P4 had to choose the text-based channel over a phone consul- tation, which then impacted how he could communicate with the doctor. In general, online channels were considered not as rich in information as of\ufb02ine in-person communications.With the awareness of the strengths and limits of AtD, our par- ticipants drew on AtD cautiously and strategically, and weaved online and of\ufb02ine resources together to ensure more reliable and cost-effective healthcare services. Below, we present our \ufb01ndings of how our participants made strategic choices online on AtD platforms, and how they strategically integrated ATD platforms into their overall healthcare practices.Making Strategic Choices OnlineIn this section, we report our \ufb01ndings on how our participants strategically used AtD platforms and the rationales behind their decision-making in terms of what consultation channel they chose (express channel or doctor channel), how they chose from a large number of doctors available on the platform, and how they assessed the diagnoses and suggestions provided by online doctors.Strategic Choices of Consultation ChannelsAs mentioned, these AtD applications support different online consultation channels: express, doctor, and sometimes bidding (e.g. on WeDoctor). We found that our participants took vari- ous factors into account to decide what forms of consultation they should choose, including the perceived severity of their symptoms, the potential cost of money and time for the consul- tation, and how they wanted to use their online consultation, e.g.for self-triage or for diagnosis and treatment.When they perceived their symptoms as minor and only sought a basic assessment, our participants preferred the express chan-Table 1: Demographics of ParticipantsParticipantsGenderAgeLength of Use YearType of Disease Consulted (for whom)OccupationP1P2MF373441liver cancer(father)cough(child)company employeeuniversity facultyP3M333congenital cerebral dysplasia(friend\u2019s child)medical professionalP4M232skin disease(self)undergraduateP5F222orthodontics(self), HPV(self)undergraduateP6M321acute gastroenteritis(self)company employeeP7F282pulpitis(self)company employeeP8F251gastroenteritis(self), hordeolum(self)postgraduateP9M381trachoma(self)entrepreneurP10F381insomnia(self), glaucoma(mother)university facultyP11M242beriber(father), folliculitis(self)postgraduateP12F557throatache(self), keloid(child), headache(husband)company employeenel as it meant a quick response with a low cost. For instance, considering the cost difference, P2 chose to use the free ex- press channel, as she only used the platform to self-triage rather than for an accurate diagnosis or treatment. It was to inquire about her child who was having a cough, and the ex- press channel was free of charge at the time: \"Considering that my child\u2019s issue wasn\u2019t severe anyway...At the same time, it\u2019s a matter of my child, so I couldn\u2019t just neglect it either. So I chose to ask on WeDoctor \ufb01rst to have someone con\ufb01rm for me whether a hospital visit was needed...I let the platform assign a doctor for me. I checked, and found that choosing a doctor on one\u2019s own would cost quite a lot, even more ex- pensive than visiting a hospital of\ufb02ine.\" Similarly, P8 told us that she chose the free express channel because she used the online consultation just for self-triage, not for treatment.To P5, she chose the express channel because she only had simple questions to ask. She considered her question about orthodontic issues so simple that most doctors on the platform would be quali\ufb01ed to answer; she chose the express channel which was cheaper:\"I input the question, and then the platform randomly assigned a doctor to answer... because my question was very simple, ... so I don\u2019t think I needed to designate a particularly famous doctor to answer it... There were fee- based consultations, but I was looking for a cheaper one...\"P6 chose the express channel for quick response because his medical condition was minor but urgent. He was suffering from acute gastroenteritis on a business trip, and could not wait for the hospital to open. He went online with Ping An Good Doctor and used the express channel for help early in the morning, because he believed it was the fastest model:\"I chose the platform-employed doctors [through the express channel]...it says full-time doctors on the platform, 24 hours online. I felt that their responses should be faster.\"On the other hand, when inquiring about severe and compli- cated diseases, participants turned to the paid doctor channel to select a doctor or even multiple doctors for consultations.4P3 is a medical professional, although he is not a clinical doctor, so often he got inquires from his friends for health-related advice or opinions, and sometimes for questions he himself could not fully answer he would turn to the AtD platform for help.For example, P1\u2019s father was in a late stage of liver cancer, and he chose to contact multiple chief physicians specialized in liver cancer for expert advice, although such services were more expensive than the express channel:\"I think this is worth- while... a bit over hundred RMB is quite cheap for a more than 10 minutes\u2019 conversation with a chief physician, right?\" Similarly, P10, P3 and P12 chose the more expensive doctors when inquiring about complicated diseases (P10:glaucoma; P3: congenital cerebral dysplasia; P12:keloidal scar).In some cases, high-priced doctors were intentionally chosen to ensure high quality of answers. When inquiring about her mother\u2019s glaucoma, P10 explained to us:\"There are different prices for different ophthalmologists. Everyone is different, 220, 180 RMB ...it\u2019s not cheap for sure. But I was very anxious about this matter, so I wanted to \ufb01nd a good service, and a reputable, experienced doctor...if the service is free, then the doctor is probably junior and inexperienced, so I didn\u2019t dare to use it.\" Although the price was equal to or even higher than that of an of\ufb02ine visit, considering that an online consultation took less time, she felt it was worthwhile.To P4, he chose the paid doctor channel not only because he also believed it was more reliable than the free, express channel, but also because he believed that knowledge should be appreciated this way. That is, he considered the payment a token of respect to the doctor\u2019s knowledge:\"I feel paid con- sultation services may be more reliable than free ones...And it\u2019s mainly about respecting the knowledge. As a law student, I have strong consciousness about this...\"One participant, P7, chose to use the bidding channel as she wanted to set her own price and believed that the bidding mech- anism could ensure faster and higher quality answers:\"...I got multiple answers after only ten minutes. The biding model is great. It can encourage doctors to improve their answer qual- ity, because patients or caregivers get to choose the perceived best answer and assign the biding payment to the doctor who gives the best answer. So this model enables a virtuous cir- cle  plus, it\u2019s great that we can set our own price based onour own economic situations.\"In summary, we found that our participants took their health issues, the expected cost of time and money, their problem\u2019s urgency, and the desired quality of services into account when making various choices on the ATD platforms, so that they could have cost-effective and reliable services.Strategic Choices of DoctorsAfter our participants had decided to use the doctor channel, they had to think about which doctor(s) to pick. Even though our participants lived in different places and had different consultation needs, we found that to \ufb01nd a good doctor, they all tended to look for the good hospitals \ufb01rst and then identify a doctor at the selected hospital for consultation.The \ufb01rst criterion of choosing an online doctor was the of\ufb01cial ranking of the doctors\u2019 af\ufb01liated hospitals. As mentioned, 3A hospitals, though rare, are commonly considered as the best hospitals, so was often used for selection. P3 explained how he chose doctors on AtD: \"Comparatively speaking, doctors in 3A hospitals are more competent, specialized, and professional. Therefore, when we have the option, choosing doctors at 3A hospitals is of course a priority.\" P1 had a similar idea and described his selection process:\"I lived in Xi\u2019an at the time. I \ufb01rst checked the local 3A hospitals, then I also checked 3A hospitals in larger cities like Shanghai.\"Besides the hospitals\u2019 of\ufb01cial ranking, hospitals\u2019 local repu- tation or reputation in speci\ufb01c clinical areas were also con- sidered. For instance, P4 picked a hospital that was locally well known:\" I live in Hai\u2019an district, and the nearby Peo- ple\u2019s Hospital has a good local reputation, so I searched the hospital \ufb01rst and then chose a doctor in the corresponding department.\" P12, who needed to consult about her daughter\u2019s skin disease, checked the hospitals\u2019 reputations in speci\ufb01c clinical areas \ufb01rst. She explained,\"I was looking for the best dermatology department in Shanghai. Based on online reviews, discussions, and recommendations, I created a list of hospitals with reputable dermatology departments, such as Huashan Hospital. I compared them one by one, and then chose the nearest one \u2013 a 3A hospital. I then looked for doctors.\"After hospitals, users further looked at doctors\u2019 titles, reviews, consultation history, and articles published online, which were all available on the AtD platforms. For example, P1 took the doctors\u2019 titles into consideration in the selection:\"It\u2019s pretty straightforward. Just choose chief physicians or at least asso- ciate chief physicians.\" P12 shared how she evaluated poten- tial candidates by integrating various sources of information: \"Those AtD platforms have a lot of information, such as each doctor\u2019s specialized diseases, how many patients each doctor has treated and how many cases are successful, number of upvotes from patients, thank you letters and comments from patients...so I checked each candidate\u2019s pro\ufb01le page for such information.\" P10 reported a similar process:\"First of all, it should be a 3A. Then I check if the doctor has treated similar diseases, how much experience the doctor has, whether the overall patient reviews are good, and whether the doctor\u2019s expertise matches the health condition I am consulting about.\"Some of our participants also considered the doctors\u2019 age, gen- der, and communication style. For instance, P1 described to ushow he chose a doctor based on how he communicated:\"This doctor was in his 30s or 40s. I found out that he loved com- municating with online patients. He even organized patient groups online...I like the fact that he\u2019s communicative.\" P4, who was worried that his symptoms might be stigmatized, ex- plained to us that he preferred to choose a young, male doctor, as he perceived that younger doctors are usually less conser- vative than older ones, and he would have felt embarrassed if it were a female doctor. Thus, he paid special attention to doctors\u2019 pro\ufb01le pictures:\" I could tell their ages by their pro\ufb01le pictures.\" For those who needed to have another consult on the same issue, they often preferred to select the same doctor they had previously consulted to ensure the continuity of care, as in the case of P6:\"...mainly because of continuity. The doctor knows my situations and medical history better.\"Online Diagnosis and Recommendation AssessmentOur participants did not just blindly follow the online doctors\u2019 diagnoses and recommendations. Rather, they assessed the results cautiously. One way was to use multiple AtD plat- forms and multiple doctors. For instance, P8 compared the diagnoses and medication recommendations provided by three different doctors on three different platforms. She was able to identify a course of action that she trusted and eventually suc- cessfully cured herself:\"I used all these three platforms (Ping An Good Doctor, ChunYu Doctor and Haodf) at the same time. Considering it\u2019s online consultation without face-to-face inter- actions with doctors, and it\u2019s about health issues, I believed it\u2019s necessary to compare so that I could decide on the best course of action...The doctors on Ping An Good Doctor and ChunYu Doctor gave me similar recommendations regarding what kinds of medication I should take, so I followed their recommendations...and cured myself. \" P10 also similarly used multiple ATD platforms to consult on the same issue. In an- other example, P7 used the biding channel to attract multiple answers from various doctors, and then assessed each answer to choose the one she perceive to be the best.Another way to be cautious is to seek additional information online to verify the doctor\u2019s recommendations. For instance, P5 actively searched for information online regarding the local hospitals and the doctors recommended by the online doc- tor:\"The doctor I consulted on YaMeMe recommended some of\ufb02ine hospitals and doctors to me...I researched the recom- mended hospitals and doctors online carefully...It\u2019s similar to online shopping...I searched and read online reviews to decide whether I should trust the recommendations.\u201dOur participants also used of\ufb02ine medicals visit to cross- validate their online diagnoses and suggestions. For example, P2 explained, \"I didn\u2019t buy the medication recommended by the doctor on the platform...I decided to do an of\ufb02ine medical visit, so then I could compare...The of\ufb02ine doctor gave the same diagnosis and recommendation, which proved that the online doctor\u2019s judgement was right.\" Similarly, P7 also went to an of\ufb02ine hospital to do an X-Ray, which con\ufb01rmed the online doctor\u2019s diagnosis and recommendation.Strategic Integration of AtD in HealthcareWhile AtD platforms provide convenient ways for online con- sultation with doctors, online consultation alone is usuallyinsuf\ufb01cient to address a health issue, due to the lack of various physical resources including medicine, lab tests, and other treatment facilities. As such, AtD was often leveraged and integrated to help improve healthcare experiences in a more cost-effective manner. In this section, we report on their strate- gic integration of AtD into their healthcare practices: how they combined AtD services with convenient of\ufb02ine resources for simple/regular care, how they used AtD for care naviga- tion \u2013 \ufb01nding the healthcare that is right for them in terms of the right doctor and the right time, and using AtD for patient engagement \u2013 learning to get informed and empowered with knowledge for better involvement with their own care.Combining AtD with Convenient Of\ufb02ine ResourcesFor some minor health issues, our participants found that online consultation, combined with medicine delivery services or pharmacy, was suf\ufb01cient to address patients\u2019 problems. For instance, P11 had used AliHealth for online consultation and medicine purchasing several times for small problems, including his father\u2019s beriberi:\"I consulted online multiple times, all for minor issues. We took the medication the online doctor recommended...and every time it worked well.\" P9, on the other hand, already purchased medicine from an of\ufb02ine pharmacy and used AtD to ensure the medication was safe and correct:\"The pharmacist said that this might be trachoma and suggested some medicine to me. But I was still worried...Then I took a photo of my eyes, and sent it to the online doctor...He told me that it was indeed trachoma...His con\ufb01rmation made me feel reassured to take the medicine.\"As these were minor health issues, our participants felt that the potential risks of online consultation, such as misdiagnosis or delayed of\ufb02ine care, were manageable and worth it. For instance, P4 had some private concerns and chose to go online for a consultation \ufb01rst, and the risk made sense to him in that case:\"If you have a big problem, you still have to go to a hospital. If it is a small problem and you can afford the delay, you can go online for a doctor\u2019s opinion \ufb01rst.\" Furthermore, he also believed that usually the doctors would avoid giving any risky suggestions:\"Online Doctors usually do not give very wrong guidance or suggestions that can cause disease exacerbation.\" Many participants reported that they chose to go online \ufb01rst since the problem was minor and not that urgent.We also found for some regular check ups, participants com- bined the online consultation remotely with doctors from 3A hospitals and an of\ufb02ine clinical visit in a nearby yet less rep- utable hospital, for a more cost-effective care solution. P2, for instance, after consulting an online doctor from a 3A hospital for her child\u2019s coughing, went to a local community hospital to get her child a blood test. She then followed up with the online doctor. For her, this was a way to combine the advantages of the AtD platform and the local hospital to achieve care she could trust:\"Doctors in the community hospitals have limited expertise compared to those online doctors from 3A hospi- tals...but it\u2019s (the local community hospital) a great place for us to get medical tests, which we cannot do on those AtD plat- forms.\" We also learned from our participants that sometimes patients who were once very sick but were currently in stable conditions would do regular check-ups at a local hospital andthen use AtD to send pictures of results to the doctor they had previously visited in Shanghai or Beijing for follow-up advice. They used this as a cost-effective approach to managing their disease without traveling and waiting.Using AtD for Care NavigationFor more complex health issues, AtD was used for care navi- gation \u2013 that is, to help them identify the right hospital/doctor and the right time for of\ufb02ine clinical visits. P12 explained this clearly:\"It (the online AtD platform) is not helpful for treat- ment but helpful to seek healthcare services for treatment.\"It was common that they used AtD to identify the right hos- pital/doctor not simply for online consultation, but also for of\ufb02ine clinical visits. P12 described to us how she used the platform to keep herself from blindly seeking healthcare ser- vices of\ufb02ine:\"You should \ufb01nd doctors who can deal with this kind of disease online...It is mainly to reduce the blindness, not for diagnosis/treatment. For diagnosis/treatment, you must go to the hospital for a face-to-face diagnosis... the online doctor, or maybe it was in fact his assistant, told me whether they could treat it and when the doctor would be available, etc...\" P1, after consulting multiple doctors through AtD, narrowed down and identi\ufb01ed one doctor for further of\ufb02ine consulta- tion:\"I made appointments with several doctors online to talk about the situation. There was one of the doctors, who is a gas- troenterologist from Shanghai Eastern Hepatobiliary Hospital and specializes in liver-related diseases...I found he\u2019s very ex- perienced [in this kind of disease], and was good at combining Traditional Chinese Medicine and Western medicine. I was not local [in Shanghai]... but my wife was in Shanghai at that time, so she brought all the test results and other documents to go see that doctor in the hospital...\" This way, through the online processes of using AtD, participants got to know and identify doctors for clinic visits of\ufb02ine.Sometimes,it was not easy to \ufb01nd the right hospital to visit, especially to \ufb01nd one that matches one\u2019s own situation, not simply the best ones which could be too crowded. For in- stance, P9, an entrepreneur who had been busy running his own startup, described to us that when he had an eye problem which turned out to be only a minor issue, he did not know where to go except directly to a well-known specialist hospital. However, well-known hospitals are usually too packed for last minute consultations, even early in the morning. Thus, he could not get urgent care in the hospital. Later, he described to us how he had limited knowledge of where to go for healthcare in many situations:\"People like us (who are busy) don\u2019t have enough time and knowledge...The \ufb01rst thought that we usually have whenever we have some issues is to visit a specialist hospital.\" As such, as people\u2019s knowledge of hospitals as well as their own health situations was often limited, and there was no family or primary care physician for such advice, the use of AtD platforms was especially valuable to address such a gap.With the use of AtD for care navigation, our participants re- ported that they gained more control of their of\ufb02ine healthcare experiences. P12 put it this way:\"If we go to hospitals directly, we don\u2019t know who the doctors are... And it\u2019s random. You usually get assigned to whoever is available. But consulting online affords us \ufb02exibility and time to do the research, andyou can \ufb01nd a doctor and then go see him of\ufb02ine, which gives you better control.\" P10 shared and compared her of\ufb02ine clini- cal experience with and without the use of AtD. At \ufb01rst, she went to visit the hospital directly:\"I went to the Department of Neurology of Huashan Hospital...it was so crowded... I was sent off in one minute, and was told, \u2019OK, I will pre- scribe you some sleeping pills\u2019. Just some sleeping pills! That couldn\u2019t really solve the problem!\" After that, she decided to use AtD:\"So I decided to \ufb01nd a specialist in insomnia. I went online to search for those doctors who specialize in insomnia on Haodf or WeDoctor... I went to see him of\ufb02ine, and made a registration. The doctor was great. He was more patient and prescribed some medication for me...and it\u2019s convenient that I could ask follow-up questions online regarding the medication without having to stand in a long queue.\" As such, before they actually went to see a doctor, they could used AtD to learn more about doctors and identify the one that could better \ufb01t their own needs.Participants also used AtD to decide the right time to \ufb01t their own situation for a hospital visit. For those with very busy schedules everyday, it was not always easy to \ufb01nd a time for a hospital visit. P7 described how the use AtD helped him make such a decision:\"I didn\u2019t have time to go to the hospital, but I was also worried about my tooth condition. I consulted the doctor online \ufb01rst to see how serious it was so that I would have some ideas in mind, and then decide whether to go to the hospital or not, and when.\" The answer from the doctor made her feel relieved that it wasn\u2019t so serious, and she then found a time to visit a hospital when she was not so busy. P12 also reported how an online assessment of urgency would shape her decision regarding when to see a doctor of\ufb02ine:\"Because sometimes we are not sure whether it\u2019s a serious or urgent issue, so consulting online \ufb01rst can help us self-triage. If the online doctor says we must hurry, we will go to the hospital as soon as possible. If it\u2019s not urgent, We will just wait until we are not busy, like on weekends. There was one time, my husband had a headache. I consulted online for him. The doctor said, \u2019you\u2019d better go to hospital and don\u2019t delay.\u2019 So we asked for leave from work and went to a hospital the next day.\"Using AtD for Patient EngagementWhen facing complex situations or needing to make dif\ufb01cult decisions, AtD was often drawn on by our participants to learn and better understand their own or their family members\u2019 health situations as well as different treatment options. This kind of patient engagement, in terms of learning to get more in- formed and empowered with knowledge, has been considered the key to effective healthcare [4]. Our study shows that such learning is supported on AtD platforms in two main ways: 1) reading the popular medical science articles and medical cases shared by the doctors and 2) talking to the doctors directly.Several participants reported reading the articles posted on AtD platforms to educate themselves before their of\ufb02ine clinic visits in order to ensure better engagement when actually talk- ing to a doctor. P12 explained why she would do this: \"If you know something about your disease, you will know better what the doctor talks about.\" She pointed out how the oftenvery brief of\ufb02ine consultations made such learning especially important:\"After all, the of\ufb02ine medical visit is very short. The Ninth Hospital in Shanghai is always so crowded. People wait for a whole day and may only have three to \ufb01ve minutes of consultation... So you need to have a clear goal, and prepare your questions and ask quickly... Learning online gives me a direction with my questions.\" She also told us that sometimes the answers from the online doctors, even when brief, pro- vided cues for her to \ufb01nd more online resources to read and study. P1, to evaluate the treatment plans for his father who had been diagnosed with end-stage cancer, read extensively on the platform before talking to the doctors, online or of\ufb02ine. Even when not engaged on AtD for a particular health issue, some participants (e.g. P2, P7, P8) reported using the plat- forms to read articles to improve their general health literacy \u2013 that is, to improve their general ability to access, understand, appraise, and apply general health information.In addition to reading, talking to the AtD doctors online was also seen as an effective way for learning, especially when compared to of\ufb02ine consultations which were usually very short. Through AtD, patients/caregivers could make an ap- pointment and could pay for a certain amount of time to talk with a doctor over the phone, and thus the length of consulta- tion could be guaranteed. P1 described it this way: \"In fact the time you can communicate with a doctor online is longer than that of\ufb02ine...you can book more than ten minutes with a doctor online.\" P10 shared how speaking with an online doctor allowed her to inquire more about medical jargon:\"It was for my mom\u2019s eyes. I did not understand some of the terms quite well. I could just ask again and again, and just continue to talk to [the online doctor].\" As such, while a phone consulta- tion might not be considered as effective for diagnosis as an in-person consultation, it was found to be more effective for learning, probably because it is based on the doctor\u2019s selected time (so the doctor is more patient) and it provides suf\ufb01cient time for answering questions more fully.Moreover, talking to different doctors, not simply for diagnos- tic assessment as mentioned earlier, was also used as a way for patients to gain deeper understanding from multiple per- spectives. P1 was aware that liver cancer was a complicated disease, and recognized that there were different schools of thoughts regarding it. Therefore, he chose to talk to multiple doctors online to understand it more clearly:\"I consulted multi- ple doctors. It was because of the nature of cancer, which hasits own attributes. That is, it is more complicated, so you can\u2019t rely on one school\u2019s perspective There are different schoolsof thoughts about what treatment plan is optimal. Therefore, I needed to synthesize the different treatment plans recom- mended by different doctors, study them, and analyze them, so that I could have a clear understanding about which plan suited my father best.\" The participants stressed that, through AtD, they could engage in this way of learning in a much more ef\ufb01cient and effective manner. P10, who talked to different doctors through AtD in order to gain knowledge so to make a decision of whether or not a surgery for her mother\u2019s glau- coma was best, told us :\"One bene\ufb01t of AtD is that we can easily make an appointment with the doctor we would like to consult over the phone. The other bene\ufb01t is that we canconsult multiple doctors at the same time, so we can learn different perspectives, compare and evaluate them.\u201dDISCUSSIONSIn the preceding sections, we illustrated our participants\u2019 de- tailed practices of using AtD. Considering their speci\ufb01c health situations and based on their awareness of the strengths and limits of both online and of\ufb02ine services, patients and care- givers cautiously and strategically made choices online and incorporated AtD into their daily healthcare practices to en- sure reliable and cost-effective healthcare services. In this section, we discuss how our study enhances our understanding of patient agency in seeking healthcare services and then pro- vides corresponding design implications. We also discuss how this particularly situated healthcare context helps highlight the value of AtD platforms.Patient Agency for HealthcareInspiring patients to go from a \"passive patient\" to an \"active agent\" has long been called on in healthcare [2] to respect patients and ensure effective healthcare outcomes [31]. Patient agency has been mainly discussed from a provider\u2019s perspec- tive in a context where the care relationship between patients and providers has already been established: how providers should be respectful and responsive to patients\u2019 needs, values, preferences, and better engage patients through education and involve them in the decision-making process [4]. In recent years, there is an increasing body of HCI work that focuses on patient agency from the patients\u2019 perspective, exploring how technologies can be designed in support of it [21, 46, 30, 5]. However, these studies are mainly carried out in clinical or hospital settings, and much less has been discussed how patients and caregivers want their healthcare services to be carried out beyond those settings and how to achieve that.With the increasing emphasis on \"self-management\" [18], more and more work has to be done by patients and their caregivers, often beyond clinical settings. HCI research is particularly concerned with how, from the patients\u2019 perspec- tive, health can be managed on a daily basis outside of the clinical setting. Previous research reveals all kinds of \"work\" that patients and their informal caregivers have to engage in in order to effectively manage their healthcare conditions [34, 58, 13, 43], especially for chronic illness management [8, 13, 42]. While some past researchers have turned their attention to the work carried out by patients and their caregivers, most of this focus remains on patient work in terms of chronic illness management, while less is on the broader work involved in how patients seek the healthcare services they want.What this study brings to the fore is, then, a kind of agency that is patient-driven, involving actively adopting, choosing, \ufb01nding, assessing, and weaving healthcare services to better suit the needs for care. What our study highlights is not simply what roles AtD platforms play in healthcare, as illustrated in [39], but also the agency patients and their caregivers enact and the kind of work they carry out in the process of seeking healthcare services. This kind of work goes beyond what pa- tients and their caregivers do when managing health conditions and daily life, especially when dealing with chronic illnesses[8, 13, 42, 43]. It is also different from the infrastructural work patients and their caregivers do in order to \ufb01x the various breakdowns in the healthcare systems composed of different institutions and organizations, as highlighted in [19]. Rather, this agency is a kind of work patients and caregivers engage in in order to get a reliable and cost-effective healthcare service to better serve their care needs.Emirbayer and Mische de\ufb01ne agency \u201cas a temporally embed- ded process of social engagement, informed by the past (in its \u2019iterational\u2019 or habitual aspect) but also oriented toward the future (as a \u2019projective\u2019 capacity to imagine alternative possibilities) and toward the present (as a \u2019practical-evaluative\u2019 capacity to contextualize past habits and future projects within the contingencies of the moment).\u201d [12]. This is exactly what we found in our study. The use of the AtD platform and the decision-making users engaged in was shaped by 1.) users\u2019 past experiences of illness and healthcare services ( e.g. their awareness of the healthcare system in China, the ranking sys- tem of hospitals, and their terrible, sometimes even shocking, experiences of long lines, overcrowding, and overly short clin- ical sessions), 2.) their current situations (e.g. their illness as minor or complex, where they are and when they want care services), and 3) the motivation to have a more controlled, better care experience, and better care effects in future (e.g. when/if to go to the hospital, choosing doctors).In other words, instead of blindly accepting or following what the existing healthcare system provides, patients and caregivers, based on their awareness of their particular situations as well as the strengths and limits of different services, strategically make choices and assessment, and weave together different online and of\ufb02ine resources in a reliable and cost-effective manner. We argue that supporting this kind of patient agency is one of the major values AtD platforms can provide.In addition, our study reveals speci\ufb01cally in what ways pa- tients and their caregivers use online AtD platforms to \ufb01nd the best healthcare service they can afford. This is made possible, because these AtD platforms do not simply make medical professionals more accessible for questions, but also make of\ufb02ine resources more transparent, e.g. the doctors\u2019 pro\ufb01les, titles, past medical cases, hospital information, price for con- sultation, and patient reviews. Transparency of this kind of information allowed patients and their caregivers the ability to make more informed choices in terms of whom they would like to go to for help, online or of\ufb02ine. When they are seen not simply as passive patients, but as active agents, we start to un- derstand, how, with simple transparency and the accessibility of resources, they were able to make choices and weave dif- ferent resources together to better address their concerns. We suggest that in addition to providing medical information for learning and education as stressed in patient engagement [31], helping patients and caregivers understand the healthcare sys- tem is important as well. Thus, in addition to making doctors\u2019 information more transparent online as shown in our study, we can also make other medical resources more transparent, such as what hospitals are nearby and what their capacities are, what facilities these hospitals have, and how packed and crowded the hospitals are at a given moment. By making vari- ous medical resources more transparent, patients could thenbetter enact their agency to better manage, control and achieve their desired healthcare services.AtD and the Situated Healthcare systemOur study reveals not only that online AtD platforms are com- plementary to of\ufb02ine services, as shown in [39], but also why that is the case. For instance, the main reason for our partici- pants to combine online advice from AtD with of\ufb02ine service from the nearby hospitals was to be able to get advice from well-known doctors from 3A hospitals that they did not have access to of\ufb02ine. Our participants also used AtD for navigation beforehand to reduce the random choice of doctors and gain a better control of the of\ufb02ine healthcare experience\u2019s uncertain waiting time. In addition, since the of\ufb02ine in-person consulta- tion is usually overly short and not conducive to learning or patient engagement, our patients/caregivers turned to AtD for paid phone consultation, and used set duration of time with doctors to ensure fuller communication, better learning experi- ences and better healthcare engagement, especially when they needed to deal with complex situations and to make dif\ufb01cult decisions. As we can see here, AtD was mainly drawn on to compensate for the various limitations patients found in the of\ufb02ine healthcare systems.In other words, as shown in our study, the adoption, use, and value of AtD services can not be understood without considering the particular healthcare context in which they are embedded. Only when we understand the inequality of healthcare resources in China, the challenges of accessing such services due to physical, geographical, and social reasons, and the limitations of the of\ufb02ine healthcare system, can we better understand why people would turn to AtD online to receive the services of their choice. Just as Miller and Slater point out that the \"virtual\" has never been apart from the \"real\", but is continuous with and embedded in other social spaces, mundane social structures, and everyday relations that it has the power to transform but cannot escape [45]. This is what we found with the use of AtD, and the values of AtD are highlighted with the particular of\ufb02ine healthcare system in which the AtD platforms are embedded .Moreover, the use of AtD is not simply shaped by of\ufb02ine healthcare systems, but also integrated with of\ufb02ine healthcare resources to form a whole healthcare service system for each user at the micro, individual level. As illustrated, our partici- pants were aware of the limitations of AtD, and strategically made the best of both AtD and of\ufb02ine services. Many AtD applications explicitly state that their services are for reference only and should not replace of\ufb02ine hospital visits. Indeed, our participants mainly used AtD as a mediating tool to gain a better of\ufb02ine healthcare experience and outcome and not to replace of\ufb02ine clinical visits except for in the case of minor issues, since they are aware that AtD does not involve face diagnosis (which is central to Chinese Traditional Medicine), lab tests or treatment facilities. In other words, AtD is mainly drawn on to complement, extend, enhance, and navigate the of\ufb02ine system, e.g. \ufb01nd the right doctor and the right time for a visit. That is, healthcare is a very materially conditioned prac- tice and the online platform is too limited a channel without the material engagement, which determines the participants\u2019reliance on of\ufb02ine healthcare systems and further determines that the use of AtD is not an stand alone phenomenon, but rather is conditioned upon, highly contextualized, and greatly shaped by the of\ufb02ine healthcare system.In spite of the limitations, one of the main values patients \ufb01nd in AtD is the ability to learn through reading popular science articles published by AtD doctors and communicating with the doctors for opinions and analysis. Sometimes, as suggested by P12, a short interaction with a doctor online could provide cues and directions for patients and their caregivers to \ufb01nd resources to learn more. Considering the usually overly short of\ufb02ine clinical sessions in Chinese hospitals, an online chan- nel, such as AtD, becomes especially important to ensuring the patients\u2019 acquisition of knowledge and the improvement of their health literacy, which has long been considered the key to effective patient engagement and patient-provider com- munication [31, 48]. The knowledge gained through these platforms evened the information asymmetrical and literacy gap between patients and doctors. We suggest that, besides making medical resources more transparent for choice mak- ing, providing reliable resources and interactions for learning and education be one direction worth exploring further when designing online AtD platforms.CONCLUSIONIn this paper, we present an interview-based study of AtD in use in China. Rather than relying on AtD directly for health- care, except for in the case of minor issues, we \ufb01nd that our participants primarily used AtD as a mediating tool to com- pensate for the various weaknesses that exist in the of\ufb02ine healthcare system and to gain better of\ufb02ine healthcare experi- ences and outcomes. While almost all our participants trusted the platforms and took advantage of them for convenience, they had a clear awareness of the limits and risks of the online healthcare services, and leveraged various strategies to ensure the safety, reliability and cost-effectiveness of their care.Recent HCI studies have brought forth the \u201cwork\u201d patients and their caregivers need to do for their healthcare, such as the self- management of chronic diseases [8, 13, 42]. Together with the recent attention on infrastructural work [19], we would like to broaden the scope of inquiry into patient\u2019s work for healthcare services beyond those taking places in clinical settings, and home settings alone, and to include the whole process patients and their caregivers go through in order to gain reliable health- care service in a cost-effective manner. We argue this is the main value AtD can provide and a direction we will explore further for the design of online healthcare services.ACKNOWLEDGMENTSWe would like to thank our participants of this study. The work is supported by the National Key Research and Development Plan of China under Grant No.2016YFB1001200, the National Natural Science Foundation of China (NSFC) under Grant No.61672167.REFERENCES\u200cElske Ammenwerth, Petra Schnell-Inderst, and Alexander Hoerbst. 2012. The impact of electronicpatient portals on patient care: a systematic review of controlled trials. Journal of medical Internet research 14, 6 (2012), e162.David Armstrong. 2014. Actors, patients and agency: a recent history. Sociology of Health & Illness 36, 2 (2014), 163\u2013174.Helen Atherton, Yannis Pappas, Carl Heneghan, and Elizabeth Murray. 2013. Experiences of using email for general practice consultations: a qualitative study. Br J Gen Pract 63, 616 (2013), e760\u2013e767.Alastair Baker. 2001. Crossing the quality chasm: a new health system for the 21st century. (2001).Andrew BL Berry, Catherine Y Lim, Tad Hirsch, Andrea L Hartzler, Linda M Kiel, Zo\u00eb A Bermet, and James D Ralston. 2019. Supporting Communication About Values Between People with Multiple Chronic Conditions and their Providers. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 470.Anna Bell Bj\u00f6rk, Helene Hillborg, Marika Augutis, and G\u00f6ran Umefjord. 2017. Evolving techniques intext-based medical consultation\u2013Physicians\u2019 long-term experiences at an Ask the doctor service. International journal of medical informatics 105 (2017), 83\u201388.CareQualityCommission. 2017. Care Quality Commission advises people to take care when using online primary care services. https://www.cqc.org.uk/n ews/releases/care-quality-commission-advises-people- take-care-when-using-online-primary-care. (2017).Yunan Chen. 2011. Health information use in chronic care cycles. In Proceedings of the ACM 2011 conference on Computer supported cooperative work. ACM, 485\u2013488.\u200cRebecca JW Cline and Katie M Haynes. 2001. Consumer health information seeking on the Internet: the state of the art. Health education research 16, 6 (2001), 671\u2013692.Mayara Costa Figueiredo, Clara Caldeira, Tera L Reynolds, Sean Victory, Kai Zheng, and Yunan Chen. 2017. Self-tracking for fertility care: collaborative support for a highly personalized problem. Proceedings of the ACM on Human-Computer Interaction 1, CSCW (2017), 36.Kolsum Deldar, Parviz Marouzi, and Reza Assadi. 2011. Teleconsultation via the web: an analysis of the type of questions that Iranian patients ask. Journal of telemedicine and telecare 17, 6 (2011), 324\u2013327.Mustafa Emirbayer and Ann Mische. 1998. What is agency? American journal of sociology 103, 4 (1998), 962\u20131023.\u200cJordan Eschler, Logan Kendall, Kathleen O\u2019Leary,  Lisa M Vizer, Paula Lozano, Jennifer B McClure, Wanda Pratt, and James D Ralston. 2015. Shared calendars for home health management. In Proceedingsof the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. ACM, 1277\u20131288.Gunther Eysenbach, John Powell, Oliver Kuss, and Eun-Ryoung Sa. 2002. Empirical studies assessing the quality of health information for consumers on the world wide web: a systematic review. Jama 287, 20 (2002), 2691\u20132700.Susannah Fox and Deborah Fallows. 2003. Internet health resources. TPRC.Jeana Frost and Michael Massagli. 2008. Social uses of personal health information within PatientsLikeMe, an online patient community: what can happen when patients have access to one another\u2019s data. Journal of medical Internet research 10, 3 (2008), e15.Mita Sanghavi Goel, Tiffany L Brown, Adam Williams, Andrew J Cooper, Romana Hasnain-Wynia, andDavid W Baker. 2011. Patient reported barriers to enrolling in a patient portal. Journal of the American Medical Informatics Association 18, Supplement_1 (2011), i8\u2013i12.Patricia A Grady and Lisa Lucio Gough. 2014.Self-management: a comprehensive approach to management of chronic conditions. American Journal of Public Health 104, 8 (2014), e25\u2013e31.Xinning Gui and Yunan Chen. 2019. Making Healthcare Infrastructure Work: Unpacking the Infrastructuring Work of Individuals. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 458.Xinning Gui, Yu Chen, Yubo Kou, Katie Pine, and Yunan Chen. 2017. Investigating Support Seeking from Peers for Pregnancy in Online Health Communities. Proceedings of the ACM on Human-Computer Interaction 1, CSCW (2017), 50.Shefali Haldar, Sonali R Mishra, Maher Kheli\ufb01, Ari H Pollack, and Wanda Pratt. 2019. Beyond the Patient Portal: Supporting Needs of Hospitalized Patients. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 366.Andrea Hartzler and Wanda Pratt. 2011. Managing the personal side of health: how patient expertise differs from the expertise of clinicians. Journal of medical Internet research 13, 3 (2011), e62.Jolie N Haun, Jason D Lind, Stephanie L Shimada, Tracey L Martin, Robert M Gosline, Nicole Antinori, Max Stewart, and Steven R Simon. 2014. Evaluating user experiences of the secure messaging tool on the Veterans Affairs\u2019 patient portal system. Journal of medical Internet research 16, 3 (2014), e75.L Kari Hironaka and Michael K Paasche-Orlow. 2008. The implications of health literacy on patient\u2013provider communication. Archives of disease in childhood 93, 5 (2008), 428\u2013432.Yifeng Hu and S Shyam Sundar. 2010. Effects of online health sources on credibility and behavioral intentions. Communication research 37, 1 (2010), 105\u2013132.Jina Huh and Mark S Ackerman. 2012. Collaborative help in chronic disease management: supporting individualized problems. In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work. ACM, 853\u2013862.Jina Huh, Rupa Patel, and Wanda Pratt. 2012. Tackling dilemmas in supporting\u2019the whole person\u2019in online patient communities. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 923\u2013926.Prospective industry research institute. 2019. China\u2019s Internet Medical Industry Status and Prospects in 2019 Good Policy + 5G Technology to Promote theReformative Development of Industry. https://bg.qianz han.com/report/detail/300/190613-d71dfea8.html.(2019).Taya Irizarry, Annette DeVito Dabbs, and Christine R Curran. 2015. Patient portals and patient engagement: a state of the science review. Journal of medical Internet research 17, 6 (2015), e148.Maia L Jacobs, James Clawson, and Elizabeth D Mynatt. 2015. Comparing health information sharing preferences of cancer patients, doctors, and navigators. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. ACM, 808\u2013818.Julia James and others. 2013. Patient engagement.Health Affairs Health Policy Brief 14, 10.1377 (2013).David C Kaelber, Ashish K Jha, Douglas Johnston, Blackford Middleton, and David W Bates. 2008. A research agenda for personal health records (PHRs). Journal of the American Medical Informatics Association 15, 6 (2008), 729\u2013736.Steven J Katz and Cheryl A Moyer. 2004. The emerging role of online communication between patients and their providers. Journal of general internal medicine 19, 9 (2004), 978\u2013983.Predrag Klasnja, Andrea Civan Hartzler, Kent T Unruh, and Wanda Pratt. 2010. Blowing in the wind: unanchored patient information work during cancer care. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 193\u2013202.Ivana Klinar, Ana Bala\u017ein, Bruno Bar\u0161ic\u00b4, and Hrvoje Tiljak. 2011. Identi\ufb01cation of general characteristics, motivation, and satisfaction of internet-based medical consultation service users in Croatia. Croatian medical journal 52, 4 (2011), 557.Yumei Li, Xiangbin Yan, and Xiaolong Song. 2019. Provision of Paid Web-Based Medical Consultation in China: Cross-Sectional Analysis of Data From a Medical Consultation Website. Journal of medical Internet research 21, 6 (2019), e12126.Leslie S Liu, Patrick C Shih, and Gillian R Hayes. 2011. Barriers to the adoption and use of personal health record systems. In Proceedings of the 2011 iConference. ACM, 363\u2013370.LV. 2018. Analysis of Online Medical APP: Ping An Good Doctor.http://www.woshipm.com/evaluating/1754515.html.(2018).Xiaojuan Ma, Xinning Gui, Jiayue Fan, Mingqian Zhao, Yunan Chen, and Kai Zheng. 2018. Professional Medical Advice at your Fingertips: An empirical study of an online. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 116.Frances Mair and Pamela Whitten. 2000. Systematic review of studies of patient satisfaction with telemedicine. Bmj 320, 7248 (2000), 1517\u20131520.Nisrine N Makarem and Jumana Antoun. 2016. Email communication in a developing country: different family physician and patient perspectives. Libyan Journal of Medicine 11, 1 (2016), 32679.Lena Mamykina, Andrew D Miller, Elizabeth D Mynatt, and Daniel Greenblatt. 2010. Constructing identities through storytelling in diabetes management. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1203\u20131212.Lena Mamykina, Elizabeth Mynatt, Patricia Davidson, and Daniel Greenblatt. 2008. MAHI: investigation of social scaffolding for re\ufb02ective thinking in diabetes management. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 477\u2013486.\u200cLena Mamykina, Drashko Nakikj, and Noemie Elhadad. 2015. Collective sensemaking in online health forums. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 3217\u20133226.Daniel Miller and Don Slater. 2000. Internet. Berg Publishers.Sonali R Mishra, Shefali Haldar, Ari H Pollack, Logan Kendall, Andrew D Miller, Maher Kheli\ufb01, and Wanda Pratt. 2016. Not Just a Receiver: Understanding Patient Behavior in the Hospital Environment. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3103\u20133114.MobTech. 2019. Mob Research Institute|Insights of Internet Healthcare Industry in 2019. https://zhuanlan.zhihu.com/p/66908625. (2019).Sun Young Park, Yunan Chen, and Shriti Raj. 2017. Beyond health literacy: supporting patient-provider communication during an emergency visit. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. ACM, 2179\u20132192.Madhavi R Patt, Thomas K Houston, Mollie W Jenckes, Daniel Z Sands, and Daniel E Ford. 2003. Doctors who are using e-mail with their patients: a qualitative exploration. Journal of Medical Internet Research 5, 2 (2003), e9.TS Raghu, Keith Frey, Yu-Hui Chang, Meng-Ru Cheng, Sharon Freimund, and Asha Patel. 2015. Using secure messaging to update medications list in ambulatory care setting. International journal of medical informatics 84, 10 (2015), 754\u2013762.Nichola Robertson, Michael Polonsky, and Lisa McQuilken. 2014. Are my symptoms serious Dr Google? A resource-based typology of valueco-destruction in online self-diagnosis. Australasian Marketing Journal (AMJ) 22, 3 (2014), 246\u2013256.Michelle Pannor Silver. 2015. Patient perspectives on online health information and communication with doctors: a qualitative study of patients 50 years old and over. Journal of medical Internet research 17, 1 (2015), e19.\u200cAnselm Straus and Juliet Corbin. 1998. Basics of qualitative research: Techniques and procedures for developing grounded theory. (1998).Si Sun, Xiaomu Zhou, Joshua C Denny, Trent S Rosenbloom, and Hua Xu. 2013. Messaging to your doctors: understanding patient-provider communications via a portal system. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1739\u20131748.\u200cPaul C Tang, Joan S Ash, David W Bates, J Marc Overhage, and Daniel Z Sands. 2006. Personal health records: de\ufb01nitions, bene\ufb01ts, and strategies for overcoming barriers to adoption. Journal of the American Medical Informatics Association 13, 2 (2006), 121\u2013126.G\u00f6ran Umefjord, G\u00f6ran Petersson, and Katarina Hamberg. 2003. Reasons for consulting a doctor on the Internet: Web survey of users of an Ask the Doctor service. Journal of Medical Internet Research 5, 4 (2003), e26.G\u00f6ran Umefjord, Herbert Sandstr\u00f6m, Hans Malker, and G\u00f6ran Petersson. 2008. Medical text-based consultations on the Internet: a 4-year study. International journal of medical informatics 77, 2 (2008), 114\u2013121.Kenton T Unruh and Wanda Pratt. 2007. Patients as actors: the patient\u2019s role in detecting, preventing, and recovering from medical errors. International journal of medical informatics 76 (2007), S236\u2013S244.Jos\u00e9 Ignacio Valenzuela, Catalina L\u00f3pez, Yuli Guzm\u00e1n, and Roosevelt Fajardo. 2010. Web-based asynchronous teleconsulting for consumers in Colombia: a 2-year follow up.. In MedInfo. 559\u2013563.Shlomo Vinker, Michael Weinfass, Lior M Kasinetz, Eliezer Kitai, and Igor Kaiserman. 2007. Web-based question-answering service of a family physician\u2014thecharacteristics of queries in a non-commercial open forum. Medical informatics and the Internet in medicine 32, 2 (2007), 123\u2013129.Ashley E Wade-Vuturo, Lindsay Satterwhite Mayberry, and Chandra Y Osborn. 2012. Secure messaging and diabetes management: experiences and perspectives of patient portal users. Journal of the American Medical Informatics Association 20, 3 (2012), 519\u2013525.Paul Wicks, Michael Massagli, Jeana Frost, Catherine Brownstein, Sally Okun, Timothy Vaughan, Richard Bradley, and James Heywood. 2010. Sharing health data for better outcomes on PatientsLikeMe. Journal of medical Internet research 12, 2 (2010), e19.", "local_uri": ["66d850c662a18d21a3aa8a8927193d16aba055f8_Image_003.jpg", "66d850c662a18d21a3aa8a8927193d16aba055f8_Image_004.jpg", "66d850c662a18d21a3aa8a8927193d16aba055f8_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Improving Crowd-Supported GUI Testing with Structural Guidance", "pdf_hash": "5052e22cf635805f4a6503ab3cf528ed80fc0fef", "year": 2020, "venue": "CHI", "alt_text": "The left hand side is a set of screenshots of three different sub-pages of a GUI. The right hand side is a event-flow graph to represent all possible event-flows that a tester can find.", "levels": [[-1], [-1]], "corpus_id": 218483555, "sentences": ["The left hand side is a set of screenshots of three different sub-pages of a GUI.", "The right hand side is a event-flow graph to represent all possible event-flows that a tester can find."], "caption": "these approaches can be overwhelming for testers to evaluate because the number of possible permutations of low-level events and targets are too large to test, especially when the context of the path is missing. So developers typically rely on manually crafting a small number of event sequences, which is not scalable.", "local_uri": ["5052e22cf635805f4a6503ab3cf528ed80fc0fef_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Patriarchy, Maternal Health and Spiritual Healing: Designing Maternal Health Interventions in Pakistan", "pdf_hash": "ffa123fe13243efefd7d5336d7aee9f0e1b74b4c", "year": 2020, "venue": "CHI", "alt_text": "Figure 1. This is an outline of the three tiers of maternal health.The first tier is primary care, the second is secondary care and the last tertiary care which consist of hospitals. The first tier includes basic health units, maternal child health centres and community midwife centres and rural health centres. The second tier consists of district health centres and tehsil headquarters and the final layer is hospitals.", "levels": null, "corpus_id": 218483179, "sentences": ["Figure 1.", "This is an outline of the three tiers of maternal health.", "The first tier is primary care, the second is secondary care and the last tertiary care which consist of hospitals.", "The first tier includes basic health units, maternal child health centres and community midwife centres and rural health centres.", "The second tier consists of district health centres and tehsil headquarters and the final layer is hospitals."], "caption": "", "local_uri": ["ffa123fe13243efefd7d5336d7aee9f0e1b74b4c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "When Screen Time Isn't Screen Time: Tensions and Needs Between Tweens and Their Parents During Nature-Based Exploration", "pdf_hash": "64d78dbdd4c558315894d7c0edb3cfc650517f5a", "year": 2021, "venue": "CHI", "alt_text": "A picture containing screenshots of NatureGo  app 1: Onboarding\n\u201cWhat are your interests?\u201d \n2: My Collections. \n3: Classification.", "levels": null, "corpus_id": 232222386, "sentences": ["A picture containing screenshots of NatureGo  app 1: Onboarding\n\u201cWhat are your interests?\u201d \n2: My Collections. \n3: Classification."], "caption": "", "local_uri": ["64d78dbdd4c558315894d7c0edb3cfc650517f5a_Image_001.png"], "annotated": false, "compound": false}
{"title": "TIP-Toy: a tactile, open-source computational toolkit to support learning across visual abilities", "pdf_hash": "45460673078f2f1131fcab15bfa5535a54f4fb70", "year": 2020, "venue": "ASSETS", "alt_text": "Tip-Toy system being used by two people, one of whom has a visual impairment. The System comprises of a camera mounted above blocks which are arranged by the users in a pattern which enables code to run. The Speak aloud button reads the code which has been made by the block pattern. The system is orientated using special blocks places at the four corners of an A3 sheet of paper. The run button runs the code. Tip-Toy was designed with expert opinion from visually impaired coders and has deepened our understanding of Low floors, raised ceilings and wide walls", "levels": null, "corpus_id": 225963313, "sentences": ["Tip-Toy system being used by two people, one of whom has a visual impairment.", "The System comprises of a camera mounted above blocks which are arranged by the users in a pattern which enables code to run.", "The Speak aloud button reads the code which has been made by the block pattern.", "The system is orientated using special blocks places at the four corners of an A3 sheet of paper.", "The run button runs the code.", "Tip-Toy was designed with expert opinion from visually impaired coders and has deepened our understanding of Low floors, raised ceilings and wide walls"], "caption": "", "local_uri": ["45460673078f2f1131fcab15bfa5535a54f4fb70_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "TIP-Toy: a tactile, open-source computational toolkit to support learning across visual abilities", "pdf_hash": "45460673078f2f1131fcab15bfa5535a54f4fb70", "year": 2020, "venue": "ASSETS", "alt_text": "The picture shows a digital representation of 6 different blocks that were shown to VI developers during the design consultation. The first block on the top right is colored in blue and has a rectangular shape with an X embossed on the top and a small notch in the lower edge of the top side. The second block is smaller, green and has a squared shape with 4 dots engraved on the top face> The third block is also squared, red and with an embossed number four in light blue. The fourth block is red and sqared shape and has the number five engraved on it. The fifth block is orange and has a rectangular shape with a squared hole on the right hand side. On the top face of the block is embossed a triangular shape pointing towards the right. The sixth block is also rectangular, but much longer than the others. It is green and has a squared hole in the middle. On the top face are embossed the words repeat times separated by the squared hole.", "levels": null, "corpus_id": 225963313, "sentences": ["The picture shows a digital representation of 6 different blocks that were shown to VI developers during the design consultation.", "The first block on the top right is colored in blue and has a rectangular shape with an X embossed on the top and a small notch in the lower edge of the top side.", "The second block is smaller, green and has a squared shape with 4 dots engraved on the top face> The third block is also squared, red and with an embossed number four in light blue.", "The fourth block is red and sqared shape and has the number five engraved on it.", "The fifth block is orange and has a rectangular shape with a squared hole on the right hand side.", "On the top face of the block is embossed a triangular shape pointing towards the right.", "The sixth block is also rectangular, but much longer than the others.", "It is green and has a squared hole in the middle.", "On the top face are embossed the words repeat times separated by the squared hole."], "caption": "Figure 2: Design of blocks showing 3D printed features that were explored with VI developers during design consulta- tion", "local_uri": ["45460673078f2f1131fcab15bfa5535a54f4fb70_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "TIP-Toy: a tactile, open-source computational toolkit to support learning across visual abilities", "pdf_hash": "45460673078f2f1131fcab15bfa5535a54f4fb70", "year": 2020, "venue": "ASSETS", "alt_text": "Top Left: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks. Three blocks are laid on the workspace a start block and two play blocks. There is a gap between the first and second play blocks that would prevent the system from detecting the latter.  Top right: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks. Three blocks are laid on the workspace a start block and two play blocks. There is a gap between the start block and first play block that would prevent the system from detecting the latter.  Bottom Left: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks. Three blocks are laid on the workspace a start block and two play blocks. The second play block is flipped horizontally so the triangle on it points towards the rest instead of the right.  Bottom right: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks. Three blocks are laid on the workspace a start block and two play blocks. The variable contained in the second play block is flipped horizontally", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 225963313, "sentences": ["Top Left: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks.", "Three blocks are laid on the workspace a start block and two play blocks.", "There is a gap between the first and second play blocks that would prevent the system from detecting the latter.", "Top right: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks.", "Three blocks are laid on the workspace a start block and two play blocks.", "There is a gap between the start block and first play block that would prevent the system from detecting the latter.", "Bottom Left: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks.", "Three blocks are laid on the workspace a start block and two play blocks.", "The second play block is flipped horizontally so the triangle on it points towards the rest instead of the right.", "Bottom right: This figure shows a typical orientation mistake that prevents TIP-Toy from reading the markers on the blocks.", "Three blocks are laid on the workspace a start block and two play blocks.", "The variable contained in the second play block is flipped horizontally"], "caption": "", "local_uri": ["45460673078f2f1131fcab15bfa5535a54f4fb70_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TIP-Toy: a tactile, open-source computational toolkit to support learning across visual abilities", "pdf_hash": "45460673078f2f1131fcab15bfa5535a54f4fb70", "year": 2020, "venue": "ASSETS", "alt_text": "Left: The figure shows a play block and a variable block  made of orange salt-dough. The triangle on the play block is made of salt dough and the six dots on the variable block are re-created with dried lentils. The play block is rectangular with a hole on the right side whereas the variable block is a rectangular block of the size of the hole  Right: The figure shows a loop block and a variable block  made of orange salt-dough. The circle, representing the loop block is made of salt dough and the four dots on the variable block are re-created with dried lentils. The variable block is a pentagon with the triangurlar side pointing towards the left and the loop block is also a pentagon with a concave triangle on the right matching the other shape", "levels": null, "corpus_id": 225963313, "sentences": ["Left: The figure shows a play block and a variable block  made of orange salt-dough.", "The triangle on the play block is made of salt dough and the six dots on the variable block are re-created with dried lentils.", "The play block is rectangular with a hole on the right side whereas the variable block is a rectangular block of the size of the hole  Right: The figure shows a loop block and a variable block  made of orange salt-dough.", "The circle, representing the loop block is made of salt dough and the four dots on the variable block are re-created with dried lentils.", "The variable block is a pentagon with the triangurlar side pointing towards the left and the loop block is also a pentagon with a concave triangle on the right matching the other shape"], "caption": "", "local_uri": ["45460673078f2f1131fcab15bfa5535a54f4fb70_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "WELCOMEFrom the inside", "pdf_hash": "735f60c4ea0eaf1646b1be2153d1ccd12713d2b0", "year": 2015, "venue": "Interactions", "alt_text": "https://encrypted-tbn2.google.com/images?q=tbn:ANd9GcTCBsX_xcaOH8eJZ92n5gKrVmrv0DMJ2rCjLwDr8R_3Hoo2_iFP", "levels": null, "corpus_id": 25208963, "sentences": ["https://encrypted-tbn2.google.com/images?q=tbn:ANd9GcTCBsX_xcaOH8eJZ92n5gKrVmrv0DMJ2rCjLwDr8R_3Hoo2_iFP"], "caption": "discounted gym memberships and a yearly health fair, and believe they have a health and wellness program in place. These are considered health and wellness activities and cannot be considered a true comprehensive program", "local_uri": ["735f60c4ea0eaf1646b1be2153d1ccd12713d2b0_Image_033.png"], "annotated": false, "compound": false}
{"title": "WELCOMEFrom the inside", "pdf_hash": "735f60c4ea0eaf1646b1be2153d1ccd12713d2b0", "year": 2015, "venue": "Interactions", "alt_text": "https://encrypted-tbn3.google.com/images?q=tbn:ANd9GcSTXeMIjOSVmZ2--beKRJpbC6L6z7MBEOQzkA33-ts1KGPWT8bNDQ", "levels": null, "corpus_id": 25208963, "sentences": ["https://encrypted-tbn3.google.com/images?q=tbn:ANd9GcSTXeMIjOSVmZ2--beKRJpbC6L6z7MBEOQzkA33-ts1KGPWT8bNDQ"], "caption": "Wellness: Read the lab results for understanding", "local_uri": ["735f60c4ea0eaf1646b1be2153d1ccd12713d2b0_Image_045.png"], "annotated": false, "compound": false}
{"title": "WELCOMEFrom the inside", "pdf_hash": "735f60c4ea0eaf1646b1be2153d1ccd12713d2b0", "year": 2015, "venue": "Interactions", "alt_text": "https://encrypted-tbn3.google.com/images?q=tbn:ANd9GcQllg6RVNZLNytMTK5_-le17gtz5FwTPv3TKKOI8Taqehx-RCuo", "levels": null, "corpus_id": 25208963, "sentences": ["https://encrypted-tbn3.google.com/images?q=tbn:ANd9GcQllg6RVNZLNytMTK5_-le17gtz5FwTPv3TKKOI8Taqehx-RCuo"], "caption": "Respirators, harnesses, level A suits, etc. EHS equipment is numerous and not easy to figure out. We must know each one and be able to teach them to our workers.", "local_uri": ["735f60c4ea0eaf1646b1be2153d1ccd12713d2b0_Image_056.png"], "annotated": false, "compound": false}
{"title": "WELCOMEFrom the inside", "pdf_hash": "735f60c4ea0eaf1646b1be2153d1ccd12713d2b0", "year": 2015, "venue": "Interactions", "alt_text": "https://encrypted-tbn0.google.com/images?q=tbn:ANd9GcQQqNFHgcSOt3rhAHPpN1Ab8B3PzW5MsMJfwKitUorKi14DojfD", "levels": null, "corpus_id": 25208963, "sentences": ["https://encrypted-tbn0.google.com/images?q=tbn:ANd9GcQQqNFHgcSOt3rhAHPpN1Ab8B3PzW5MsMJfwKitUorKi14DojfD"], "caption": "So what does sitting do to your body? Prolonged sitting causes changes in our metabolism, so sitting disease is part of what is known as metabolic syndrome. An enzyme, lipoprotein lipase, which resides in blood vessels, is essentially turned off with inactivity. This enzyme is responsible for metabolizing fats and sugars in the bloodstream. Physical movement stimulates enzyme activity, which in turn improves cholesterol and helps regulate blood sugar. Lack of movement lowers enzyme activity, which in turn contributes to weight gain, diabetes and reduction of the good cholesterol, HDL. In the first study cited above, the authors stated that \u201cexperimentally reducing normal spontaneous standing and ambulatory time had a much greater effect on LPL regulation than adding vigorous exercise training on top of the normal level of nonexercise activity. Those studies also found that inactivity initiated unique cellular processes that were qualitatively different from the exercise responses.\u201d In other words, standing, walking and fidgeting every hour (spontaneous movement) throughout the day seems to have a greater effect on lipoprotein lipase enzymes than exercising an hour per day but sitting the rest of the day.", "local_uri": ["735f60c4ea0eaf1646b1be2153d1ccd12713d2b0_Image_069.png"], "annotated": false, "compound": false}
{"title": "WELCOMEFrom the inside", "pdf_hash": "735f60c4ea0eaf1646b1be2153d1ccd12713d2b0", "year": 2015, "venue": "Interactions", "alt_text": "Description: https://encrypted-tbn0.google.com/images?q=tbn:ANd9GcTiqxN9sKqzb8bOcUWdXhtnFTi_UKtdvgefeGlw6z2HqdyD6U-m", "levels": null, "corpus_id": 25208963, "sentences": ["Description: https://encrypted-tbn0.google.com/images?q=tbn:ANd9GcTiqxN9sKqzb8bOcUWdXhtnFTi_UKtdvgefeGlw6z2HqdyD6U-m"], "caption": "", "local_uri": ["735f60c4ea0eaf1646b1be2153d1ccd12713d2b0_Image_097.jpg"], "annotated": false, "compound": false}
{"title": "Chasing Play on TikTok from Populations with Disabilities to Inspire Playful and Inclusive Technology Design", "pdf_hash": "259f5b5d7f7e4eee935318ed2e6cee59588aecb5", "year": 2021, "venue": "CHI", "alt_text": "5 TikTok Screenshots: 1) A blind person showing how she can pour drinks, 2) a drummer with one hand, 3) a person doing 1-handed push-ups while controlling a maze overlay, 4) a person with partial paralysis singing \"Head, shoulder, knees, and toes\", and 5) an electric wheelchair user confidently navigating her house backwards.", "levels": null, "corpus_id": 233987833, "sentences": ["5 TikTok Screenshots: 1) A blind person showing how she can pour drinks, 2) a drummer with one hand, 3) a person doing 1-handed push-ups while controlling a maze overlay, 4) a person with partial paralysis singing \"Head, shoulder, knees, and toes\", and 5) an electric wheelchair user confidently navigating her house backwards."], "caption": "", "local_uri": ["259f5b5d7f7e4eee935318ed2e6cee59588aecb5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Chasing Play on TikTok from Populations with Disabilities to Inspire Playful and Inclusive Technology Design", "pdf_hash": "259f5b5d7f7e4eee935318ed2e6cee59588aecb5", "year": 2021, "venue": "CHI", "alt_text": "There were 29 videos that were rated as totally introspective, 23 were given a rating of 2, 45 had elements of both, 45 were given a rating of 4, and 105 were totally performative. There were 86 play potentials that could inspire Assistive Technology/Devices, 47 that could inspire IoT designs, 69 that could inspire Social Augmentation/Wearables, 31 that could inspire mobile applications, and 142 that could inspire Social Media Design.", "levels": null, "corpus_id": 233987833, "sentences": ["There were 29 videos that were rated as totally introspective, 23 were given a rating of 2, 45 had elements of both, 45 were given a rating of 4, and 105 were totally performative.", "There were 86 play potentials that could inspire Assistive Technology/Devices, 47 that could inspire IoT designs, 69 that could inspire Social Augmentation/Wearables, 31 that could inspire mobile applications, and 142 that could inspire Social Media Design."], "caption": "", "local_uri": ["259f5b5d7f7e4eee935318ed2e6cee59588aecb5_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "deformTable: Crafting a Shape-changing Device for Creative Appropriations Over Time", "pdf_hash": "d78f0cbaf3083aedccbf905e707e83c271199c53", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "deformTable is a shape-changing artifact that can go up and down according to the weight of applied objects on the wooden surface.", "levels": null, "corpus_id": 235663024, "sentences": ["deformTable is a shape-changing artifact that can go up and down according to the weight of applied objects on the wooden surface."], "caption": "In this pictorial, we present critical reflections on the design process of crafting the deformTable. The table is a shape-changing device that can go up and down based on the weight of objects placed on it. This pictorial focuses on key design decisions in the making of a shape-changing artifact like the deformTable that explores how deformability, temporality, and materiality can encourage creative appropriations over time. We believe this pictorial contributes to how to design research products that can investigate shape-changing artifacts in everyday settings over time.", "local_uri": ["d78f0cbaf3083aedccbf905e707e83c271199c53_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "1 Finding Mnemo : Hybrid Intelligence Memory in a Crowd-Powered Dialog System", "pdf_hash": "780b79016da0aef4d8b6864c800b65102b15ffa3", "year": 2018, "venue": "", "alt_text": "Mnemo's interface consists of four parts: (1) DialogView, which displays raw dialog lines; (2) NoteView: displays already-saved notes; (3) NoteSummary: allows workers to summarize notes; (4) TimeSelect: allows workers to estimate a note's longevity.", "levels": null, "corpus_id": 13887284, "sentences": ["Mnemo's interface consists of four parts: (1) DialogView, which displays raw dialog lines; (2) NoteView: displays already-saved notes; (3) NoteSummary: allows workers to summarize notes; (4) TimeSelect: allows workers to estimate a note's longevity."], "caption": "", "local_uri": ["780b79016da0aef4d8b6864c800b65102b15ffa3_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "What Can CHI Do About Dark Patterns?", "pdf_hash": "b7c48cc57e08ee8ace433c32c5d0816370bb40e5", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Screenshot of the checkout cart for an e-commerce website for buying flowers. In addition to flowers, a greeting card appears in the cart.", "levels": null, "corpus_id": 233987223, "sentences": ["Screenshot of the checkout cart for an e-commerce website for buying flowers.", "In addition to flowers, a greeting card appears in the cart."], "caption": "Figure 1: Sneak into Basket on avasfowers.net, one of 1,818 instances of dark patterns found in [12]. Despite not requesting a greeting card, one is automatically added at checkout for $3.99.", "local_uri": ["b7c48cc57e08ee8ace433c32c5d0816370bb40e5_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "What Can CHI Do About Dark Patterns?", "pdf_hash": "b7c48cc57e08ee8ace433c32c5d0816370bb40e5", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "This figure shows two sketches. The first is a room with light entering on two sides. The second is a building with many rooms, each of which has light entering on at least two sides.", "levels": null, "corpus_id": 233987223, "sentences": ["This figure shows two sketches.", "The first is a room with light entering on two sides.", "The second is a building with many rooms, each of which has light entering on at least two sides."], "caption": "", "local_uri": ["b7c48cc57e08ee8ace433c32c5d0816370bb40e5_Image_006.png"], "annotated": false, "compound": false}
{"title": "Decoraction: a Catalogue for Interactive Home Decor of the Nearest-Future", "pdf_hash": "64efcb8d676f2c4a156087f704b4c334b8c17616", "year": 2021, "venue": "TEI", "alt_text": "This is figure 6 that shows the Decoraction catalogue (page 5) showing MORVAZ. Image is the copyright of Sara Nabil.", "levels": null, "corpus_id": 231919069, "sentences": ["This is figure 6 that shows the Decoraction catalogue (page 5) showing MORVAZ.", "Image is the copyright of Sara Nabil."], "caption": "Fig. 6 The Decoraction catalogue (page 5) showing MORVAZ \u00a9 Sara Nabil", "local_uri": ["64efcb8d676f2c4a156087f704b4c334b8c17616_Image_020.jpg", "64efcb8d676f2c4a156087f704b4c334b8c17616_Image_021.jpg", "64efcb8d676f2c4a156087f704b4c334b8c17616_Image_022.jpg", "64efcb8d676f2c4a156087f704b4c334b8c17616_Image_025.jpg", "64efcb8d676f2c4a156087f704b4c334b8c17616_Image_026.jpg", "64efcb8d676f2c4a156087f704b4c334b8c17616_Image_027.jpg", "64efcb8d676f2c4a156087f704b4c334b8c17616_Image_028.jpg"], "annotated": false, "compound": true}
{"title": "Decoraction: a Catalogue for Interactive Home Decor of the Nearest-Future", "pdf_hash": "64efcb8d676f2c4a156087f704b4c334b8c17616", "year": 2021, "venue": "TEI", "alt_text": "This is Figure 8 that shows the TacTILE (interactive tiles).\nThe illustration on the left shows designing and crafting TacTiles: from the digital design and pattern design, to laser-cutting, weaving Nichrome wire, and programming.", "levels": null, "corpus_id": 231919069, "sentences": ["This is Figure 8 that shows the TacTILE (interactive tiles).", "The illustration on the left shows designing and crafting TacTiles: from the digital design and pattern design, to laser-cutting, weaving Nichrome wire, and programming."], "caption": "", "local_uri": ["64efcb8d676f2c4a156087f704b4c334b8c17616_Image_032.jpg"], "annotated": false, "compound": false}
{"title": "Decoraction: a Catalogue for Interactive Home Decor of the Nearest-Future", "pdf_hash": "64efcb8d676f2c4a156087f704b4c334b8c17616", "year": 2021, "venue": "TEI", "alt_text": "This is Figure 11 that shows WATERFALL and WATERDROP (the interactive colour-changing wall-painting and cushion). The illustration shows the design and circuit; the crafting of the WaterFall painting; and the WaterDrop Cushion in both states off and on.", "levels": null, "corpus_id": 231919069, "sentences": ["This is Figure 11 that shows WATERFALL and WATERDROP (the interactive colour-changing wall-painting and cushion).", "The illustration shows the design and circuit; the crafting of the WaterFall painting; and the WaterDrop Cushion in both states off and on."], "caption": "1) Illustration of", "local_uri": ["64efcb8d676f2c4a156087f704b4c334b8c17616_Image_035.jpg"], "annotated": false, "compound": false}
{"title": "Decoraction: a Catalogue for Interactive Home Decor of the Nearest-Future", "pdf_hash": "64efcb8d676f2c4a156087f704b4c334b8c17616", "year": 2021, "venue": "TEI", "alt_text": "This is Fig. 12 that shows the Making process of LITHER (the interactive leather rug). The process explores how can we design a shape-changing rug? If motors are not a practical solution for a floor rug, can shape-memory alloy (SMA) be sewn to deform it? The figure shows how this is a process of iterative experimentation, extensive trials, documentation, reflectionn and re-design. In such way, the making process is design research that yields scientific knowledge.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 231919069, "sentences": ["This is Fig.", "12 that shows the Making process of LITHER (the interactive leather rug).", "The process explores how can we design a shape-changing rug?", "If motors are not a practical solution for a floor rug, can shape-memory alloy (SMA) be sewn to deform it?", "The figure shows how this is a process of iterative experimentation, extensive trials, documentation, reflectionn and re-design.", "In such way, the making process is design research that yields scientific knowledge."], "caption": "Motors are not practical. Can Shape-Memory Alloy (SMA) be sewn to deform it?", "local_uri": ["64efcb8d676f2c4a156087f704b4c334b8c17616_Image_049.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "HapticSeer: A Multi-channel, Black-box, Platform-agnostic Approach to Detecting Video Game Events for Real-time Haptic Feedback", "pdf_hash": "341ee80cdaeb3403beae340824f7127cb9d77622", "year": 2021, "venue": "CHI", "alt_text": "The first line graph showing that the discrete output from a Speed Detector almost fit the ground truth.  The second line graph showing that the longitudinal acceleration output from an Inertia Detector has a similar trend with ground truth, but with a smaller magnitude.  The third line graph showing that the lateral acceleration output from an Inertia Detector has a similar trend with the ground truth, but cramped to -20 to 20.", "levels": [[3], [3], [3, 2]], "corpus_id": 233987644, "sentences": ["The first line graph showing that the discrete output from a Speed Detector almost fit the ground truth.", "The second line graph showing that the longitudinal acceleration output from an Inertia Detector has a similar trend with ground truth, but with a smaller magnitude.", "The third line graph showing that the lateral acceleration output from an Inertia Detector has a similar trend with the ground truth, but cramped to -20 to 20."], "caption": "We concluded that", "local_uri": ["341ee80cdaeb3403beae340824f7127cb9d77622_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback", "pdf_hash": "9dee532c99e44cd49473110a6735e6cfd25723b7", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"WalkingVibe prototype with 2 vibration motors behind the ears, which provide  vibrotactile stimulation synchronized to foot steps in VR. Our 240-person study showed that it significantly reduced discomfort and VR sickness, and significantly improved the realism of the virtual walking experience\"", "levels": null, "corpus_id": 218482816, "sentences": ["Figure 1: \"WalkingVibe prototype with 2 vibration motors behind the ears, which provide  vibrotactile stimulation synchronized to foot steps in VR.", "Our 240-person study showed that it significantly reduced discomfort and VR sickness, and significantly improved the realism of the virtual walking experience\""], "caption": "Figure 1. WalkingVibe prototype with 2 vibration motors behind the ears, which provide vibrotactile stimulation synchronized to footsteps in VR. Our 240-person study showed that it signi\ufb01cantly reduced dis- comfort and VR sickness, and signi\ufb01cantly improved the realism of the virtual walking experience in VR.", "local_uri": ["9dee532c99e44cd49473110a6735e6cfd25723b7_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback", "pdf_hash": "9dee532c99e44cd49473110a6735e6cfd25723b7", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"The setup for each category of tactile feedback: (a) 2-sided vibration, (b) backside vibration, (c) 2-sided tapping feedback replicated from PhantomLegs [42] project.\"", "levels": null, "corpus_id": 218482816, "sentences": ["Figure 3: \"The setup for each category of tactile feedback: (a) 2-sided vibration, (b) backside vibration, (c) 2-sided tapping feedback replicated from PhantomLegs [42] project.\""], "caption": "Figure 3. The setup for each category of tactile feedback: (a) 2-sided vibration, (b) backside vibration, (c) 2-sided tapping feedback replicated from PhantomLegs [37] project.", "local_uri": ["9dee532c99e44cd49473110a6735e6cfd25723b7_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback", "pdf_hash": "9dee532c99e44cd49473110a6735e6cfd25723b7", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: \"Screenshots of three scenes and paths of the virtual walking environment: (a)(d) the city; (b)(e) the forest; (c)(f) the Sci-fi passage\"", "levels": null, "corpus_id": 218482816, "sentences": ["Figure 4: \"Screenshots of three scenes and paths of the virtual walking environment: (a)(d) the city; (b)(e) the forest; (c)(f) the Sci-fi passage\""], "caption": "Figure 4. Screenshots of three scenes and paths of the virtual walking environment: (a)(d) the city, (b)(e) the forest, and (c)(f) the sci-\ufb01 Passage.", "local_uri": ["9dee532c99e44cd49473110a6735e6cfd25723b7_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for The Web as History Using Web Archives to Understand the Past and the Present", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for The Web as History Using Web Archives to Understand the Past and the Present"], "caption": "The World Wide Web has now been in use for more than 20 years. From early browsers to today\u2019s principal source of information, entertainment and much else, the Web is an integral part of our daily lives, to the extent that some people believe \u2018if it\u2019s not online, it", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for The Euro Crisis and European Iden-tities Political and Media Discourse in Germany, Ireland and Poland", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for The Euro Crisis and European Iden-tities Political and Media Discourse in Germany, Ireland and Poland"], "caption": "This book builds upon our knowledge of the far-reaching economic, political and social effects of the Euro crisis on the European Union by providing a unique study of European identities. In partic- ular, it considers the impact on the construction of European", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Compact Cinematics The Moving Image in the Age of Bit-Sized Me-dia", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Compact Cinematics The Moving Image in the Age of Bit-Sized Me-dia"], "caption": "Compact Cinematics chal- lenges the dominant under- standing of cinema to focus on the various compact, short, miniature, pocket-sized forms of cinematics that have existed from even before its standardi- zation in theatrical form, and in recent years have multiplied", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_021.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for From Superman to Social Realism Children's Media and Scandinavian Childhood", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for From Superman to Social Realism Children's Media and Scandinavian Childhood"], "caption": "Can children\u2019s media be a source of education and em- powerment? Or is the com- mercial media market a threat to their sense of social and democratic values? Such ques- tions about the appropriate- ness of children\u2019s media con- sumption have recurred in", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_025.png"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Music Video After MTV Audiovisual Studies, New Media, and Popular Music", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Music Video After MTV Audiovisual Studies, New Media, and Popular Music"], "caption": "Since the 1980s, music videos have been everywhere, and to- day almost all of the most- viewed clips on YouTube are music videos. However, in ac- ademia, music videos do not currently share this popularity. Music Video After MTV gives music video its due aca-", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_029.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Style, Mediation, and Change Sociolinguistic Perspectives on Talking Media", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Style, Mediation, and Change Sociolinguistic Perspectives on Talking Media"], "caption": "When talk circulates through technological media - through television or radio and through the activities they support, like the dissemina- tion of news, product adver- tising or entertainment - it takes on distinctive character- istics, functions and styles.", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_032.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Danmarks f\u00f8rste lydoptagelser Edisons fonograf i 1890'ernes K\u00f8benhavn", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Danmarks f\u00f8rste lydoptagelser Edisons fonograf i 1890'ernes K\u00f8benhavn"], "caption": "I efter\u00e5ret 1889 vandt Thomas Edisons fonograf genklang i K\u00f8benhavn. Nu blev det for alvor muligt at optage og gen- give lyd, og et betaget publi- kum str\u00f8mmede til for at h\u00f8re vidundermaskinen. Frem til midten af 1890\u2019erne optog", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_033.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Paranoia p\u00e5 Film Om overv\u00e5gning, panik og konspirationsteori i thriller- og science fiction-genren", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Paranoia p\u00e5 Film Om overv\u00e5gning, panik og konspirationsteori i thriller- og science fiction-genren"], "caption": "Filmbog med hovedfokus p\u00e5 temaet overv\u00e5gning i thriller og science fiction-genren.", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_034.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for In the Aftermath of Gezi From Social Movement to Social Change?", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for In the Aftermath of Gezi From Social Movement to Social Change?"], "caption": "This edited volume addresses various aspects of social and political development in Tur- key and the latter\u2019s role within a global context. Paradigmati- cally and theoretically, it is situated in the realm of com- munication and/for social change. The chapters thread", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_036.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Rethinking Journalism Again Societal role and public relevance in a digital age", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Rethinking Journalism Again Societal role and public relevance in a digital age"], "caption": "It\u2019s easy to make a rhetorical case for the value of journal- ism. Because, it is a necessary precondition for democracy; it speaks to the people and for the people; it informs citizens and enables them to make ra- tional decisions; it functions as their watchdog on govern-", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_037.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Corporate Social Responsibility Strategy, Communication, Governance", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Corporate Social Responsibility Strategy, Communication, Governance"], "caption": "This upper-level textbook of- fers an up-to-date introduction to issues in corporate social responsibility (CSR) from a global perspective. Written by an international team of ex- perts, it guides students through key themes in CSR", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_038.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Culture War Affective Cultural Politics, Tepid Nationalism and Art Activism", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Culture War Affective Cultural Politics, Tepid Nationalism and Art Activism"], "caption": "The culture wars, intertwining art, culture, and politics, have sparked prominent political debates across the globe for many years, but particularly in Europe and America since 2001. Focusing specifically on the experience of Denmark", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_039.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Nordisk Films Kompagni 1906-1924 The Rise and Fall of the Polar Bear", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Nordisk Films Kompagni 1906-1924 The Rise and Fall of the Polar Bear"], "caption": "Nordisk Films Kompagni 1906-1924: The Rise and Fall of the Polar Bear is the first comprehensive study of the Danish film company, Nordisk Films Kompagni, in the silent era. Based on ar- chival research, primarily in the company\u2019s surviving busi-", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_042.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Revolution of Innovation Manage-ment The Digital Breakthrough", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Revolution of Innovation Manage-ment The Digital Breakthrough"], "caption": "This edited collection explores how digitalization is changing the management of innova- tion, and the subsequent im- plications for the next phases in its development. The au- thors identify and examine rel- evant phenomena which are related to the ongoing digital", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_043.jpg"], "annotated": false, "compound": false}
{"title": "Denmark Current literature Mediated intimacies Connectivities , relationalities and proximities", "pdf_hash": "76fb457009b907167b7afd6d218958ac04bb7e3a", "year": 2018, "venue": "", "alt_text": "Billedresultat for Advancing Multimodal and Critical Discourse Studies Interdisciplinary Research Inspired by Theo Van Leeuwen Social Semiotics", "levels": null, "corpus_id": 198959347, "sentences": ["Billedresultat for Advancing Multimodal and Critical Discourse Studies Interdisciplinary Research Inspired by Theo Van Leeuwen Social Semiotics"], "caption": "This volume highlights the dy- namic and groundbreaking contributions of Theo van Leeuwen to the fields of mul- timodality, social semiotics, and critical discourse analysis, and demonstrates how the key themes in his work have influ- enced and will continue to", "local_uri": ["76fb457009b907167b7afd6d218958ac04bb7e3a_Image_047.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Reality Games for People Using Wheelchairs", "pdf_hash": "a06653f43592cdfecf73b6961a666fc98e4ad2c8", "year": 2020, "venue": "CHI", "alt_text": "The figure shows a screenshots of the 3D downhill skiing environment for Karamaisu Slope and an embedded image showing the interaction with the game while using a wheelchair.", "levels": null, "corpus_id": 218483080, "sentences": ["The figure shows a screenshots of the 3D downhill skiing environment for Karamaisu Slope and an embedded image showing the interaction with the game while using a wheelchair."], "caption": "", "local_uri": ["a06653f43592cdfecf73b6961a666fc98e4ad2c8_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Reality Games for People Using Wheelchairs", "pdf_hash": "a06653f43592cdfecf73b6961a666fc98e4ad2c8", "year": 2020, "venue": "CHI", "alt_text": "The screenshot shows a dark dungeon with an enemy attacking for Dungeon Spell, an embedded image also shows the interaction carried out by the player at that point in the game.", "levels": null, "corpus_id": 218483080, "sentences": ["The screenshot shows a dark dungeon with an enemy attacking for Dungeon Spell, an embedded image also shows the interaction carried out by the player at that point in the game."], "caption": "Figure 1. Screenshot of Karamaisu Slope.                                                                 Figure 2. Screenshot of Dungeon Spell.\u200c", "local_uri": ["a06653f43592cdfecf73b6961a666fc98e4ad2c8_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Reality Games for People Using Wheelchairs", "pdf_hash": "a06653f43592cdfecf73b6961a666fc98e4ad2c8", "year": 2020, "venue": "CHI", "alt_text": "The screenshot shows a space battle through the front of a spaceship for Space Travel, an embedded image also shows the interaction carried out by the player at that point in the game.", "levels": null, "corpus_id": 218483080, "sentences": ["The screenshot shows a space battle through the front of a spaceship for Space Travel, an embedded image also shows the interaction carried out by the player at that point in the game."], "caption": "Figure 3. Screenshot of Space Travel.", "local_uri": ["a06653f43592cdfecf73b6961a666fc98e4ad2c8_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Virtual Reality Games for People Using Wheelchairs", "pdf_hash": "a06653f43592cdfecf73b6961a666fc98e4ad2c8", "year": 2020, "venue": "CHI", "alt_text": "The sketch shows a person who uses a manual wheelchair wearing VR hardware (headset) and graphical elements around the corners of the sketch to indicate that this was a player who engaged with the space game.", "levels": [[-1]], "corpus_id": 218483080, "sentences": ["The sketch shows a person who uses a manual wheelchair wearing VR hardware (headset) and graphical elements around the corners of the sketch to indicate that this was a player who engaged with the space game."], "caption": "Figure 4. Sketch of a player engaging with Space Travel.", "local_uri": ["a06653f43592cdfecf73b6961a666fc98e4ad2c8_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"You're Giving Me Mixed Signals!\": A Comparative Analysis of Methods that Capture Players' Emotional Response to Games", "pdf_hash": "c09974ac85492833d183afdc330dcb457f3792d0", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A picture containing photo, wall, indoor, bathroom  Description generated with very high confidence", "levels": null, "corpus_id": 5046949, "sentences": ["A picture containing photo, wall, indoor, bathroom  Description generated with very high confidence"], "caption": "", "local_uri": ["c09974ac85492833d183afdc330dcb457f3792d0_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "\"You're Giving Me Mixed Signals!\": A Comparative Analysis of Methods that Capture Players' Emotional Response to Games", "pdf_hash": "c09974ac85492833d183afdc330dcb457f3792d0", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A picture containing photo, different, indoor, wall  Description generated with high confidence", "levels": null, "corpus_id": 5046949, "sentences": ["A picture containing photo, different, indoor, wall  Description generated with high confidence"], "caption": "", "local_uri": ["c09974ac85492833d183afdc330dcb457f3792d0_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "transTexture Lamp: Understanding Lived Experiences with Deformation Through a Materiality Lens", "pdf_hash": "33abded76c0254908aa3e47b15431ed069691547", "year": 2020, "venue": "CHI", "alt_text": "Clockwise from top-left: a power switch installed at the base of transTexture; a rotary button which can control digital light and deformation simultaneously; the fabric lampshade in a deformable state; and a static state.", "levels": null, "corpus_id": 218483073, "sentences": ["Clockwise from top-left: a power switch installed at the base of transTexture; a rotary button which can control digital light and deformation simultaneously; the fabric lampshade in a deformable state; and a static state."], "caption": "Figure 2. Clockwise from top-left: a power switch installed at the base of transTexture; a rotary button which can control digital light and deformation simultaneously; the fabric lampshade in a deformable state; and a static state.", "local_uri": ["33abded76c0254908aa3e47b15431ed069691547_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "transTexture Lamp: Understanding Lived Experiences with Deformation Through a Materiality Lens", "pdf_hash": "33abded76c0254908aa3e47b15431ed069691547", "year": 2020, "venue": "CHI", "alt_text": "From left to right. Eva-H1 worked with her drawn transTexture; Wilma-H2 placed transTexture in her kitchen; Rita-H3 pressed the texture patterns daily in her living room.", "levels": null, "corpus_id": 218483073, "sentences": ["From left to right.", "Eva-H1 worked with her drawn transTexture; Wilma-H2 placed transTexture in her kitchen; Rita-H3 pressed the texture patterns daily in her living room."], "caption": "Figure 3. From left to right. Eva-H1 worked with her drawn transTexture; Wilma-H2 placed transTexture in her kitchen; Rita-H3 pressed the texture patterns daily in her living room.", "local_uri": ["33abded76c0254908aa3e47b15431ed069691547_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "transTexture Lamp: Understanding Lived Experiences with Deformation Through a Materiality Lens", "pdf_hash": "33abded76c0254908aa3e47b15431ed069691547", "year": 2020, "venue": "CHI", "alt_text": "The comparison between the original transTexture and the sketched transTexture.", "levels": null, "corpus_id": 218483073, "sentences": ["The comparison between the original transTexture and the sketched transTexture."], "caption": "Figure 3. The comparison between the original transTexture and the sketched transTexture.", "local_uri": ["33abded76c0254908aa3e47b15431ed069691547_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families: Camera Work for Three-party Mobile Video Calls", "pdf_hash": "fbd31b58de8e6de8647b65e34a982d99f03e930b", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: Transcript of a video extract. Child holds and shakes the phone. Then the child drops the phone to bed. Mum complains about phone shaking.   01 ((child holds and shakes the phone.))  02 ((child drops the phone to bed))  03 ((grandfather grabs the phone back))  04 Mum: is she holding the phone or are your holding?   05 Grandfather: she is holding the phone   06 Mum: the phone is always shaking", "levels": null, "corpus_id": 218482386, "sentences": ["Figure 2: Transcript of a video extract.", "Child holds and shakes the phone.", "Then the child drops the phone to bed.", "Mum complains about phone shaking.", "01 ((child holds and shakes the phone.))  02 ((child drops the phone to bed))  03 ((grandfather grabs the phone back))  04 Mum: is she holding the phone or are your holding?   05 Grandfather: she is holding the phone   06 Mum: the phone is always shaking"], "caption": "Figure 2: A child shaking and dropping the phone", "local_uri": ["fbd31b58de8e6de8647b65e34a982d99f03e930b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families: Camera Work for Three-party Mobile Video Calls", "pdf_hash": "fbd31b58de8e6de8647b65e34a982d99f03e930b", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: Different ways of grandparents holding the phone for parent-child video calls. There are six small images showing how grandparents position the phone in front of the child's head at the beginning of video calls.", "levels": null, "corpus_id": 218482386, "sentences": ["Figure 3: Different ways of grandparents holding the phone for parent-child video calls.", "There are six small images showing how grandparents position the phone in front of the child's head at the beginning of video calls."], "caption": "Figure 3: Grandparents holding the phone for facilitated three-party parent-child video calls", "local_uri": ["fbd31b58de8e6de8647b65e34a982d99f03e930b_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families: Camera Work for Three-party Mobile Video Calls", "pdf_hash": "fbd31b58de8e6de8647b65e34a982d99f03e930b", "year": 2020, "venue": "CHI", "alt_text": "Figure 4: Transcript of a vide extract. A child is attempting to grab the phone.   01 ((Child moves hand and tries to grab the phone.))   02 ((Child walks toward the phone))  03 ((Grandma moves the phone away))", "levels": null, "corpus_id": 218482386, "sentences": ["Figure 4: Transcript of a vide extract.", "A child is attempting to grab the phone.", "01 ((Child moves hand and tries to grab the phone.))   02 ((Child walks toward the phone))  03 ((Grandma moves the phone away))"], "caption": "Figure 4: Child attempting to grab the phone", "local_uri": ["fbd31b58de8e6de8647b65e34a982d99f03e930b_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families: Camera Work for Three-party Mobile Video Calls", "pdf_hash": "fbd31b58de8e6de8647b65e34a982d99f03e930b", "year": 2020, "venue": "CHI", "alt_text": "Figure 8: Transcript of a video extract. The boy is eating ice cream. He wants to show am empty ice cream stick to mum. Although he moves the stick toward screen, the stick is not visible on screen for remote mum.   01 Mum: big brother, have you eaten up yours?   02 (0.5 seconds) ((boy is eating an ice cream))  03 Boy: hey hey he ha   04 Boy: ((moves empty ice cream stick toward screen, shows it to screen, but it is not visible at all for mum))", "levels": null, "corpus_id": 218482386, "sentences": ["Figure 8: Transcript of a video extract.", "The boy is eating ice cream.", "He wants to show am empty ice cream stick to mum. Although he moves the stick toward screen, the stick is not visible on screen for remote mum.", "01 Mum: big brother, have you eaten up yours?   02 (0.5 seconds) ((boy is eating an ice cream))  03 Boy: hey hey he ha   04 Boy: ((moves empty ice cream stick toward screen, shows it to screen, but it is not visible at all for mum))"], "caption": "Figure 8: Boy trying to show the empty ice cream stick to remote mum", "local_uri": ["fbd31b58de8e6de8647b65e34a982d99f03e930b_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families: Camera Work for Three-party Mobile Video Calls", "pdf_hash": "fbd31b58de8e6de8647b65e34a982d99f03e930b", "year": 2020, "venue": "CHI", "alt_text": "Figure 9: Examples of grandparent self-checking their phone screen. Grandparent moves phone back to themselves to see what is shown on the screen.", "levels": null, "corpus_id": 218482386, "sentences": ["Figure 9: Examples of grandparent self-checking their phone screen.", "Grandparent moves phone back to themselves to see what is shown on the screen."], "caption": "Figure 9: Examples of grandparent self-checking the phone screen", "local_uri": ["fbd31b58de8e6de8647b65e34a982d99f03e930b_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families: Camera Work for Three-party Mobile Video Calls", "pdf_hash": "fbd31b58de8e6de8647b65e34a982d99f03e930b", "year": 2020, "venue": "CHI", "alt_text": "Figure 11: Transcript of a video extract. Grandfather holds the phone. the child is dancing. But the camera does not show the child. Grandmother tells grandfather that he didn't show the child. Then grandfather correct the camera position.   01 ((Grandfather holds the phone. The girl is dancing for dad, but she is not visible on phone screen))  02  Grandmother talks to grandfather, you didn't show her   03  ((grandfather changes camera position, and the right phone screen now shows the girl dancing))", "levels": null, "corpus_id": 218482386, "sentences": ["Figure 11: Transcript of a video extract.", "Grandfather holds the phone.", "the child is dancing. But the camera does not show the child.", "Grandmother tells grandfather that he didn't show the child.", "Then grandfather correct the camera position.", "01 ((Grandfather holds the phone.", "The girl is dancing for dad, but she is not visible on phone screen))  02  Grandmother talks to grandfather, you didn't show her   03  ((grandfather changes camera position, and the right phone screen now shows the girl dancing))"], "caption": "Figure 11: Co-present grandmother complaining about camera work", "local_uri": ["fbd31b58de8e6de8647b65e34a982d99f03e930b_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Moregrasp: restoration of upper limb function in Individuals with High spinal cord injury by Multimodal Neuroprostheses for Interaction in Daily Activities", "pdf_hash": "23327b017df9b25bd558f0fea5959f7354c0f658", "year": 2017, "venue": "GBCIC", "alt_text": "https://lh3.googleusercontent.com/nJVGEELmKKVm1c-7Oh6aKESBRo-6IF0H2SKa0At883knQWc4dmyIYNIZkhIkrShNLiKwMyGNS2WxcaweVj_tVRSnWPOo1WAjzL3UujHTFllkxcXANwXy199Ni166J-RXm308CqBI", "levels": null, "corpus_id": 35763797, "sentences": ["https://lh3.googleusercontent.com/nJVGEELmKKVm1c-7Oh6aKESBRo-6IF0H2SKa0At883knQWc4dmyIYNIZkhIkrShNLiKwMyGNS2WxcaweVj_tVRSnWPOo1WAjzL3UujHTFllkxcXANwXy199Ni166J-RXm308CqBI"], "caption": "Figure 3: Classification accuracies of 5 EUs with SCI for grasps and pronation/supination (icon paradigm). The dashed line is the significance level.", "local_uri": ["23327b017df9b25bd558f0fea5959f7354c0f658_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Moregrasp: restoration of upper limb function in Individuals with High spinal cord injury by Multimodal Neuroprostheses for Interaction in Daily Activities", "pdf_hash": "23327b017df9b25bd558f0fea5959f7354c0f658", "year": 2017, "venue": "GBCIC", "alt_text": "https://lh3.googleusercontent.com/Ha3p7O2KSXyz-2eGhAqrjK0zP1UUAQGvZ1POi5HLWlLWw7v5RODIY-Hb3bs-kARL_mmW1G8ZTRAcP3N5IAV7OVIwVL9vMnAUjXfdGBcP7ZZH3bF503e7GvX9PBQm_62wLoIyUGW2", "levels": null, "corpus_id": 35763797, "sentences": ["https://lh3.googleusercontent.com/Ha3p7O2KSXyz-2eGhAqrjK0zP1UUAQGvZ1POi5HLWlLWw7v5RODIY-Hb3bs-kARL_mmW1G8ZTRAcP3N5IAV7OVIwVL9vMnAUjXfdGBcP7ZZH3bF503e7GvX9PBQm_62wLoIyUGW2"], "caption": "Figure 4: Classification accuracies of 3 EUs with SCI for grasps and hand open (object paradigm). The dashed line represents the significance level.", "local_uri": ["23327b017df9b25bd558f0fea5959f7354c0f658_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Moregrasp: restoration of upper limb function in Individuals with High spinal cord injury by Multimodal Neuroprostheses for Interaction in Daily Activities", "pdf_hash": "23327b017df9b25bd558f0fea5959f7354c0f658", "year": 2017, "venue": "GBCIC", "alt_text": "https://lh6.googleusercontent.com/n4XU1U2S48c3HRaIrjXtZzcSfN5520Q9SUddpUUnU2F7D1Ss6dgaPuxLX9F11pZdWHxYIQP0tJfEDNWrbhcSqM0F2oCu6Tu0gJCG_yoM5wfT8K_eM7QV7BhhWQfx4rcoifVyOh42", "levels": null, "corpus_id": 35763797, "sentences": ["https://lh6.googleusercontent.com/n4XU1U2S48c3HRaIrjXtZzcSfN5520Q9SUddpUUnU2F7D1Ss6dgaPuxLX9F11pZdWHxYIQP0tJfEDNWrbhcSqM0F2oCu6Tu0gJCG_yoM5wfT8K_eM7QV7BhhWQfx4rcoifVyOh42"], "caption": "", "local_uri": ["23327b017df9b25bd558f0fea5959f7354c0f658_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Moregrasp: restoration of upper limb function in Individuals with High spinal cord injury by Multimodal Neuroprostheses for Interaction in Daily Activities", "pdf_hash": "23327b017df9b25bd558f0fea5959f7354c0f658", "year": 2017, "venue": "GBCIC", "alt_text": "https://lh6.googleusercontent.com/MjG-tPKJjPMbOtH-DIAwOMJr0-4msSb9bfpyh11t6P1lra2gidw51PTik-YYq9PcMjUdRgXX43G5_7liEXwd3dMOX_zonjoXS3HOEK5sxbbqNfBgCDTlEq3vgolsRhvfdw9ZhjJF", "levels": null, "corpus_id": 35763797, "sentences": ["https://lh6.googleusercontent.com/MjG-tPKJjPMbOtH-DIAwOMJr0-4msSb9bfpyh11t6P1lra2gidw51PTik-YYq9PcMjUdRgXX43G5_7liEXwd3dMOX_zonjoXS3HOEK5sxbbqNfBgCDTlEq3vgolsRhvfdw9ZhjJF"], "caption": "", "local_uri": ["23327b017df9b25bd558f0fea5959f7354c0f658_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Moregrasp: restoration of upper limb function in Individuals with High spinal cord injury by Multimodal Neuroprostheses for Interaction in Daily Activities", "pdf_hash": "23327b017df9b25bd558f0fea5959f7354c0f658", "year": 2017, "venue": "GBCIC", "alt_text": "https://lh3.googleusercontent.com/fLGg6nDyBEjFpQSRTZlqPZ-ARSnNMlLGAz3xGKjuGW8kIOPPAuYfA2vOejMhQtVEsgHtgRQSXuYFFd9qbx4rEB91IoRtdFI_LWTGpteCyFq_2a6EIh8gjeDO4HuesfmPN6GU4cjX", "levels": null, "corpus_id": 35763797, "sentences": ["https://lh3.googleusercontent.com/fLGg6nDyBEjFpQSRTZlqPZ-ARSnNMlLGAz3xGKjuGW8kIOPPAuYfA2vOejMhQtVEsgHtgRQSXuYFFd9qbx4rEB91IoRtdFI_LWTGpteCyFq_2a6EIh8gjeDO4HuesfmPN6GU4cjX"], "caption": "Figure 7: First prototype of a multi-electrode forearm electrode sleeve. (a) individual gypsum model of the forearm with electrode cavities, (b) prototype of personalized arm sleeve made from non-conductive medical silicon with integrated conductive silicon electrodes and cables, and Inertial Measurement Units (IMUs) for measurement of the wrist and elbow position and calculation of the wrist rotation angle.", "local_uri": ["23327b017df9b25bd558f0fea5959f7354c0f658_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Moregrasp: restoration of upper limb function in Individuals with High spinal cord injury by Multimodal Neuroprostheses for Interaction in Daily Activities", "pdf_hash": "23327b017df9b25bd558f0fea5959f7354c0f658", "year": 2017, "venue": "GBCIC", "alt_text": "https://lh5.googleusercontent.com/aGOiLfshyBU8IjzL7PqQFjPFTjTr99rgHZD-TqTnNGrHEbtg0mWXNeVNDMgh3fNkWcTp2SfpE54jfUKDr-OWFRxTazQcROK9b2vFoxH2FnTryvHdsRGHJ74uSaw82pNNleL9I2_O", "levels": null, "corpus_id": 35763797, "sentences": ["https://lh5.googleusercontent.com/aGOiLfshyBU8IjzL7PqQFjPFTjTr99rgHZD-TqTnNGrHEbtg0mWXNeVNDMgh3fNkWcTp2SfpE54jfUKDr-OWFRxTazQcROK9b2vFoxH2FnTryvHdsRGHJ74uSaw82pNNleL9I2_O"], "caption": "", "local_uri": ["23327b017df9b25bd558f0fea5959f7354c0f658_Image_008.png"], "annotated": false, "compound": false}
{"title": "Transwall: a transparent double-sided touch display facilitating co-located face-to-face interactions", "pdf_hash": "8747432ba547c32d72d69b3313859468fb82d356", "year": 2014, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2. Implementation of TransWall. Users manipulating the content, \u201cChromatic rubber band.\u201d", "levels": null, "corpus_id": 6604955, "sentences": ["Figure 2.", "Implementation of TransWall.", "Users manipulating the content, \u201cChromatic rubber band.\u201d"], "caption": "", "local_uri": ["8747432ba547c32d72d69b3313859468fb82d356_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display", "pdf_hash": "1b01af668d6ab2e47de2dd278eb83700d330a6c6", "year": 2021, "venue": "CHI", "alt_text": "The illustration of three hands and each type of methods with the vibration motors. For example, the Baseline method was described with four same-colored vibration motor icons", "levels": null, "corpus_id": 233987029, "sentences": ["The illustration of three hands and each type of methods with the vibration motors. For example, the Baseline method was described with four same-colored vibration motor icons"], "caption": "", "local_uri": ["1b01af668d6ab2e47de2dd278eb83700d330a6c6_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display", "pdf_hash": "1b01af668d6ab2e47de2dd278eb83700d330a6c6", "year": 2021, "venue": "CHI", "alt_text": "Three arm postures, Forward and Right and Down, are described with a simple illustration.", "levels": null, "corpus_id": 233987029, "sentences": ["Three arm postures, Forward and Right and Down, are described with a simple illustration."], "caption": "Figure 3: Three arm postures tested in Experiment 1.", "local_uri": ["1b01af668d6ab2e47de2dd278eb83700d330a6c6_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display", "pdf_hash": "1b01af668d6ab2e47de2dd278eb83700d330a6c6", "year": 2021, "venue": "CHI", "alt_text": "The photo of WTD prototype we implemented. Four ERM vibration motors are attached to 3D printed watch frame with the spacing of 30 mm.", "levels": null, "corpus_id": 233987029, "sentences": ["The photo of WTD prototype we implemented.", "Four ERM vibration motors are attached to 3D printed watch frame with the spacing of 30 mm."], "caption": "", "local_uri": ["1b01af668d6ab2e47de2dd278eb83700d330a6c6_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display", "pdf_hash": "1b01af668d6ab2e47de2dd278eb83700d330a6c6", "year": 2021, "venue": "CHI", "alt_text": "Figure 7. A photo of the experimental environment. A person is wearing a noise-canceling headset with the wrist-worn tactile display prototype put on. He is sitting on a chair and watching the laptop on the desk to see a GUI testing program.    Figure 8. The bar chart of the accuracies from the preliminary study. The accuracies for each condition is shown in Table 1.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987029, "sentences": ["Figure 7.", "A photo of the experimental environment.", "A person is wearing a noise-canceling headset with the wrist-worn tactile display prototype put on.", "He is sitting on a chair and watching the laptop on the desk to see a GUI testing program.", "Figure 8.", "The bar chart of the accuracies from the preliminary study.", "The accuracies for each condition is shown in Table 1."], "caption": "", "local_uri": ["1b01af668d6ab2e47de2dd278eb83700d330a6c6_Image_008.jpg", "1b01af668d6ab2e47de2dd278eb83700d330a6c6_Image_009.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "A photograph of four people taken from the sky with their shadows taking up the majority of the picture", "levels": [[-1]], "corpus_id": 235663008, "sentences": ["A photograph of four people taken from the sky with their shadows taking up the majority of the picture"], "caption": "An example of a spontaneous playful appropriation: Adopting the drone\u2019s perspective to play with shadows in the outdoors. Credits: debetchris, Instagram", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "A man proposes to a woman in a park. He has a knee on the floor. A drone has landed next to them after delivering an engagement ring.", "levels": null, "corpus_id": 235663008, "sentences": ["A man proposes to a woman in a park.", "He has a knee on the floor.", "A drone has landed next to them after delivering an engagement ring."], "caption": "Drone proposal", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "A person operating a drone to which hair clippers are attached to cut the sitting person's hair", "levels": null, "corpus_id": 235663008, "sentences": ["A person operating a drone to which hair clippers are attached to cut the sitting person's hair"], "caption": "Drone-assisted haircut", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "On the left side of the screen, a football field from the drone's perspective. On the right side, the Pok\u00e9mon Go interface.", "levels": null, "corpus_id": 235663008, "sentences": ["On the left side of the screen, a football field from the drone's perspective.", "On the right side, the Pok\u00e9mon Go interface."], "caption": "Hacking Pokemon Go", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "A person holding a small drone to which a dollar bill is attached with a \"prank gone wrong\" inscription", "levels": null, "corpus_id": 235663008, "sentences": ["A person holding a small drone to which a dollar bill is attached with a \"prank gone wrong\" inscription"], "caption": "Flying money bill prank", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "Two drones with googly eyes have been placed on a roof with their propellers deployed to give them human-looking limbs.", "levels": null, "corpus_id": 235663008, "sentences": ["Two drones with googly eyes have been placed on a roof with their propellers deployed to give them human-looking limbs."], "caption": "Googly-eyed drones", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_019.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "A person is holding a tennis ball while a drone in the background is \"waiting\" for it to be thrown", "levels": null, "corpus_id": 235663008, "sentences": ["A person is holding a tennis ball while a drone in the background is \"waiting\" for it to be thrown"], "caption": "An excited dog-like drone", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_021.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "The shape of an asian temple is formed in the dark sky thanks to hundreds of drones with lights", "levels": null, "corpus_id": 235663008, "sentences": ["The shape of an asian temple is formed in the dark sky thanks to hundreds of drones with lights"], "caption": "Drone swarm show", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_022.jpg"], "annotated": false, "compound": false}
{"title": "Drawing From Social Media to Inspire Increasingly Playful and Social Drone Futures", "pdf_hash": "3022437d4278a5850d9e76e9c2e17b691b2ee5a2", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "A lightpainting picture showing rays of lights in a random trajectory in an urban environment", "levels": null, "corpus_id": 235663008, "sentences": ["A lightpainting picture showing rays of lights in a random trajectory in an urban environment"], "caption": "702", "local_uri": ["3022437d4278a5850d9e76e9c2e17b691b2ee5a2_Image_026.jpg"], "annotated": false, "compound": false}
{"title": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "pdf_hash": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "year": 2021, "venue": "CHI", "alt_text": "Bar chart of number of participants that considered to stop using their tracker, shows that 40% of the participants somewhat to strongly agree to having considered to stop using their tracker.", "levels": [[2, 1]], "corpus_id": 233987550, "sentences": ["Bar chart of number of participants that considered to stop using their tracker, shows that 40% of the participants somewhat to strongly agree to having considered to stop using their tracker."], "caption": "", "local_uri": ["09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_002.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "pdf_hash": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "year": 2021, "venue": "CHI", "alt_text": "Diagram consisting of a square in the centre depicting facilitated reflection, with two attached loops. The upper loops visualises the temporal cycle, the lower loop the conceptual cycle.", "levels": null, "corpus_id": 233987550, "sentences": ["Diagram consisting of a square in the centre depicting facilitated reflection, with two attached loops.", "The upper loops visualises the temporal cycle, the lower loop the conceptual cycle."], "caption": "", "local_uri": ["09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_005.png"], "annotated": false, "compound": false}
{"title": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "pdf_hash": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "year": 2021, "venue": "CHI", "alt_text": "Bar chart of the regularity of which participants check their tracker data, 63% of the participants check their data at least once a day.", "levels": [[2, 1]], "corpus_id": 233987550, "sentences": ["Bar chart of the regularity of which participants check their tracker data, 63% of the participants check their data at least once a day."], "caption": "", "local_uri": ["09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "pdf_hash": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "year": 2021, "venue": "CHI", "alt_text": "Bar chart of number of participants that reflect on their data spontaneously, shows that 85% of the participants somewhat to strongly agree that reflection happens spontaneously.", "levels": [[2, 1]], "corpus_id": 233987550, "sentences": ["Bar chart of number of participants that reflect on their data spontaneously, shows that 85% of the participants somewhat to strongly agree that reflection happens spontaneously."], "caption": "(a)", "local_uri": ["09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "pdf_hash": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "year": 2021, "venue": "CHI", "alt_text": "Bar chart of number of participants are satisfied about how the tracker data reflects their daily activities, shows that 95% of the participants somewhat to strongly agree to this.", "levels": [[2, 1]], "corpus_id": 233987550, "sentences": ["Bar chart of number of participants are satisfied about how the tracker data reflects their daily activities, shows that 95% of the participants somewhat to strongly agree to this."], "caption": "", "local_uri": ["09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "pdf_hash": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "year": 2021, "venue": "CHI", "alt_text": "Diagram consisting of a square in the centre depicting facilitated reflection, with two attached loops. The upper loop visualises the temporal cycle, the lower loop the conceptual cycle.The drawn circles connected to arrow describe specific examples of a participants tracking experience.", "levels": [[-1], [-1], [-1]], "corpus_id": 233987550, "sentences": ["Diagram consisting of a square in the centre depicting facilitated reflection, with two attached loops.", "The upper loop visualises the temporal cycle, the lower loop the conceptual cycle.", "The drawn circles connected to arrow describe specific examples of a participants tracking experience."], "caption": "Figure 6: P7\u2019s personal informatics journey on the TMRM. Our model help identify key turning points in the user\u2019s refection process. These points show where additional support for refection is needed.", "local_uri": ["09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_011.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Pseudo-code to distinguish between errors and edits. The function receives two strings (backspaced and final text) and returns whether the backspaced text was and edit or error. The algorithm is described in detail in the paper.", "levels": [[-1], [-1], [-1]], "corpus_id": 202233477, "sentences": ["Pseudo-code to distinguish between errors and edits.", "The function receives two strings (backspaced and final text) and returns whether the backspaced text was and edit or error.", "The algorithm is described in detail in the paper."], "caption": "Figure 1. Algorithm to distinguish between an edit and error.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_013.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "The figure shows four examples of backspaced and final text and how the algortihm classifies each of the input streams (whether error or edit).", "levels": [[-1]], "corpus_id": 202233477, "sentences": ["The figure shows four examples of backspaced and final text and how the algortihm classifies each of the input streams (whether error or edit)."], "caption": "Figure 2. The figure illustrates four input streams and how to distinguish errors from edits. a) backspaces are classified as errors, since all character corrections are adjacent; b) the backspaced text is a different and valid word, showing a change in mind from black to brown; c) the spellchecker return a suggestion that is equal to the re-entered text, meaning that backspaces were errors; d) spellchecker does not return results, but words are significantly different, thus we assume it is an edit.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_014.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Participants slowly improve performance from week to week; however, for P2 there is a significant performance drop in week 3, followed by a steady improvement, and for P4 there is a sudden performance improvement in week 7.", "levels": null, "corpus_id": 202233477, "sentences": ["Participants slowly improve performance from week to week; however, for P2 there is a significant performance drop in week 3, followed by a steady improvement, and for P4 there is a sudden performance improvement in week 7."], "caption": "Figure 3. Words per minute over 8 weeks.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_025.png"], "annotated": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Error rates decrease over the 8-week period. Substitutions are clearly the most common error type.", "levels": [[3], [2]], "corpus_id": 202233477, "sentences": ["Error rates decrease over the 8-week period.", "Substitutions are clearly the most common error type."], "caption": "Figure 5. Total error rate for each type of error.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_028.png"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "There is a clear predominance of horizontal overlaps, particularly on keys N and M.", "levels": [[-1]], "corpus_id": 202233477, "sentences": ["There is a clear predominance of horizontal overlaps, particularly on keys N and M."], "caption": "Figure 6. Polygons encompass hit points within a standard deviation of", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_030.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Bubbles are clearly larger in week one (twice the size of week 8). Some keys such as S, E, and backspace have larger bubbles in week 1.", "levels": null, "corpus_id": 202233477, "sentences": ["Bubbles are clearly larger in week one (twice the size of week 8).", "Some keys such as S, E, and backspace have larger bubbles in week 1."], "caption": "Figure 7. A circle indicates a pause; size represents its duration. Left - week 1 for P1, Right - week 8 for P1.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_031.jpg"], "annotated": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Lift points are scattered over intended keys. There is no clear offset pattern. Keys nears edges have lower variability.", "levels": [[-1], [-1], [-1]], "corpus_id": 202233477, "sentences": ["Lift points are scattered over intended keys.", "There is no clear offset pattern.", "Keys nears edges have lower variability."], "caption": "Figure 8. Lift points for all participants in week eight.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_032.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Participants slowly improve performance from week to week as in laboratory settings; however, everyday typing performance is consistently higher than lab performance (average 1.5 times).", "levels": [[3, 2]], "corpus_id": 202233477, "sentences": ["Participants slowly improve performance from week to week as in laboratory settings; however, everyday typing performance is consistently higher than lab performance (average 1.5 times)."], "caption": "Figure 9. Words per minute for each participant over 12 weeks.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_037.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Investigating laboratory and everyday typing performances of blind users", "pdf_hash": "3aa4cfaadcfc1172d3439ca6cfd55177420589ae", "year": 2017, "venue": "", "alt_text": "Uncorrected error rate is slightly higher in everyday typing tasks compared with laboratory performance. This results holds true for all participants.", "levels": [[3], [3]], "corpus_id": 202233477, "sentences": ["Uncorrected error rate is slightly higher in everyday typing tasks compared with laboratory performance.", "This results holds true for all participants."], "caption": "Figure 10. Uncorrected error rate for each participant over 12 weeks.", "local_uri": ["3aa4cfaadcfc1172d3439ca6cfd55177420589ae_Image_038.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "User Experiences with Online Status Indicators", "pdf_hash": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "year": 2020, "venue": "CHI", "alt_text": "A sequence of 5 images shown to participants in our survey. The first image shows only a green dot. The second image shows a green dot near a profile picture of Oprah Winfrey with her name next to her picture. The third image shows the second image but inside of a box with the header 'Online Friends (1).' In the fourth image, the words 'Online Now' are added near the green dot. In the final image, the words `Online Now' are shown in the same color as the green dot. Below these images is the prompt shown to participants: `If you opened an app and saw a dot like the one in the image above, what would you think the dot means? (That is, what information is the dot meant to convey?)", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483208, "sentences": ["A sequence of 5 images shown to participants in our survey.", "The first image shows only a green dot.", "The second image shows a green dot near a profile picture of Oprah Winfrey with her name next to her picture.", "The third image shows the second image but inside of a box with the header 'Online Friends (1).'", "In the fourth image, the words 'Online Now' are added near the green dot.", "In the final image, the words `Online Now' are shown in the same color as the green dot.", "Below these images is the prompt shown to participants: `If you opened an app and saw a dot like the one in the image above, what would you think the dot means? (", "That is, what information is the dot meant to convey?)"], "caption": "Figure 1. In the experimental component of our survey, participants saw this progression of images and, after each image, answered the question in the top left. A control group saw these images in gray scale, and other groups saw the images with OSI components\u2019 (dot and \u201conline now\u201d text) in green (as in this \ufb01gure), blue, or orange.", "local_uri": ["6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "User Experiences with Online Status Indicators", "pdf_hash": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "year": 2020, "venue": "CHI", "alt_text": "A collage of OSIs from a variety of apps, which are not specified. Most are green dot icons, but the image also shows text such as `Active a few minutes ago,' a blue dot icon, and an orange dot icon.", "levels": null, "corpus_id": 218483208, "sentences": ["A collage of OSIs from a variety of apps, which are not specified.", "Most are green dot icons, but the image also shows text such as `Active a few minutes ago,' a blue dot icon, and an orange dot icon."], "caption": "Figure 2. This image and explanation were shown to participants to minimize the possible impacts of which experimental condition they ex- perienced in the previous section of the survey.", "local_uri": ["6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "User Experiences with Online Status Indicators", "pdf_hash": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "year": 2020, "venue": "CHI", "alt_text": "Four plots are shown, corresponding to the four experimental conditions. The y-axis on each plot shows the percent of participants who correctly identified that the dot was an OSI. The x-axis has 5 marks to correspond to the 5 images shown to participants. All four graphs show a monotonically increasing number of participants getting the correct response. The plot for the green condition shows a significantly greater area under the curve that connects the 5 data points, because more participants answered correctly for earlier images in the sequence.", "levels": [[1], [1], [1], [2], [4, 3]], "corpus_id": 218483208, "sentences": ["Four plots are shown, corresponding to the four experimental conditions.", "The y-axis on each plot shows the percent of participants who correctly identified that the dot was an OSI.", "The x-axis has 5 marks to correspond to the 5 images shown to participants.", "All four graphs show a monotonically increasing number of participants getting the correct response.", "The plot for the green condition shows a significantly greater area under the curve that connects the 5 data points, because more participants answered correctly for earlier images in the sequence."], "caption": "", "local_uri": ["6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3, 4], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "User Experiences with Online Status Indicators", "pdf_hash": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "year": 2020, "venue": "CHI", "alt_text": "A stacked bar chart showing how many participants guessed that an app does or does not have OSIs and how many participants were not sure. Each bar is the same height, so that the bars show the percent of people answering about this app that gave each answer choice, and the total number of participants giving each answer is overlaid on top of the bars. The top 15 most popular apps, used by at least 10% of participants, are shown, with responses ranging from mostly correctly stating that the app had OSIs to mostly expressing uncertainty about whether the app had OSIs.\" Table 4: \"A table conveying the mean difference, SE, DF, t value, p value, and 95% CI between the green dot experimental condition and each other color (blue, gray, orange).", "levels": [[1], [1], [1], [0]], "corpus_id": 218483208, "sentences": ["A stacked bar chart showing how many participants guessed that an app does or does not have OSIs and how many participants were not sure.", "Each bar is the same height, so that the bars show the percent of people answering about this app that gave each answer choice, and the total number of participants giving each answer is overlaid on top of the bars.", "The top 15 most popular apps, used by at least 10% of participants, are shown, with responses ranging from mostly correctly stating that the app had OSIs to mostly expressing uncertainty about whether the app had OSIs.\"", "Table 4: \"A table conveying the mean difference, SE, DF, t value, p value, and 95% CI between the green dot experimental condition and each other color (blue, gray, orange)."], "caption": "Discord had OSIs, but only a few knew that MyFitnessPal had them. Some differences may be related to how OSIs are designed in each app. For instance, OSIs on Instagram are only visible between users of the messaging feature, so it is plausible that responses for Instagram correlate with whether each participant uses that feature.", "local_uri": ["6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "User Experiences with Online Status Indicators", "pdf_hash": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "year": 2020, "venue": "CHI", "alt_text": "5 bars show the percent of participants who answered yes, no, or not sure as to whether they had experienced each of the scenarios listed in the paper text. Corresponding to the order that they are listed in the methods section, the percent of participants who said yes to each question is 36.5%, 41.5%, 53.5%, 61%, and 69.5%.", "levels": [[-1], [-1]], "corpus_id": 218483208, "sentences": ["5 bars show the percent of participants who answered yes, no, or not sure as to whether they had experienced each of the scenarios listed in the paper text.", "Corresponding to the order that they are listed in the methods section, the percent of participants who said yes to each question is 36.5%, 41.5%, 53.5%, 61%, and 69.5%."], "caption": "", "local_uri": ["6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "User Experiences with Online Status Indicators", "pdf_hash": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "year": 2020, "venue": "CHI", "alt_text": "A hand-drawn image showing a representation of a mobile phone screen. There is a drawing of a person's face, meant to look like a profile picture and a green dot icon but that looks like a pie chart with only a small section shown in green and the rest in white. Next to this icon is a small triangle pointing down, like those used for expanding menus. An `explanation' in the app says `Starting App. You will appear online in 5 ... 4 ... 3 ...' and an annotation pointing to the triangle next to the green pie chart icon says `click here to avoid appearing as online.'", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483208, "sentences": ["A hand-drawn image showing a representation of a mobile phone screen.", "There is a drawing of a person's face, meant to look like a profile picture and a green dot icon but that looks like a pie chart with only a small section shown in green and the rest in white.", "Next to this icon is a small triangle pointing down, like those used for expanding menus.", "An `explanation' in the app says `Starting App.", "You will appear online in 5 ... 4 ... 3 ...' and an annotation pointing to the triangle next to the green pie chart icon says `click here to avoid appearing as online.'"], "caption": "Figure 6. Illustration of a design recommendations to let users turn off their OSI as they open an app.", "local_uri": ["6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "LEGOWorld: Repurposing Commodity Tools & Technologies to Create an Accessible and Customizable Programming Environment", "pdf_hash": "5796c478310ac34b2fc81c0a119c5f67afe0339e", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Shows an overview of the system. A large LEGO base plate and on top a set of components that make up the system: the Ozobot robot, the different caps (start, end, and path), LEGO caps with top codes and tactile arrows, and a dialog between a child and the system - What's the instruction? Move Forward. How many times? Two.", "levels": null, "corpus_id": 233987318, "sentences": ["Shows an overview of the system.", "A large LEGO base plate and on top a set of components that make up the system: the Ozobot robot, the different caps (start, end, and path), LEGO caps with top codes and tactile arrows, and a dialog between a child and the system - What's the instruction? Move Forward. How many times? Two."], "caption": "Figure 1: LEGOWorld. A small-scale robot travels over a path of 3D-printed LEGO caps, assembled over a LEGO base plate. Two interaction modes are represented: Tangible Programming- LEGO blocks with 3D printed programming caps are assembled to create a program, recognized by a mobile device camera; and Voice Programming- programs are created conversationally in a dialog.", "local_uri": ["5796c478310ac34b2fc81c0a119c5f67afe0339e_Image_001.png"], "annotated": false, "compound": false}
{"title": "LEGOWorld: Repurposing Commodity Tools & Technologies to Create an Accessible and Customizable Programming Environment", "pdf_hash": "5796c478310ac34b2fc81c0a119c5f67afe0339e", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Overview of the system. It is possible to see all the different components: physical map, that contains a green plate where a path is built with LEGO, a robot walking the path between the obstacles provided by the LEGO blocks. There is a white plate with tangible blocks on the right of the green plate. The tangible blocks form a horizontally placed sequence. They have arrows that enables blind people to identify them and a code that allows an app to recognise them. On the right of the white plate there is a box with stored  LEGO. Above this three components there is a circle containing the same components but with tripod and holding a smartphone to allow the user interaction.", "levels": null, "corpus_id": 233987318, "sentences": ["Overview of the system.", "It is possible to see all the different components: physical map, that contains a green plate where a path is built with LEGO, a robot walking the path between the obstacles provided by the LEGO blocks.", "There is a white plate with tangible blocks on the right of the green plate.", "The tangible blocks form a horizontally placed sequence.", "They have arrows that enables blind people to identify them and a code that allows an app to recognise them.", "On the right of the white plate there is a box with stored  LEGO.", "Above this three components there is a circle containing the same components but with tripod and holding a smartphone to allow the user interaction."], "caption": "", "local_uri": ["5796c478310ac34b2fc81c0a119c5f67afe0339e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "LEGOWorld: Repurposing Commodity Tools & Technologies to Create an Accessible and Customizable Programming Environment", "pdf_hash": "5796c478310ac34b2fc81c0a119c5f67afe0339e", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "this figure is divided in two images. The one on the left contains the real green plate and a path built with LEGOS. The one on the right contains each of the path blocks with a description of them. First appears a white block with horizontal lines as texture that is responsible for being the start block, then there is a blue block with wavy lines as texture and compse the end block of the path, on the right there is a green block that allows the robot to walk over it, on the right of the green one there is a red LEGO that represents the obstacles or \"walls\" that define the path", "levels": null, "corpus_id": 233987318, "sentences": ["this figure is divided in two images.", "The one on the left contains the real green plate and a path built with LEGOS.", "The one on the right contains each of the path blocks with a description of them.", "First appears a white block with horizontal lines as texture that is responsible for being the start block, then there is a blue block with wavy lines as texture and compse the end block of the path, on the right there is a green block that allows the robot to walk over it, on the right of the green one there is a red LEGO that represents the obstacles or \"walls\" that define the path"], "caption": "", "local_uri": ["5796c478310ac34b2fc81c0a119c5f67afe0339e_Image_005.jpg", "5796c478310ac34b2fc81c0a119c5f67afe0339e_Image_006.png"], "annotated": false, "compound": true}
{"title": "LEGOWorld: Repurposing Commodity Tools & Technologies to Create an Accessible and Customizable Programming Environment", "pdf_hash": "5796c478310ac34b2fc81c0a119c5f67afe0339e", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figure is divided in two images. The left one contains the elements that compose a single programming block. This block has in the downside a LEGO brick and in the upper side a white platform that contains a Topcode for an app recognise the instruction of the block and an arrow as a relief for blind people to be able to identify the instruction.   The right image, shows tangible blocks placed in a vertical sequence that forms the algorithm.", "levels": null, "corpus_id": 233987318, "sentences": ["The figure is divided in two images.", "The left one contains the elements that compose a single programming block.", "This block has in the downside a LEGO brick and in the upper side a white platform that contains a Topcode for an app recognise the instruction of the block and an arrow as a relief for blind people to be able to identify the instruction.", "The right image, shows tangible blocks placed in a vertical sequence that forms the algorithm."], "caption": "Figure 4: Programming Blocks. (Left) Breakdown of a programming block. A LEGO with a 3D-printed cap with a Topcode and a tangible turn-right arrow; (Right) Programming Blocks horizontally arranged over the base plate.", "local_uri": ["5796c478310ac34b2fc81c0a119c5f67afe0339e_Image_007.png", "5796c478310ac34b2fc81c0a119c5f67afe0339e_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Technology for Situated and Emergent Play: A Bridging Concept and Design Agenda", "pdf_hash": "4d42d561dc2aeba86d7f741fed6eec6c6ec80346", "year": 2020, "venue": "CHI", "alt_text": "Figure featuring four playful technologies that are a source of positive affect. A: StreetPong. B: What If You Were In. C: the Mood Squeezer spheres and LED floor. D: inpatient and a caregiver interacting with PhySeEar.", "levels": null, "corpus_id": 218483112, "sentences": ["Figure featuring four playful technologies that are a source of positive affect.", "A: StreetPong. B: What If You Were In. C: the Mood Squeezer spheres and LED floor.", "D: inpatient and a caregiver interacting with PhySeEar."], "caption": "Figure 2. Playful technologies that are a source of positive affect. A: StreetPong [21]. B: What If You Were In...[1]. C: the Mood Squeezer [24] spheres and LED floor. D: inpatient and a caregiver interacting with PhySeEar [54].", "local_uri": ["4d42d561dc2aeba86d7f741fed6eec6c6ec80346_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Technology for Situated and Emergent Play: A Bridging Concept and Design Agenda", "pdf_hash": "4d42d561dc2aeba86d7f741fed6eec6c6ec80346", "year": 2020, "venue": "CHI", "alt_text": "Figure featuring two playful technologies that afford exploration. A: the Drift Table. B: the Speaker Prototypes.", "levels": null, "corpus_id": 218483112, "sentences": ["Figure featuring two playful technologies that afford exploration.", "A: the Drift Table. B: the Speaker Prototypes."], "caption": "Figure 3. Playful technologies that afford exploration. A: the Drift Table [25]. B: the Speaker Prototypes [82].", "local_uri": ["4d42d561dc2aeba86d7f741fed6eec6c6ec80346_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Technology for Situated and Emergent Play: A Bridging Concept and Design Agenda", "pdf_hash": "4d42d561dc2aeba86d7f741fed6eec6c6ec80346", "year": 2020, "venue": "CHI", "alt_text": "Figure showing the basic functioning of Pinsight. Left: how to add content to a Pin. Center-left: a Pin. Center-right: Pin locations on a map. Right: someone interacting with a Pin.", "levels": null, "corpus_id": 218483112, "sentences": ["Figure showing the basic functioning of Pinsight.", "Left: how to add content to a Pin.", "Center-left: a Pin.", "Center-right: Pin locations on a map.", "Right: someone interacting with a Pin."], "caption": "Figure 5. Basic functioning of Pinsight [52]. Left: how to add content to a Pin. Center-left: a Pin. Center-right: Pin locations on a map. Right: someone interacting with a Pin.", "local_uri": ["4d42d561dc2aeba86d7f741fed6eec6c6ec80346_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Technology for Situated and Emergent Play: A Bridging Concept and Design Agenda", "pdf_hash": "4d42d561dc2aeba86d7f741fed6eec6c6ec80346", "year": 2020, "venue": "CHI", "alt_text": "Figure featuring the Keep-Up-With-Me table. The plate to the left is lower than the one to the right because its owner is eating faster.", "levels": null, "corpus_id": 218483112, "sentences": ["Figure featuring the Keep-Up-With-Me table.", "The plate to the left is lower than the one to the right because its owner is eating faster."], "caption": "Figure 7. The Keep-Up-With-Me table [60]. One plate (left) is lower than the other (right) because its owner is eating faster.", "local_uri": ["4d42d561dc2aeba86d7f741fed6eec6c6ec80346_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Technology for Situated and Emergent Play: A Bridging Concept and Design Agenda", "pdf_hash": "4d42d561dc2aeba86d7f741fed6eec6c6ec80346", "year": 2020, "venue": "CHI", "alt_text": "Figure featuring two playful technologies that bring people together. A: people sitting around the Bonding Buffet table, inviting others to sit. B: two persons playing with Pixel Motion.", "levels": null, "corpus_id": 218483112, "sentences": ["Figure featuring two playful technologies that bring people together.", "A: people sitting around the Bonding Buffet table, inviting others to sit.", "B: two persons playing with Pixel Motion."], "caption": "Figure 8. Playful technologies that bring people together. A: people sitting around the Bonding Buffet table [73], inviting others to sit. B: two persons playing with Pixel Motion [66].", "local_uri": ["4d42d561dc2aeba86d7f741fed6eec6c6ec80346_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Leaving the Field: Designing a Socio-Material Toolkit for Teachers to Continue to Design Technology with Children", "pdf_hash": "1fffa5bf1c9cd8bf519791a5810950b31cedbf3e", "year": 2021, "venue": "CHI", "alt_text": "Schedule for design workshop activities as displayed in teacher handbook. The modules are: Introduction, Technological Opportunities, Ideas, Building, Presenting; the optional modules are Combining technologies, Fantasy role playing and Technology and topic.", "levels": null, "corpus_id": 233986949, "sentences": ["Schedule for design workshop activities as displayed in teacher handbook.", "The modules are: Introduction, Technological Opportunities, Ideas, Building, Presenting; the optional modules are Combining technologies, Fantasy role playing and Technology and topic."], "caption": "Figure 1: Impression of activities during the design work- shop with the teachers.", "local_uri": ["1fffa5bf1c9cd8bf519791a5810950b31cedbf3e_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Leaving the Field: Designing a Socio-Material Toolkit for Teachers to Continue to Design Technology with Children", "pdf_hash": "1fffa5bf1c9cd8bf519791a5810950b31cedbf3e", "year": 2021, "venue": "CHI", "alt_text": "Picture taken during the co-design workshop, showing the first prototype of the diary for children, first prototypes of card decks, and littlebits.", "levels": null, "corpus_id": 233986949, "sentences": ["Picture taken during the co-design workshop, showing the first prototype of the diary for children, first prototypes of card decks, and littlebits."], "caption": "", "local_uri": ["1fffa5bf1c9cd8bf519791a5810950b31cedbf3e_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Leaving the Field: Designing a Socio-Material Toolkit for Teachers to Continue to Design Technology with Children", "pdf_hash": "1fffa5bf1c9cd8bf519791a5810950b31cedbf3e", "year": 2021, "venue": "CHI", "alt_text": "Showing final prototype of technology cards (distance sensor, cable, sound) and Design trigger card (protection). The cards are black and white, and have different icons and patterns on it.", "levels": null, "corpus_id": 233986949, "sentences": ["Showing final prototype of technology cards (distance sensor, cable, sound) and Design trigger card (protection).", "The cards are black and white, and have different icons and patterns on it."], "caption": "Figure 4: Final prototype of technology cards (motion sensor, cable, sound) and design trigger card (protection).", "local_uri": ["1fffa5bf1c9cd8bf519791a5810950b31cedbf3e_Image_013.png"], "annotated": false, "compound": false}
{"title": "Leaving the Field: Designing a Socio-Material Toolkit for Teachers to Continue to Design Technology with Children", "pdf_hash": "1fffa5bf1c9cd8bf519791a5810950b31cedbf3e", "year": 2021, "venue": "CHI", "alt_text": "Schematic showing associated infrastructures in the socio-material toolkit:  Material kits (technological samples, card decks) Method kits (Guidelines for Teachers, Suggestions for design activities) Diaries (Record of childrens experiences, design process records) Online Platform (Forum for Experience exchange, support network, Digital materials, Technology Videos) Collaborations (Fablabs, Policy makers, Universities for teachers)", "levels": null, "corpus_id": 233986949, "sentences": ["Schematic showing associated infrastructures in the socio-material toolkit:  Material kits (technological samples, card decks) Method kits (Guidelines for Teachers, Suggestions for design activities) Diaries (Record of childrens experiences, design process records) Online Platform (Forum for Experience exchange, support network, Digital materials, Technology Videos) Collaborations (Fablabs, Policy makers, Universities for teachers)"], "caption": "", "local_uri": ["1fffa5bf1c9cd8bf519791a5810950b31cedbf3e_Image_014.jpg"], "annotated": false, "compound": false}
{"title": "Student Engagement in Sensitive Design Contexts: A Case Study in Dementia Care", "pdf_hash": "49735e8df339d7d5e284b76638da38529263fa04", "year": 2020, "venue": "CHI", "alt_text": "This picture displays two examples of the finished life story boxes. The boxes contain physical objects design by the students and residents.", "levels": null, "corpus_id": 218482406, "sentences": ["This picture displays two examples of the finished life story boxes.", "The boxes contain physical objects design by the students and residents."], "caption": "Figure 1: Examples of the Life Story Box", "local_uri": ["49735e8df339d7d5e284b76638da38529263fa04_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Designing and Evaluating Livefonts", "pdf_hash": "0bc157f953ea87ab91323fd70ff95af28332e818", "year": 2017, "venue": "UIST", "alt_text": "Box-plots of Area Ratio (script/Latin), which ranges on the y-axis from 0.0-2.0. The scripts plotted are Matilda, Tricolor Braille, Version 2, and Version 1. For each, low-vision and sighted groups are plotted separately. The medians are approximately: Matilda (low-vision: 1.0, sighted: 1.2); Tricolor Braille (low-vision: 0.9, sighted: 0.6); Version 2 (low-vision: 0.7, sighted: 0.6); Version 1 (low-vision: 0.5, sighted: 0.55).", "levels": [[1], [1], [1], [2]], "corpus_id": 6851931, "sentences": ["Box-plots of Area Ratio (script/Latin), which ranges on the y-axis from 0.0-2.0.", "The scripts plotted are Matilda, Tricolor Braille, Version 2, and Version 1.", "For each, low-vision and sighted groups are plotted separately.", "The medians are approximately: Matilda (low-vision: 1.0, sighted: 1.2); Tricolor Braille (low-vision: 0.9, sighted: 0.6); Version 2 (low-vision: 0.7, sighted: 0.6); Version 1 (low-vision: 0.5, sighted: 0.55)."], "caption": "", "local_uri": ["0bc157f953ea87ab91323fd70ff95af28332e818_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Designing and Evaluating Livefonts", "pdf_hash": "0bc157f953ea87ab91323fd70ff95af28332e818", "year": 2017, "venue": "UIST", "alt_text": "Box-plot of Time Ration (script/Latin). Scripts plotted, in order: Matilda, Version 1, Version 2, Tricolor Braille, Armenian, Hebrew, Arabic, Devangari, Chinese. Low-vision and sighted groups are plotted separately for each. Medians range from about 1 for Matilda to about 3 for Chinese. The spread for Chinese is the largest by far.", "levels": [[1], [1], [1], [2], [2]], "corpus_id": 6851931, "sentences": ["Box-plot of Time Ration (script/Latin).", "Scripts plotted, in order: Matilda, Version 1, Version 2, Tricolor Braille, Armenian, Hebrew, Arabic, Devangari, Chinese.", "Low-vision and sighted groups are plotted separately for each.", "Medians range from about 1 for Matilda to about 3 for Chinese.", "The spread for Chinese is the largest by far."], "caption": "", "local_uri": ["0bc157f953ea87ab91323fd70ff95af28332e818_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A colorful screenshot shows two rows of strawberries, one with six berries and the label \"6,\" one with eight berries and the label \"8.\" There is a \"+\" between them. At the bottom of the screen are the choices: 22, 14, 16, and 19. At the top of the screen is the text: \"Use the model to add.\"", "levels": null, "corpus_id": 53224149, "sentences": ["A colorful screenshot shows two rows of strawberries, one with six berries and the label \"6,\" one with eight berries and the label \"8.\" There is a \"+\" between them.", "At the bottom of the screen are the choices: 22, 14, 16, and 19.", "At the top of the screen is the text: \"Use the model to add.\""], "caption": "", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A screenshot shows a split screen where the left side shows several cartoon farm animals in a room and says \"take care of animals in the barn!\" The right side shows several animals and people in a field of corn and says \"raise your crops in the field!\"", "levels": [[-1], [-1]], "corpus_id": 53224149, "sentences": ["A screenshot shows a split screen where the left side shows several cartoon farm animals in a room and says \"take care of animals in the barn!\"", "The right side shows several animals and people in a field of corn and says \"raise your crops in the field!\""], "caption": "Figure 1: Examples of commercially available structured and exploratory games. Left: \u201cSplash Math,\u201d aims to teach math content through exercises organized into structured levels. Right: \u201cToca Life, Farm\u201d enables children to explore objects and animals on a farm without fixed goals or \u201ccorrect\u201d interactions.", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Three screenshots in a row. Left: a grid of buttons with the numbers from 1-12 against a plain blue background. Middle: A smiling monster shows its teeth, which are dirty and full of food. Right: A complex machine contains conveyor belts, buttons, and tubes; at the bottom of the screen Cookie Monster (a blue cartoon monster) smiles and raises his hands in the air in excitement.", "levels": null, "corpus_id": 53224149, "sentences": ["Three screenshots in a row.", "Left: a grid of buttons with the numbers from 1-12 against a plain blue background.", "Middle: A smiling monster shows its teeth, which are dirty and full of food.", "Right: A complex machine contains conveyor belts, buttons, and tubes; at the bottom of the screen Cookie Monster (a blue cartoon monster) smiles and raises his hands in the air in excitement."], "caption": "Figure 2: Screenshots from Cookie Monster\u2019s Challenge (CMC). Left: structured levels organize the game. Middle: Children must complete fixed tasks under time pressure, like completely cleaning the monster\u2019s teeth. Right: A Rube-Goldberg machine is assembled bit by bit, with each piece of the machine marking progress made by the child.", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Three screenshots in a row. Left: A green storefront with a red-and-white-striped awning and bins of fruits and vegetables. Arrows are shown on the far sides of the screen pointing outward. Middle: The inside of a grocery store. A prince stands behind the cash register. Food items line shelves on the wall. The door is open and the street is visible outside. Right: A close-up view of the conveyor belt next to the cash register. A single banana is on it.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 53224149, "sentences": ["Three screenshots in a row.", "Left: A green storefront with a red-and-white-striped awning and bins of fruits and vegetables.", "Arrows are shown on the far sides of the screen pointing outward.", "Middle: The inside of a grocery store.", "A prince stands behind the cash register.", "Food items line shelves on the wall.", "The door is open and the street is visible outside.", "Right: A close-up view of the conveyor belt next to the cash register.", "A single banana is on it."], "caption": "Figure 3: Screenshots from Explore Daniel Tiger\u2019s Neighborhood (DT). The game is open-ended and the child can explore without fixed goals by going into a shop of their choosing (left), meandering the store (middle), or zooming into small tasks, like scanning items at the grocery store check-out (right).", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A parent sits on the left and a child sits on the right with the tablet on the table propped up and centered between them. The parent's hand covers the child's.", "levels": null, "corpus_id": 53224149, "sentences": ["A parent sits on the left and a child sits on the right with the tablet on the table propped up and centered between them.", "The parent's hand covers the child's."], "caption": "", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "An iPad is propped up on a table. A child sits in front of it but does not touch it, and their hands are resting on the table with fists closed. An adult hand reaches out from the right side of the child and is touching the screen.", "levels": null, "corpus_id": 53224149, "sentences": ["An iPad is propped up on a table.", "A child sits in front of it but does not touch it, and their hands are resting on the table with fists closed.", "An adult hand reaches out from the right side of the child and is touching the screen."], "caption": "", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A child sits facing an iPad propped up on a table and touching the screen. A parent sits apart, about four feet from the child, with their head propped up on their and with their elbow on the table, leaning back.", "levels": null, "corpus_id": 53224149, "sentences": ["A child sits facing an iPad propped up on a table and touching the screen.", "A parent sits apart, about four feet from the child, with their head propped up on their and with their elbow on the table, leaning back."], "caption": "", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_013.jpg"], "annotated": false, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A bubble chart shows a matrix of bubbles, where the rows are countries (U.S., China, Taiwan) and the columns are behavior categories. Each bubble has a number label corresponding to its area, which reflects the average number of a times a parent from the sample (row) performed the specific behavior (column) during the instructional game session.  Taiwan: Intervention: 17.75 Instruction: 16.25 Open-Ended: 12.83 Augmentation: 8.58 Warmth: 12.83  China: Intervention: 7.3 Instruction: 8.2 Open-Ended: 8.0 Augmentation: 7.4 Warmth: 3.8  U.S. Intervention: 2.4 Instruction: 6.26 Open-Ended: 7.6 Augmentation: 9.6 Warmth: 22.8", "levels": [[1], [1], [2]], "corpus_id": 53224149, "sentences": ["A bubble chart shows a matrix of bubbles, where the rows are countries (U.S., China, Taiwan) and the columns are behavior categories.", "Each bubble has a number label corresponding to its area, which reflects the average number of a times a parent from the sample (row) performed the specific behavior (column) during the instructional game session.", "Taiwan: Intervention: 17.75 Instruction: 16.25 Open-Ended: 12.83 Augmentation: 8.58 Warmth: 12.83  China: Intervention: 7.3 Instruction: 8.2 Open-Ended: 8.0 Augmentation: 7.4 Warmth: 3.8  U.S. Intervention: 2.4 Instruction: 6.26 Open-Ended: 7.6 Augmentation: 9.6 Warmth: 22.8"], "caption": "Figure 10: The average number of times each parent displayed each form of JME during the CMC session.", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_019.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "pdf_hash": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "year": 2018, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "A bubble chart shows a matrix of bubbles, where the rows are countries (U.S., China, Taiwan) and the columns are behavior categories. Each bubble has a number label corresponding to its area, which reflects the average number of a times a parent from the sample (row) performed the specific behavior (column) during the instructional game session.  Taiwan: Intervention: 4.17 Instruction: 7.7 Open-Ended: 6 Augmentation: 8.83 Warmth: 4.83  China: Intervention: 2.4 Instruction: 5.0 Open-Ended: 7.4 Augmentation: 7.4 Warmth: 1.6  U.S. Intervention: 0.5 Instruction: 4.5 Open-Ended: 8.1 Augmentation: 20.9 Warmth: 9.9", "levels": [[1], [1], [2]], "corpus_id": 53224149, "sentences": ["A bubble chart shows a matrix of bubbles, where the rows are countries (U.S., China, Taiwan) and the columns are behavior categories.", "Each bubble has a number label corresponding to its area, which reflects the average number of a times a parent from the sample (row) performed the specific behavior (column) during the instructional game session.", "Taiwan: Intervention: 4.17 Instruction: 7.7 Open-Ended: 6 Augmentation: 8.83 Warmth: 4.83  China: Intervention: 2.4 Instruction: 5.0 Open-Ended: 7.4 Augmentation: 7.4 Warmth: 1.6  U.S. Intervention: 0.5 Instruction: 4.5 Open-Ended: 8.1 Augmentation: 20.9 Warmth: 9.9"], "caption": "", "local_uri": ["c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_020.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Understanding the Authoring and Playthrough of Nonvisual Smartphone Tutorials", "pdf_hash": "a3863edd4d06083ddf21a1386acbdc6d3da8f85f", "year": 2019, "venue": "INTERACT", "alt_text": "1) screenshot of the WhatsApp App in the group creation screen, with two examples of the step description by two different authors, one blind and one sighted. Now we do the up down gesture to go to the last element of the page. Now we are on Next we double tap. (BA) We click forward. (SA); 2) screenshot of the RadioNet App with a list of the radios available to be selected after a search., again with two examples. Now its showing a list of radio stations, we are going to swipe from left to right with one finger until Radio Comercial. It will say radio logo, radio and name. (\u0192)(BA) Multiple stations appear, we choose the one we want, Radio Comercial. (SA)", "levels": null, "corpus_id": 201671924, "sentences": ["1) screenshot of the WhatsApp App in the group creation screen, with two examples of the step description by two different authors, one blind and one sighted.", "Now we do the up down gesture to go to the last element of the page.", "Now we are on Next we double tap. (BA) We click forward. (SA); 2) screenshot of the RadioNet App with a list of the radios available to be selected after a search., again with two examples.", "Now its showing a list of radio stations, we are going to swipe from left to right with one finger until Radio Comercial.", "It will say radio logo, radio and name. (\u0192)(BA) Multiple stations appear, we choose the one we want, Radio Comercial. (SA)"], "caption": "", "local_uri": ["a3863edd4d06083ddf21a1386acbdc6d3da8f85f_Image_002.png"], "annotated": false, "compound": false}
{"title": "Understanding the Telework Experience of People with Disabilities", "pdf_hash": "cf5d5e65a7743e62ef70456dd3897dbe5f2f21b0", "year": 2021, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Picture showing a mobile phone on a small tripod showing a video call with an ASL interpreter positioned in front of a laptop connected to a meeting video call with video tiles of meeting participants, screen sharing of slides, and live captioning of the meeting audio.", "levels": null, "corpus_id": 233353894, "sentences": ["Picture showing a mobile phone on a small tripod showing a video call with an ASL interpreter positioned in front of a laptop connected to a meeting video call with video tiles of meeting participants, screen sharing of slides, and live captioning of the meeting audio."], "caption": "", "local_uri": ["cf5d5e65a7743e62ef70456dd3897dbe5f2f21b0_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Telework Experience of People with Disabilities", "pdf_hash": "cf5d5e65a7743e62ef70456dd3897dbe5f2f21b0", "year": 2021, "venue": "Proc. ACM Hum. Comput. Interact.", "alt_text": "Screenshot of video call with a matrix showing 3 video windows of participants and one black window with a static profile picture of a participant. There is also a row of icons of additional participants including an icon with just two initials and an icon of a phone caller.", "levels": null, "corpus_id": 233353894, "sentences": ["Screenshot of video call with a matrix showing 3 video windows of participants and one black window with a static profile picture of a participant.", "There is also a row of icons of additional participants including an icon with just two initials and an icon of a phone caller."], "caption": "", "local_uri": ["cf5d5e65a7743e62ef70456dd3897dbe5f2f21b0_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202017-10-09%20at%204.49.07%20PM", "levels": [[-1]], "corpus_id": 5046448, "sentences": ["../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202017-10-09%20at%204.49.07%20PM"], "caption": "Shoes are often still designed with pen, paper, and lasts (foot forms) (see fig 1 and 2). Our approach in this case study is one of \u201cUltra Personalization\u201d that includes data in the design process. While this level of personalization is sometimes achieved in bespoke shoemaking, we wanted to make the shoe with a 3D printer. Using soft and flexible materials allowed us to work with a 3-D form with the added dimension of movement (4D Printing). Designing a shoe to be fully 4D printed is very different than traditional shoe making, see fig. 3.", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_006.jpg", "935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../../../Desktop/Case%20Study%20Project%20J/Schermafdruk%202015-07-31%2015.06.48", "levels": null, "corpus_id": 5046448, "sentences": ["../../../Desktop/Case%20Study%20Project%20J/Schermafdruk%202015-07-31%2015.06.48"], "caption": "Figure 8 Digital Last Development", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_015.jpg", "935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_017.jpg"], "annotated": false, "compound": true}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../../../Desktop/Case%20Study%20Project%20J/Schermafdruk%202015-08-03%2016.05.21", "levels": null, "corpus_id": 5046448, "sentences": ["../../../Desktop/Case%20Study%20Project%20J/Schermafdruk%202015-08-03%2016.05.21"], "caption": "Figure 11 Grasshopper Definition", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_021.jpg", "935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_023.jpg"], "annotated": false, "compound": true}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202015-08-18%20at%2022.39.32.p", "levels": null, "corpus_id": 5046448, "sentences": ["../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202015-08-18%20at%2022.39.32.p"], "caption": "", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_022.jpg"], "annotated": false, "compound": false}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../../../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202015-09-02%20at%2007.5", "levels": null, "corpus_id": 5046448, "sentences": ["../../../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202015-09-02%20at%2007.5"], "caption": "Figure 15 Inverted Normals Geometry Problems", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_025.jpg"], "annotated": false, "compound": false}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../../../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202015-09-02%20at%2008.1", "levels": null, "corpus_id": 5046448, "sentences": ["../../../Desktop/Case%20Study%20Project%20J/Screen%20Shot%202015-09-02%20at%2008.1"], "caption": "Figure 16 Manifold Geometry Problems", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_027.jpg", "935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_029.jpg"], "annotated": false, "compound": true}
{"title": "Towards Ultra Personalized 4D Printed Shoes", "pdf_hash": "935e474e40ed4b42af2f7e13e7801c57cfec4245", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "../../../Desktop/Case%20Study%20Project%20J/Schermafdruk%202015-08-06%2018.00.01", "levels": null, "corpus_id": 5046448, "sentences": ["../../../Desktop/Case%20Study%20Project%20J/Schermafdruk%202015-08-06%2018.00.01"], "caption": "Figure 19 Grasshopper \u201cDefinition\u201d of the shoe", "local_uri": ["935e474e40ed4b42af2f7e13e7801c57cfec4245_Image_031.jpg"], "annotated": false, "compound": false}
{"title": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars", "pdf_hash": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b", "year": 2020, "venue": "CHI", "alt_text": "Two sets of experiences shows the Miniature Haptics' concept of shrinkgin haptic feedback and applying it to hand-based and embodied avatars. The first set shows that in virtual scene, an avatar kicks a football, while in reality it is two finger acting in finger walking in place(FWIP) and kick the football prop with index finger as foot. The football prop is actuated by a custom device that actuates prop according to the virtual scene. The second set shows a man avatar jogging through a forest in the virtual scene, and in reality the fingers are running on a 2-axis tilting platform that gives feedback for the changin slope.", "levels": null, "corpus_id": 218483279, "sentences": ["Two sets of experiences shows the Miniature Haptics' concept of shrinkgin haptic feedback and applying it to hand-based and embodied avatars.", "The first set shows that in virtual scene, an avatar kicks a football, while in reality it is two finger acting in finger walking in place(FWIP) and kick the football prop with index finger as foot.", "The football prop is actuated by a custom device that actuates prop according to the virtual scene.", "The second set shows a man avatar jogging through a forest in the virtual scene, and in reality the fingers are running on a 2-axis tilting platform that gives feedback for the changin slope."], "caption": "Figure 1. Miniature Haptics introduces the concept of shrinking haptic feedback and applying it to hand-based and embodied avatars. Here we use Finger Walking in Place (FWIP) as the avatar control and embodiment method, and show two examples of: (a) kicking a football, with haptic feedback provided by 6 linear actuators, and (b) jogging through a forest, with haptic feedback for the changing slope provided through a 2-axis tilting platform.", "local_uri": ["ac5ed73b8241b1de45a45ca6aad1e34726dc236b_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars", "pdf_hash": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b", "year": 2020, "venue": "CHI", "alt_text": "Visualization of skeletal model participants mapped onto hands in both closed-hand and opened-hand poses. The closed hand shows the head maps to the wrist, hips map to metacarpophalangeal joint of index finger and middle finger for left and right hips, respectively; knees map to proximal interphalangeal joint of index finger and middle finger, and feet map to the tip of index finger and middle finger. The shoulders map to back of the palm, elbows and hand are very close, mapping to close to interphalangeal joint of thumb and pinky for left and right hand, respectively. The open hand gesture shows similar results, but the hand parts are extended to the tip of thumb and pinky.", "levels": null, "corpus_id": 218483279, "sentences": ["Visualization of skeletal model participants mapped onto hands in both closed-hand and opened-hand poses.", "The closed hand shows the head maps to the wrist, hips map to metacarpophalangeal joint of index finger and middle finger for left and right hips, respectively; knees map to proximal interphalangeal joint of index finger and middle finger, and feet map to the tip of index finger and middle finger.", "The shoulders map to back of the palm, elbows and hand are very close, mapping to close to interphalangeal joint of thumb and pinky for left and right hand, respectively.", "The open hand gesture shows similar results, but the hand parts are extended to the tip of thumb and pinky."], "caption": "Figure 5. Visualization of skeletal model participants mapped onto hands in both closed-hand and opened-hand poses, by overlaying all body areas drawn by each participant and averaging the location of each joint: (a) the skeleton model with limbs and joints; (b) skeletal mapping when \ufb01nger walking in the hand-closed pose; (c) skeletal mapping when \ufb01nger walking in the hand-opened pose.", "local_uri": ["ac5ed73b8241b1de45a45ca6aad1e34726dc236b_Image_005.png"], "annotated": false, "compound": false}
{"title": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars", "pdf_hash": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b", "year": 2020, "venue": "CHI", "alt_text": "Miniature Haptics football prototype, consists of 6 linear actuators actuating football props, touch sensors on each football prop to sense kicking the ball, and vicon markers on user's hand to detect the motion of the hand. The footballs randomly arrive at 1 of 6 locations and the player use hands to control the avatar to return the ball using head or feet.", "levels": null, "corpus_id": 218483279, "sentences": ["Miniature Haptics football prototype, consists of 6 linear actuators actuating football props, touch sensors on each football prop to sense kicking the ball, and vicon markers on user's hand to detect the motion of the hand.", "The footballs randomly arrive at 1 of 6 locations and the player use hands to control the avatar to return the ball using head or feet."], "caption": "", "local_uri": ["ac5ed73b8241b1de45a45ca6aad1e34726dc236b_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars", "pdf_hash": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b", "year": 2020, "venue": "CHI", "alt_text": "Average rating of enjoyment, realism, and intuitiveness on a 7-point Likert scale for the football experience, showing that Miniature Haptics significantly improved enjoyment and realism compared. to controller with vibration feedback and finger walking. There are significant difference between following pairs: enjoyment, MiniatureHaptics vs finger walking and controller; realism, all 3 pairs.", "levels": [[3], [3], [3]], "corpus_id": 218483279, "sentences": ["Average rating of enjoyment, realism, and intuitiveness on a 7-point Likert scale for the football experience, showing that Miniature Haptics significantly improved enjoyment and realism compared.", "to controller with vibration feedback and finger walking.", "There are significant difference between following pairs: enjoyment, MiniatureHaptics vs finger walking and controller; realism, all 3 pairs."], "caption": "", "local_uri": ["ac5ed73b8241b1de45a45ca6aad1e34726dc236b_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars", "pdf_hash": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b", "year": 2020, "venue": "CHI", "alt_text": "A pie chart showing distribution of the most preferred interaction method for the football experience. It shows that Miniature Haptics was the most preferred interaction method at 58%. The controller and finger walking are 25% and 17% respectively.", "levels": [[1], [2], [2]], "corpus_id": 218483279, "sentences": ["A pie chart showing distribution of the most preferred interaction method for the football experience.", "It shows that Miniature Haptics was the most preferred interaction method at 58%.", "The controller and finger walking are 25% and 17% respectively."], "caption": "Figure 9. Distribution of the most preferred interaction method for the football experience, showing Miniature Haptics was the most preferred interaction method.", "local_uri": ["ac5ed73b8241b1de45a45ca6aad1e34726dc236b_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Playing With Shadows: An Exploration of Calm Game Interaction", "pdf_hash": "7ceab70dff42941e9d6e06ef8cebf4911679416f", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "A group of people sitting at a table using a computer  Description automatically generated", "levels": null, "corpus_id": 220324645, "sentences": ["A group of people sitting at a table using a computer  Description automatically generated"], "caption": "mechanic, and in many games, in-game action speedily progresses and game mechanics put pressure on players. In our work, we want to push back against the idea of rushing the player, and instead explore interaction paradigms and game mechanics that support calm game interaction. We understand calm gameplay as the combination of game interfaces that support pleasant and effortless player input with the absence of direct pressure to (re)act within the game so that players have freedom to contemplate and explore. To explore the concept, we present Shadventures, a side-scrolling adventure game that is controlled by projecting the shadow created by one of the player\u2019s hand movement into the game.", "local_uri": ["7ceab70dff42941e9d6e06ef8cebf4911679416f_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Designing Personalized Therapy Tools for People with Dementia", "pdf_hash": "60affef2448f6e53c0084124e50cee2bef6f82c9", "year": 2019, "venue": "W4A", "alt_text": "Main cognitive stimulation activities of Scrapbook represented by screen captures. From left to right a. Reminiscence therapy, a screen with a picture in the center (session control buttons hidden); b. Flashcard, a screen with 8 squares, some with a visible picture (already flipped), and some blank; c. Street View navigation, a  3D view from one monument of the city; d. Quiz, screen with a question on the top, a picture about the question on the left, and the options on the right; e. Touch, with several images flying from the left to the right of the screen; f. Puzzle (in fullscreen), with a picture divided in 9 random parts.", "levels": null, "corpus_id": 199501863, "sentences": ["Main cognitive stimulation activities of Scrapbook represented by screen captures.", "From left to right a. Reminiscence therapy, a screen with a picture in the center (session control buttons hidden); b. Flashcard, a screen with 8 squares, some with a visible picture (already flipped), and some blank; c. Street View navigation, a  3D view from one monument of the city; d. Quiz, screen with a question on the top, a picture about the question on the left, and the options on the right; e. Touch, with several images flying from the left to the right of the screen; f. Puzzle (in fullscreen), with a picture divided in 9 random parts."], "caption": "2 Related Work\u200c", "local_uri": ["60affef2448f6e53c0084124e50cee2bef6f82c9_Image_002.png"], "annotated": false, "compound": false}
{"title": "Designing Personalized Therapy Tools for People with Dementia", "pdf_hash": "60affef2448f6e53c0084124e50cee2bef6f82c9", "year": 2019, "venue": "W4A", "alt_text": "Timeline of study procedure. With boxes, from left to right, representing each phase of the study. Each box presents a new iteration of the tool and a deployment with psychologists. First phase with 1 psychologist and 3 patients. Second phase with 3 psychologists and 11 patients, and third phase with 3 psychologists and 12 patients. Box 1 is associated with goals 1 and 2, box 2, with goal 3 and box 3 with goals 4 and 5.", "levels": null, "corpus_id": 199501863, "sentences": ["Timeline of study procedure.", "With boxes, from left to right, representing each phase of the study.", "Each box presents a new iteration of the tool and a deployment with psychologists.", "First phase with 1 psychologist and 3 patients.", "Second phase with 3 psychologists and 11 patients, and third phase with 3 psychologists and 12 patients.", "Box 1 is associated with goals 1 and 2, box 2, with goal 3 and box 3 with goals 4 and 5."], "caption": "Figure 2. Timeline of study procedure.", "local_uri": ["60affef2448f6e53c0084124e50cee2bef6f82c9_Image_003.png"], "annotated": false, "compound": false}
{"title": "Designing Personalized Therapy Tools for People with Dementia", "pdf_hash": "60affef2448f6e53c0084124e50cee2bef6f82c9", "year": 2019, "venue": "W4A", "alt_text": "Patient performing a jigsaw puzzle. A patient sitting on a chair, with a tablet in her hands is performing a jigsaw puzzle.", "levels": null, "corpus_id": 199501863, "sentences": ["Patient performing a jigsaw puzzle.", "A patient sitting on a chair, with a tablet in her hands is performing a jigsaw puzzle."], "caption": "Designing Personalized Therapy Tools for People with Dementia      W4A \u201919, May 13\u201315, 2019, San Francisco, CA, USA", "local_uri": ["60affef2448f6e53c0084124e50cee2bef6f82c9_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "When to Add Human Narration to Photo-Sharing Social Media", "pdf_hash": "192ac65963c6ff1e299ec09560f8001e581eead9", "year": 2020, "venue": "ASSETS", "alt_text": "Figure 1a shows a person is standing on a rock looking at a snow-covered mountain range while Figure 1b shows a person is standing on a rock looking at a green plain.", "levels": null, "corpus_id": 225963174, "sentences": ["Figure 1a shows a person is standing on a rock looking at a snow-covered mountain range while Figure 1b shows a person is standing on a rock looking at a green plain."], "caption": "(a) Description in frst person perspective: \u201dI (b) Description in third person perspective: stand atop a tilted rock surrounded by ice.\u201dA woman stands atop a rock surrounded by My back is turned as I overlook the sunset greenery. Her back is turned as she gazes beyond the mountain range in the distance.\u201d at the sprawling plain.\u201d \u00a9 Philipp Lublasser", "local_uri": ["192ac65963c6ff1e299ec09560f8001e581eead9_Image_003.jpg", "192ac65963c6ff1e299ec09560f8001e581eead9_Image_004.jpg"], "annotated": false, "compound": true}
{"title": "A Demo of Talkit++: Interacting with 3D Printed Models Using an iOS Device", "pdf_hash": "00e493b70313cf2dd8af6f73e436f4c4559b557c", "year": 2018, "venue": "ASSETS", "alt_text": "A visually impaired student is using Talkit++ on an iPad to learn the Plane model. The student is exploring the model, and the Talkit++ application is recognizing her gestures and providing audio feedback. The iPad is in a stand so that the camera can capture the student\u2019s interactions with the model.", "levels": null, "corpus_id": 52939800, "sentences": ["A visually impaired student is using Talkit++ on an iPad to learn the Plane model.", "The student is exploring the model, and the Talkit++ application is recognizing her gestures and providing audio feedback.", "The iPad is in a stand so that the camera can capture the student\u2019s interactions with the model."], "caption": "Figure1. A visually impaired student is using Talkit++ on an iPad to learn the Plane model. Talkit++ speaks audio information when the student explores the model. The iPad is in a stand so that the camera can capture the student\u2019s interactions with the model.", "local_uri": ["00e493b70313cf2dd8af6f73e436f4c4559b557c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "A Demo of Talkit++: Interacting with 3D Printed Models Using an iOS Device", "pdf_hash": "00e493b70313cf2dd8af6f73e436f4c4559b557c", "year": 2018, "venue": "ASSETS", "alt_text": "Three printed models: the Map model, the Volcano model, and the Plane model. The Map model is a Washington State Regions map, and has three 2D trackers on the corners of the model. The Volcano model illustrates the internal structure of a volcano, and has two 2D trackers on the bottom of the model. The Plane model illustrates a typical shape of a passenger plane, and has a 3D tracker mounted on the body of the plane.", "levels": null, "corpus_id": 52939800, "sentences": ["Three printed models: the Map model, the Volcano model, and the Plane model.", "The Map model is a Washington State Regions map, and has three 2D trackers on the corners of the model.", "The Volcano model illustrates the internal structure of a volcano, and has two 2D trackers on the bottom of the model.", "The Plane model illustrates a typical shape of a passenger plane, and has a 3D tracker mounted on the body of the plane."], "caption": "Figure 2. We designed three interactive models: (a) the Map model, (b) the Volcano model, and (c) the Plane model.", "local_uri": ["00e493b70313cf2dd8af6f73e436f4c4559b557c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "TACTOPI: a Playful Approach to Promote Computational Thinking for Visually Impaired Children", "pdf_hash": "c17ae0348a1fd6eae8a11d422f71624fa28d4c1a", "year": 2020, "venue": "ASSETS", "alt_text": "Schematic illustration of TACTOPI's Prototype composed by a a world map, on the background, a steering wheel, as a rudder, in the center and the robot as a ship on the right side. The illustrations contains descriptions of key features from the prototype, on the map these are braille (pointing to an example of Angola written in braille) and the borders are tactile. In the center, the interactive steering wheel integrates a game controller with joystick, buttons and haptic feedback. On the right side we observe the mobile robot that will be contained in the ship, pointing to a buzzer with a label description of sound feedback and obstacle avoidance sensors.", "levels": null, "corpus_id": 225947952, "sentences": ["Schematic illustration of TACTOPI's Prototype composed by a a world map, on the background, a steering wheel, as a rudder, in the center and the robot as a ship on the right side.", "The illustrations contains descriptions of key features from the prototype, on the map these are braille (pointing to an example of Angola written in braille) and the borders are tactile.", "In the center, the interactive steering wheel integrates a game controller with joystick, buttons and haptic feedback.", "On the right side we observe the mobile robot that will be contained in the ship, pointing to a buzzer with a label description of sound feedback and obstacle avoidance sensors."], "caption": "Figure 1: TACTOPI, a playful environment designed from the ground up to be rich in both its story (a nautical game) and its mechanics (e.g., a physical robot-boat controlled with a 3D printed wheel), tailored to promote computational thinking at diferent levels (4 to 8 years old). The map has been designed using resources from Freepik.com", "local_uri": ["c17ae0348a1fd6eae8a11d422f71624fa28d4c1a_Image_001.png"], "annotated": false, "compound": false}
{"title": "Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations", "pdf_hash": "e06c93b3b1e062ebaae9f3b570eab2398b284785", "year": 2017, "venue": "UIST", "alt_text": "Figure 8. A histogram of the time participants took to locate an element, categorized and colored by models.", "levels": null, "corpus_id": 12097260, "sentences": ["Figure 8.", "A histogram of the time participants took to locate an element, categorized and colored by models."], "caption": "", "local_uri": ["e06c93b3b1e062ebaae9f3b570eab2398b284785_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations", "pdf_hash": "e06c93b3b1e062ebaae9f3b570eab2398b284785", "year": 2017, "venue": "UIST", "alt_text": "Figure 9. The mean time participants spent on each model, broken down by the different sections in Markit.", "levels": [[0], [1]], "corpus_id": 12097260, "sentences": ["Figure 9.", "The mean time participants spent on each model, broken down by the different sections in Markit."], "caption": "", "local_uri": ["e06c93b3b1e062ebaae9f3b570eab2398b284785_Image_010.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Capturing Experts' Mental Models to Organize a Collection of Haptic Devices: Affordances Outweigh Attributes", "pdf_hash": "07ffdba9d5617e70010ec4612616bfb4ab0c07e1", "year": 2020, "venue": "CHI", "alt_text": "The image shows a mock-up of the card-sorting interface at the top and three icons denoting the uber-attributes, device categories, and device similarities at the bottom.", "levels": null, "corpus_id": 215806819, "sentences": ["The image shows a mock-up of the card-sorting interface at the top and three icons denoting the uber-attributes, device categories, and device similarities at the bottom."], "caption": "Figure 1: Expert mental categories for haptic devices. To depict the device similarities, experts sometimes created non- overlapping \u201cbins\u201d and sometimes \u201ctagged\u201d devices by copy- ing them into multiple bins. We derived device uber-attributes, categories, and similarities from the responses of 18 experts.", "local_uri": ["07ffdba9d5617e70010ec4612616bfb4ab0c07e1_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Capturing Experts' Mental Models to Organize a Collection of Haptic Devices: Affordances Outweigh Attributes", "pdf_hash": "07ffdba9d5617e70010ec4612616bfb4ab0c07e1", "year": 2020, "venue": "CHI", "alt_text": "The image has three vertical sections presenting the four stages of the study, the data analysis, and the resulting categories and uber-attributes.", "levels": [[-1]], "corpus_id": 215806819, "sentences": ["The image has three vertical sections presenting the four stages of the study, the data analysis, and the resulting categories and uber-attributes."], "caption": "Figure 3: Overview of the data collected during our study as well as our analytical procedures and results.", "local_uri": ["07ffdba9d5617e70010ec4612616bfb4ab0c07e1_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Capturing Experts' Mental Models to Organize a Collection of Haptic Devices: Affordances Outweigh Attributes", "pdf_hash": "07ffdba9d5617e70010ec4612616bfb4ab0c07e1", "year": 2020, "venue": "CHI", "alt_text": "The seven (including 1 `miscellaneous') GFF categories that we derived for the 75 devices using a clustering analysis.", "levels": null, "corpus_id": 215806819, "sentences": ["The seven (including 1 `miscellaneous') GFF categories that we derived for the 75 devices using a clustering analysis."], "caption": "\u2264", "local_uri": ["07ffdba9d5617e70010ec4612616bfb4ab0c07e1_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Friendsourcing the unmet needs of people with dementia", "pdf_hash": "59e0a8b077cc2aa5640a27a653f47fc06f3d966f", "year": 2014, "venue": "W4A", "alt_text": "Image of the system. Data is collected from a mobile device and fed to the database. The events are extarcted from the database and inluded in the semantic network. Incomplete events generate questions to the social network.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 15356584, "sentences": ["Image of the system.", "Data is collected from a mobile device and fed to the database.", "The events are extarcted from the database and inluded in the semantic network.", "Incomplete events generate questions to the social network."], "caption": "Figure 1. High-level system design.", "local_uri": ["59e0a8b077cc2aa5640a27a653f47fc06f3d966f_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Friendsourcing the unmet needs of people with dementia", "pdf_hash": "59e0a8b077cc2aa5640a27a653f47fc06f3d966f", "year": 2014, "venue": "W4A", "alt_text": "An automatic question is posed on the user's Facebook group. It is a post asking where was Jo\u00e3o that morning with 4 comments with possible answers. Selection of the correct answer is made by the system based on the quanity of \"likes\".", "levels": null, "corpus_id": 15356584, "sentences": ["An automatic question is posed on the user's Facebook group.", "It is a post asking where was Jo\u00e3o that morning with 4 comments with possible answers.", "Selection of the correct answer is made by the system based on the quanity of \"likes\"."], "caption": "Figure 2. A question posed automatically in the PwD\u2019s private group. The members answer by using the \u201clike\u201d mechanism.", "local_uri": ["59e0a8b077cc2aa5640a27a653f47fc06f3d966f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Involving Users and Experts in Motion-Based Game Design for Older Adults", "pdf_hash": "12b16859dfbb36992edf01f172716470aa13af5c", "year": 2013, "venue": "", "alt_text": "https://lh3.googleusercontent.com/YGQmz_2w-d8Uku_1iGIHN87eqkA_REKrqfFzaxWw23bIEmz8yHZfERcDpSC34Ci3pzxcAoZqA0Jz9peq1aLzjfqSSvWexPoHG342VMFxgnvZm8FM9HFu", "levels": null, "corpus_id": 192824, "sentences": ["https://lh3.googleusercontent.com/YGQmz_2w-d8Uku_1iGIHN87eqkA_REKrqfFzaxWw23bIEmz8yHZfERcDpSC34Ci3pzxcAoZqA0Jz9peq1aLzjfqSSvWexPoHG342VMFxgnvZm8FM9HFu"], "caption": "", "local_uri": ["12b16859dfbb36992edf01f172716470aa13af5c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Teaser figure with 6 photos. A: photo shows the ironing tool, with the front spool visible. B: various materials with applied tapes and patches. C: D: different fabrics with flexible wiring and capacitive button and slider controls. E: photo of an ironed NFC tag. F: fabric with an ironed thin electroluminescence wire.", "levels": null, "corpus_id": 218482526, "sentences": ["Teaser figure with 6 photos.", "A: photo shows the ironing tool, with the front spool visible.", "B: various materials with applied tapes and patches.", "C: D: different fabrics with flexible wiring and capacitive button and slider controls.", "E: photo of an ironed NFC tag.", "F: fabric with an ironed thin electroluminescence wire."], "caption": "Figure 1. Rapid Iron-On User Interfaces constitute a new approach for fabrication of custom textile prototypes in a fast and intuitive sketching-like manner. By using a handheld ironing tool (A+B) for applying adhesive functional tapes and patches (C-E), the designer can create rich interactive functionality, drawing from a library of thin and \ufb02exible wiring, input and output (D-F) as well as computing components.", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Collage of five photos showing the ironing tool from multiple perspectives. A: ironing tool when assembled, the front spool and the spring-loaded heating block are visible. B: side view of an opened ironing tool; both spools and the inserted tape is visible. The heating block is lifted up. C: similar to photo B, but with the heating block pressed down, pushing the tape onto the textile. D: small blade that cuts a four-wire tape. The blade is mounted on actuated leadscrew. E: four exchangeable heating blocks: 1: flat shape 2: u-shaped 3: t-shaped 4: l-shaped.", "levels": null, "corpus_id": 218482526, "sentences": ["Collage of five photos showing the ironing tool from multiple perspectives.", "A: ironing tool when assembled, the front spool and the spring-loaded heating block are visible.", "B: side view of an opened ironing tool; both spools and the inserted tape is visible.", "The heating block is lifted up.", "C: similar to photo B, but with the heating block pressed down, pushing the tape onto the textile.", "D: small blade that cuts a four-wire tape.", "The blade is mounted on actuated leadscrew.", "E: four exchangeable heating blocks: 1: flat shape 2: u-shaped 3: t-shaped 4: l-shaped."], "caption": "Figure 2. Iron-On working principle: A functional textile component is fabricated using the dispenser (A). When not pressed down, the tape is slightly lifted above the surface (B). When pressed down by the ex- changeable, spring-loaded heating block (E), the tape is ironed onto the textile (C+A). The cutting blade (D) moves along an actuated leadscrew.", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Collage of the process of applying a rapid iron-on patch. A: taking out a slider patch (two parallel triangles) from a patch collection. B: the patch is hold in front of a textile with two wires, the ironing tool is visible in the background. C: the patch is placed on the textile, aligned with the two existing traces and the ironing tool is put on. D: the patch is applied and the carrier and protection layer is getting removed. E: the applied patch is touched with a finger.", "levels": null, "corpus_id": 218482526, "sentences": ["Collage of the process of applying a rapid iron-on patch.", "A: taking out a slider patch (two parallel triangles) from a patch collection.", "B: the patch is hold in front of a textile with two wires, the ironing tool is visible in the background.", "C: the patch is placed on the textile, aligned with the two existing traces and the ironing tool is put on.", "D: the patch is applied and the carrier and protection layer is getting removed.", "E: the applied patch is touched with a finger."], "caption": "Figure 3. A selection of different patch modules showing various pre-manufactured functional multi-layer patches (A) and the application of a small- sized capacitive slider (B-E).", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "hoto of different raw materials (all labeled). A: shows a generic transfer material. B: isolation material with a smooth surface. C: conductive material with a fine structure. D: piezoelectric material with a very dark look. E: resistive material, looking more rough.", "levels": null, "corpus_id": 218482526, "sentences": ["hoto of different raw materials (all labeled).", "A: shows a generic transfer material.", "B: isolation material with a smooth surface.", "C: conductive material with a fine structure.", "D: piezoelectric material with a very dark look.", "E: resistive material, looking more rough."], "caption": "Figure 4. Different raw materials can be used for various applications. For instance, a generic transfer (A) can iron-on other materials, iso- lations (B) can shield or hide circuits, while conductive (C), piezoelec- tric (D) and resistive (E) fabric can be utilized for traces and sensors.", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Photo collage showing how the RIO device works. A: Press; a hand presses the device and its spring block down; a short part of the applied trace tape is visible. B: Move; the device is still pressed down but was moved to the left; a bigger part of the applied trace tape is visible. C: Release, the hand lifts the spring block up.", "levels": null, "corpus_id": 218482526, "sentences": ["Photo collage showing how the RIO device works.", "A: Press; a hand presses the device and its spring block down; a short part of the applied trace tape is visible.", "B: Move; the device is still pressed down but was moved to the left; a bigger part of the applied trace tape is visible.", "C: Release, the hand lifts the spring block up."], "caption": "Figure 5. To create functional traces, the user has to press the material- loaded RIO device down (A), hold & move the device in the desired di- rection (B) and \ufb01nally release the iron to \ufb01nish (C).", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Photo collage of spools of different trace types. A: standard wires made of conductive fabric B : elastic multi-wire traces C: a shielded single-wire trace - a detail view shows the construction of the layer composition.", "levels": null, "corpus_id": 218482526, "sentences": ["Photo collage of spools of different trace types.", "A: standard wires made of conductive fabric B : elastic multi-wire traces C: a shielded single-wire trace - a detail view shows the construction of the layer composition."], "caption": "Figure 6. The rapid iron-on approach supports different types of traces, offering various design options regarding the number of wires, their thickness and stretchability.", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Photo collage for applying an angled connection patch. A: traces on fabrics. B: connector stencil is placed on existing traces. C: stencil is ironed with a small ironing tool. D: RIO device is put above the stencil. E: RIO device is moved across stencil. F: the result is shown; additional traces are now visible placed in a 90 degree angle. G: conductivity is shown by measuring the connection with a multimeter.", "levels": null, "corpus_id": 218482526, "sentences": ["Photo collage for applying an angled connection patch.", "A: traces on fabrics.", "B: connector stencil is placed on existing traces.", "C: stencil is ironed with a small ironing tool.", "D: RIO device is put above the stencil.", "E: RIO device is moved across stencil.", "F: the result is shown; additional traces are now visible placed in a 90 degree angle.", "G: conductivity is shown by measuring the connection with a multimeter."], "caption": "Figure 7. If a user wants to attach an angled connection to existing traces (A), a connector stencil (B) could be placed above and ironed- on (C). Afterwards, our RIO tool could be moved above the sten- cil (D+E). The front axis previews the tape\u2019s alignment. As a result (F) all traces are bridged building a high conductive (2.9\u03a9) connection (G).", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Photo of a zipper with ironed conductive traces on both sides of the zipper. On the upper side, a LilyPad microcontroller is visible and connected to the traces.", "levels": null, "corpus_id": 218482526, "sentences": ["Photo of a zipper with ironed conductive traces on both sides of the zipper.", "On the upper side, a LilyPad microcontroller is visible and connected to the traces."], "caption": "Figure 8. Zippers can be used to physically bridge fabrics with reversible connections.", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes", "pdf_hash": "6133ae6f9a606e21e18bc62d53866a69ea4657dc", "year": 2020, "venue": "CHI", "alt_text": "Photo collage of ironing a pressure-touch sensing matrix. A: sketching conductive trace rows with the RIO. B: appyling a piezoresistive layer with the RIO tool. C: sketching sensing columns traces. D: result with highlighted rows and columns in a non-rectangular shape.", "levels": null, "corpus_id": 218482526, "sentences": ["Photo collage of ironing a pressure-touch sensing matrix.", "A: sketching conductive trace rows with the RIO.", "B: appyling a piezoresistive layer with the RIO tool.", "C: sketching sensing columns traces.", "D: result with highlighted rows and columns in a non-rectangular shape."], "caption": "Figure 9. Pressure-Touch sensing matrix consisting of 4x4 electrodes and piezoresistive fabric.", "local_uri": ["6133ae6f9a606e21e18bc62d53866a69ea4657dc_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "pdf_hash": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "year": 2019, "venue": "IDC", "alt_text": "Screenshots of 3 mini-games in Cookie Monster's Challenge. Left: Whac-A-Mole style mini-game. It shows a finger is hovering over a dog in a hat and not over a cat in a hat. Middle: Button tapping mini-game. It shows Cookie Monster wearing orange cat ears and whiskers, a blue button, and a red button. Right: no touch pig mini-game. It shows a pink pig (cartoon) walking across the blue background.", "levels": null, "corpus_id": 174801549, "sentences": ["Screenshots of 3 mini-games in Cookie Monster's Challenge.", "Left: Whac-A-Mole style mini-game.", "It shows a finger is hovering over a dog in a hat and not over a cat in a hat.", "Middle: Button tapping mini-game.", "It shows Cookie Monster wearing orange cat ears and whiskers, a blue button, and a red button.", "Right: no touch pig mini-game.", "It shows a pink pig (cartoon) walking across the blue background."], "caption": "Figure 1: Three mini-games in PBS Kids and Sesame Workshop\u2019s Cookie Monster\u2019s Challenge that require inhibitory control.", "local_uri": ["beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_001.png"], "annotated": false, "compound": false}
{"title": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "pdf_hash": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "year": 2019, "venue": "IDC", "alt_text": "Shows a finger hovering over the red button in the button tapping mini-game. Shows a finger moving away (to the left) from the iPad and the red button in the button tapping mini-game, as an orange cartoon cat pops out on the side of the screen. Shows the button tapping mini-game with an orange cartoon cat popping out on the side of the screen. The child's finger is resting on the table next to the iPad.", "levels": null, "corpus_id": 174801549, "sentences": ["Shows a finger hovering over the red button in the button tapping mini-game.", "Shows a finger moving away (to the left) from the iPad and the red button in the button tapping mini-game, as an orange cartoon cat pops out on the side of the screen.", "Shows the button tapping mini-game with an orange cartoon cat popping out on the side of the screen.", "The child's finger is resting on the table next to the iPad."], "caption": "interactions between each child and the application, parents and the application, and child-parent or parent-child interac- tions (orientation, posture, gesture, talk, etc.). This analytical process resulted in 31 codes, with 46 sub-codes (e.g., par- ent: instruction guidance for child, changing wording of app; child: looking to parent for reassurance, physical). Here, we discuss seven overarching themes, regarding patterns of in-", "local_uri": ["beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_004.png"], "annotated": false, "compound": false}
{"title": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "pdf_hash": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "year": 2019, "venue": "IDC", "alt_text": "This shows a child holding the iPad on both sides with his left thumb on the screen, during the no touch pig mini-game. The pig is in the middle of the screen. His mother's hand is on his left wrist. Then, it shows a child holding the iPad on both sides with his left and right thumbs on the screen, during the no touch pig mini-game. The pig is almost off-screen, with only its behind showing on the right side. The child's right thumb is directly below it. Finally, it  shows a child holding the iPad on both sides with his left and right thumbs on the screen, during the no touch pig mini-game. The pig is almost completely off-screen, with only its tail showing on the right side. The child's right thumb is touching the pig.", "levels": null, "corpus_id": 174801549, "sentences": ["This shows a child holding the iPad on both sides with his left thumb on the screen, during the no touch pig mini-game.", "The pig is in the middle of the screen.", "His mother's hand is on his left wrist. Then, it shows a child holding the iPad on both sides with his left and right thumbs on the screen, during the no touch pig mini-game.", "The pig is almost off-screen, with only its behind showing on the right side.", "The child's right thumb is directly below it.", "Finally, it  shows a child holding the iPad on both sides with his left and right thumbs on the screen, during the no touch pig mini-game.", "The pig is almost completely off-screen, with only its tail showing on the right side.", "The child's right thumb is touching the pig."], "caption": "M", "local_uri": ["beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_005.png"], "annotated": false, "compound": false}
{"title": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "pdf_hash": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "year": 2019, "venue": "IDC", "alt_text": "Line graph comparing the standardized MEFS scores between children who played CMC and children who played DT from baseline to the short-term post-test. From baseline to short-term post-test, the CMC line goes up from about 0.4 to 0.5. From baseline to short-term post-test, the DT line goes slightly down from 0.2 to about -0.5.", "levels": [[1], [3, 2], [3, 2]], "corpus_id": 174801549, "sentences": ["Line graph comparing the standardized MEFS scores between children who played CMC and children who played DT from baseline to the short-term post-test.", "From baseline to short-term post-test, the CMC line goes up from about 0.4 to 0.5.", "From baseline to short-term post-test, the DT line goes slightly down from 0.2 to about -0.5."], "caption": "Mother: The pig. C26:    I will!", "local_uri": ["beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "pdf_hash": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "year": 2019, "venue": "IDC", "alt_text": "Left: Mockup of app screen. There is a timer bar at the top, completely full. Below the bar, there are three piles of dirt with three sprouts of grass growing out of them. Underneath this picture, it says \"Don't touch the grass; let it grow!\" Middle: Mockup of app screen. There is a timer bar at the top, a little more than halfway full. Below the bar, there are three piles of dirt with three plants growing out of them. There is a watering can on the side, with water coming out. Underneath this picture, it says \"Water and wait for the plants to grow!\" Right: Mockup of app screen. There is a timer bar at the top, almost empty. Below the bar, there are three trees with acorns on them. Underneath this picture, it says \"You waited, and the plants grew into trees! Now you can collect acorns.\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 174801549, "sentences": ["Left: Mockup of app screen.", "There is a timer bar at the top, completely full.", "Below the bar, there are three piles of dirt with three sprouts of grass growing out of them.", "Underneath this picture, it says \"Don't touch the grass; let it grow!\"", "Middle: Mockup of app screen.", "There is a timer bar at the top, a little more than halfway full.", "Below the bar, there are three piles of dirt with three plants growing out of them.", "There is a watering can on the side, with water coming out.", "Underneath this picture, it says \"Water and wait for the plants to grow!\"", "Right: Mockup of app screen.", "There is a timer bar at the top, almost empty.", "Below the bar, there are three trees with acorns on them.", "Underneath this picture, it says \"You waited, and the plants grew into trees!", "Now you can collect acorns.\""], "caption": "they might do so more efectively with more guidance. It would be useful to explore whether the system can scafold parents\u2019 scafolding [73] and guide parents toward support- ing children productively.", "local_uri": ["beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "pdf_hash": "454af1728ba52b16965f7d18fc9301d0ffb6d708", "year": 2020, "venue": "", "alt_text": "Left side show solar system with humans orbiting around Algorithms and AI. with arrow pointg to right side that should AI orbiting around humans.", "levels": null, "corpus_id": 226748434, "sentences": ["Left side show solar system with humans orbiting around Algorithms and AI. with arrow pointg to right side that should AI orbiting around humans."], "caption": "", "local_uri": ["454af1728ba52b16965f7d18fc9301d0ffb6d708_Image_020.jpg"], "annotated": false, "compound": false}
{"title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "pdf_hash": "454af1728ba52b16965f7d18fc9301d0ffb6d708", "year": 2020, "venue": "", "alt_text": "A table with Emulation Goal on the left and Appication Goal on the right. In the middle are combine designs. four ros are 1. Intelligent Agent and Powerful Tool 2. Simulated Teammate and Tele-Operated Device 3. Autonomous System and Supervisory Control 4. Humanoid Robot and Mechanical-like Appliance", "levels": null, "corpus_id": 226748434, "sentences": ["A table with Emulation Goal on the left and Appication Goal on the right. In the middle are combine designs.", "four ros are 1. Intelligent Agent and Powerful Tool 2. Simulated Teammate and Tele-Operated Device 3.", "Autonomous System and Supervisory Control 4. Humanoid Robot and Mechanical-like Appliance"], "caption": "", "local_uri": ["454af1728ba52b16965f7d18fc9301d0ffb6d708_Image_039.jpg"], "annotated": false, "compound": false}
{"title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "pdf_hash": "454af1728ba52b16965f7d18fc9301d0ffb6d708", "year": 2020, "venue": "", "alt_text": "robot hands shaking human hands, with big red X acros them showing this is not desirable", "levels": null, "corpus_id": 226748434, "sentences": ["robot hands shaking human hands, with big red X acros them showing this is not desirable"], "caption": "", "local_uri": ["454af1728ba52b16965f7d18fc9301d0ffb6d708_Image_040.jpg"], "annotated": false, "compound": false}
{"title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "pdf_hash": "454af1728ba52b16965f7d18fc9301d0ffb6d708", "year": 2020, "venue": "", "alt_text": "two images of human-like robots, with big red X acros them showing this is not desirable", "levels": null, "corpus_id": 226748434, "sentences": ["two images of human-like robots, with big red X acros them showing this is not desirable"], "caption": "Figure 7. Clich\u00e9-ridden images of humanoid robot hands and social robots", "local_uri": ["454af1728ba52b16965f7d18fc9301d0ffb6d708_Image_041.jpg"], "annotated": false, "compound": false}
{"title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "pdf_hash": "454af1728ba52b16965f7d18fc9301d0ffb6d708", "year": 2020, "venue": "", "alt_text": "three examples of control panels for dishwasher, clothes washer, and clothes dryer", "levels": null, "corpus_id": 226748434, "sentences": ["three examples of control panels for dishwasher, clothes washer, and clothes dryer"], "caption": "", "local_uri": ["454af1728ba52b16965f7d18fc9301d0ffb6d708_Image_045.jpg"], "annotated": false, "compound": false}
{"title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "pdf_hash": "454af1728ba52b16965f7d18fc9301d0ffb6d708", "year": 2020, "venue": "", "alt_text": "Three nest ovals: Innor one is for reliable systems in software engineering teams, the surrounding oval is safety culture in organization design for organizations, and theouter oval is trustworthy certification by external reviews within an industry", "levels": null, "corpus_id": 226748434, "sentences": ["Three nest ovals: Innor one is for reliable systems in software engineering teams, the surrounding oval is safety culture in organization design for organizations, and theouter oval is trustworthy certification by external reviews within an industry"], "caption": "Figure 9. Governance structures to guide teams, organizations, and industry leaders", "local_uri": ["454af1728ba52b16965f7d18fc9301d0ffb6d708_Image_047.jpg"], "annotated": false, "compound": false}
{"title": "Machinoia, Machine of Multiple Me: Integrating with Past, Future and Alternative Selves", "pdf_hash": "abe41e92397a2d3beb68b16ba0560a29b5b35ae5", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Alternative AI-generated versions of the wearer smiling, having a different age, and having a different gender.", "levels": null, "corpus_id": 233987574, "sentences": ["Alternative AI-generated versions of the wearer smiling, having a different age, and having a different gender."], "caption": "Figure 3: Alternative versions of the wearer generated by StyleGAN with feature specifc vectors applied to them.", "local_uri": ["abe41e92397a2d3beb68b16ba0560a29b5b35ae5_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Self-Calibrating Head-Mounted Eye Trackers Using Egocentric Visual Saliency", "pdf_hash": "8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5", "year": 2015, "venue": "UIST", "alt_text": "Figure 2: Example pictures of the eye tracker setup in this work. Video-based Pupil Pro eye tracker and EOG-based tracker using the TMSI Mobi6.", "levels": null, "corpus_id": 8930560, "sentences": ["Figure 2: Example pictures of the eye tracker setup in this work.", "Video-based Pupil Pro eye tracker and EOG-based tracker using the TMSI Mobi6."], "caption": "Figure 2: Left: video-based Pupil Pro eye tracker, right: EOG-based tracker based on the TMSI Mobi6.", "local_uri": ["8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5_Image_004.jpg", "8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Self-Calibrating Head-Mounted Eye Trackers Using Egocentric Visual Saliency", "pdf_hash": "8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5", "year": 2015, "venue": "UIST", "alt_text": "Figure 6: Example eye video frames showing a large position drift. The observed eye position is clearly different after recording sessions.", "levels": [[-1], [-1]], "corpus_id": 8930560, "sentences": ["Figure 6: Example eye video frames showing a large position drift.", "The observed eye position is clearly different after recording sessions."], "caption": "Eye video frame at session V0      (b) Eye video frame at session V2Figure 6: Example eye video frames of the participant with the largest calibration drift.ing the running average of the input EOG signal. Blinks were removed in the same manner as described in Bulling et al. [8].Drift CharacteristicsThe box plots in Figure 5 provide an overview of the calibra\u00ad tion drift for all 20 participants for the video-based (5a) and EOG-based (5b) eye tracker for the three validation sessions (V0, V1, V2) and two recording sessions (R1, R2).As can be seen from Figure 5a, the gaze estimation error clearly increased over the course of the recording. While at V0 the mean error was 3.2\u00b0 (SD=1.3\u00b0), in V2 the max\u00ad imum estimation error reached up to 40 degrees (M=10.1\u00b0, SD=9.2\u00b0). Figure 7 shows a more detailed analysis for the video-based tracker. We divided participants into two groups according to the error at V2. A participant is categorised into the calibration drifted group if the error was outside the 95% con\ufb01dence interval of the initial error distribution at V0, while the others become the calibration maintained group. 12 out of 20 participants showed calibration drift which in most cases increased gradually at a more or less constant rate. This error is likely caused by shifts of the eye tracking headset and thus pupil positions (see Figure 6 for an example). More signi\ufb01cant drift occurred in the top two cases and was likely caused by external factors, such as users touching the headset.Figure 5b shows the corresponding results for the EOG-based eye tracker. In this case, the \ufb01rst validation session (V0) is used to calibrate the baseline estimator. We take a linearFigure 7: Time series variations of estimation errors of the video-based setting. Video-based setting                        (b) EOG-based setting", "local_uri": ["8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5_Image_012.jpg", "8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5_Image_013.jpg"], "annotated": false, "compound": true}
{"title": "Self-Calibrating Head-Mounted Eye Trackers Using Egocentric Visual Saliency", "pdf_hash": "8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5", "year": 2015, "venue": "UIST", "alt_text": "Figure 9: A diagram of the self-calibration method. The self-calibration process consists of two sequential steps, the aggregation step and the mapping step. The saliency maps are clustered according to the similarity between associated input features in the aggregation step, and the relationship between the input feature space and gaze coordinate space is established in the mapping step.", "levels": null, "corpus_id": 8930560, "sentences": ["Figure 9: A diagram of the self-calibration method.", "The self-calibration process consists of two sequential steps, the aggregation step and the mapping step.", "The saliency maps are clustered according to the similarity between associated input features in the aggregation step, and the relationship between the input feature space and gaze coordinate space is established in the mapping step."], "caption": "Aggregation", "local_uri": ["8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Demonstration of JetController: High-speed Ungrounded Force Feedback Controllers Using Air Propulsion Jets", "pdf_hash": "89e2fdadc30740d5621ee09d5e2a05762e7df2d4", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The bar charts show the maximum frequencies under different tubing configurations and different force magnitude.", "levels": [[1]], "corpus_id": 233987764, "sentences": ["The bar charts show the maximum frequencies under different tubing configurations and different force magnitude."], "caption": "Figure 3: Maximum impulse frequency at diferent force magnitudes for diferent tubing sizes and tubing lengths, cal- culated based on force rise time and fall time. (The 4.0N fre- quency is unavailable for 6mm x 250cm tubing because it could only achieve a maximum force of 3.3N.)", "local_uri": ["89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Demonstration of JetController: High-speed Ungrounded Force Feedback Controllers Using Air Propulsion Jets", "pdf_hash": "89e2fdadc30740d5621ee09d5e2a05762e7df2d4", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figure contains 4 different haptic events in our demonstration, and each of them has 3 different force feedback techniques to simulate the haptic event.", "levels": null, "corpus_id": 233987764, "sentences": ["The figure contains 4 different haptic events in our demonstration, and each of them has 3 different force feedback techniques to simulate the haptic event."], "caption": "Driving on grassy roads", "local_uri": ["89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_005.jpg", "89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_006.jpg", "89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_007.jpg", "89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_008.jpg"], "annotated": false, "compound": true}
{"title": "Demonstration of JetController: High-speed Ungrounded Force Feedback Controllers Using Air Propulsion Jets", "pdf_hash": "89e2fdadc30740d5621ee09d5e2a05762e7df2d4", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The figure contains two force curves, 50Hz full impulses at 1.0N under 6mm x 100cm tubing and 20Hz full impulses at 4.0N under 8mm x 100cm tubing.", "levels": null, "corpus_id": 233987764, "sentences": ["The figure contains two force curves, 50Hz full impulses at 1.0N under 6mm x 100cm tubing and 20Hz full impulses at 4.0N under 8mm x 100cm tubing."], "caption": "Figure 4: Actual force curve measurements of JetController, showing 50Hz of full impulses at 1.0N (6mm x 100cm tubing) and 20Hz of full impulses at 4.0N (8mm x 100cm tubing).", "local_uri": ["89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_009.png"], "annotated": false, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "The palm of the non-dominant hand is placed on a rectangular pin display slightly larger than the palm. The pins protrude from the display to stimulate the palm. In Shoot 'em Up, two (T-shaped) enemy planes are facing the (reverse T-shaped) player's aircraft. In Whack-A-Mole, two moles are each represented by a single protruding pin. The index and middle fingers of the dominant hand are touching the back of the non-dominant hand to select the two moles simultaneously. In Multi-player Pong, two vertical columns (one on the left side and one on the right side of the palm) are indicating the field of the game. Two paddles and one ball are each displayed by a protruding pin on each corresponding location. In Rhythm game, a horizontal line is displayed near the bottom of the palm while each target location is indicated by a single pin.", "levels": null, "corpus_id": 233987866, "sentences": ["The palm of the non-dominant hand is placed on a rectangular pin display slightly larger than the palm.", "The pins protrude from the display to stimulate the palm.", "In Shoot 'em Up, two (T-shaped) enemy planes are facing the (reverse T-shaped) player's aircraft.", "In Whack-A-Mole, two moles are each represented by a single protruding pin.", "The index and middle fingers of the dominant hand are touching the back of the non-dominant hand to select the two moles simultaneously.", "In Multi-player Pong, two vertical columns (one on the left side and one on the right side of the palm) are indicating the field of the game.", "Two paddles and one ball are each displayed by a protruding pin on each corresponding location.", "In Rhythm game, a horizontal line is displayed near the bottom of the palm while each target location is indicated by a single pin."], "caption": "Figure 1: ThroughHand interaction and applications. (a) The user recognizes the location and characteristics of multiple ob- jects on a 2D tactile display using the palm of the non-dominant hand. Simultaneously, the user inputs by touching the corre- sponding location on the back of the non-dominant hand with fngers of the dominant hand. (b) Multi-touch Whack-A-Mole,", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_001.png"], "annotated": false, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "The main board is a large rectangular board that includes an arduino board (bottom) and circuitry for data (left) and power (right) transfer. Card boards are smaller rectangular boards that can be attached to the main board. A card board includes twelve linear servo motors that each drive an acrylic pin. The display is assembled by attaching the card boards on the main board and fastening an acrylic cover on top to only reveal a rectangular area of pins. On the top left corner of the cover is an optical marker used to track the location of the display. Textured stickers are attached on pins located at the tips of the thumb, middle finger, and little finger of the non-dominant hand. The user participates in the experiment by placing the non-dominant hand on top of the pin display while seated. An optical marker is attached on the nail of the index finger of the dominant hand. Cameras for optical tracking are fixed above the display. The experimenter inspects the status of the pin display through the monitor screen.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 233987866, "sentences": ["The main board is a large rectangular board that includes an arduino board (bottom) and circuitry for data (left) and power (right) transfer.", "Card boards are smaller rectangular boards that can be attached to the main board.", "A card board includes twelve linear servo motors that each drive an acrylic pin.", "The display is assembled by attaching the card boards on the main board and fastening an acrylic cover on top to only reveal a rectangular area of pins.", "On the top left corner of the cover is an optical marker used to track the location of the display.", "Textured stickers are attached on pins located at the tips of the thumb, middle finger, and little finger of the non-dominant hand.", "The user participates in the experiment by placing the non-dominant hand on top of the pin display while seated.", "An optical marker is attached on the nail of the index finger of the dominant hand.", "Cameras for optical tracking are fixed above the display.", "The experimenter inspects the status of the pin display through the monitor screen."], "caption": "", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_009.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "The study consisted of 3 parts and an interview at the end. Part 1 lasted 25 minutes and consisted of an introduction, optical tracker calibration, and the pointing accuracy task. Part 2 took 25 minutes and involved target selection task, shape recognition task, and moving target selection task. Part 3 consisted of playing the ThroughHand games (Whack-A-Mole, Shoot 'em Up, Multi-player Pong, and Rhythm game) for 40 minutes. The semi-structured interview took about 10 minutes.", "levels": null, "corpus_id": 233987866, "sentences": ["The study consisted of 3 parts and an interview at the end.", "Part 1 lasted 25 minutes and consisted of an introduction, optical tracker calibration, and the pointing accuracy task.", "Part 2 took 25 minutes and involved target selection task, shape recognition task, and moving target selection task.", "Part 3 consisted of playing the ThroughHand games (Whack-A-Mole, Shoot 'em Up, Multi-player Pong, and Rhythm game) for 40 minutes.", "The semi-structured interview took about 10 minutes."], "caption": "", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_010.png"], "annotated": false, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "On the far left is a bar graph of the number of perceivable pins for each participant. All participants were able to perceive more pins in the lower area of the palm. On the center left is a diagram of the pin display showing the locations of pins that participants could recognize. The perceivable pins produce a shape similar to the left palm. Pins on the edges of the shape could only be perceived by participants with large hands while pins in the inside could be recognized by most (more than seven) participants. On the center right is a bar graph of the distance errors for each participant. P5 had the highest error of approximately 19~mm while P3 had the lowest error of approximately 10~mm. On the far right is a diagram of the pin display showing the average error for each perceivable pin. The error is lower towards the fingertips while it is highest at the bottom of the palm.", "levels": [[1], [3], [1], [3], [3], [1], [2], [1], [3]], "corpus_id": 233987866, "sentences": ["On the far left is a bar graph of the number of perceivable pins for each participant.", "All participants were able to perceive more pins in the lower area of the palm.", "On the center left is a diagram of the pin display showing the locations of pins that participants could recognize.", "The perceivable pins produce a shape similar to the left palm.", "Pins on the edges of the shape could only be perceived by participants with large hands while pins in the inside could be recognized by most (more than seven) participants.", "On the center right is a bar graph of the distance errors for each participant.", "P5 had the highest error of approximately 19~mm while P3 had the lowest error of approximately 10~mm.", "On the far right is a diagram of the pin display showing the average error for each perceivable pin.", "The error is lower towards the fingertips while it is highest at the bottom of the palm."], "caption": "", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_011.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "On the left are the six shapes used in shape recognition task: a single pin, two pins in horizontal orientation, two pins in vertical orientation, a square with width and height of two pins, four pins in a reverse T shape (three pins on the top row and one pin on the bottom row), and four pins in a T shape (three pins on the bottom row and one pin on the top row). On the right is the layout of the pin display during the moving target selection task.", "levels": null, "corpus_id": 233987866, "sentences": ["On the left are the six shapes used in shape recognition task: a single pin, two pins in horizontal orientation, two pins in vertical orientation, a square with width and height of two pins, four pins in a reverse T shape (three pins on the top row and one pin on the bottom row), and four pins in a T shape (three pins on the bottom row and one pin on the top row).", "On the right is the layout of the pin display during the moving target selection task."], "caption": "", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_012.png"], "annotated": false, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "On the left is a bar graph of the average selection times for each participant. The maximum value is about 1150~ms (P2) while the minimum value is about 700~ms (P9). On the right is a histogram of the selection times of all participants. Most selections occurred between 500~ms and 800~ms. However, there were about 30 selections that took over 1500~ms.", "levels": [[1], [2], [1], [2], [2]], "corpus_id": 233987866, "sentences": ["On the left is a bar graph of the average selection times for each participant.", "The maximum value is about 1150~ms (P2) while the minimum value is about 700~ms (P9).", "On the right is a histogram of the selection times of all participants.", "Most selections occurred between 500~ms and 800~ms.", "However, there were about 30 selections that took over 1500~ms."], "caption": "Figure 7: Results of shape recognition task: (a) Average ac- curacy, (b) stimulus-response confusion matrix. (Error bar represents standard error)", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_013.png", "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_014.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": true}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "Time error graph of the moving target selection task. Most participants produced shorter time errors for faster target speeds while P2 produced a constant error of approximately 200~ms regardless of the target speed.", "levels": [[1], [2]], "corpus_id": 233987866, "sentences": ["Time error graph of the moving target selection task.", "Most participants produced shorter time errors for faster target speeds while P2 produced a constant error of approximately 200~ms regardless of the target speed."], "caption": "Figure 8: Results of the moving target selection task (error bars represent standard error).", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_015.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "pdf_hash": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "year": 2021, "venue": "CHI", "alt_text": "On the far left is a bar graph of the Whack-A-Mole scores for each participant. The highest score was about 90 points (P10) while the lowest was about 60 points (P7). On the center left is a bar graph of the number of planes destroyed and number of collisions in Shoot 'em Up. All participants destroyed more than 13 enemy planes. On the center is a bar graph of the number of rallies in Multi-player Pong for each participant. P7 and P8 performed around 9 rallies on average, while P9 performed less than 2 rallies on average. On the center right is a bar graph of the Rhythm game scores for each participant. On the far right is a scatter plot of Rhythm game score against moving target selection error. The Rhythm game scores (excluding that of P1) were analyzed to show that participants with lower time error in the moving target selection task (study 2) scored higher in Rhythm game.", "levels": [[1], [2], [1], [2], [1], [2], [1], [1], [3]], "corpus_id": 233987866, "sentences": ["On the far left is a bar graph of the Whack-A-Mole scores for each participant.", "The highest score was about 90 points (P10) while the lowest was about 60 points (P7).", "On the center left is a bar graph of the number of planes destroyed and number of collisions in Shoot 'em Up.", "All participants destroyed more than 13 enemy planes.", "On the center is a bar graph of the number of rallies in Multi-player Pong for each participant.", "P7 and P8 performed around 9 rallies on average, while P9 performed less than 2 rallies on average.", "On the center right is a bar graph of the Rhythm game scores for each participant.", "On the far right is a scatter plot of Rhythm game score against moving target selection error.", "The Rhythm game scores (excluding that of P1) were analyzed to show that participants with lower time error in the moving target selection task (study 2) scored higher in Rhythm game."], "caption": "", "local_uri": ["3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_016.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Characterizing Human vs. Automated Coaching: Preliminary Results", "pdf_hash": "3dad3a747463969ccf7aa054e005afa71bb3d2a9", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "This violin plot shows a similar side-by-side distribution of the response rate to messages between the \"human coach\" and \"wizard-of-oz\" groups as in Figure 3. On the left, the density for the \"human coach\" group centered on a higher point, the mean of 71%. On the right, the plot for the \"woz\" group is relatively unchanged from how it appeared in Figure 3, with the mass of the distribution centered on the mean response rate of 54%.", "levels": [[1], [2, 1], [2, 1]], "corpus_id": 218482557, "sentences": ["This violin plot shows a similar side-by-side distribution of the response rate to messages between the \"human coach\" and \"wizard-of-oz\" groups as in Figure 3.", "On the left, the density for the \"human coach\" group centered on a higher point, the mean of 71%.", "On the right, the plot for the \"woz\" group is relatively unchanged from how it appeared in Figure 3, with the mass of the distribution centered on the mean response rate of 54%."], "caption": "", "local_uri": ["3dad3a747463969ccf7aa054e005afa71bb3d2a9_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Exploring the Potential of Apple Face ID as a Drag, Queer and Trans Technology Design Tool", "pdf_hash": "0c87789d0bdb7adc19fd5cf43a7726dee408b495", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "Extracted image from Apple Face ID set up process. It shows a user wearing a facial prosthesis, training an validating a new identity.", "levels": null, "corpus_id": 235662866, "sentences": ["Extracted image from Apple Face ID set up process.", "It shows a user wearing a facial prosthesis, training an validating a new identity."], "caption": "Figure 1: Apple Face ID setting up a new identity.", "local_uri": ["0c87789d0bdb7adc19fd5cf43a7726dee408b495_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Exploring the Potential of Apple Face ID as a Drag, Queer and Trans Technology Design Tool", "pdf_hash": "0c87789d0bdb7adc19fd5cf43a7726dee408b495", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "The image contains seventeen selfies in order to show that some angles and volumes between different users\u2019 prostheses from the same facial features, had non-identical verification accuracy. The image is divided in two. On the left side, the ones with higher accuracy. On the right side, the ones with lower accuracy.", "levels": null, "corpus_id": 235662866, "sentences": ["The image contains seventeen selfies in order to show that some angles and volumes between different users\u2019 prostheses from the same facial features, had non-identical verification accuracy.", "The image is divided in two.", "On the left side, the ones with higher accuracy.", "On the right side, the ones with lower accuracy."], "caption": "Figure 7: Some angles and volumes between different users\u2019 prostheses from the same facial features, had non-identical verification accuracy. On the left side, the ones with higher accuracy. On the right side, the ones with lower accuracy.", "local_uri": ["0c87789d0bdb7adc19fd5cf43a7726dee408b495_Image_044.jpg", "0c87789d0bdb7adc19fd5cf43a7726dee408b495_Image_045.jpg"], "annotated": false, "compound": true}
{"title": "Exploring the Potential of Apple Face ID as a Drag, Queer and Trans Technology Design Tool", "pdf_hash": "0c87789d0bdb7adc19fd5cf43a7726dee408b495", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "Four images are representing documented dynamics, methodologies and experiences during the workshop. \n- Image 1: PLA prosthesis on the table.\n- Image 2: A participant is interviewed after the workshop, showing it's designed prosthesis and table of content.\n- Image 3: Participant in the process of training prosthesis, setting up Apple Face ID.\n- Image 4: Participant in the process of training a prosthesis, setting up Apple Face ID.", "levels": null, "corpus_id": 235662866, "sentences": ["Four images are representing documented dynamics, methodologies and experiences during the workshop. \n- Image 1: PLA prosthesis on the table.\n- Image 2: A participant is interviewed after the workshop, showing it's designed prosthesis and table of content.", "- Image 3: Participant in the process of training prosthesis, setting up Apple Face ID.\n- Image 4: Participant in the process of training a prosthesis, setting up Apple Face ID."], "caption": "", "local_uri": ["0c87789d0bdb7adc19fd5cf43a7726dee408b495_Image_046.jpg"], "annotated": false, "compound": false}
{"title": "Exploring the Potential of Apple Face ID as a Drag, Queer and Trans Technology Design Tool", "pdf_hash": "0c87789d0bdb7adc19fd5cf43a7726dee408b495", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "8 images represent different approaches from participants into the creation of their prosthesis. The images are extracted from their own iPhone during the Apple Face ID set up process. 4 images are examples of validated new identities. 4 images are examples of rejected identities.", "levels": null, "corpus_id": 235662866, "sentences": ["8 images represent different approaches from participants into the creation of their prosthesis.", "The images are extracted from their own iPhone during the Apple Face ID set up process.", "4 images are examples of validated new identities.", "4 images are examples of rejected identities."], "caption": "Figure 8: Documented dynamics, methodologies and experiences during the workshop.       Figure 9: Participants free exploration of prosthesis. An example of validated and", "local_uri": ["0c87789d0bdb7adc19fd5cf43a7726dee408b495_Image_047.jpg"], "annotated": false, "compound": false}
{"title": "Exploring the Potential of Apple Face ID as a Drag, Queer and Trans Technology Design Tool", "pdf_hash": "0c87789d0bdb7adc19fd5cf43a7726dee408b495", "year": 2021, "venue": "Conference on Designing Interactive Systems", "alt_text": "Three images of Ignacio wearing her prosthesis while walking on the street, talking on the phone and walking the dog.", "levels": null, "corpus_id": 235662866, "sentences": ["Three images of Ignacio wearing her prosthesis while walking on the street, talking on the phone and walking the dog."], "caption": "", "local_uri": ["0c87789d0bdb7adc19fd5cf43a7726dee408b495_Image_054.jpg"], "annotated": false, "compound": false}
{"title": "SketCHI 3.0: Hands-On Special Interest Group on Sketching Education in HCI", "pdf_hash": "803c0407224e84ce6c2ade9ba1b7f188bde0a506", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "A sketch drawn in pink pen depicting people outside a deli. There are four figures in the sketch. Above the deli are large windows.", "levels": null, "corpus_id": 218483034, "sentences": ["A sketch drawn in pink pen depicting people outside a deli.", "There are four figures in the sketch.", "Above the deli are large windows."], "caption": "SketCHI 3.0: Hands-On Special Interest Group on Sketching Education in HCI", "local_uri": ["803c0407224e84ce6c2ade9ba1b7f188bde0a506_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "SketCHI 3.0: Hands-On Special Interest Group on Sketching Education in HCI", "pdf_hash": "803c0407224e84ce6c2ade9ba1b7f188bde0a506", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "A table hosts a collection of sketched responses to the conference environment. The image is too small to see the individual sketches.", "levels": null, "corpus_id": 218483034, "sentences": ["A table hosts a collection of sketched responses to the conference environment.", "The image is too small to see the individual sketches."], "caption": "Fig. 2: Collected SIG sketches from CHI 2019", "local_uri": ["803c0407224e84ce6c2ade9ba1b7f188bde0a506_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "SketCHI 3.0: Hands-On Special Interest Group on Sketching Education in HCI", "pdf_hash": "803c0407224e84ce6c2ade9ba1b7f188bde0a506", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Nicolai Marquardt, researcher, stands in front of three flipcharts covered in post it notes and written comments on paper templates.", "levels": [[-1]], "corpus_id": 218483034, "sentences": ["Nicolai Marquardt, researcher, stands in front of three flipcharts covered in post it notes and written comments on paper templates."], "caption": "Fig. 3: Combining/discussing attendee thoughts on the role of sketching in HCI (CHI 2019)", "local_uri": ["803c0407224e84ce6c2ade9ba1b7f188bde0a506_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Un-authorised View: Leveraging Volunteer Expertise in Heritage", "pdf_hash": "b305669a3f006e63d17b21f103a0601bf9dcda46", "year": 2020, "venue": "CHI", "alt_text": "A group of four images labelled a), b), c) and d) illustrating the cultural probes. Image a) shows a transparent plastic envelope which holds three empty probe cards. Image b) shows laid out completed probe cards by different participants. Image c) shows two completed probe cards in a close up. A free letter card is positioned above of a postcard. Image d) shows the full set of three probe cards completed by the same participant. A gift card probe is open at the top; free letter card is at the bottom left, and postcard probe positioned at the bottom right side of the image.", "levels": null, "corpus_id": 218483001, "sentences": ["A group of four images labelled a), b), c) and d) illustrating the cultural probes.", "Image a) shows a transparent plastic envelope which holds three empty probe cards.", "Image b) shows laid out completed probe cards by different participants.", "Image c) shows two completed probe cards in a close up.", "A free letter card is positioned above of a postcard.", "Image d) shows the full set of three probe cards completed by the same participant.", "A gift card probe is open at the top; free letter card is at the bottom left, and postcard probe positioned at the bottom right side of the image."], "caption": "Figure 1. a). The probe pack; b). Various completed probes; c). Completed postcard and free letter card; d). A completed pack", "local_uri": ["b305669a3f006e63d17b21f103a0601bf9dcda46_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Four images arranged horizontally each showing a colored gapped circle composed of dots on a grey background also composed of dots. Gapped circle color from left to right: red, magenta, blue, white.", "levels": null, "corpus_id": 201036718, "sentences": ["Four images arranged horizontally each showing a colored gapped circle composed of dots on a grey background also composed of dots.", "Gapped circle color from left to right: red, magenta, blue, white."], "caption": "Figure 1: CDT stimuli: four stills of gapped circles at max\u00ad imum difference from background in the computerized color differentiation test by Flatla and Gutwin.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Plot of how mean discrimination ellipsoid volume (dependent) varies across eight different lighting ratios (independent). As lighting ratio increases (i.e., the room gets brighter or the screen gets darker), ellipsoid volume increases (i.e., participant color differentiation abilities decrease).", "levels": [[1], [3]], "corpus_id": 201036718, "sentences": ["Plot of how mean discrimination ellipsoid volume (dependent) varies across eight different lighting ratios (independent).", "As lighting ratio increases (i.e., the room gets brighter or the screen gets darker), ellipsoid volume increases (i.e., participant color differentiation abilities decrease)."], "caption": "Figure 2: Mean discrimination ellipsoid volumes (in CIE L*u*v* units3) \u00b1 s.e. for each of eight lighting ratios.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Screenshot of WebCDT test in our LabintheWild deployment. Shows a white gapped circle on a square grey background. The square is surrounded by input buttons (labelled with icons showing different orientations of gapped circles) for participants to input the orientation of the gapped circle. Also shows the \"I can't tell\" button for when the orientation is not visible.", "levels": null, "corpus_id": 201036718, "sentences": ["Screenshot of WebCDT test in our LabintheWild deployment.", "Shows a white gapped circle on a square grey background.", "The square is surrounded by input buttons (labelled with icons showing different orientations of gapped circles) for participants to input the orientation of the gapped circle.", "Also shows the \"I can't tell\" button for when the orientation is not visible."], "caption": "", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Histogram plot of mean discrimination volumes (bin size = 250) for our participants. Rises sharply from 0-250 (over 500 participants) to a peak at 500-750 (over 3000 participants). Histogram then falls off with a very long tail. Minimum volume is 21.68, 25th quartile is at volume 804.62, 50th quartile (median) is at volume 1558.38, 75th quartile is at volume 3223.60, maximum volume is 1058397.75. Histogram is cut off above volumes of 12000.", "levels": [[1], [3, 2], [3], [2], [2]], "corpus_id": 201036718, "sentences": ["Histogram plot of mean discrimination volumes (bin size = 250) for our participants.", "Rises sharply from 0-250 (over 500 participants) to a peak at 500-750 (over 3000 participants).", "Histogram then falls off with a very long tail.", "Minimum volume is 21.68, 25th quartile is at volume 804.62, 50th quartile (median) is at volume 1558.38, 75th quartile is at volume 3223.60, maximum volume is 1058397.75.", "Histogram is cut off above volumes of 12000."], "caption": "Figure 4: Histogram of number of participants versus mean discrimination ellipsoid volumes. Participants with ellipsoid volumes above 12,000 are not shown for space reasons.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Plots of lower and upper estimates of the number of unique differentiable colors (independent) versus discrimination ellipsoid volume (dependent). Plots start very high (above 20000 unique colors), fall quickly as ellipsoid volume increases, and then level out into a long-tail. At volume=1000, upper estimate is about 10000 colors and the lower estimate is about 5000 colors; at volume=2000, upper=5000 and lower=2000; at volume 4000, upper=3000 and lower=1000.", "levels": [[1], [3, 2], [2]], "corpus_id": 201036718, "sentences": ["Plots of lower and upper estimates of the number of unique differentiable colors (independent) versus discrimination ellipsoid volume (dependent).", "Plots start very high (above 20000 unique colors), fall quickly as ellipsoid volume increases, and then level out into a long-tail.", "At volume=1000, upper estimate is about 10000 colors and the lower estimate is about 5000 colors; at volume=2000, upper=5000 and lower=2000; at volume 4000, upper=3000 and lower=1000."], "caption": "Figure 5: Estimate of the number of unique differentiable col\u00ad ors versus discrimination ellipsoid volume. Minimum and maximum values for color gamut based on ellipsoid shape are shown as green and blue lines respectively. Vertical lines show positions of the mean and the \ufb01rst standard deviation for our participant population.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_009.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Demonstration of ColorCheck's masking. Two images of a kitten sniffing a flower, original on left and masked on right. Masked image shows color pairs that are not differentiable by 15% of the population. A large portion of the background foliage and flowers are masked, however the kitten and some bright pink flowers are unmasked (still differentiable).", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 201036718, "sentences": ["Demonstration of ColorCheck's masking.", "Two images of a kitten sniffing a flower, original on left and masked on right.", "Masked image shows color pairs that are not differentiable by 15% of the population.", "A large portion of the background foliage and flowers are masked, however the kitten and some bright pink flowers are unmasked (still differentiable)."], "caption": "Original image                  (b) Color pairs that are not differen\u00adtiable by 15% of the population are masked.Figure 6: ColorCheck creates masked images, which black out color pairs that are not differentiable by a certain percent\u00ad age of the population.By using a color differentiation model collected from a broad population (such as the ellipsoid volumes computed from our cleaned dataset of 23,120 participants), ColorCheck is able to measure the proportion of the population that is unable to differentiate any pair of image colors. ColorCheck identi\ufb01es the ratio of pixels within the input image that are not differ\u00ad entiable. The output of this process is a data \ufb01le containing a two dimensional description of color differentiability for a given proportion of the population (e.g., one value in this de\u00ad scription might be that 60% of a given image\u2019s colors cannot be distinguished by 20% of the population).In addition to this broad understanding about how an image may be seen by a population, ColorCheck is able to layer a mask (shown as black pixels) on top of an image which shows the color differentiability for a speci\ufb01c part of the population. Using this, designers can easily identify the areas of an image that contain problematic color pairs. This is demonstrated in Figure 6: ColorCheck takes an original image (Figure 6a) and blacks out those pixels that are not differentiable by a given percentage of the population (Figure 6b).Note that these masked images are not intended to show how an image would be perceived by the population \u2013 this would be a dif\ufb01cult task given the variability of color perceptionwithin the population. Instead, ColorCheck allows design\u00ad ers to see if the critical parts of their images are perceivable (in terms of color differentiability) by their target population: Color-coded regions that are blacked out usually suggest that users lose important visual cues to understand data. If adja\u00ad cent colors are black, users will be unable to separate content regions. Finally, if large parts of an image are set to black, de\u00ad signers can assume that users will be unable to decipher the website or image\u2019s message, and aesthetic appreciation of the image could be signi\ufb01cantly altered.In practice, we expect that designers would view the same image with various differentiability thresholds, for example that of 50% or 80%. In the images generated by ColorCheck, all color pairs that are not differentiable by 50% and 20% of the population (respectively) will have been set to black. A designer can then use this information to decide whether the masked parts are essential for interpreting the content. If yes, they may decide to recolor critical areas, such as by choosing colors that are further apart in the color space.In its current form, ColorCheck leaves the decision which color pairs need to be differentiable to the designer. This decision was made because of the dif\ufb01culty to computation\u00ad ally determine the intentions of designers, and in particu\u00ad lar, which colors designers necessarily require to be differen\u00ad tiable. To support designers in this process, ColorCheck pro\u00ad vides a batch mode for processing 0-100% of the population, generating a separate masked image for each percentage. By visually scanning each image in increasing percentage order, pairs of problem colors can be identi\ufb01ed when both regions of the image become masked at the same percentage level.As ColorCheck relies on a data set of ellipsoid volumes, the output currently shows which parts of an image are not differ\u00ad entiable by our web-based participant population. This sam\u00ad ple might not be representative of the average computer user, and likely does not balance usage among situational lighting conditions and devices.  Our experiment therefore remains online; we plan to regularly update the tool with the result\u00ad ing data to achieve more representative forecasts of users\u2019 ability to differentiate colors. In addition, ColorCheck users can plug in their own data sets (e.g., the ellipsoid volumes of their target group obtained through the use of WebCDT). It is also possible to reduce the input data set to a speci\ufb01c popu\u00ad lation of interest based on demographics, device and monitorFigure 7: Mean proportion of image pixels differentiable for different percentages of the population.To estimate how many images in our datasets contain col\u00ad ors that are non-differentiable for different percentages of our participant population, we calculated the average proportion of differentiable colors within each image dataset (across all websites, and across all infographics, respectively). The re\u00ad sults of this analysis shows that 12% of our population can differentiate all colors (100% of pixels) in the websites and infographics in our dataset (see Figure 7). These are par\u00ad ticipants whose ellipsoid volumes were below 500 (see also Figure 4 for the distribution of ellipsoid volumes). Ellipsoid volumes larger than 500 result in a steady loss of differentia\u00ad bility in our image datasets. Roughly half of the population (52%) is unable to differentiate 10% of the colors in an aver\u00ad age website or infographic. As we increase the proportion of the population we are targeting, the mean differentiability fur\u00ad ther decreases to a near-plateau between 90% and 99% of the population differentiating just under 60% of website content and just over 40% of infographic content. One of the rea\u00ad sons for infographics to perform more poorly than websites is that infographics often use homogeneous color schemes, so neighboring colors vary only in intensity. We also observe a sharp decline when targeting more than 99% of the popu\u00ad lation, which are people with severe color differentiation dif\u00ad \ufb01culties in our dataset (i.e., those whose test data resulted in extremely high ellipsoid volume levels).Table 2: Comparison of situational lighting conditions and demographics of the 10% participants with the worst color differentiability, and the remaining 90%. Tests of the equality of two proportions report on Pearson\u2019s chi-squared test.Factor      Worst 10%   Top 90%            StatisticsOutdoors     30.05%     23.14%       \u03c72 = 5.38, p < .05settings, situational lighting conditions, or color vision de\ufb01\u00ad(1,20420)Monitor     M=62.81,    M=68.22,      t               \u22121 97 p     05ciency (e.g., users over a certain age or those on mobile de\u00adbrightness    SD=27.96   SD=29.10(106.03) =. , < .vices). Designers can sub-sample our open-access dataset, orAmbientM=53.81,M=47.10,t(142.93) = 3.50, p < .001use WebCDT to generate their own custom dataset.Color Differentiability in Websites and InfographicsWe employed ColorCheck to evaluate the level of differenti\u00ad ation between two example image datasets: (1) 450 website screenshots, selected to represent a range of domains and to vary in colorfulness, and (2) 3,000 infographics from the on- line community visual.ly.66These datasets have been previously published in [33] and [18] and were used with permission. \u00a0 brightness \u00a0 \u00a0SD=22.84  \u00a0SD=23.79 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Age         M=40.29,    M=30.21,     t(111.04) = 5.27, p < .0001(2,23120) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0SD=20.10   SD=14.83 \u00a0 \u00a0Male 37.39% 26.58% \u03c72 = 13.64, p < .01(1,23120) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 \u00a0 \u00a0CVD 23.04% 2.24% \u03c72 = 413.50, p < .0001But who are the 10% of participants that are most severely affected by a lack of color differentiability in websites and infographics? From our earlier analysis, we know that par\u00ad ticipants who reported being in brighter surroundings, and those who had set their monitors to lower brightness values performed worse. We also saw that being older, male, and  Original websiteColors pairs that are not differentiable by 20%      (c) Colors pairs that are not differentiable by 10% of the population have been set to black.                     of the population have been set to black.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_012.jpg", "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_013.jpg"], "annotated": false, "compound": true}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Plot of mean proportion of image pixels differentiable (independent) for 0% - 100% of the population (dependent) for websites and infographics. Increasing from 0% of the population, both plots start at 100% differentiable and gradually fall to 80% differentiable at 75% of the population. Plots begin to diverge here as they both fall off more quickly until a discontinuity plateau is reached at 88% of the population (websites = 60% differentiable, infographics = 50% differentiable). Plateau gradually declines to 99% of population (websites = 55% differentiable, infographics = 45% differentiable), and then both plots fall to 0% differentiable for 100% of the population.", "levels": [[1], [3, 2], [3, 2], [3, 2]], "corpus_id": 201036718, "sentences": ["Plot of mean proportion of image pixels differentiable (independent) for 0% - 100% of the population (dependent) for websites and infographics.", "Increasing from 0% of the population, both plots start at 100% differentiable and gradually fall to 80% differentiable at 75% of the population.", "Plots begin to diverge here as they both fall off more quickly until a discontinuity plateau is reached at 88% of the population (websites = 60% differentiable, infographics = 50% differentiable).", "Plateau gradually declines to 99% of population (websites = 55% differentiable, infographics = 45% differentiable), and then both plots fall to 0% differentiable for 100% of the population."], "caption": "Figure 7: Mean proportion of image pixels differentiable for different percentages of the population.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "University of Dundee Enabling designers to foresee which colors users cannot", "pdf_hash": "0f06c57940475cfc741de29099847b6aa66c8bc0", "year": 2016, "venue": "", "alt_text": "Image masks generated by ColorCheck for one sample website and one sample infographic showing original image, masked at not differentiable for 20% of the population, and masked at not differentiable for 10% of the population. Website: ColorCheck finds minor differentiability problems at 20% and most text is still legible at 10% although some problems with gradient background colors are highlighted. Infographic: ColorCheck finds substantial differentiation problems at 20% (6 segments (half) of main pie chart are not differentiable) and some text difficulties. At 10%, most of the infographic colors are not differentiable.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 201036718, "sentences": ["Image masks generated by ColorCheck for one sample website and one sample infographic showing original image, masked at not differentiable for 20% of the population, and masked at not differentiable for 10% of the population.", "Website: ColorCheck finds minor differentiability problems at 20% and most text is still legible at 10% although some problems with gradient background colors are highlighted.", "Infographic: ColorCheck finds substantial differentiation problems at 20% (6 segments (half) of main pie chart are not differentiable) and some text difficulties.", "At 10%, most of the infographic colors are not differentiable."], "caption": "Original websiteColors pairs that are not differentiable by 20%      (c) Colors pairs that are not differentiable by 10% of the population have been set to black.                     of the population have been set to black.", "local_uri": ["0f06c57940475cfc741de29099847b6aa66c8bc0_Image_020.jpg", "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_021.jpg", "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_022.jpg", "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_023.jpg", "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_024.jpg", "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_025.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture", "pdf_hash": "db65c33bd25bcb3fe54426ef5bd0041b84d87d4d", "year": 2018, "venue": "UIST", "alt_text": "Figure 2 shows an excerpt from a chat log between the end user and remote crowd worker. The crowd worker says, \"Want me to select a time too?\" and the end user replies with \"3pm.\" The crowd worker then proposes the click action on 3pm, which appears in the chat log as \"CrowdWorker wants to click 3:00 PM.\" Beneath this message, there are four options available to the end user: Accept, Reject, Focus, and Request Label.", "levels": null, "corpus_id": 51937592, "sentences": ["Figure 2 shows an excerpt from a chat log between the end user and remote crowd worker.", "The crowd worker says, \"Want me to select a time too?\" and the end user replies with \"3pm.\" The crowd worker then proposes the click action on 3pm, which appears in the chat log as \"CrowdWorker wants to click 3:00 PM.\" Beneath this message, there are four options available to the end user: Accept, Reject, Focus, and Request Label."], "caption": "Figure 2. When a remote client worker proposes a page action, a de- scription of that action is sent to the end user for approval. The end user can perform one of four actions: 1) accept the action, which will perform it on their browser; 2) reject the action; 3) focus which will direct their keyboard focus and screen reader to the target element; or 4) request a label, which will ask a crowd worker to replace the ARIA label of the target element.", "local_uri": ["db65c33bd25bcb3fe54426ef5bd0041b84d87d4d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Opportunities for In-Home Augmented Reality Guidance", "pdf_hash": "1ac54dd26a3dfd9af0a0d9276a8f4af4a710004d", "year": 2019, "venue": "CHI Extended Abstracts", "alt_text": "Two graphs, showing average error for a point versus its horizontal position and vertical position. As horizontal position changes, error stays relatively constant. As vertical position gets higher than the calibration marker, error increases.", "levels": [[1], [3], [3]], "corpus_id": 144207381, "sentences": ["Two graphs, showing average error for a point versus its horizontal position and vertical position.", "As horizontal position changes, error stays relatively constant.", "As vertical position gets higher than the calibration marker, error increases."], "caption": "", "local_uri": ["1ac54dd26a3dfd9af0a0d9276a8f4af4a710004d_Image_004.png"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Opportunities for In-Home Augmented Reality Guidance", "pdf_hash": "1ac54dd26a3dfd9af0a0d9276a8f4af4a710004d", "year": 2019, "venue": "CHI Extended Abstracts", "alt_text": "A box and whisker plot showing NASA Task Load Index score in each of six categories and the average.", "levels": [[1]], "corpus_id": 144207381, "sentences": ["A box and whisker plot showing NASA Task Load Index score in each of six categories and the average."], "caption": "", "local_uri": ["1ac54dd26a3dfd9af0a0d9276a8f4af4a710004d_Image_005.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Powering interactive intelligent systems with the crowd", "pdf_hash": "0333746cbe16d657505549930019774cd064f406", "year": 2014, "venue": "UIST'14 Adjunct", "alt_text": "View interface. On the left is a chatbox with audio players in place of user messages, and on the right is a streaming video from the user's phone, with screenshots taken by the worker below.", "levels": null, "corpus_id": 1298020, "sentences": ["View interface.", "On the left is a chatbox with audio players in place of user messages, and on the right is a streaming video from the user's phone, with screenshots taken by the worker below."], "caption": "Figure 3. View\u2019s worker interface. Workers are shown a video streamed from the user\u2019s phone, and asked to reply to spoken queries using the chat interface. In this example, the recorded message asks workers \u201cHow do I cook this?\u201d a question that often involves multiple image- framing steps and was often challenging using VizWiz.", "local_uri": ["0333746cbe16d657505549930019774cd064f406_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Death of a Robot: Social Media Reactions and Language Usage when a Robot Stops Operating", "pdf_hash": "20c30babb6d3e22833f1d5b0142b63d8e2ff556a", "year": 2020, "venue": "2020 15th ACM/IEEE International Conference on Human-Robot Interaction (HRI)", "alt_text": "This is a confusion matrix of the TF-IDF vector values for each of the nine topics that shows the similarities in words used to tweet about them. The tweets about humans were relatively similar to each other (ranging from 0.27 to 0.50). Grumpy Cat and Keyboard Cat were similar to each other (0.32) and somewhat similar to the humans (0.53 to 0.65). Jibo and Kuri were similar to each other (0.31) but to no other topics. Oppy, Notre Dame, and Koko were not very similar to other topics, with scores ranging from 0.63 to 0.92.", "levels": null, "corpus_id": 212549025, "sentences": ["This is a confusion matrix of the TF-IDF vector values for each of the nine topics that shows the similarities in words used to tweet about them.", "The tweets about humans were relatively similar to each other (ranging from 0.27 to 0.50).", "Grumpy Cat and Keyboard Cat were similar to each other (0.32) and somewhat similar to the humans (0.53 to 0.65).", "Jibo and Kuri were similar to each other (0.31) but to no other topics.", "Oppy, Notre Dame, and Koko were not very similar to other topics, with scores ranging from 0.63 to 0.92."], "caption": "Figure 3: Confusion matrix showing the similarity between topics according to cosine distance in TF-IDF vector space.", "local_uri": ["20c30babb6d3e22833f1d5b0142b63d8e2ff556a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Designing for Sensory Appreciation: Cultivating Somatic Approaches to Experience Design", "pdf_hash": "3c9d2ffca6defa53bd3bd45f21fb513c39cbb1e3", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 4: \"whisper interaction based on somatic facilitation, Schiphorst, 2011\"", "levels": null, "corpus_id": 218483583, "sentences": ["Figure 4: \"whisper interaction based on somatic facilitation, Schiphorst, 2011\""], "caption": "References", "local_uri": ["3c9d2ffca6defa53bd3bd45f21fb513c39cbb1e3_Image_006.png"], "annotated": false, "compound": false}
{"title": "Exploring accessible programming with educators and visually impaired children", "pdf_hash": "1ba993c58ec6183dc7a2d194314a54a9570c652f", "year": 2020, "venue": "IDC", "alt_text": "Photo taken during the Workshop with children. Two visually impaired children perform a goal-directed programming activity using a map located on the table. The activity consists of moving the robot two steps forward to reach the duck. The map is made of 4 EVA foam tiles with two different colors. The child on the right is concentrated in arranging the blocks while the child at the left is touching the duck after the robot reached it which. This child is smiling and have much of his body on the table because the duck is located two foam tiles away from his body.", "levels": null, "corpus_id": 219688982, "sentences": ["Photo taken during the Workshop with children.", "Two visually impaired children perform a goal-directed programming activity using a map located on the table.", "The activity consists of moving the robot two steps forward to reach the duck.", "The map is made of 4 EVA foam tiles with two different colors.", "The child on the right is concentrated in arranging the blocks while the child at the left is touching the duck after the robot reached it which.", "This child is smiling and have much of his body on the table because the duck is located two foam tiles away from his body."], "caption": "Figure 1. Two visually impaired children performing a goal-directed spatial programming activity in Study 2.", "local_uri": ["1ba993c58ec6183dc7a2d194314a54a9570c652f_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Exploring accessible programming with educators and visually impaired children", "pdf_hash": "1ba993c58ec6183dc7a2d194314a54a9570c652f", "year": 2020, "venue": "IDC", "alt_text": "Illustration of the 4 environments used in Study 2 at the left of the image, and at the right, it has lines indicating the age group reference for each environment. The robot DOC and map were developed for children aged between 4 and 7 years old; the robot DASH and BLOCKLY App for children over 6 years old; Osmo AWBIE Code app with tangible blocks was build for children over 5 years old; and finally, the robot DASH and PUZZLETS blocks and tray were developed for children over 4 years old.", "levels": null, "corpus_id": 219688982, "sentences": ["Illustration of the 4 environments used in Study 2 at the left of the image, and at the right, it has lines indicating the age group reference for each environment.", "The robot DOC and map were developed for children aged between 4 and 7 years old; the robot DASH and BLOCKLY App for children over 6 years old; Osmo AWBIE Code app with tangible blocks was build for children over 5 years old; and finally, the robot DASH and PUZZLETS blocks and tray were developed for children over 4 years old."], "caption": "Figure 2. Environments used in Study 1. From top to bottom: robot DOC and map; robot DASH and BLOCKLY App; Osmo Coding AW- BIE App and blocks; and robot DASH, PUZZLETS blocks and tray.", "local_uri": ["1ba993c58ec6183dc7a2d194314a54a9570c652f_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Exploring accessible programming with educators and visually impaired children", "pdf_hash": "1ba993c58ec6183dc7a2d194314a54a9570c652f", "year": 2020, "venue": "IDC", "alt_text": "The bespoke environment used in Study 2. At the left is illustrated a complete sequence of actions (walk right; walk forward, walk right and walk forward) made by the blocks with augmented arrows and the play button at the bottom of the sequence. In the middle is illustrate the blocks with Bluetooth Audio button and augmented tactile cues with felt pads. Finally at the right is represented the robot DASH and its augmented tactile cues, eyebrows made with felt pads, as also augmented feedforward mechanisms.", "levels": null, "corpus_id": 219688982, "sentences": ["The bespoke environment used in Study 2.", "At the left is illustrated a complete sequence of actions (walk right; walk forward, walk right and walk forward) made by the blocks with augmented arrows and the play button at the bottom of the sequence.", "In the middle is illustrate the blocks with Bluetooth Audio button and augmented tactile cues with felt pads.", "Finally at the right is represented the robot DASH and its augmented tactile cues, eyebrows made with felt pads, as also augmented feedforward mechanisms."], "caption": "Figure 3. Tangible and audio-rich blocks and the robot with augmented physicality, feedback and feedforward mechanisms.", "local_uri": ["1ba993c58ec6183dc7a2d194314a54a9570c652f_Image_003.png"], "annotated": false, "compound": false}
{"title": "Effect of Fixed and Infinite Ray Length on Distal 3D Pointing in Virtual Reality", "pdf_hash": "b51674a41f4d8ba24c467ae9d237e84fc90dfa59", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3(a): \"VR environment for experiment, two controllers and cursor on the target. Target is colored in green\"", "levels": null, "corpus_id": 218482777, "sentences": ["Figure 3(a): \"VR environment for experiment, two controllers and cursor on the target.", "Target is colored in green\""], "caption": "\u00b1", "local_uri": ["b51674a41f4d8ba24c467ae9d237e84fc90dfa59_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Effect of Fixed and Infinite Ray Length on Distal 3D Pointing in Virtual Reality", "pdf_hash": "b51674a41f4d8ba24c467ae9d237e84fc90dfa59", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 3(b): \"VR environment, two controllers and target with the fixed ray length. Target is gray colored since the cursor does not interact with cursor\"", "levels": null, "corpus_id": 218482777, "sentences": ["Figure 3(b): \"VR environment, two controllers and target with the fixed ray length.", "Target is gray colored since the cursor does not interact with cursor\""], "caption": "(b)", "local_uri": ["b51674a41f4d8ba24c467ae9d237e84fc90dfa59_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Effect of Fixed and Infinite Ray Length on Distal 3D Pointing in Virtual Reality", "pdf_hash": "b51674a41f4d8ba24c467ae9d237e84fc90dfa59", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 7(a): \"Throughput increased in fixed ray length but not in infinite ray length\"", "levels": null, "corpus_id": 218482777, "sentences": ["Figure 7(a): \"Throughput increased in fixed ray length but not in infinite ray length\""], "caption": "(a)", "local_uri": ["b51674a41f4d8ba24c467ae9d237e84fc90dfa59_Image_017.jpg"], "annotated": false, "compound": false}
{"title": "Effect of Fixed and Infinite Ray Length on Distal 3D Pointing in Virtual Reality", "pdf_hash": "b51674a41f4d8ba24c467ae9d237e84fc90dfa59", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 7(b): \"Throughput is increased in each depth condition for fixed ray length compared to infinite ray length\"", "levels": null, "corpus_id": 218482777, "sentences": ["Figure 7(b): \"Throughput is increased in each depth condition for fixed ray length compared to infinite ray length\""], "caption": "(b)", "local_uri": ["b51674a41f4d8ba24c467ae9d237e84fc90dfa59_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "Fake Moods: Can Users Trick an Emotion-Aware VoiceBot?", "pdf_hash": "4f932661157e43e2df1eaa16e0cfeaa7443ac338", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "The web page for collecting voice samples. The emoticons in the upper row can be clicked to select the emotion to enter. In the beginning, all emoticons were gray. After successfully mimicking an emotion, the corresponding emoticon becomes yellow.", "levels": null, "corpus_id": 233987615, "sentences": ["The web page for collecting voice samples.", "The emoticons in the upper row can be clicked to select the emotion to enter.", "In the beginning, all emoticons were gray.", "After successfully mimicking an emotion, the corresponding emoticon becomes yellow."], "caption": "Figure 1: The web page for collecting voice samples. The emoticons in the upper row can be clicked to select the emo- tion to enter. In the beginning, all emoticons were gray. Af- ter successfully mimicking an emotion, the corresponding emoticon becomes yellow.", "local_uri": ["4f932661157e43e2df1eaa16e0cfeaa7443ac338_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Fake Moods: Can Users Trick an Emotion-Aware VoiceBot?", "pdf_hash": "4f932661157e43e2df1eaa16e0cfeaa7443ac338", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Number of trials and success rates of all 26 participants for all basic emotion they tried to mimic.", "levels": [[1]], "corpus_id": 233987615, "sentences": ["Number of trials and success rates of all 26 participants for all basic emotion they tried to mimic."], "caption": "(a) Number of trials until success                                                                               (b) Success rates\u200c", "local_uri": ["4f932661157e43e2df1eaa16e0cfeaa7443ac338_Image_006.jpg", "4f932661157e43e2df1eaa16e0cfeaa7443ac338_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": true}
{"title": "Understanding the Role of Technology to Support Breastfeeding", "pdf_hash": "79b8dcb17db71e75b3401e6743e194cf0943b960", "year": 2021, "venue": "CHI", "alt_text": "A diagram illustrating constructs of the integrated behavioural model, which includes knowledge, environmental constraints, intention, salience and habit as its core components. The intention construct is further composed of three sub-constructs: attitude (experiential attitude and instrumental attitude), perceived norm (injunctive norm and descriptive norm) and personal agency (perceived control and self-efficacy). All of these constructs are the predictors of a health related behaviour.", "levels": null, "corpus_id": 233987274, "sentences": ["A diagram illustrating constructs of the integrated behavioural model, which includes knowledge, environmental constraints, intention, salience and habit as its core components.", "The intention construct is further composed of three sub-constructs: attitude (experiential attitude and instrumental attitude), perceived norm (injunctive norm and descriptive norm) and personal agency (perceived control and self-efficacy).", "All of these constructs are the predictors of a health related behaviour."], "caption": "Figure 1: The integrated behavioural model", "local_uri": ["79b8dcb17db71e75b3401e6743e194cf0943b960_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Understanding the Role of Technology to Support Breastfeeding", "pdf_hash": "79b8dcb17db71e75b3401e6743e194cf0943b960", "year": 2021, "venue": "CHI", "alt_text": "A stacked bar chart illustrating the distributions of categories on how parents with different infant feeding roles make use of digital technology to support their infant feeding practice. The category of technology use includes relating to the feeding experiences of others, assuring infant feeding practice, finding solutions to challenges in infant feeding and logging infant feeding practice. For the category of relating to the feeding experiences of others, 12.8% of breastfeeding parents responded neither agree nor disagree, 38.40% agreed, 36.80% strongly agreed. No bottle-feeding parents responded strongly disagree and disagree. 25.00% responded neither agree nor disagree, 41.67% responded agree, 33.33% responded strongly agree. 23.53% of partners responded responded neither agree nor disagree and 41.18% responded agree. For the assuring infant feeding practice category, 32.54% of breastfeeding parents responded agree while 34.92% responded strongly agree. 50% of bottle-feeding parents responded responded agree and 25.00% responded strongly agree. 23.53% of partners responded disagree, and 41.18% responded agree. For the finding solutions to challenges in infant feeding practice category, 37.7% of breastfeeding parents responded agree and 45.90% responded strongly agree. 33.33% of bottle-feeding parents responded agree and 33.33% responded strongly agree. 37.5% of partners responded agree, 31.25% responded strongly agree. For the logging infant feeding practice category, 34.92% of breastfeeding parents responded strongly disagree, and 25.40% responded strongly agree. 50% of bottle-feeding parents responded strongly agree. 31.25% of partners responded strongly disagree, and 31.25% responded strongly agree.", "levels": [[1], [1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 233987274, "sentences": ["A stacked bar chart illustrating the distributions of categories on how parents with different infant feeding roles make use of digital technology to support their infant feeding practice.", "The category of technology use includes relating to the feeding experiences of others, assuring infant feeding practice, finding solutions to challenges in infant feeding and logging infant feeding practice.", "For the category of relating to the feeding experiences of others, 12.8% of breastfeeding parents responded neither agree nor disagree, 38.40% agreed, 36.80% strongly agreed.", "No bottle-feeding parents responded strongly disagree and disagree.", "25.00% responded neither agree nor disagree, 41.67% responded agree, 33.33% responded strongly agree.", "23.53% of partners responded responded neither agree nor disagree and 41.18% responded agree.", "For the assuring infant feeding practice category, 32.54% of breastfeeding parents responded agree while 34.92% responded strongly agree.", "50% of bottle-feeding parents responded responded agree and 25.00% responded strongly agree.", "23.53% of partners responded disagree, and 41.18% responded agree.", "For the finding solutions to challenges in infant feeding practice category, 37.7% of breastfeeding parents responded agree and 45.90% responded strongly agree.", "33.33% of bottle-feeding parents responded agree and 33.33% responded strongly agree.", "37.5% of partners responded agree, 31.25% responded strongly agree.", "For the logging infant feeding practice category, 34.92% of breastfeeding parents responded strongly disagree, and 25.40% responded strongly agree.", "50% of bottle-feeding parents responded strongly agree.", "31.25% of partners responded strongly disagree, and 31.25% responded strongly agree."], "caption": "", "local_uri": ["79b8dcb17db71e75b3401e6743e194cf0943b960_Image_003.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Understanding the Role of Technology to Support Breastfeeding", "pdf_hash": "79b8dcb17db71e75b3401e6743e194cf0943b960", "year": 2021, "venue": "CHI", "alt_text": "A bar chart illustrating the distribution of the IBM constructs on 5-point Likert scales. For attitude, breastfeeding parents responded strongly agree(59.95%). Bottle-feeding parents responded strongly disagree(30.56%), strongly agree(33.33%). Partners responded strongly agree(35.29%). Parents-to-be responded neither agree nor disagree(33.33%), strongly agree(33.33%). For norm, breastfeeding parents responded strongly disagree(27.32%), disagree(27.32%). Bottle-feeding parents responded strongly disagree(25%) and strongly agree(25%). Partners responded neither agree nor disagree(23.53%), agree(32.94%). Parents-to-be responded neither agree nor disagree(43%). For personal agency, breastfeeding parents responded agree(23.22%), strongly agree(35.95%). Bottle-feeding parents responded strongly disagree(39.58%). Partners responded disagree(25%), agree(25%) and strongly agree(21.32%). Parents-to-be responded neither agree nor disagree(32.14%). For environment, breastfeeding parents responded neither agree nor disagree(25.1%), responded agree(23.28%). Bottle-feeding parents responded neither agree nor disagree(27.08%), strongly agree(27.08%). Partners responded neither agree nor disagree(26.94%), agree(25%). Parents-to-be responded neither agree nor disagree(47.5%). For knowledge, breastfeeding parent responded strongly agree(38.88%). Bottle-feeding parents responded agree(23.33%), strongly agree(36.67%). Partners responded agree(33.05%), strongly agree(46.39%). Parents-to-be responded neither agree nor disagree(28.33%), agree(33.33%). For salience, breastfeeding parents responded strongly agree(59.79%). Bottle-feeding parents responded agree(33.33%), strongly agree(33.33%). Partners responded strongly agree(41.18%). Parents-to-be responded strongly agree(40%). For habit, Breastfeeding parents responded agree(30.65%), strongly agree(46.77%). Bottle-feeding parents responded strongly agree(58.33%). Partners responded agree(47.06%).", "levels": [[1], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2]], "corpus_id": 233987274, "sentences": ["A bar chart illustrating the distribution of the IBM constructs on 5-point Likert scales.", "For attitude, breastfeeding parents responded strongly agree(59.95%).", "Bottle-feeding parents responded strongly disagree(30.56%), strongly agree(33.33%).", "Partners responded strongly agree(35.29%).", "Parents-to-be responded neither agree nor disagree(33.33%), strongly agree(33.33%).", "For norm, breastfeeding parents responded strongly disagree(27.32%), disagree(27.32%).", "Bottle-feeding parents responded strongly disagree(25%) and strongly agree(25%).", "Partners responded neither agree nor disagree(23.53%), agree(32.94%).", "Parents-to-be responded neither agree nor disagree(43%).", "For personal agency, breastfeeding parents responded agree(23.22%), strongly agree(35.95%).", "Bottle-feeding parents responded strongly disagree(39.58%).", "Partners responded disagree(25%), agree(25%) and strongly agree(21.32%).", "Parents-to-be responded neither agree nor disagree(32.14%).", "For environment, breastfeeding parents responded neither agree nor disagree(25.1%), responded agree(23.28%).", "Bottle-feeding parents responded neither agree nor disagree(27.08%), strongly agree(27.08%).", "Partners responded neither agree nor disagree(26.94%), agree(25%).", "Parents-to-be responded neither agree nor disagree(47.5%).", "For knowledge, breastfeeding parent responded strongly agree(38.88%).", "Bottle-feeding parents responded agree(23.33%), strongly agree(36.67%).", "Partners responded agree(33.05%), strongly agree(46.39%).", "Parents-to-be responded neither agree nor disagree(28.33%), agree(33.33%).", "For salience, breastfeeding parents responded strongly agree(59.79%).", "Bottle-feeding parents responded agree(33.33%), strongly agree(33.33%).", "Partners responded strongly agree(41.18%).", "Parents-to-be responded strongly agree(40%).", "For habit, Breastfeeding parents responded agree(30.65%), strongly agree(46.77%).", "Bottle-feeding parents responded strongly agree(58.33%).", "Partners responded agree(47.06%)."], "caption": "", "local_uri": ["79b8dcb17db71e75b3401e6743e194cf0943b960_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "Magic Touch: Interacting with 3D Printed Graphics", "pdf_hash": "8b82b86897a86655304835196b07374bf2815f23", "year": 2016, "venue": "ASSETS", "alt_text": "Figure 1, We made three interactive graphics with our trackers: (a) a globe, (b) a cell model and (c) a tactile map.", "levels": [[-1]], "corpus_id": 6073099, "sentences": ["Figure 1, We made three interactive graphics with our trackers: (a) a globe, (b) a cell model and (c) a tactile map."], "caption": "Figure 1. We made three interactive graphics with our trackers: (a) a globe, (b) a cell model and (c) a tactile map.", "local_uri": ["8b82b86897a86655304835196b07374bf2815f23_Image_001.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Phasking on Paper: Accessing a Continuum of PHysically Assisted SKetchING", "pdf_hash": "46bba332133f1ad3c0212f83ac89abb692a8e578", "year": 2020, "venue": "CHI", "alt_text": "Figure 5: Ballpoint drive and sensing in 2D. Motors generated torques Tm are transmitted to the surface contact ball through the gears causing the ball to roll over the surface. The friction force Fr between the contact ball and the surface produces motion in x-y direction.", "levels": null, "corpus_id": 218483425, "sentences": ["Figure 5: Ballpoint drive and sensing in 2D.", "Motors generated torques Tm are transmitted to the surface contact ball through the gears causing the ball to roll over the surface.", "The friction force Fr between the contact ball and the surface produces motion in x-y direction."], "caption": "", "local_uri": ["46bba332133f1ad3c0212f83ac89abb692a8e578_Image_015.jpg"], "annotated": false, "compound": false}
{"title": "Phasking on Paper: Accessing a Continuum of PHysically Assisted SKetchING", "pdf_hash": "46bba332133f1ad3c0212f83ac89abb692a8e578", "year": 2020, "venue": "CHI", "alt_text": "Figure 6: Phasking Pen ballpoint Drive design iterations. From left (earliest): (a) Early version with plastic gear between motors and rollers, and rubber connecting ball between roller and surface contact ball. (b) Pulley belt connection between motors and rollers, and clock gears with micro cogs between roller and the contact ball. (c) Metal gears between motors and roller, for a lower gear ratio of 1:1. (d) Similar mechanism with a customized higher gear ratio of 4:1.", "levels": null, "corpus_id": 218483425, "sentences": ["Figure 6: Phasking Pen ballpoint Drive design iterations.", "From left (earliest): (a) Early version with plastic gear between motors and rollers, and rubber connecting ball between roller and surface contact ball. (b) Pulley belt connection between motors and rollers, and clock gears with micro cogs between roller and the contact ball. (c) Metal gears between motors and roller, for a lower gear ratio of 1:1. (d) Similar mechanism with a customized higher gear ratio of 4:1."], "caption": "", "local_uri": ["46bba332133f1ad3c0212f83ac89abb692a8e578_Image_018.jpg"], "annotated": false, "compound": false}
{"title": "Phasking on Paper: Accessing a Continuum of PHysically Assisted SKetchING", "pdf_hash": "46bba332133f1ad3c0212f83ac89abb692a8e578", "year": 2020, "venue": "CHI", "alt_text": "Figure 7: Paper tool palette, which can be hand-drawn and customized, or printed on a full sheet or slip of paper.", "levels": null, "corpus_id": 218483425, "sentences": ["Figure 7: Paper tool palette, which can be hand-drawn and customized, or printed on a full sheet or slip of paper."], "caption": "", "local_uri": ["46bba332133f1ad3c0212f83ac89abb692a8e578_Image_029.jpg"], "annotated": false, "compound": false}
{"title": "Phasking on Paper: Accessing a Continuum of PHysically Assisted SKetchING", "pdf_hash": "46bba332133f1ad3c0212f83ac89abb692a8e578", "year": 2020, "venue": "CHI", "alt_text": "Figure 8: Force generation performance. (a) BOSE test setup.(b) Max output force response to PWM voltage pulses.  (c)Force-tracking response to a slow sinusoid of PWM excitation.", "levels": null, "corpus_id": 218483425, "sentences": ["Figure 8: Force generation performance. (a) BOSE test setup.(b) Max output force response to PWM voltage pulses.", "(c)Force-tracking response to a slow sinusoid of PWM excitation."], "caption": "", "local_uri": ["46bba332133f1ad3c0212f83ac89abb692a8e578_Image_030.jpg", "46bba332133f1ad3c0212f83ac89abb692a8e578_Image_031.jpg", "46bba332133f1ad3c0212f83ac89abb692a8e578_Image_032.jpg"], "annotated": false, "compound": true}
{"title": "Towards Risk Indication In Mountain Biking Using Smart Wearables", "pdf_hash": "89f5e36948e752ff3dacfe8c25d3be475dce5886", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A watch communicates to a mountain biker their current risk of crashing, as computed from internal and external factors", "levels": null, "corpus_id": 233987651, "sentences": ["A watch communicates to a mountain biker their current risk of crashing, as computed from internal and external factors"], "caption": "Figure 1: A watch communicates to a mountain biker their current risk of crashing, as computed from internal and external factors", "local_uri": ["89f5e36948e752ff3dacfe8c25d3be475dce5886_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Towards Risk Indication In Mountain Biking Using Smart Wearables", "pdf_hash": "89f5e36948e752ff3dacfe8c25d3be475dce5886", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "A risk level, that persistently exceeds a certain threshold will be indicated by additional modalities, such as vibration, sound or flashing of the screen.", "levels": null, "corpus_id": 233987651, "sentences": ["A risk level, that persistently exceeds a certain threshold will be indicated by additional modalities, such as vibration, sound or flashing of the screen."], "caption": "", "local_uri": ["89f5e36948e752ff3dacfe8c25d3be475dce5886_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Molder: An Accessible Design Tool for Tactile Maps", "pdf_hash": "e4c9da1d58adca2895c3340bfa323aa5b49bd385", "year": 2020, "venue": "CHI", "alt_text": "Figure 2. This figure shows the Physical Ruler, which consists of two rulers at a right angle to each other. Each ruler has six empty slots. There are barcode markers at the end of each ruler and at the corner where the rulers are attached to each other. On top of the ruler is the Indicator, which is a movable piece of five barcode markers with two stoppers on each end.", "levels": [[-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483460, "sentences": ["Figure 2.", "This figure shows the Physical Ruler, which consists of two rulers at a right angle to each other.", "Each ruler has six empty slots.", "There are barcode markers at the end of each ruler and at the corner where the rulers are attached to each other.", "On top of the ruler is the Indicator, which is a movable piece of five barcode markers with two stoppers on each end."], "caption": "Figure 2. The Physical Ruler and The Indicator. There are three markers and one stopper on the Physical Ruler. The Indicator has two stoppers and a rigid tactile pattern on its right side.", "local_uri": ["e4c9da1d58adca2895c3340bfa323aa5b49bd385_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Molder: An Accessible Design Tool for Tactile Maps", "pdf_hash": "e4c9da1d58adca2895c3340bfa323aa5b49bd385", "year": 2020, "venue": "CHI", "alt_text": "Figure 3. This figure includes two sub-images referred to as the letters A and B. Image A shows two Tangible Widgets. The top one has four printed tactile square pieces representing four functions: Braille, Icon, Line, and Dot. One piece with a braille label represents the Braille function, one piece with a raised half-sphere represents the Icon function, one piece with a raised line pattern represents the Line function, and one piece with a raised 3x3 dot array represents the Dot function. The Widget on the bottom has four printed tactile square pieces representing another four functions: Undo, Scale, Delete, and Label. One piece with a leftward arrow tactile symbol represents the Undo function, one piece with a dot and a circle tactile symbols represents the Scale function, one piece with an X tactile shape represents the Delete function, and one piece with an L tactile shape represents the label function. Each widget has a barcode marker on the left and right side of the widget. Image B shows both Tangible Widgets positioned in a 3D printed palette.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483460, "sentences": ["Figure 3.", "This figure includes two sub-images referred to as the letters A and B. Image A shows two Tangible Widgets.", "The top one has four printed tactile square pieces representing four functions: Braille, Icon, Line, and Dot.", "One piece with a braille label represents the Braille function, one piece with a raised half-sphere represents the Icon function, one piece with a raised line pattern represents the Line function, and one piece with a raised 3x3 dot array represents the Dot function.", "The Widget on the bottom has four printed tactile square pieces representing another four functions: Undo, Scale, Delete, and Label.", "One piece with a leftward arrow tactile symbol represents the Undo function, one piece with a dot and a circle tactile symbols represents the Scale function, one piece with an X tactile shape represents the Delete function, and one piece with an L tactile shape represents the label function.", "Each widget has a barcode marker on the left and right side of the widget.", "Image B shows both Tangible Widgets positioned in a 3D printed palette."], "caption": "Figure 3. The design of the Tangible Widgets. (A) The top Tangible Widget has four functions: Braille, Icon, Line, and Dot. The Tangible Widget on the bottom has another four functions: Undo, Scale, Delete, and Label. (B) Designers can combine the two Tangible Widgets into a palette.", "local_uri": ["e4c9da1d58adca2895c3340bfa323aa5b49bd385_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Molder: An Accessible Design Tool for Tactile Maps", "pdf_hash": "e4c9da1d58adca2895c3340bfa323aa5b49bd385", "year": 2020, "venue": "CHI", "alt_text": "Figure 4. This figure shows three representations of the same building on a dark grey background. Each building, marked in yellow, has a different tactile pattern represented in light grey. The tactile patterns from left to right are: Icon, Line, and Dot.", "levels": null, "corpus_id": 218483460, "sentences": ["Figure 4.", "This figure shows three representations of the same building on a dark grey background.", "Each building, marked in yellow, has a different tactile pattern represented in light grey.", "The tactile patterns from left to right are: Icon, Line, and Dot."], "caption": "Figure 4. Three tactile patterns on the same building. From left to right, the added tactile patterns are Icon, Line, and Dot. The building is marked in yellow and the added patterns are marked in grey.", "local_uri": ["e4c9da1d58adca2895c3340bfa323aa5b49bd385_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Molder: An Accessible Design Tool for Tactile Maps", "pdf_hash": "e4c9da1d58adca2895c3340bfa323aa5b49bd385", "year": 2020, "venue": "CHI", "alt_text": "Figure 8. This figure shows the graphical user interface of the Molder server. It is a user interface of Blender with two menus: the Molder menu on the left, and the selection window on the right. The center of the interface shows the rendered model, which is a model of a map with six buildings. A user can select a building in the selection window on the right and perform Molder functions on the left menu. The model will be updated in real time.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483460, "sentences": ["Figure 8.", "This figure shows the graphical user interface of the Molder server.", "It is a user interface of Blender with two menus: the Molder menu on the left, and the selection window on the right.", "The center of the interface shows the rendered model, which is a model of a map with six buildings.", "A user can select a building in the selection window on the right and perform Molder functions on the left menu.", "The model will be updated in real time."], "caption": "Figure 8. The user interface of the Molder server. A user can select a building in the selection window and perform Molder functions. The model will be updated in real time.", "local_uri": ["e4c9da1d58adca2895c3340bfa323aa5b49bd385_Image_008.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Molder: An Accessible Design Tool for Tactile Maps", "pdf_hash": "e4c9da1d58adca2895c3340bfa323aa5b49bd385", "year": 2020, "venue": "CHI", "alt_text": "Figure 9. This figure includes four sub-images referred to as the letters A to D. The four images demonstrate how Molder automatically creates a draft model. Image A shows the outlines from the imported 2D buildings. There are seven dark red buildings with different shapes on a dark grey background. Image B shows that Molder combines overlapping units to a single building and assigns high-contrast colors and IDs to all buildings. In images B, C, and D, all six buildings are in different colors. Image C shows that Molder adds a base and a small cube on the top left corner to indicate the orientation of the model.Image D shows that Molder extrudes the 2D building outlines to 3D meshes, resulting in a 3D model with the desired buildings.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218483460, "sentences": ["Figure 9.", "This figure includes four sub-images referred to as the letters A to D. The four images demonstrate how Molder automatically creates a draft model.", "Image A shows the outlines from the imported 2D buildings.", "There are seven dark red buildings with different shapes on a dark grey background.", "Image B shows that Molder combines overlapping units to a single building and assigns high-contrast colors and IDs to all buildings.", "In images B, C, and D, all six buildings are in different colors.", "Image C shows that Molder adds a base and a small cube on the top left corner to indicate the orientation of the model.", "Image D shows that Molder extrudes the 2D building outlines to 3D meshes, resulting in a 3D model with the desired buildings."], "caption": "Figure 9. Molder automatically creates a draft model with the geographic information provided by a designer. (A) First, Molder imports 2D building outlines. (B) Then, it combines overlapping units to a single building (e.g., the aqua building has four units) and assigns high-contrast colors and IDs to all buildings. (C) Molder also adds a base (marked in grey) and a small cube (marked in red) on the top left corner to indicate the orientation of the model. (D) Last, Molder extrudes the 2D building outlines to 3D meshes.", "local_uri": ["e4c9da1d58adca2895c3340bfa323aa5b49bd385_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Flippo the Robo-Shoe-Fly: A Foot Dwelling Social Wearable Companion", "pdf_hash": "eb589744456cc714b77462bb7d2455bb48ca2b47", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 6: The Flippo design, on its own, not worn and glowing with lights, both red and green to indicate how many rounds of interaction it had and requested.", "levels": null, "corpus_id": 218483177, "sentences": ["Figure 6: The Flippo design, on its own, not worn and glowing with lights, both red and green to indicate how many rounds of interaction it had and requested."], "caption": "", "local_uri": ["eb589744456cc714b77462bb7d2455bb48ca2b47_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Sooner or later?: immediate feedback as a source of inspiration in electronic brainstorming", "pdf_hash": "59dd7438c0f333c504a800f9daaba1c7383c3c67", "year": 2017, "venue": "OZCHI", "alt_text": "Resulting proposition: People who feel motivated and able prefer deferred feedback, while the feedback mechanism serves as facilitator and/or motivator to others.", "levels": null, "corpus_id": 2572431, "sentences": ["Resulting proposition: People who feel motivated and able prefer deferred feedback, while the feedback mechanism serves as facilitator and/or motivator to others."], "caption": "Figure 1: Resulting proposition: People who feel motivated and able prefer deferred feedback, while the feedback mechanism serves as facilitator and/or motivator to others.", "local_uri": ["59dd7438c0f333c504a800f9daaba1c7383c3c67_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Mental Models of AI Agents in a Cooperative Game Setting", "pdf_hash": "a109274aa61679a5d95058b4bd20fa7acba0df52", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: \"Image of a person with a thought bubble. Inside the thought bubble is a cartoon robot with three arrows pointing to the three phrases: global behavior, knowledge distribution, local behavior.\"", "levels": [[-1], [-1]], "corpus_id": 218482474, "sentences": ["Figure 1: \"Image of a person with a thought bubble.", "Inside the thought bubble is a cartoon robot with three arrows pointing to the three phrases: global behavior, knowledge distribution, local behavior.\""], "caption": "Figure 1: A mental model of an AI agent has three components: behavior at a large scale, the agent\u2019s knowledge of various topics, and behavior at the scale of an individual output.", "local_uri": ["a109274aa61679a5d95058b4bd20fa7acba0df52_Image_001.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Mental Models of AI Agents in a Cooperative Game Setting", "pdf_hash": "a109274aa61679a5d95058b4bd20fa7acba0df52", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: \"Rounded box with two columns. The left column has at the top an icon of a robot and the word Hints. The right column has at the top an icon of a person and the word Guesses. The Hints column has the hints salad, red, fruit. The Guesses column has the guesses lettuce, radish, tomato. At the bottom of the box in large font it says: 'You won! The target word was tomato.'\"", "levels": null, "corpus_id": 218482474, "sentences": ["Figure 2: \"Rounded box with two columns.", "The left column has at the top an icon of a robot and the word Hints.", "The right column has at the top an icon of a person and the word Guesses.", "The Hints column has the hints salad, red, fruit.", "The Guesses column has the guesses lettuce, radish, tomato.", "At the bottom of the box in large font it says: 'You won! The target word was tomato.'\""], "caption": "", "local_uri": ["a109274aa61679a5d95058b4bd20fa7acba0df52_Image_002.png"], "annotated": false, "compound": false}
{"title": "Mental Models of AI Agents in a Cooperative Game Setting", "pdf_hash": "a109274aa61679a5d95058b4bd20fa7acba0df52", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: \"Four rounded boxes in a T formation. The center box is labeled 'WordBot' and contains the words 'Trained Neural Network'. The left and bottom box have arrows pointing at the center box. The left box is labeled 'Game State' and contains the words 'Target: cat; Hints: meow; Guesses: kitten'. The bottom box is labeled 'Knowledge Base' and contains the words 'cat -> type of feline; cat -> related term is pet; cat -> wants nap; cat -> in a bag; ...'. The right box has an arrow pointing at it from the center box. It is labeled 'Next Hint' and contains the word 'Dog'.\"", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 218482474, "sentences": ["Figure 3: \"Four rounded boxes in a T formation.", "The center box is labeled 'WordBot' and contains the words 'Trained Neural Network'.", "The left and bottom box have arrows pointing at the center box.", "The left box is labeled 'Game State' and contains the words 'Target: cat; Hints: meow; Guesses: kitten'.", "The bottom box is labeled 'Knowledge Base' and contains the words 'cat -> type of feline; cat -> related term is pet; cat -> wants nap; cat -> in a bag; ...'.", "The right box has an arrow pointing at it from the center box.", "It is labeled 'Next Hint' and contains the word 'Dog'.\""], "caption": "Figure 3: Diagram of the \u2018giver\u2019 AI agent, called WordBot. WordBot is a trained neural network, which has encoded infor- mation from the training data. In addition, information from a knowledge base is used as input along with the game state.", "local_uri": ["a109274aa61679a5d95058b4bd20fa7acba0df52_Image_003.png"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "pdf_hash": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "year": 2017, "venue": "UIST", "alt_text": "The screen capture of SktechExpress UI. On the left side there is a canvas. In the canvas, the Super Mario game (a side scrolling game) is depicted. e.g. Mario Jumping, Mushroom approaching, etc. On the right side, the list of animation and associated functionality are there. Please read the caption for the further detail.", "levels": null, "corpus_id": 25231320, "sentences": ["The screen capture of SktechExpress UI.", "On the left side there is a canvas.", "In the canvas, the Super Mario game (a side scrolling game) is depicted.", "e.g. Mario Jumping, Mushroom approaching, etc. On the right side, the list of animation and associated functionality are there. Please read the caption for the further detail."], "caption": "Figure 1. SketchExpress allows crowd workers to prototype interactive behaviors. A requester describes aloud how a user interface should behave and crowd workers quickly create complex interactive behaviors. The interface contains the following features for crowd workers to easily create interactive behaviors (animations) as follows: (1) A synchronized canvas that supports simultaneous interactions between a requester and workers. (2) the ability to select and replay multiple animations at once; (3) reset functionality that places elements in the animation back to their initial state (position, color, etc.); (4) recording button to record a worker\u2019s demonstrations; (5) a chat box for helpers to ask clari\ufb01cation questions if needed. (6) labels that show the current state of the animation [replaying/remixing] to prevent multiple workers from concurrently working on the same animation.", "local_uri": ["0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "pdf_hash": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "year": 2017, "venue": "UIST", "alt_text": "Description of combinging two differen animations. Rotation operation and translation operation together can be replayed and such combination can enable a\"rolling stone\" animation. In the picture, the first image shows a rock rotating where it stands, the 2nd image shows translating the rock on the ground from left to right, and the third image depicts the rolling stone animation.", "levels": null, "corpus_id": 25231320, "sentences": ["Description of combinging two differen animations.", "Rotation operation and translation operation together can be replayed and such combination can enable a\"rolling stone\" animation.", "In the picture, the first image shows a rock rotating where it stands, the 2nd image shows translating the rock on the ground from left to right, and the third image depicts the rolling stone animation."], "caption": "", "local_uri": ["0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "pdf_hash": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "year": 2017, "venue": "UIST", "alt_text": "Description of Recording and Replaying by Delta value. The figure shows three images of a clock and the hour hand rotate by 60 degree per image. The first image is one o'clock the 2nd iamge is three o'clock and the last image is 5 o'clock. From the first image, if you run the rotatioin animation twice, it will reach the third image.", "levels": null, "corpus_id": 25231320, "sentences": ["Description of Recording and Replaying by Delta value.", "The figure shows three images of a clock and the hour hand rotate by 60 degree per image.", "The first image is one o'clock the 2nd iamge is three o'clock and the last image is 5 o'clock.", "From the first image, if you run the rotatioin animation twice, it will reach the third image."], "caption": "Figure 2. (1) Initial State: arrow pointed vertically upwards. (2) After \ufb01rst replay: arrow pointed 60 degrees clockwise. (3) After second replay: arrow pointed 120 degrees clockwise. Reset will restore state (1).", "local_uri": ["0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "pdf_hash": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "year": 2017, "venue": "UIST", "alt_text": "The screen capture of remixing user interface. It has the list of operation in a table. See the caption for the further detail.", "levels": null, "corpus_id": 25231320, "sentences": ["The screen capture of remixing user interface.", "It has the list of operation in a table.", "See the caption for the further detail."], "caption": "Figure 4. Remixing helps workers make more expressive animations. The interface consists of: (1) Operation list: provides workers with a discrete view of an animation as a series of operations. (2) Operation duration: if clicked, a container of remix functions is expanded. (3) Re- play options: you can choose for each element if it will be skipped (not displayed), if it will appear instantly, or if it will appear in real-time. (4) Slider and input: modify the operation\u2019s speed and duration. (5) Trash icon: skip the operation or delay. (6) Check mark and highlight bor- der: indicate the current operation in preview. (7) Element replacement: reuses the animation for one element as the animation for another one.", "local_uri": ["0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "pdf_hash": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "year": 2017, "venue": "UIST", "alt_text": "The bar graph for precision and recall for three different conditions. Total six bars are present. (Precision/Recall for three condition from C1, C2, to C3). The values are as follows: C1 Precision : 86.9%, C1 Recall 62.7%, C2 Precision 94.7%, C2 Recall 66.8%, C3 Precision 99.2%, C3 Recall 90.0%", "levels": [[1], [1], [1], [2]], "corpus_id": 25231320, "sentences": ["The bar graph for precision and recall for three different conditions.", "Total six bars are present. (", "Precision/Recall for three condition from C1, C2, to C3).", "The values are as follows: C1 Precision : 86.9%, C1 Recall 62.7%, C2 Precision 94.7%, C2 Recall 66.8%, C3 Precision 99.2%, C3 Recall 90.0%"], "caption": "", "local_uri": ["0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "pdf_hash": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "year": 2017, "venue": "UIST", "alt_text": "The bar graph for the completion time of each animation. Total 6 bars are present (1st demo and the 2nd demo per condition (C1, C2, and C3)). The values are as follows: C1- 1st Demo:39.4s, C1-2nd Demo:35.6s, C1-2nd Demo(Green portion):22.5s, C2-1st Demo:78.3s, C2-2nd Demo:19.1s, C2-2nd Demo (Green portion):4.6s, C3-1st Demo:174.5s, C2-2nd Demo:12.4s, C2-2nd Demo (Green portion):1.1s", "levels": [[1], [1], [2]], "corpus_id": 25231320, "sentences": ["The bar graph for the completion time of each animation.", "Total 6 bars are present (1st demo and the 2nd demo per condition (C1, C2, and C3)).", "The values are as follows: C1- 1st Demo:39.4s, C1-2nd Demo:35.6s, C1-2nd Demo(Green portion):22.5s, C2-1st Demo:78.3s, C2-2nd Demo:19.1s, C2-2nd Demo (Green portion):4.6s, C3-1st Demo:174.5s, C2-2nd Demo:12.4s, C2-2nd Demo (Green portion):1.1s"], "caption": "Figure 6. Latency of the 1st demo(blue) and the 2nd demo(yellow); The time it takes to demo-remix-replay an animation is longer than the other two conditions, but once an animation is created (the 2nd demo), a worker can respond quickly by replaying the animation. This is because the amount of time needed to respond to the demonstration request is the time it takes to restore the initial state needed to reproduce the requester behavior (the portion of the green bar in the yellow one).", "local_uri": ["0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2], "has_1_2_3": false, "has_1_2": true, "compound": false}
{"title": "\"You Have to Piece the Puzzle Together\": Implications for Designing Decision Support in Intensive Care", "pdf_hash": "3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "A sketched scene from contextual inquiry in ICU 1: The scene shows a patient room. A critically ill patient is lying in a hospital bed, being ventilated via hoses with a ventilation machine. In the foreground, there are three healthcare professionals in hospital clothing standing around the bed. One of the three (a man) bends over the patient, another man looks at the patient's vital signs, which are displayed on a monitoring device. The third person, a woman, makes a handwritten note on a piece of paper. Next to the bed, perfusors for medication and a computer with a keyboard and mouse are mounted on the wall.", "levels": null, "corpus_id": 220323868, "sentences": ["A sketched scene from contextual inquiry in ICU 1: The scene shows a patient room.", "A critically ill patient is lying in a hospital bed, being ventilated via hoses with a ventilation machine.", "In the foreground, there are three healthcare professionals in hospital clothing standing around the bed.", "One of the three (a man) bends over the patient, another man looks at the patient's vital signs, which are displayed on a monitoring device.", "The third person, a woman, makes a handwritten note on a piece of paper.", "Next to the bed, perfusors for medication and a computer with a keyboard and mouse are mounted on the wall."], "caption": "Figure 1. Sketched scene from a contextual inquiry in ICU 1. The pic- ture shows the people involved (residents, nurse, patient) in the physi- cian\u2019s examination of a patient. Information is exchanged between the physician, the nurse and the patient (depending on responsiveness). A bed-side computer allows documentation in electronic health records.", "local_uri": ["3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "\"You Have to Piece the Puzzle Together\": Implications for Designing Decision Support in Intensive Care", "pdf_hash": "3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Visualization of an abstract User Experience Map. The map is divided vertically into 4 areas. At the top, next to the title Day Shift in ICU, there is a timeline divided into different phases. An orange timeline is assigned in color to the persona Peter Busch, specialist nurse. It starts at 6 o'clock in the morning with the early shift and ends at 14:30. A petrol-blue timeline is assigned to the persona Dr. Hanna Tolle, resident, by color. It starts at 7:30 and ends at 16:30.  Below the timelines begins an area that shows Peter Busch's goals, needs, decisions, stress levels and actions along the timeline. One goal in the afternoon is: \"wants to notice changes at an early stage\". A need of Peter in the morning at the beginning of the shift is: \"needs info on 24 hour volume therapy\" and a decision in the morning \"how much volume in the next hour\". Stress levels and actions are represented by a curve and icons. The curve fluctuates strongly, there are hardly any low stress values to be seen. Actions by Peter and Hanna Tolle on the patient are indicated in the form of bars along 4 lines, the first three representing Peters and Hannas mutual patients, the other one representing all other patients of Hanna. In the area below, goals, Needs, Decisions, Stress Levels and actions of Hanna Tolle are displayed. Hanna's stress level is high throughout her shift, but fluctuates and reaches high outliers shortly before the end of the shift.  Hanna's patient interactions alternate with Peter's and partially overlap.  A decision Hanna makes in the afternoon is: \"how much volume in next 24 hours\". A need around noon: \"needs help proritizing\". In the morning Hanna has the goal: \"Collect all \"pieces\" of information \"of the puzzle\".", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 220323868, "sentences": ["Visualization of an abstract User Experience Map.", "The map is divided vertically into 4 areas.", "At the top, next to the title Day Shift in ICU, there is a timeline divided into different phases.", "An orange timeline is assigned in color to the persona Peter Busch, specialist nurse.", "It starts at 6 o'clock in the morning with the early shift and ends at 14:30.", "A petrol-blue timeline is assigned to the persona Dr. Hanna Tolle, resident, by color.", "It starts at 7:30 and ends at 16:30.", "Below the timelines begins an area that shows Peter Busch's goals, needs, decisions, stress levels and actions along the timeline.", "One goal in the afternoon is: \"wants to notice changes at an early stage\".", "A need of Peter in the morning at the beginning of the shift is: \"needs info on 24 hour volume therapy\" and a decision in the morning \"how much volume in the next hour\".", "Stress levels and actions are represented by a curve and icons.", "The curve fluctuates strongly, there are hardly any low stress values to be seen.", "Actions by Peter and Hanna Tolle on the patient are indicated in the form of bars along 4 lines, the first three representing Peters and Hannas mutual patients, the other one representing all other patients of Hanna.", "In the area below, goals, Needs, Decisions, Stress Levels and actions of Hanna Tolle are displayed.", "Hanna's stress level is high throughout her shift, but fluctuates and reaches high outliers shortly before the end of the shift.", "Hanna's patient interactions alternate with Peter's and partially overlap.", "A decision Hanna makes in the afternoon is: \"how much volume in next 24 hours\".", "A need around noon: \"needs help proritizing\".", "In the morning Hanna has the goal: \"Collect all \"pieces\" of information \"of the puzzle\"."], "caption": "", "local_uri": ["3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed_Image_002.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"You Have to Piece the Puzzle Together\": Implications for Designing Decision Support in Intensive Care", "pdf_hash": "3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Picture of the persona Dr. Hanna Tolle. The persona shows a picture of a young woman with glasses next to the title: Resident and ICU Beginner. Next to this picture is a quote from Hanna: \"Sometimes it's hard for me to decide where to start. I'm glad I have my colleagues.\" Below, basic information about Hanna is described. She is 29 years old and has been working for 6 months in a pneumological intensive care unit with 18 beds. Shift work is divided into three shifts.  The rest of the page describes daily work, Hanna's context and decision making, her goals and pain points.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 220323868, "sentences": ["Picture of the persona Dr. Hanna Tolle.", "The persona shows a picture of a young woman with glasses next to the title: Resident and ICU Beginner.", "Next to this picture is a quote from Hanna: \"Sometimes it's hard for me to decide where to start.", "I'm glad I have my colleagues.\"", "Below, basic information about Hanna is described.", "She is 29 years old and has been working for 6 months in a pneumological intensive care unit with 18 beds.", "Shift work is divided into three shifts.", "The rest of the page describes daily work, Hanna's context and decision making, her goals and pain points."], "caption": "Figure 3. Excerpt of the primary persona Dr. Hanna Tolle: She is an ICU beginner, who started to work as a resident in an ICU six months ago. It takes her long to weigh all information and derive therapeutic measures. She asks her colleagues for advice and help regularly.", "local_uri": ["3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed_Image_003.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "\"You Have to Piece the Puzzle Together\": Implications for Designing Decision Support in Intensive Care", "pdf_hash": "3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed", "year": 2020, "venue": "Conference on Designing Interactive Systems", "alt_text": "Picture of the persona Peter Busch. The persona shows a picture of a man with a beard next to the title: Specialist nurse & ICU advanced. Next to this picture is a quote from Peter: \"You're responsible non-stop. But it is never the decision of the doctor or nurse alone, it always interlocks.\" The rest of the page describes daily work, Peters context and decision making, his goals and pain points.  Below, basic information about Peter is described. He is 36 years old and has been working for 12 years in a pneumological intensive care unit with 18 beds. Shift work is divided into three shifts. Peter had previously worked as a nurse for 12 years.", "levels": [[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], "corpus_id": 220323868, "sentences": ["Picture of the persona Peter Busch.", "The persona shows a picture of a man with a beard next to the title: Specialist nurse & ICU advanced.", "Next to this picture is a quote from Peter: \"You're responsible non-stop.", "But it is never the decision of the doctor or nurse alone, it always interlocks.\"", "The rest of the page describes daily work, Peters context and decision making, his goals and pain points.", "Below, basic information about Peter is described.", "He is 36 years old and has been working for 12 years in a pneumological intensive care unit with 18 beds.", "Shift work is divided into three shifts.", "Peter had previously worked as a nurse for 12 years."], "caption": "Figure 4. Excerpt of the primary persona Peter Busch: He is an ad- vanced specialist ICU nurse, who works at an ICU for 12 years. His ex- pertise and practical experience automate his decisions. If he questions a decision, he will discuss this with the physician.", "local_uri": ["3d8ccf7a132d0831fcd6b8f6b7c561a40d21afed_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "TinyBlackBox: Supporting Mobile In-The-Wild Studies", "pdf_hash": "3fbbe989292fe23dad02837af5a1045d41cbdac9", "year": 2015, "venue": "ASSETS", "alt_text": "Screen shot of the web-based TBB Player. three sections, 1) the rendered screen content of the mobile device showing the app drawer page. 2) the page DOM tree structure and 3) the individual node information for the Facebook icon page element.", "levels": null, "corpus_id": 13076217, "sentences": ["Screen shot of the web-based TBB Player.", "three sections, 1) the rendered screen content of the mobile device showing the app drawer page. 2) the page DOM tree structure and 3) the individual node information for the Facebook icon page element."], "caption": "", "local_uri": ["3fbbe989292fe23dad02837af5a1045d41cbdac9_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Voice+Tactile: Augmenting In-vehicle Voice User Interface with Tactile Touchpad Interaction", "pdf_hash": "395baeac820075a001d755fb8f3628b6dc87ccea", "year": 2020, "venue": "CHI", "alt_text": "Voice+Tactile interaction. (a) A user can invoke the interface via a 3-finger double-tap. (b) In a listening state, concentric patterns indicate the interface is attending. (c) Inverted concentric patterns indicate input speech is detected. (d) A rotating fan-shape pattern indicates processing. (e) A feedforward indicates the finger used in the next step. (f-g) The user can scroll through speech output on a ladder pattern. (h) The user selects an item by double-tapping one finger.", "levels": null, "corpus_id": 218483074, "sentences": ["Voice+Tactile interaction.", "(a) A user can invoke the interface via a 3-finger double-tap.", "(b) In a listening state, concentric patterns indicate the interface is attending. (c) Inverted concentric patterns indicate input speech is detected.", "(d) A rotating fan-shape pattern indicates processing.", "(e) A feedforward indicates the finger used in the next step. (f-g) The user can scroll through speech output on a ladder pattern.", "(h) The user selects an item by double-tapping one finger."], "caption": "", "local_uri": ["395baeac820075a001d755fb8f3628b6dc87ccea_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Voice+Tactile: Augmenting In-vehicle Voice User Interface with Tactile Touchpad Interaction", "pdf_hash": "395baeac820075a001d755fb8f3628b6dc87ccea", "year": 2020, "venue": "CHI", "alt_text": "The Voice+Tactile prototype: (a) the PinPad, a tactile display with 40 times 25 pins, and (b) the system diagram of the prototype.", "levels": null, "corpus_id": 218483074, "sentences": ["The Voice+Tactile prototype: (a) the PinPad, a tactile display with 40 times 25 pins, and (b) the system diagram of the prototype."], "caption": "\u00d7", "local_uri": ["395baeac820075a001d755fb8f3628b6dc87ccea_Image_005.png"], "annotated": false, "compound": false}
{"title": "Voice+Tactile: Augmenting In-vehicle Voice User Interface with Tactile Touchpad Interaction", "pdf_hash": "395baeac820075a001d755fb8f3628b6dc87ccea", "year": 2020, "venue": "CHI", "alt_text": "The driving simulator setup. Participants wore an eye tracker. The tactile interface is on a small table.", "levels": null, "corpus_id": 218483074, "sentences": ["The driving simulator setup.", "Participants wore an eye tracker.", "The tactile interface is on a small table."], "caption": "Figure 4: The driving simulator setup. Participants wore an eye tracker. The tactile interface is on a small table.", "local_uri": ["395baeac820075a001d755fb8f3628b6dc87ccea_Image_006.png"], "annotated": false, "compound": false}
{"title": "Voice+Tactile: Augmenting In-vehicle Voice User Interface with Tactile Touchpad Interaction", "pdf_hash": "395baeac820075a001d755fb8f3628b6dc87ccea", "year": 2020, "venue": "CHI", "alt_text": "The Input Adjustment interactions: Fine-Tuning interaction (a) and Text Edit interaction (b-d). (a) A jog dial tactile pattern appears at the finger position. The user can fine-tune the music volume or light brightness by moving the finger clockwise or counter-clockwise. (b) A horizontal ladder-like tactile pattern for reviewing and editing a voice input is shown. The user can review commands at a word-level via the pattern. (c) The user can locate a misrecognized word by hearing each word. (d) The user can edit a wrong word by saying the correct word.", "levels": null, "corpus_id": 218483074, "sentences": ["The Input Adjustment interactions: Fine-Tuning interaction (a) and Text Edit interaction (b-d).", "(a) A jog dial tactile pattern appears at the finger position.", "The user can fine-tune the music volume or light brightness by moving the finger clockwise or counter-clockwise.", "(b) A horizontal ladder-like tactile pattern for reviewing and editing a voice input is shown.", "The user can review commands at a word-level via the pattern.", "(c) The user can locate a misrecognized word by hearing each word. (d) The user can edit a wrong word by saying the correct word."], "caption": "Figure 5: The Input Adjustment interactions: Fine-Tuning interaction (a) and Text Edit interaction (b-d). (a) A jog dial tactile pattern appears at the \ufb01nger position. The user can \ufb01ne-tune the music volume or light brightness by moving the \ufb01nger clockwise or counter-clockwise. (b) A horizontal ladder-like tactile pattern for reviewing and editing a voice input is shown. The user can review commands at a word-level via the pattern. (c) The user can locate a misrecognized word by hearing each word. (d) The user can edit a wrong word by saying the correct word.", "local_uri": ["395baeac820075a001d755fb8f3628b6dc87ccea_Image_007.png"], "annotated": false, "compound": false}
{"title": "A Critical Examination of Virtual Reality Technology in the Context of the Minority Body", "pdf_hash": "d90ad38de9da3f207a68e159c1bf882a34273bc6", "year": 2021, "venue": "CHI", "alt_text": "Drawing of a person with long hair who uses a cane and is wearing VR headset, with controllers dangling from their wrists. On the left side of the person, we illustrate three degrees of freedom provided by the VR headset and head tracking through arrows, on the right side, we illustrate the additional three degrees of freedom that are achieved through full-body interaction through a coordinate system that illustrates the axes on which a user is expected to move.", "levels": null, "corpus_id": 233987226, "sentences": ["Drawing of a person with long hair who uses a cane and is wearing VR headset, with controllers dangling from their wrists.", "On the left side of the person, we illustrate three degrees of freedom provided by the VR headset and head tracking through arrows, on the right side, we illustrate the additional three degrees of freedom that are achieved through full-body interaction through a coordinate system that illustrates the axes on which a user is expected to move."], "caption": "", "local_uri": ["d90ad38de9da3f207a68e159c1bf882a34273bc6_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "A Critical Examination of Virtual Reality Technology in the Context of the Minority Body", "pdf_hash": "d90ad38de9da3f207a68e159c1bf882a34273bc6", "year": 2021, "venue": "CHI", "alt_text": "Drawing of a person standing up using a VR system, holding the controllers in their hands, and wearing a VR headset. The cable of the headset is also shown. Headset, controllers and cable are highlighted to underline the hardware elements of the VR system.", "levels": null, "corpus_id": 233987226, "sentences": ["Drawing of a person standing up using a VR system, holding the controllers in their hands, and wearing a VR headset.", "The cable of the headset is also shown.", "Headset, controllers and cable are highlighted to underline the hardware elements of the VR system."], "caption": "handheld controllers to support gesture-based input (unless system supports hands-free tracking of arms, hands and fngers, e.g., Ocu- lus Quest) and input through buttons. Some handheld controllers also support haptic feedback (e.g., vibration). (3) Positional tracking devices \u2013 often optical systems \u2013 that support the determination of location and pose of a person within a dedicated interaction space (e.g., the HTC Vive Lighthouses). To enter VR, people subject themselves to the system by attaching relevant devices to their bod- ies, and limiting their radius of movement to the interaction space that is defned by the technology (see also, Figure 2). To engage with these systems, people need to use both arms and hands to hold two controllers, and are expected to precisely push sometimes small buttons with their fngers. Some controllers provide haptic feedback (i.e., vibration) that is directly transferred to the hands. In the case of hands-free tracking systems, people need to be able to gesture with their arms, hands, and fngers. Additionally, many headsets are reasonably heavy (e.g., 470g/1lbs for Oculus Rift, and 555g/1.2lbs for the HTC Vive [5]), which puts additional strain on the spine [26].", "local_uri": ["d90ad38de9da3f207a68e159c1bf882a34273bc6_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "A Critical Examination of Virtual Reality Technology in the Context of the Minority Body", "pdf_hash": "d90ad38de9da3f207a68e159c1bf882a34273bc6", "year": 2021, "venue": "CHI", "alt_text": "Drawing of the upper body of a person. They are shown from the front, wearing a VR headset and reaching up with their left arm. We can see their palm. Hand and head are highlighted as these are the body parts involved in this specific action.", "levels": null, "corpus_id": 233987226, "sentences": ["Drawing of the upper body of a person.", "They are shown from the front, wearing a VR headset and reaching up with their left arm.", "We can see their palm.", "Hand and head are highlighted as these are the body parts involved in this specific action."], "caption": "", "local_uri": ["d90ad38de9da3f207a68e159c1bf882a34273bc6_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "A Critical Examination of Virtual Reality Technology in the Context of the Minority Body", "pdf_hash": "d90ad38de9da3f207a68e159c1bf882a34273bc6", "year": 2021, "venue": "CHI", "alt_text": "Drawing of the upper body of a person wearing a VR headset and holding VR controllers, shown from the side. Body parts that are directly in touch with hardware (hands, head) are highlighted.", "levels": null, "corpus_id": 233987226, "sentences": ["Drawing of the upper body of a person wearing a VR headset and holding VR controllers, shown from the side.", "Body parts that are directly in touch with hardware (hands, head) are highlighted."], "caption": "", "local_uri": ["d90ad38de9da3f207a68e159c1bf882a34273bc6_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "A Critical Examination of Virtual Reality Technology in the Context of the Minority Body", "pdf_hash": "d90ad38de9da3f207a68e159c1bf882a34273bc6", "year": 2021, "venue": "CHI", "alt_text": "Drawing of a wheelchair user wearing a VR headset, making input into the virtual environment with their arms and hands. The floor area is indicated to highlight the space within the environment that is required for interaction.", "levels": null, "corpus_id": 233987226, "sentences": ["Drawing of a wheelchair user wearing a VR headset, making input into the virtual environment with their arms and hands.", "The floor area is indicated to highlight the space within the environment that is required for interaction."], "caption": "", "local_uri": ["d90ad38de9da3f207a68e159c1bf882a34273bc6_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "A participant solves compound remote associate questions under a time limit on a computer. Two tactors, attached to the abdomen, deliver personalized paced vibrations with which participants can synchronize their breathing. The answer to the question above would be \"bill,\" a word which is related to all three words shown", "levels": null, "corpus_id": 218483018, "sentences": ["A participant solves compound remote associate questions under a time limit on a computer.", "Two tactors, attached to the abdomen, deliver personalized paced vibrations with which participants can synchronize their breathing.", "The answer to the question above would be \"bill,\" a word which is related to all three words shown"], "caption": "Figure 1. A participant solves compound remote associate questions under a time limit on a computer. Two tactors, attached to the abdomen, deliver personalized paced vibrations with which participants can synchronize their breathing. The answer to the question above would be \"bill,\" a word which is related to all three words shown.", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "Procedure  flowchart  of  the  study. All  participants went through the same procedure until end of Post-stressor 1.  At that point,participants who were randomly  assigned to the control group went through Block 2 while receiving no vibrations (indicated by a solid blackline), while participants who were randomly assigned to the treatment group go through Block 2 while receiving vibrations. Self-reported measures of affect (positive and negative) as well as STAI-6 were collectedat five conditions of Baseline, V-Breathing Practice, Pre-stressor 1, Post-stressor 1, and Post-stressor 2 (indicated by color-coded pen-and-paper icon)", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 218483018, "sentences": ["Procedure  flowchart  of  the  study.", "All  participants went through the same procedure until end of Post-stressor 1.", "At that point,participants who were randomly  assigned to the control group went through Block 2 while receiving no vibrations (indicated by a solid blackline), while participants who were randomly assigned to the treatment group go through Block 2 while receiving vibrations.", "Self-reported measures of affect (positive and negative) as well as STAI-6 were collectedat five conditions of Baseline, V-Breathing Practice, Pre-stressor 1, Post-stressor 1, and Post-stressor 2 (indicated by color-coded pen-and-paper icon)"], "caption": "", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_006.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "Left: Average STAI-6 scores during five conditions for both treatment and control groups. An interaction effect (circled) was observed between group and post-stressor 1 and post-stressor 2 conditions: the treatment group receiving vibrotactile patterns during post-stressor 2 experienced a drop in anxiety compared to the control group. Solid line indicates treatment group; dotted line indicates control group. Center, Right: Average positive affect (left) and negative affect (center) during five conditions as reported on a scale of 1 to 100 by both treatment and control groups. No interaction effect was observed between group (treatment and control) and condition (post-stressor 1 and post-stressor 2): we did not find that receiving vibrotactile patterns had an influence on either positive or negative affect.", "levels": [[1], [3], [1], [1], [3]], "corpus_id": 218483018, "sentences": ["Left: Average STAI-6 scores during five conditions for both treatment and control groups.", "An interaction effect (circled) was observed between group and post-stressor 1 and post-stressor 2 conditions: the treatment group receiving vibrotactile patterns during post-stressor 2 experienced a drop in anxiety compared to the control group.", "Solid line indicates treatment group; dotted line indicates control group.", "Center, Right: Average positive affect (left) and negative affect (center) during five conditions as reported on a scale of 1 to 100 by both treatment and control groups.", "No interaction effect was observed between group (treatment and control) and condition (post-stressor 1 and post-stressor 2): we did not find that receiving vibrotactile patterns had an influence on either positive or negative affect."], "caption": "Figure 3. Left: Average STAI-6 scores during \ufb01ve conditions for both treatment and control groups. An interaction effect (circled) was observed between group and post-stressor 1 and post-stressor 2 conditions: the treatment group receiving vibrotactile patterns during post-stressor 2 experienced a drop in anxiety compared to the control group. Solid line indicates treatment group; dotted line indicates control group. Center, Right: Average positive affect (left) and negative affect (center) during \ufb01ve conditions as reported on a scale of 1 to 100 by both treatment and control groups. No interaction effect was observed between group (treatment and control) and condition (post-stressor 1 and post-stressor 2): we did not \ufb01nd that receiving vibrotactile patterns had an in\ufb02uence on either positive or negative affect.", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_008.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "Linear mixed model fit to STAI-6 scores. The model fits horizontal lines for the control group and lines with negative slopes for the treatment group (shown in red), in both cases considering individualized intercepts. The model predicts a significant drop in anxiety for the treatment group going from the post-stressor 1 to post-stressor 2 condition.", "levels": null, "corpus_id": 218483018, "sentences": ["Linear mixed model fit to STAI-6 scores.", "The model fits horizontal lines for the control group and lines with negative slopes for the treatment group (shown in red), in both cases considering individualized intercepts.", "The model predicts a significant drop in anxiety for the treatment group going from the post-stressor 1 to post-stressor 2 condition."], "caption": "", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "Left and Right: Average user-technology engagement difficulty at three levels of perception, cognition, and action in the presence and absence of a stressor, with effect sizes displayed. Left: Synchronization with vibrations is more difficult than both noticing and differentiating vibrations. This is true both without a stressor (at V-Breathing Practice) and with a stressor (at Post-stressor 2). Right: When a stressor is introduced, synchronizing becomes significantly more difficult than both noticing and differentiating. Note that there is data for both treatment and control groups in the absent stressor condition (at V-Breathing Practice), but only for the treatment group in the stressor condition (at Post-stressor 2).", "levels": [[1], [3], [3], [3], [3]], "corpus_id": 218483018, "sentences": ["Left and Right: Average user-technology engagement difficulty at three levels of perception, cognition, and action in the presence and absence of a stressor, with effect sizes displayed.", "Left: Synchronization with vibrations is more difficult than both noticing and differentiating vibrations.", "This is true both without a stressor (at V-Breathing Practice) and with a stressor (at Post-stressor 2).", "Right: When a stressor is introduced, synchronizing becomes significantly more difficult than both noticing and differentiating.", "Note that there is data for both treatment and control groups in the absent stressor condition (at V-Breathing Practice), but only for the treatment group in the stressor condition (at Post-stressor 2)."], "caption": "Figure 5. Left and Right: Average user-technology engagement dif\ufb01culty at three levels of perception, cognition, and action in the presence and absence of a stressor, with effect sizes displayed. Left: Synchronization with vibrations is more dif\ufb01cult than both noticing and differentiating vibrations. This is true both without a stressor (at V-Breathing Practice) and with a stressor (at Post-stressor 2). Right: When a stressor is introduced, synchronizing becomes signi\ufb01cantly more dif\ufb01cult than both noticing and differentiating. Note that there is data for both treatment and control groups in the absent stressor condition (at V-Breathing Practice), but only for the treatment group in the stressor condition (at Post-stressor 2).", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_011.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "SHAP feature importance measured as the mean absolute Shapley values. The rating of how much a participant desired to turn PIV vibrations off was the most important feature, changing the predicted anxiety level on average by 1.42 points. Right: SHAP summary plot showing the importance and the effect of features. Low numbers of willingness to turn-off vibrations contribute to anxiety drop, and large numbers to increase in anxiety.", "levels": [[1], [2], [1], [4, 3]], "corpus_id": 218483018, "sentences": ["SHAP feature importance measured as the mean absolute Shapley values.", "The rating of how much a participant desired to turn PIV vibrations off was the most important feature, changing the predicted anxiety level on average by 1.42 points.", "Right: SHAP summary plot showing the importance and the effect of features.", "Low numbers of willingness to turn-off vibrations contribute to anxiety drop, and large numbers to increase in anxiety."], "caption": "Figure 6. Left: SHAP feature importance measured as the mean absolute Shapley values. The rating of how much a participant desired to turn PIV vibrations off was the most important feature, changing the predicted anxiety level on average by 1.42 points. Right: SHAP summary plot showing the importance and the effect of features. Low numbers of willingness to turn-off vibrations contribute to anxiety drop, and large numbers to increase in anxiety.", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_012.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3, 4], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "pdf_hash": "031c5426862746aeef25ca784afa9191fcdb0eff", "year": 2020, "venue": "CHI", "alt_text": "Left: Average STAI-6 scores. An interaction effect (circled) was observed between group and Post-stressor 1 and Post-stressor 2 for Openness score < 33 but not for Openness score > 33. Center: An interaction effect was observed between Pre- and Post-stressor 1 as well as between Post-stressor 1 and 2 which make the results inconclusive. Right: The same as left but for  Reappraisal scores cutoff at 71.", "levels": [[1], [3, 2], [3], [2]], "corpus_id": 218483018, "sentences": ["Left: Average STAI-6 scores.", "An interaction effect (circled) was observed between group and Post-stressor 1 and Post-stressor 2 for Openness score < 33 but not for Openness score > 33.", "Center: An interaction effect was observed between Pre- and Post-stressor 1 as well as between Post-stressor 1 and 2 which make the results inconclusive.", "Right: The same as left but for  Reappraisal scores cutoff at 71."], "caption": "", "local_uri": ["031c5426862746aeef25ca784afa9191fcdb0eff_Image_014.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1, 2, 3], "has_1_2_3": true, "has_1_2": true, "compound": false}
{"title": "Mobile, Hands-free, Silent Speech Texting Using SilentSpeller", "pdf_hash": "1fb80dbba2b67cbc5b1afae8214b4fc9b0ba3b87", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "a) A SilentSpeller user wears the SmartPalate retainer whose 124 electrodes sense the position of the tongue at 100Hz. Applications include b) Hands-busy situations where speech is socially inappropriate c) users with low manual dexterity working in open office environments d) United Nations operations where silent communication is necessary", "levels": null, "corpus_id": 233987510, "sentences": ["a) A SilentSpeller user wears the SmartPalate retainer whose 124 electrodes sense the position of the tongue at 100Hz.", "Applications include b) Hands-busy situations where speech is socially inappropriate c) users with low manual dexterity working in open office environments d) United Nations operations where silent communication is necessary"], "caption": "", "local_uri": ["1fb80dbba2b67cbc5b1afae8214b4fc9b0ba3b87_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Mobile, Hands-free, Silent Speech Texting Using SilentSpeller", "pdf_hash": "1fb80dbba2b67cbc5b1afae8214b4fc9b0ba3b87", "year": 2021, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2.a A person is sitting on a chair. A dentist is putting his hands to make a dental impression needed for custom-fit SmartPalate.   Figure 2.b The image of SmartPalate sensor we use in our SilentSpeller.   Figure 2.c Palatogram and electrode map. Notethat individual letters are not recognized in real-time but are added to the image for illustration purposes.", "levels": null, "corpus_id": 233987510, "sentences": ["Figure 2.a A person is sitting on a chair.", "A dentist is putting his hands to make a dental impression needed for custom-fit SmartPalate.", "Figure 2.b The image of SmartPalate sensor we use in our SilentSpeller.", "Figure 2.c Palatogram and electrode map.", "Notethat individual letters are not recognized in real-time but are added to the image for illustration purposes."], "caption": "A", "local_uri": ["1fb80dbba2b67cbc5b1afae8214b4fc9b0ba3b87_Image_004.jpg", "1fb80dbba2b67cbc5b1afae8214b4fc9b0ba3b87_Image_005.jpg"], "annotated": false, "compound": true}
{"title": "Experiencing Simulated Confrontations in Virtual Reality", "pdf_hash": "a5cc57b26c628b139670c6a6722b05b744888896", "year": 2021, "venue": "CHI", "alt_text": "Two side-by-side images, one showing a user wearing a VR headset, and another showing a user watching the character on a large TV screen.", "levels": null, "corpus_id": 233987532, "sentences": ["Two side-by-side images, one showing a user wearing a VR headset, and another showing a user watching the character on a large TV screen."], "caption": "Figure 3: Setup for our study conditions", "local_uri": ["a5cc57b26c628b139670c6a6722b05b744888896_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Metaprobes, Metaphysical Workshops and Sketchy Philosophy", "pdf_hash": "047aa58be6b6a8350d2dbf820b2e78a1c183068e", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: In a Thingly Ontology an object is neither constrained through components nor dissolved through relations", "levels": null, "corpus_id": 218483254, "sentences": ["Figure 1: In a Thingly Ontology an object is neither constrained through components nor dissolved through relations"], "caption": "Figure 1 In a Thingly Ontology an object is neither constrained through components nor dissolved through relations. Image credit: the first author.", "local_uri": ["047aa58be6b6a8350d2dbf820b2e78a1c183068e_Image_001.png"], "annotated": false, "compound": false}
{"title": "Metaprobes, Metaphysical Workshops and Sketchy Philosophy", "pdf_hash": "047aa58be6b6a8350d2dbf820b2e78a1c183068e", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: The Poetic Dreamcatcher. The device (bottom). Retina analysis procedure undergone by participants during Stage 2 (Top right). Poetry in the process of printing (Top center). Actress simulating the functioning of the base module (Top left)", "levels": null, "corpus_id": 218483254, "sentences": ["Figure 2: The Poetic Dreamcatcher.", "The device (bottom).", "Retina analysis procedure undergone by participants during Stage 2 (Top right).", "Poetry in the process of printing (Top center).", "Actress simulating the functioning of the base module (Top left)"], "caption": "Figure 2 The Poetic Dreamcatcher. The device (bottom). Retina analysis procedure undergone by participants during Stage 2 (Top right). Poetry in the process of printing (Top center). Actress simulating the functioning of the base module (Top left). Image credit: the first author.", "local_uri": ["047aa58be6b6a8350d2dbf820b2e78a1c183068e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Aidme: interactive non-visual smartphone tutorials", "pdf_hash": "7f0c72a3bd2c5e794a3bad9c9d19ac6ebe86306f", "year": 2018, "venue": "MobileHCI Adjunct", "alt_text": "A diagram representing the steps for tutorial authoring. 1) start by pressing the button on top of the screen to initiate recording. 2) Narrate each step and demonstrate the task on the smartphone, repeat 2) until the task is complete. 3) Select the overaly button on the top of the screen, a prompt will pop-up to name and describe the tutorial.", "levels": null, "corpus_id": 52097191, "sentences": ["A diagram representing the steps for tutorial authoring.", "1) start by pressing the button on top of the screen to initiate recording.", "2) Narrate each step and demonstrate the task on the smartphone, repeat 2) until the task is complete.", "3) Select the overaly button on the top of the screen, a prompt will pop-up to name and describe the tutorial."], "caption": "", "local_uri": ["7f0c72a3bd2c5e794a3bad9c9d19ac6ebe86306f_Image_002.png"], "annotated": false, "compound": false}
{"title": "Designing smartglasses applications for people with low vision", "pdf_hash": "42ea99b7f70c0e5f06e5a9fb8a78da4c37c90f4a", "year": 2017, "venue": "ASAC", "alt_text": "A virtual triangle shown on the Epson Moverio smartglasses during the study. The triangle is shown in 5 colors (white, yellow, red,  blue, and green) over dark and light backgrounds.", "levels": null, "corpus_id": 21820037, "sentences": ["A virtual triangle shown on the Epson Moverio smartglasses during the study.", "The triangle is shown in 5 colors (white, yellow, red,  blue, and green) over dark and light backgrounds."], "caption": "", "local_uri": ["42ea99b7f70c0e5f06e5a9fb8a78da4c37c90f4a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Designing smartglasses applications for people with low vision", "pdf_hash": "42ea99b7f70c0e5f06e5a9fb8a78da4c37c90f4a", "year": 2017, "venue": "ASAC", "alt_text": "The phrase \"Turn Left\" as presented on the Epson Moverio smartglasses during the study. The phrase is presented in 5 colors (white, yellow, red, blue, and green) and over two backgrounds, dark and light.", "levels": null, "corpus_id": 21820037, "sentences": ["The phrase \"Turn Left\" as presented on the Epson Moverio smartglasses during the study.", "The phrase is presented in 5 colors (white, yellow, red, blue, and green) and over two backgrounds, dark and light."], "caption": "Figure 1. Actual stimuli presented on smartglasses in our study, shown over dark (top) and light (bottom) backgrounds. The triangle and a short phrase are shown in the different colors used in the study. In addition to color, we also varied the stimulus size, thickness, and font face (for text).", "local_uri": ["42ea99b7f70c0e5f06e5a9fb8a78da4c37c90f4a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Blind People Interacting with Large Touch Surfaces: Strategies for One-handed and Two-handed Exploration", "pdf_hash": "71cb80944729a6a294b0c2a70508ec0f236473a0", "year": 2015, "venue": "ITS", "alt_text": "It shows two examples of the focused strategy, one with One-hand and the other with Two-hand. In both examples, the users explore the screen horizontally, row by row. However, in the Two-hand condition, users divide the screen and each hand explores (symmetrically) the respective side", "levels": null, "corpus_id": 17496473, "sentences": ["It shows two examples of the focused strategy, one with One-hand and the other with Two-hand.", "In both examples, the users explore the screen horizontally, row by row.", "However, in the Two-hand condition, users divide the screen and each hand explores (symmetrically) the respective side"], "caption": "", "local_uri": ["71cb80944729a6a294b0c2a70508ec0f236473a0_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Blind People Interacting with Large Touch Surfaces: Strategies for One-handed and Two-handed Exploration", "pdf_hash": "71cb80944729a6a294b0c2a70508ec0f236473a0", "year": 2015, "venue": "ITS", "alt_text": "It shows an example of the focused strategy. It shows that the user was exploring the screen in a very limited area, close to the target.", "levels": null, "corpus_id": 17496473, "sentences": ["It shows an example of the focused strategy.", "It shows that the user was exploring the screen in a very limited area, close to the target."], "caption": "Figure 3. Focused strategy interaction traces of a Relocate task.", "local_uri": ["71cb80944729a6a294b0c2a70508ec0f236473a0_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Blind People Interacting with Large Touch Surfaces: Strategies for One-handed and Two-handed Exploration", "pdf_hash": "71cb80944729a6a294b0c2a70508ec0f236473a0", "year": 2015, "venue": "ITS", "alt_text": "It shows an example of the to-the-point strategy. It shows a direct movement to the desired target", "levels": null, "corpus_id": 17496473, "sentences": ["It shows an example of the to-the-point strategy.", "It shows a direct movement to the desired target"], "caption": "Figure 4. Interaction trace of a Relocate task presenting a To- the-point strategy.", "local_uri": ["71cb80944729a6a294b0c2a70508ec0f236473a0_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Blind People Interacting with Large Touch Surfaces: Strategies for One-handed and Two-handed Exploration", "pdf_hash": "71cb80944729a6a294b0c2a70508ec0f236473a0", "year": 2015, "venue": "ITS", "alt_text": "It shows an example of the freeform strategy. The movements are very unstructured and there are several unvisited areas, and others that were visited more than once", "levels": null, "corpus_id": 17496473, "sentences": ["It shows an example of the freeform strategy.", "The movements are very unstructured and there are several unvisited areas, and others that were visited more than once"], "caption": "apply them in the subsequent locate and relocate tasks using the To-The-Point strategy.", "local_uri": ["71cb80944729a6a294b0c2a70508ec0f236473a0_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Blind People Interacting with Large Touch Surfaces: Strategies for One-handed and Two-handed Exploration", "pdf_hash": "71cb80944729a6a294b0c2a70508ec0f236473a0", "year": 2015, "venue": "ITS", "alt_text": "It shows an example of the Freeform Symmetry strategy. Alike the freeform strategy, there are several areas that were not visited and others that were visited more than once. However, in this case each hand explores the respective side of the screen, in a way that seem symmetrical, even though without aparent structure.", "levels": null, "corpus_id": 17496473, "sentences": ["It shows an example of the Freeform Symmetry strategy.", "Alike the freeform strategy, there are several areas that were not visited and others that were visited more than once.", "However, in this case each hand explores the respective side of the screen, in a way that seem symmetrical, even though without aparent structure."], "caption": "Figure 6. Interaction traces during a Relate task showing Freeform Symmetry strategy.", "local_uri": ["71cb80944729a6a294b0c2a70508ec0f236473a0_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Blind People Interacting with Large Touch Surfaces: Strategies for One-handed and Two-handed Exploration", "pdf_hash": "71cb80944729a6a294b0c2a70508ec0f236473a0", "year": 2015, "venue": "ITS", "alt_text": "It shows an example of the Trailing Finger strategy. Alike the freeform strategy, there are several areas that were not visited and others that were visited more than once. Even though each hand explores the respective side of the screen, it seems that one had is leading while the other follows the first one's movements with an horizontal offset", "levels": null, "corpus_id": 17496473, "sentences": ["It shows an example of the Trailing Finger strategy.", "Alike the freeform strategy, there are several areas that were not visited and others that were visited more than once.", "Even though each hand explores the respective side of the screen, it seems that one had is leading while the other follows the first one's movements with an horizontal offset"], "caption": "Figure 7. Interaction traces of a Relate task showing a Trailing Finger strategy.", "local_uri": ["71cb80944729a6a294b0c2a70508ec0f236473a0_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "MirrorPad: Mirror on Touchpad for Direct Pen Interaction in the Laptop Environment", "pdf_hash": "89ce35d22b833c4969ec79eb95a573b492b4c3ca", "year": 2020, "venue": "CHI", "alt_text": "Experimental setup for user evaluation experiment: (a) setting for MirrorPad condition, (b) setting for Laptop condition, and (c) setting for Tablet condition.", "levels": null, "corpus_id": 218483134, "sentences": ["Experimental setup for user evaluation experiment: (a) setting for MirrorPad condition, (b) setting for Laptop condition, and (c) setting for Tablet condition."], "caption": "", "local_uri": ["89ce35d22b833c4969ec79eb95a573b492b4c3ca_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "MirrorPad: Mirror on Touchpad for Direct Pen Interaction in the Laptop Environment", "pdf_hash": "89ce35d22b833c4969ec79eb95a573b492b4c3ca", "year": 2020, "venue": "CHI", "alt_text": "Experimental tasks: (a) three Keyboard tasks in a single file, (b) two Pen tasks in a 2-column layout, (c) one Pen task in a 1-column layout, and (d) the organization of the 10 sessions.", "levels": null, "corpus_id": 218483134, "sentences": ["Experimental tasks: (a) three Keyboard tasks in a single file, (b) two Pen tasks in a 2-column layout, (c) one Pen task in a 1-column layout, and (d) the organization of the 10 sessions."], "caption": "Figure 4. Experimental tasks: (a) three Keyboard tasks in a single \ufb01le,", "local_uri": ["89ce35d22b833c4969ec79eb95a573b492b4c3ca_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "MirrorPad: Mirror on Touchpad for Direct Pen Interaction in the Laptop Environment", "pdf_hash": "89ce35d22b833c4969ec79eb95a573b492b4c3ca", "year": 2020, "venue": "CHI", "alt_text": "Mean completion times. Error bars show the standard deviations, and the asterisks indicate significant differences.", "levels": [[1], [1]], "corpus_id": 218483134, "sentences": ["Mean completion times.", "Error bars show the standard deviations, and the asterisks indicate significant differences."], "caption": "Figure 5. Mean completion times. Error bars show the standard devia\u00ad tions, and the asterisks indicate signi\ufb01cant differences (p < 0.001).", "local_uri": ["89ce35d22b833c4969ec79eb95a573b492b4c3ca_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MirrorPad: Mirror on Touchpad for Direct Pen Interaction in the Laptop Environment", "pdf_hash": "89ce35d22b833c4969ec79eb95a573b492b4c3ca", "year": 2020, "venue": "CHI", "alt_text": "Means of NASA TLX overall score and SUS score. Error bars show the standard deviations, and the asterisks indicate significant differences.", "levels": [[1], [1]], "corpus_id": 218483134, "sentences": ["Means of NASA TLX overall score and SUS score.", "Error bars show the standard deviations, and the asterisks indicate significant differences."], "caption": "Figure 6. Means of NASA TLX overall score and SUS score. Error bars show the standard deviations, and the asterisks indicate signi\ufb01cant differences (p < 0.01).", "local_uri": ["89ce35d22b833c4969ec79eb95a573b492b4c3ca_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MirrorPad: Mirror on Touchpad for Direct Pen Interaction in the Laptop Environment", "pdf_hash": "89ce35d22b833c4969ec79eb95a573b492b4c3ca", "year": 2020, "venue": "CHI", "alt_text": "Means of NASA TLX weighted workload scores. Error bars show the standard deviations, and the asterisks indicate significant differences.", "levels": [[1], [1]], "corpus_id": 218483134, "sentences": ["Means of NASA TLX weighted workload scores.", "Error bars show the standard deviations, and the asterisks indicate significant differences."], "caption": "Figure 7. Means of NASA TLX weighted workload scores. Error bars show the standard deviations, and the asterisks indicate signi\ufb01cant dif\u00ad ferences (p < 0.01).", "local_uri": ["89ce35d22b833c4969ec79eb95a573b492b4c3ca_Image_007.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "MagTouch identifies the contact finger among index, middle, and ring fingers using an embedded magnetometer in a smartwatch and a permanent magnet ring. A user can invoke three different functions on a single button by simply switching between fingers. Based on the touch location and the magnetic field vector, MagTouch can identify the contact finger when a user moves the hand to switch between fingers and when the user only switches the contact fingers without moving the hand.", "levels": null, "corpus_id": 218482708, "sentences": ["MagTouch identifies the contact finger among index, middle, and ring fingers using an embedded magnetometer in a smartwatch and a permanent magnet ring.", "A user can invoke three different functions on a single button by simply switching between fingers.", "Based on the touch location and the magnetic field vector, MagTouch can identify the contact finger when a user moves the hand to switch between fingers and when the user only switches the contact fingers without moving the hand."], "caption": "Figure 1. MagTouch identi\ufb01es the contact \ufb01nger among index, middle, and ring \ufb01ngers using an embedded magnetometer in a smartwatch and a permanent magnet ring. (a) A user can invoke three different functions on a single button by simply switching between \ufb01ngers. Based on the touch location and the magnetic \ufb01eld vector, MagTouch can identify the contact \ufb01nger when a user moves the hand to switch between \ufb01ngers (b,c) and when the user only switches the contact \ufb01ngers without moving the hand (d,e).", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_001.png"], "annotated": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "Magnetic field data when touching various locations of the smartwatch's touchscreen. Magnetic field can be similar even if a touching finger is different. Touch location data is necessary for such cases.", "levels": [[1], [1], [0]], "corpus_id": 218482708, "sentences": ["Magnetic field data when touching various locations of the smartwatch's touchscreen.", "Magnetic field can be similar even if a touching finger is different.", "Touch location data is necessary for such cases."], "caption": "Figure 2. Magnetic \ufb01eld data when touching various locations of the smartwatch\u2019s touchscreen. Magnetic \ufb01eld can be similar even if a touch- ing \ufb01nger is different. Touch location data is necessary for such cases.", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_002.png"], "annotated": true, "is_plot": true, "uniq_levels": [0, 1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "Change in the magnitude of magnetometer vector data with the change in the ambient magnetic field. The magnetometer vector data is a vector sum of the magnetic field of the magnet ring and the ambient magnetic field. The reference and 6'o clock directions denotes two opposite directions.", "levels": [[1], [1], [1]], "corpus_id": 218482708, "sentences": ["Change in the magnitude of magnetometer vector data with the change in the ambient magnetic field.", "The magnetometer vector data is a vector sum of the magnetic field of the magnet ring and the ambient magnetic field.", "The reference and 6'o clock directions denotes two opposite directions."], "caption": "", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_003.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "The MagTouch method: (a) physical model and (b) the inverse problem for finger identification.", "levels": null, "corpus_id": 218482708, "sentences": ["The MagTouch method: (a) physical model and (b) the inverse problem for finger identification."], "caption": "that the classi\ufb01er margins of the resultant machine learning model could successfully accommodate the variation of rela- tive hand positions and orientations occurring during normal smartwatch use.", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_004.png"], "annotated": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "MagTouch hardware implementation. a) Smartwatch and magnet ring worn in hands. Magnet rings made with b) ABS plastic and c) resin. d) Magnet ring with rubber band used for experiments.", "levels": null, "corpus_id": 218482708, "sentences": ["MagTouch hardware implementation.", "a) Smartwatch and magnet ring worn in hands.", "Magnet rings made with b) ABS plastic and c) resin.", "d) Magnet ring with rubber band used for experiments."], "caption": "Figure 5. MagTouch hardware implementation. a) Smartwatch and magnet ring worn in hands. Magnet rings made with b) ABS plastic and c) resin. d) Magnet ring with rubber band used for experiments.", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "MagTouch system overview. The first part, which we call CAME, estimates the magnetic field of the magnet ring. The second part is a machine learning classifier, which predicts a touching finger with a touch location and a magnetic field vector measured at the sensor.", "levels": null, "corpus_id": 218482708, "sentences": ["MagTouch system overview.", "The first part, which we call CAME, estimates the magnetic field of the magnet ring.", "The second part is a machine learning classifier, which predicts a touching finger with a touch location and a magnetic field vector measured at the sensor."], "caption": "", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_006.png"], "annotated": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "F1 scores of different indexes for the magnetic field distortion by different thresholds. Bold lines indicate mean values for different participants and the colored areas indicate the range of standard deviations.", "levels": [[1], [1]], "corpus_id": 218482708, "sentences": ["F1 scores of different indexes for the magnetic field distortion by different thresholds.", "Bold lines indicate mean values for different participants and the colored areas indicate the range of standard deviations."], "caption": "Figure 10. F1 scores of different indexes for the magnetic \ufb01eld distortion by different thresholds. Bold lines indicate mean values for different par- ticipants and the colored areas indicate the range of standard deviations.", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_010.png"], "annotated": true, "is_plot": true, "uniq_levels": [1], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "pdf_hash": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "year": 2020, "venue": "CHI", "alt_text": "a) The smartwatch's diameter was 35mm (320 pixels), and the target's diameter was 11mm (100 pixels.) b) The four facing and rotational directions for Test 1. The reference direction was used in training. A participant c) reads a book and d) enters text using a keyboard for Test 2.", "levels": null, "corpus_id": 218482708, "sentences": ["a) The smartwatch's diameter was 35mm (320 pixels), and the target's diameter was 11mm (100 pixels.) b) The four facing and rotational directions for Test 1.", "The reference direction was used in training.", "A participant c) reads a book and d) enters text using a keyboard for Test 2."], "caption": "", "local_uri": ["0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_011.png"], "annotated": false, "compound": false}
{"title": "Enabling Biographical Cognitive Stimulation for People with Dementia", "pdf_hash": "0612562df50ad8a484306719600a17f833826d12", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "Representation of Scrapbook's non-pharmacological activities through the presentation of computer screens (print screens of a web browser)  : (a) reminiscence therapy (with an image filling the majority of the screen and some buttons to control the session and provide feedback about the image) , (b) jigsaw puzzle (a picture broken in several pieces, two feedback buttons and another to adjust the difficulty of the puzzle), (c) memory flashcard (several squares with images, some already visible and some still hidden),  (d) street view navigation (a map on the left side with several points marked with an option that allows navigation through each of these points, and on the right side a list of cities and a search box)", "levels": [[-1]], "corpus_id": 5046417, "sentences": ["Representation of Scrapbook's non-pharmacological activities through the presentation of computer screens (print screens of a web browser)  : (a) reminiscence therapy (with an image filling the majority of the screen and some buttons to control the session and provide feedback about the image) , (b) jigsaw puzzle (a picture broken in several pieces, two feedback buttons and another to adjust the difficulty of the puzzle), (c) memory flashcard (several squares with images, some already visible and some still hidden),  (d) street view navigation (a map on the left side with several points marked with an option that allows navigation through each of these points, and on the right side a list of cities and a search box)"], "caption": "Figure 3: Scrapbook's non- pharmacological activities: (a) reminiscence therapy, (b) jigsaw puzzle, (c) memory flashcard, (d) street view navigation.", "local_uri": ["0612562df50ad8a484306719600a17f833826d12_Image_004.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Data Donors: Sharing Knowledge for Mobile Accessibility", "pdf_hash": "80e45f697c7eda45af8f577aad649df976d52c74", "year": 2018, "venue": "CHI Extended Abstracts", "alt_text": "A screenshot of the assistive play donor aplication. On the top side it shows a screenshot of fruit ninja with two traces of two different swipes. Bellow a list of options to annotate which actions those traces correspond to. Finnally, in the last third of the screenshot a timeline is shown with the limits of each gesture identifed by assistive play.", "levels": null, "corpus_id": 5041400, "sentences": ["A screenshot of the assistive play donor aplication.", "On the top side it shows a screenshot of fruit ninja with two traces of two different swipes.", "Bellow a list of options to annotate which actions those traces correspond to.", "Finnally, in the last third of the screenshot a timeline is shown with the limits of each gesture identifed by assistive play."], "caption": "", "local_uri": ["80e45f697c7eda45af8f577aad649df976d52c74_Image_002.png"], "annotated": false, "compound": false}
{"title": "ToonNote: Improving Communication in Computational Notebooks Using Interactive Data Comics", "pdf_hash": "50c0020c1d03ee152d178a4d188036bdc972a6f7", "year": 2021, "venue": "CHI", "alt_text": "Study materials that are used for the second study. COVID-19 data set and Netflix data set", "levels": [[-1], [-1]], "corpus_id": 233987138, "sentences": ["Study materials that are used for the second study.", "COVID-19 data set and Netflix data set"], "caption": "", "local_uri": ["50c0020c1d03ee152d178a4d188036bdc972a6f7_Image_007.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "ToonNote: Improving Communication in Computational Notebooks Using Interactive Data Comics", "pdf_hash": "50c0020c1d03ee152d178a4d188036bdc972a6f7", "year": 2021, "venue": "CHI", "alt_text": "Quantitative results of the second study. Time, Accuracy, NASA TLX, Intention, Comprehension, Enjoyment, Engagement, Features, Tool Evaluation, If you can make this to whom do you want to share, Preferable Notebook Format per Stage", "levels": null, "corpus_id": 233987138, "sentences": ["Quantitative results of the second study.", "Time, Accuracy, NASA TLX, Intention, Comprehension, Enjoyment, Engagement, Features, Tool Evaluation, If you can make this to whom do you want to share, Preferable Notebook Format per Stage"], "caption": "", "local_uri": ["50c0020c1d03ee152d178a4d188036bdc972a6f7_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Why doesn't it work?: voice-driven interfaces and young children's communication repair strategies", "pdf_hash": "873c0e1f1de9017b80a56b1105d6f6f411a5d64c", "year": 2018, "venue": "IDC", "alt_text": "The image shows a tablet screen with a yellow background and cartoon duck in the foreground. A mostly obscured child is leaning in toward the screen. Larger hands next to the child's hands indicate that the child is accompanied by an adult.", "levels": null, "corpus_id": 195352412, "sentences": ["The image shows a tablet screen with a yellow background and cartoon duck in the foreground.", "A mostly obscured child is leaning in toward the screen.", "Larger hands next to the child's hands indicate that the child is accompanied by an adult."], "caption": "Figure 1: A parent and child play a voice-driven mini- game together (\u201cCookie Monster\u2019s Challenge\u201d by Sesame Workshop and PBS Kids). The app tells him, \u201cQuick, say quack out loud!\u201d and the child responds by leaning for- ward and saying \u201cquack!\u201d to the screen.", "local_uri": ["873c0e1f1de9017b80a56b1105d6f6f411a5d64c_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Why doesn't it work?: voice-driven interfaces and young children's communication repair strategies", "pdf_hash": "873c0e1f1de9017b80a56b1105d6f6f411a5d64c", "year": 2018, "venue": "IDC", "alt_text": "Image of cartoon duck in front of a yellow background. The duck holds its wing up to its ear, as if it is listening for a sound.", "levels": null, "corpus_id": 195352412, "sentences": ["Image of cartoon duck in front of a yellow background.", "The duck holds its wing up to its ear, as if it is listening for a sound."], "caption": "", "local_uri": ["873c0e1f1de9017b80a56b1105d6f6f411a5d64c_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Why doesn't it work?: voice-driven interfaces and young children's communication repair strategies", "pdf_hash": "873c0e1f1de9017b80a56b1105d6f6f411a5d64c", "year": 2018, "venue": "IDC", "alt_text": "Image of a cartoon duck on a yellow background. The duck is almost to the left edge of the image and its feet are slightly lifted to indicate that it is walking.", "levels": null, "corpus_id": 195352412, "sentences": ["Image of a cartoon duck on a yellow background.", "The duck is almost to the left edge of the image and its feet are slightly lifted to indicate that it is walking."], "caption": "Figure 2. Screenshot of the \u201cQuack\u201d bonus game in Cookie Monster\u2019s Challenge. Left: The duck holds its wing up to its ear to imply that the player should speak (and specifically, that the player should say \u201cquack\u201d). Right: When the game begins, the duck starts pacing back and forth, walking on and off the screen. The speed at which the duck walks and the amount of time it spends on screen varies by level, such that the game becomes progressively more difficult as the level increases.", "local_uri": ["873c0e1f1de9017b80a56b1105d6f6f411a5d64c_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Why doesn't it work?: voice-driven interfaces and young children's communication repair strategies", "pdf_hash": "873c0e1f1de9017b80a56b1105d6f6f411a5d64c", "year": 2018, "venue": "IDC", "alt_text": "A cartoon shows a child saying \"Alexa! Play five lil monkeys\" and a drawing of an Amazon Echo Dot says \"Ok, you said 'play five what monkeys?'\" Then a bubble shows the child saying \"Little\" and the Echo responding \"You said 'play five little monkeys?'\" Circular arrows have text that says \"back-and-forth follow-up question\"", "levels": null, "corpus_id": 195352412, "sentences": ["A cartoon shows a child saying \"Alexa! Play five lil monkeys\" and a drawing of an Amazon Echo Dot says \"Ok, you said 'play five what monkeys?'\" Then a bubble shows the child saying \"Little\" and the Echo responding \"You said 'play five little monkeys?'\" Circular arrows have text that says \"back-and-forth follow-up question\""], "caption": "", "local_uri": ["873c0e1f1de9017b80a56b1105d6f6f411a5d64c_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Why doesn't it work?: voice-driven interfaces and young children's communication repair strategies", "pdf_hash": "873c0e1f1de9017b80a56b1105d6f6f411a5d64c", "year": 2018, "venue": "IDC", "alt_text": "Child side says: - Continuous repetition - Varied tones - Varied pronunciation - Rarely giving up  Parent side says: - Technical troubleshooting - Giving up after a few tries - Pronouncements  Shared middle says: - Stop-and-go repetition - Increased volume - Statements of resignation", "levels": null, "corpus_id": 195352412, "sentences": ["Child side says: - Continuous repetition - Varied tones - Varied pronunciation - Rarely giving up  Parent side says: - Technical troubleshooting - Giving up after a few tries - Pronouncements  Shared middle says: - Stop-and-go repetition - Increased volume - Statements of resignation"], "caption": "", "local_uri": ["873c0e1f1de9017b80a56b1105d6f6f411a5d64c_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "Screenshot of a the Image > Transform menu from GIMP, with Flip Vertically hovered over. To the right, an extended Detour Submenu is displayed, with names, tooltip text, and visual previews for the Flip Vertically command, and two alternative commands under an \"Are you looking for...\" heading.", "levels": null, "corpus_id": 12184485, "sentences": ["Screenshot of a the Image > Transform menu from GIMP, with Flip Vertically hovered over.", "To the right, an extended Detour Submenu is displayed, with names, tooltip text, and visual previews for the Flip Vertically command, and two alternative commands under an \"Are you looking for...\" heading."], "caption": "Figure 1. A Detour Submenu for the Image > Transform > Flip Vertically menu item in GIMP.", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_001.png"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "Screenshot of the Review ribbon from Microsoft Word. Highlighted are two groups of three commands - Delete, Previous, and Next in the Comments area, and Reject, Previous, and Next in the Changes area.", "levels": null, "corpus_id": 12184485, "sentences": ["Screenshot of the Review ribbon from Microsoft Word.", "Highlighted are two groups of three commands - Delete, Previous, and Next in the Comments area, and Reject, Previous, and Next in the Changes area."], "caption": "", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "Screenshot of the Format ribbon from Microsoft Word. Highlighted are two groups of three commands each - Shape Fill, Shape Outline, and Shape Effects in the Shape Styles area, and Text Fill, Text Outline, and Text Effects in the WordArt Styles area.", "levels": null, "corpus_id": 12184485, "sentences": ["Screenshot of the Format ribbon from Microsoft Word.", "Highlighted are two groups of three commands each - Shape Fill, Shape Outline, and Shape Effects in the Shape Styles area, and Text Fill, Text Outline, and Text Effects in the WordArt Styles area."], "caption": "Figure 2. Sets of commands with similar appearances and layout on the Review ribbon in Microsoft Word (top) and the Format ribbon in PowerPoint (bottom).", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "Shows the Home ribbon from Microsoft Word, with the Borders command icon (a 2x2 grid) highlighted.", "levels": null, "corpus_id": 12184485, "sentences": ["Shows the Home ribbon from Microsoft Word, with the Borders command icon (a 2x2 grid) highlighted."], "caption": "Figure 3. The Borders dropdown on the Home ribbon could be interpreted as a command for inserting tables into a document.", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "A 2-axis design space. The y-axis is Command Type, consisting of \"No Parameters\", \"Parameters Required (e.g., in a dialog)\" and \"Direct-Manipulation Required\". The x-axis is Time of Intervention, consisting of \"Searching for and evaluating commands\", \"Executing an incorrect command\", and \"After an incorrect command has been executed\". The techniques presented in the text are displayed in the areas they occupy within the design space created by the two axes.", "levels": [[-1], [-1], [-1], [-1]], "corpus_id": 12184485, "sentences": ["A 2-axis design space.", "The y-axis is Command Type, consisting of \"No Parameters\", \"Parameters Required (e.g., in a dialog)\" and \"Direct-Manipulation Required\".", "The x-axis is Time of Intervention, consisting of \"Searching for and evaluating commands\", \"Executing an incorrect command\", and \"After an incorrect command has been executed\".", "The techniques presented in the text are displayed in the areas they occupy within the design space created by the two axes."], "caption": "Figure 4. A design space for addressing false feedforward errors, with the four techniques we have developed located within it.", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_005.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "Screenshot of the GIMP toolbox. The mouse is hovered over the Flip Tool. To the right of the toolbox is a menu showing the name, tooltip text, and a visual preview for the Flip Tool, and four alternative commands under an \"Are you looking for...\" heading.", "levels": null, "corpus_id": 12184485, "sentences": ["Screenshot of the GIMP toolbox.", "The mouse is hovered over the Flip Tool.", "To the right of the toolbox is a menu showing the name, tooltip text, and a visual preview for the Flip Tool, and four alternative commands under an \"Are you looking for...\" heading."], "caption": "", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "The Canvas Size... dialog from GIMP. To the right of the dialog is a menu showing the name, tooltip text, and a visual preview for the Canvas Size tool, and four alternative commands under an \"Similar commands...\" heading. Callouts describe how the visual previews \"show the effect of entered settings on alternative commands\" and that \"Selecting an alternative in the menu will swap the dialog in place, transferring settings when possible.\"", "levels": null, "corpus_id": 12184485, "sentences": ["The Canvas Size... dialog from GIMP.", "To the right of the dialog is a menu showing the name, tooltip text, and a visual preview for the Canvas Size tool, and four alternative commands under an \"Similar commands...\" heading.", "Callouts describe how the visual previews \"show the effect of entered settings on alternative commands\" and that \"Selecting an alternative in the menu will swap the dialog in place, transferring settings when possible.\""], "caption": "Figure 6. A Dialog Link-Up for the Set Image Canvas Size dialog in GIMP.", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "Screenshot of a figure being selected with the Intelligent Scissors tool in GIMP. To the right of the canvas, a menu shows the name and tooltip text for Intelligent Scissors, and an alternative command (Free Select) under a \"Similar tools...(click to try)\" heading. Diagram shows that clicking an alternative migrates clicks already made with the Intelligent Scissors tool to the alternative tool.", "levels": null, "corpus_id": 12184485, "sentences": ["Screenshot of a figure being selected with the Intelligent Scissors tool in GIMP.", "To the right of the canvas, a menu shows the name and tooltip text for Intelligent Scissors, and an alternative command (Free Select) under a \"Similar tools...(click to try)\" heading.", "Diagram shows that clicking an alternative migrates clicks already made with the Intelligent Scissors tool to the alternative tool."], "caption": "", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software", "pdf_hash": "933896c5916d37b2652b75d45f0fe022c124461e", "year": 2015, "venue": "UIST", "alt_text": "An Undo History dialog, showing a list of commands. Below the most recent command, is a \"Did you mean...(Press 'Z' to cycle\" heading with a list of alternatives. Each alternative includes a name, tooltip text, and a visual preview.", "levels": [[-1], [-1], [-1]], "corpus_id": 12184485, "sentences": ["An Undo History dialog, showing a list of commands.", "Below the most recent command, is a \"Did you mean...(Press 'Z' to cycle\" heading with a list of alternatives.", "Each alternative includes a name, tooltip text, and a visual preview."], "caption": "Figure 6. Did-You-Mean History adds alternatives to the existing Undo History dialog, which can be selected or cycled through with the Or-do command.", "local_uri": ["933896c5916d37b2652b75d45f0fe022c124461e_Image_009.jpg"], "annotated": true, "is_plot": false, "uniq_levels": [], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Community Collectives: Low-tech Social Support for Digitally-Engaged Entrepreneurship", "pdf_hash": "a9616e14a5c96ee3923b6c559d3e7c07deff550d", "year": 2020, "venue": "CHI", "alt_text": "Figure 1: This image has two images--left and right. On the left is an example of how a filled out tour idea worksheet was used to inform one local entrepreneur's Facebook Event description, which is pictured on the right.", "levels": null, "corpus_id": 218483580, "sentences": ["Figure 1: This image has two images--left and right.", "On the left is an example of how a filled out tour idea worksheet was used to inform one local entrepreneur's Facebook Event description, which is pictured on the right."], "caption": "Figure 1. Example of how a \ufb01lled out tour idea worksheet was used to inform one local entrepreneur\u2019s Facebook Event description.", "local_uri": ["a9616e14a5c96ee3923b6c559d3e7c07deff550d_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Community Collectives: Low-tech Social Support for Digitally-Engaged Entrepreneurship", "pdf_hash": "a9616e14a5c96ee3923b6c559d3e7c07deff550d", "year": 2020, "venue": "CHI", "alt_text": "Figure 2: This image has two images--left and right. On the left is an example of how a filled out tour planning worksheet was used create a Google Maps plan, pictured on the right.", "levels": null, "corpus_id": 218483580, "sentences": ["Figure 2: This image has two images--left and right.", "On the left is an example of how a filled out tour planning worksheet was used create a Google Maps plan, pictured on the right."], "caption": "Figure 2. One local entrepreneur only learned about using Google Maps after describing her plans on paper with peers during a meeting.", "local_uri": ["a9616e14a5c96ee3923b6c559d3e7c07deff550d_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Community Collectives: Low-tech Social Support for Digitally-Engaged Entrepreneurship", "pdf_hash": "a9616e14a5c96ee3923b6c559d3e7c07deff550d", "year": 2020, "venue": "CHI", "alt_text": "Figure 3: This image is a diagram. The top lists existing tech-mediated supports, like MOOCS, online tutorials, blogs, and online discussion groups, that may foster digital engagement among those with adequate resources and comfort using online technologies. The bottom lists low-tech social support identified in this study: resource-connecting organizations, regular in-person meetings, paper planning tools, and practice and validation. These elements encouraged digital engagement among entrepreneurs in under-resourced contexts.", "levels": null, "corpus_id": 218483580, "sentences": ["Figure 3: This image is a diagram.", "The top lists existing tech-mediated supports, like MOOCS, online tutorials, blogs, and online discussion groups, that may foster digital engagement among those with adequate resources and comfort using online technologies.", "The bottom lists low-tech social support identified in this study: resource-connecting organizations, regular in-person meetings, paper planning tools, and practice and validation.", "These elements encouraged digital engagement among entrepreneurs in under-resourced contexts."], "caption": "Figure 3. Existing tech-mediated supports (above) may foster digital engagement among those with adequate resources and comfort using on- line technologies. But, we found that low-tech social support (below) en- couraged digital engagement among entrepreneurs in under-resourced contexts.", "local_uri": ["a9616e14a5c96ee3923b6c559d3e7c07deff550d_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Soma Design and Sensory Misalignment", "pdf_hash": "4ed8bf54a5cd40f6bd85cc8fae043ce353160d17", "year": 2020, "venue": "CHI", "alt_text": "Three frames showing people participating in contact imporvisation, a kind of improvised dance where people are in contact.", "levels": null, "corpus_id": 218482566, "sentences": ["Three frames showing people participating in contact imporvisation, a kind of improvised dance where people are in contact."], "caption": "Figure 2. Example of a body map used in the workshop to document bodily experiences before and after each workshop activity", "local_uri": ["4ed8bf54a5cd40f6bd85cc8fae043ce353160d17_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Soma Design and Sensory Misalignment", "pdf_hash": "4ed8bf54a5cd40f6bd85cc8fae043ce353160d17", "year": 2020, "venue": "CHI", "alt_text": "A body map - a hand written notation sheet with a picture of a body and various descriptive words and sketched in areas", "levels": null, "corpus_id": 218482566, "sentences": ["A body map - a hand written notation sheet with a picture of a body and various descriptive words and sketched in areas"], "caption": "Figure 3. Contact improvisation exercise focused on exploring balance", "local_uri": ["4ed8bf54a5cd40f6bd85cc8fae043ce353160d17_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Soma Design and Sensory Misalignment", "pdf_hash": "4ed8bf54a5cd40f6bd85cc8fae043ce353160d17", "year": 2020, "venue": "CHI", "alt_text": "Three panels, the first shows a person lying on a Soma Bit, which looks like a cushion, the second shows a person wearing a VR headset and balancing on a steel scaffolding pole, the third shows a person hanging upside down from a scaffolding rig with two other people either supporting them or holding soma bits against their body.", "levels": null, "corpus_id": 218482566, "sentences": ["Three panels, the first shows a person lying on a Soma Bit, which looks like a cushion, the second shows a person wearing a VR headset and balancing on a steel scaffolding pole, the third shows a person hanging upside down from a scaffolding rig with two other people either supporting them or holding soma bits against their body."], "caption": "Figure 4. (left to right) a) Some Soma Bits that were used for exploring actuation on the body, b) tightrope prototype, c) \ufb02ying harness prototype", "local_uri": ["4ed8bf54a5cd40f6bd85cc8fae043ce353160d17_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 2: Transcript of a video extract. Child holds and shakes the phone. Then the child drops the phone to bed. Mum complains about phone shaking.  01 ((child holds and shakes the phone.)) 02 ((child drops the phone to bed)) 03 ((grandfather grabs the phone back)) 04 Mum: is she holding the phone or are your holding?  05 Grandfather: she is holding the phone  06 Mum: the phone is always shaking", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 2: Transcript of a video extract.", "Child holds and shakes the phone.", "Then the child drops the phone to bed.", "Mum complains about phone shaking.", "01 ((child holds and shakes the phone.)) 02 ((child drops the phone to bed)) 03 ((grandfather grabs the phone back)) 04 Mum: is she holding the phone or are your holding?  05 Grandfather: she is holding the phone  06 Mum: the phone is always shaking"], "caption": "Figure 2: A child shaking and dropping the phone", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 3: Different ways of grandparents holding the phone for parent-child video calls. There are six small images showing how grandparents position the phone in front of the child's head at the beginning of video calls.", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 3: Different ways of grandparents holding the phone for parent-child video calls.", "There are six small images showing how grandparents position the phone in front of the child's head at the beginning of video calls."], "caption": "Figure 3: Grandparents holding the phone for facilitated three-party parent-child video calls", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_003.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 4: Transcript of a vide extract. A child is attempting to grab the phone.  01 ((Child moves hand and tries to grab the phone.))  02 ((Child walks toward the phone)) 03 ((Grandma moves the phone away))", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 4: Transcript of a vide extract.", "A child is attempting to grab the phone.", "01 ((Child moves hand and tries to grab the phone.))  02 ((Child walks toward the phone)) 03 ((Grandma moves the phone away))"], "caption": "Figure 4: Child attempting to grab the phone", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_004.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 5: \u201cTranscript of a video extract. This is the beginning of a video call, grandfather moves the phone toward the child, and positions the phone in front of the child. 01 ((grandfather calls mum, and he appears on screen)); 02 ((grandfather moves the phone toward the child, and the child appears on screen)); 03  Grandfather says take a look at this. 04  Mum appears on phone screen. Mum and child talking head configuration))\u201d", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 5: \u201cTranscript of a video extract.", "This is the beginning of a video call, grandfather moves the phone toward the child, and positions the phone in front of the child.", "01 ((grandfather calls mum, and he appears on screen)); 02 ((grandfather moves the phone toward the child, and the child appears on screen)); 03  Grandfather says take a look at this. 04  Mum appears on phone screen.", "Mum and child talking head configuration))\u201d"], "caption": "Figure 5: Grandparent positioning phone & prompting child", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_005.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 6: \u201cTranscript of a video extract. Child does not look at the phone screen. Grandfather touches the child's shoulder, then the child moves head to look at phone screen.01 ((child looks aside, grandfather touches her shoulder)); 02 ((child moves head and gaze to screen))\u201d", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 6: \u201cTranscript of a video extract.", "Child does not look at the phone screen.", "Grandfather touches the child's shoulder, then the child moves head to look at phone screen.01 ((child looks aside, grandfather touches her shoulder)); 02 ((child moves head and gaze to screen))\u201d"], "caption": "Figure 6: Grandparent getting the child\u2019s attention", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_006.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 7: \u201cThe left image shows a grandmother frames the child but only shows the part of child's head. The middle image shows a grandmother framing the child but only showing an unrelated part of the room. The right images shows a grandmother positions the phone toward a child but only shows the ceiling.\u201d", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 7: \u201cThe left image shows a grandmother frames the child but only shows the part of child's head.", "The middle image shows a grandmother framing the child but only showing an unrelated part of the room.", "The right images shows a grandmother positions the phone toward a child but only shows the ceiling.\u201d"], "caption": "Figure 7: Problems in grandparents\u2019 framing", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_007.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 8: Transcript of a video extract. The boy is eating ice cream. He wants to show am empty ice cream stick to mum. Although he moves the stick toward screen, the stick is not visible on screen for remote mum.  01 Mum: big brother, have you eaten up yours?  02 (0.5 seconds) ((boy is eating an ice cream)) 03 Boy: hey hey he ha  04 Boy: ((moves empty ice cream stick toward screen, shows it to screen, but it is not visible at all for mum))", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 8: Transcript of a video extract.", "The boy is eating ice cream.", "He wants to show am empty ice cream stick to mum. Although he moves the stick toward screen, the stick is not visible on screen for remote mum.", "01 Mum: big brother, have you eaten up yours?  02 (0.5 seconds) ((boy is eating an ice cream)) 03 Boy: hey hey he ha  04 Boy: ((moves empty ice cream stick toward screen, shows it to screen, but it is not visible at all for mum))"], "caption": "Figure 8: Boy trying to show the empty ice cream stick to remote mum", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_008.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 9: Examples of grandparent self-checking their phone screen. Grandparent moves phone back to themselves to see what is shown on the screen.", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 9: Examples of grandparent self-checking their phone screen.", "Grandparent moves phone back to themselves to see what is shown on the screen."], "caption": "Figure 9: Examples of grandparent self-checking the phone screen", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_009.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 10: \u201cTranscript of a video extract. This is the ending of a video call. The child waves hand to say goodbye to mum, but mum says I didn't see you. Then grandmother re-positions the camera. The child's head and waving appear on the phone screen. 01 Child: mum, bye bye ((but child not visible on screen)); 02 (0.3 seconds); 03 Grandmother: say again mum bye bye; 04 Grandmother: say dad; 05 Child: dad; 06 Mum: I didn't see you! 07  ((grandmother changes camera position)); 08  ((and now the phone shows the child)).\u201d", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 10: \u201cTranscript of a video extract.", "This is the ending of a video call.", "The child waves hand to say goodbye to mum, but mum says I didn't see you.", "Then grandmother re-positions the camera.", "The child's head and waving appear on the phone screen.", "01 Child: mum, bye bye ((but child not visible on screen)); 02 (0.3 seconds); 03 Grandmother: say again mum bye bye; 04 Grandmother: say dad; 05 Child: dad; 06 Mum: I didn't see you! 07  ((grandmother changes camera position)); 08  ((and now the phone shows the child)).\u201d"], "caption": "Figure 10: Remote parent complaining about camera work", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_010.jpg"], "annotated": false, "compound": false}
{"title": "Connecting Distributed Families", "pdf_hash": "00f333d0ecabc9bb9d093688019aaf4040cda46a", "year": 2020, "venue": "", "alt_text": "Figure 11: Transcript of a video extract. Grandfather holds the phone. the child is dancing. But the camera does not show the child. Grandmother tells grandfather that he didn't show the child. Then grandfather correct the camera position.  01 ((Grandfather holds the phone. The girl is dancing for dad, but she is not visible on phone screen)) 02  Grandmother talks to grandfather, you didn't show her  03  ((grandfather changes camera position, and the right phone screen now shows the girl dancing))", "levels": null, "corpus_id": 211039687, "sentences": ["Figure 11: Transcript of a video extract.", "Grandfather holds the phone.", "the child is dancing. But the camera does not show the child.", "Grandmother tells grandfather that he didn't show the child.", "Then grandfather correct the camera position.", "01 ((Grandfather holds the phone.", "The girl is dancing for dad, but she is not visible on phone screen)) 02  Grandmother talks to grandfather, you didn't show her  03  ((grandfather changes camera position, and the right phone screen now shows the girl dancing))"], "caption": "Figure 11: Co-present grandmother complaining about camera work", "local_uri": ["00f333d0ecabc9bb9d093688019aaf4040cda46a_Image_011.jpg"], "annotated": false, "compound": false}
{"title": "Exploring Potentially Abusive Ethical, Social and Political Implications of Mixed Reality Research in HCI", "pdf_hash": "80ae088f9b10af8d9700c77c39033ab8a6f99215", "year": 2020, "venue": "CHI Extended Abstracts", "alt_text": "Figure 2: \"Top: A user wearing a VR HMD with external displays showing the Coca Cola logo.Bottom: the perspective of the user in which he sees a similar Coca Cola superimposed on world\"", "levels": null, "corpus_id": 218482389, "sentences": ["Figure 2: \"Top: A user wearing a VR HMD with external displays showing the Coca Cola logo.", "Bottom: the perspective of the user in which he sees a similar Coca Cola superimposed on world\""], "caption": "", "local_uri": ["80ae088f9b10af8d9700c77c39033ab8a6f99215_Image_009.jpg", "80ae088f9b10af8d9700c77c39033ab8a6f99215_Image_010.jpg"], "annotated": false, "compound": true}
{"title": "Effect of target size on non-visual text-entry", "pdf_hash": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "year": 2016, "venue": "MobileHCI", "alt_text": "It shows the 4 keyboard sizes in the same picture to assess their relative size. Each size is bigger than all of the smallest sizes put together. Large ocuppies the same space as the three smaller conditions; medium the same space as small and tiny; small is about twice the size of the tiny keyboard.", "levels": null, "corpus_id": 17354027, "sentences": ["It shows the 4 keyboard sizes in the same picture to assess their relative size.", "Each size is bigger than all of the smallest sizes put together.", "Large ocuppies the same space as the three smaller conditions; medium the same space as small and tiny; small is about twice the size of the tiny keyboard."], "caption": "Touch-enabled devices have a growing variety of screen sizes; however, there is little knowledge on the effect of key size on non-visual text-entry performance. We conducted a user study with 12 blind participants to investigate how non- visual input performance varies with four QWERTY keyboard sizes (ranging from 15mm to 2.5mm). This paper presents an analysis of typing performance and touch behaviors discussing its implications for future research. Our findings show that there is an upper limit to the benefits of larger target sizes between 10mm and 15mm. Input speed decreases from 4.5 to 2.4 words per minute (WPM) for targets sizes below 10mm. The smallest size was deemed unusable by participants even though performance was in par with previous work.", "local_uri": ["f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_001.jpg"], "annotated": false, "compound": false}
{"title": "Effect of target size on non-visual text-entry", "pdf_hash": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "year": 2016, "venue": "MobileHCI", "alt_text": "It show the tablet with the sponge cutout for the medium size keyboard. The sponged is is attached to the screen and only a small hole in the center bottom of the tablet is cutout in order for the participants to be able to detect where the keyboard is and where its borders are.", "levels": null, "corpus_id": 17354027, "sentences": ["It show the tablet with the sponge cutout for the medium size keyboard.", "The sponged is is attached to the screen and only a small hole in the center bottom of the tablet is cutout in order for the participants to be able to detect where the keyboard is and where its borders are."], "caption": "Figure 2 - Experimental setup for the medium size keyboard with the sponge cutout fixed to the tablet.", "local_uri": ["f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_002.jpg"], "annotated": false, "compound": false}
{"title": "Effect of target size on non-visual text-entry", "pdf_hash": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "year": 2016, "venue": "MobileHCI", "alt_text": "The graph shows a gradual increase of about 10% of the amount of total errors from large to tiny. On the other hand, the uncorrected error rate remains constant.", "levels": [[3, 2], [3]], "corpus_id": 17354027, "sentences": ["The graph shows a gradual increase of about 10% of the amount of total errors from large to tiny.", "On the other hand, the uncorrected error rate remains constant."], "caption": "Figure 4 - Total and Uncorrected error rate of the four conditions.", "local_uri": ["f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_004.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Effect of target size on non-visual text-entry", "pdf_hash": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "year": 2016, "venue": "MobileHCI", "alt_text": "The graph shows a significant increase of the relative path lenght as size gets smaller. From a relative distance smaller than 10 pixels to a 30 pixel difference on tiny. The same effect it is also noticible on the task axis lengh but the difference between tiny and large is of only about 8 relative pixels.", "levels": [[3], [3, 2], [3, 2]], "corpus_id": 17354027, "sentences": ["The graph shows a significant increase of the relative path lenght as size gets smaller.", "From a relative distance smaller than 10 pixels to a 30 pixel difference on tiny.", "The same effect it is also noticible on the task axis lengh but the difference between tiny and large is of only about 8 relative pixels."], "caption": "Figure 3- Relative Path Length and Task Axis Length in pixels.", "local_uri": ["f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_005.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2, 3], "has_1_2_3": false, "has_1_2": false, "compound": false}
{"title": "Effect of target size on non-visual text-entry", "pdf_hash": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "year": 2016, "venue": "MobileHCI", "alt_text": "The bar graph depicts no differences between the number of slips of large, medium and small size but a considerable difference to tiny that above 60% slip errors.", "levels": [[2]], "corpus_id": 17354027, "sentences": ["The bar graph depicts no differences between the number of slips of large, medium and small size but a considerable difference to tiny that above 60% slip errors."], "caption": "", "local_uri": ["f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_006.jpg"], "annotated": true, "is_plot": true, "uniq_levels": [2], "has_1_2_3": false, "has_1_2": false, "compound": false}
